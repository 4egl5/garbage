{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## This notebook outlines the concepts involved in Topic Modeling\n",
    "\n",
    "\n",
    "Topic modeling is a statistical model to **discover** the abstract \"topics\" that occur in a collection of documents\n",
    "\n",
    "It is commonly used in text document. But nowadays, in social media analysis, topic modeling is an emerging research area.\n",
    "\n",
    "One of the most popular algorithms used is **Latent Dirichlet Allocation** which was proposed by\n",
    "David Blei et al in 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "https://raw.githubusercontent.com/subashgandyer/datasets/main/kaggledatasets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Install the necessary library\n",
    "- Import the necessary libraries\n",
    "- Download the dataset\n",
    "- Load the dataset\n",
    "- Pre-process the dataset\n",
    "    - Tokenize\n",
    "    - Stop words removal\n",
    "    - Non-alphabetic words removal\n",
    "    - Lowercase\n",
    "- Create a dictionary for the document\n",
    "- Filter low frequency words\n",
    "- Create an Index to word dictionary\n",
    "- Train the Topic Model\n",
    "- Predict on the dataset\n",
    "- Visualize the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `'stopwords''\n",
      "/bin/bash: -c: line 1: ` nltk.download('stopwords')'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "! nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/subashgandyer/datasets/main/kaggledatasets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Versions</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>License</th>\n",
       "      <th>Views</th>\n",
       "      <th>Download</th>\n",
       "      <th>Kernels</th>\n",
       "      <th>Topics</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Card Fraud Detection</td>\n",
       "      <td>Anonymized credit card transactions labeled as...</td>\n",
       "      <td>Machine Learning Group - ULB</td>\n",
       "      <td>1241</td>\n",
       "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
       "      <td>crime\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>442,136 views</td>\n",
       "      <td>53,128 downloads</td>\n",
       "      <td>1,782 kernels</td>\n",
       "      <td>26 topics</td>\n",
       "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
       "      <td>The datasets contains transactions made by cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>Hugo Mathien</td>\n",
       "      <td>1046</td>\n",
       "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
       "      <td>association football\\neurope</td>\n",
       "      <td>SQLite</td>\n",
       "      <td>299 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>396,214 views</td>\n",
       "      <td>46,367 downloads</td>\n",
       "      <td>1,459 kernels</td>\n",
       "      <td>75 topics</td>\n",
       "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMDB 5000 Movie Dataset</td>\n",
       "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
       "      <td>The Movie Database (TMDb)</td>\n",
       "      <td>1024</td>\n",
       "      <td>Version 2,2017-09-28</td>\n",
       "      <td>film</td>\n",
       "      <td>CSV</td>\n",
       "      <td>44 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>446,255 views</td>\n",
       "      <td>62,002 downloads</td>\n",
       "      <td>1,394 kernels</td>\n",
       "      <td>46 topics</td>\n",
       "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
       "      <td>Background\\nWhat can we say about the success ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global Terrorism Database</td>\n",
       "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
       "      <td>START Consortium</td>\n",
       "      <td>789</td>\n",
       "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
       "      <td>crime\\nterrorism\\ninternational relations</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>187,877 views</td>\n",
       "      <td>26,309 downloads</td>\n",
       "      <td>608 kernels</td>\n",
       "      <td>11 topics</td>\n",
       "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
       "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bitcoin Historical Data</td>\n",
       "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
       "      <td>Zielak</td>\n",
       "      <td>618</td>\n",
       "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
       "      <td>history\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>119 MB</td>\n",
       "      <td>CC4</td>\n",
       "      <td>146,734 views</td>\n",
       "      <td>16,868 downloads</td>\n",
       "      <td>68 kernels</td>\n",
       "      <td>13 topics</td>\n",
       "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
       "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title  \\\n",
       "0  Credit Card Fraud Detection   \n",
       "1     European Soccer Database   \n",
       "2      TMDB 5000 Movie Dataset   \n",
       "3    Global Terrorism Database   \n",
       "4      Bitcoin Historical Data   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0  Anonymized credit card transactions labeled as...   \n",
       "1  25k+ matches, players & teams attributes for E...   \n",
       "2                Metadata on ~5,000 movies from TMDb   \n",
       "3  More than 170,000 terrorist attacks worldwide,...   \n",
       "4  Bitcoin data at 1-min intervals from select ex...   \n",
       "\n",
       "                          Owner  Votes  \\\n",
       "0  Machine Learning Group - ULB   1241   \n",
       "1                  Hugo Mathien   1046   \n",
       "2     The Movie Database (TMDb)   1024   \n",
       "3              START Consortium    789   \n",
       "4                        Zielak    618   \n",
       "\n",
       "                                            Versions  \\\n",
       "0          Version 2,2016-11-05|Version 1,2016-11-03   \n",
       "1  Version 10,2016-10-24|Version 9,2016-10-24|Ver...   \n",
       "2                               Version 2,2017-09-28   \n",
       "3          Version 2,2017-07-19|Version 1,2016-12-08   \n",
       "4  Version 11,2018-01-11|Version 10,2017-11-17|Ve...   \n",
       "\n",
       "                                        Tags Data Type    Size License  \\\n",
       "0                             crime\\nfinance       CSV  144 MB    ODbL   \n",
       "1               association football\\neurope    SQLite  299 MB    ODbL   \n",
       "2                                       film       CSV   44 MB   Other   \n",
       "3  crime\\nterrorism\\ninternational relations       CSV  144 MB   Other   \n",
       "4                           history\\nfinance       CSV  119 MB     CC4   \n",
       "\n",
       "           Views          Download        Kernels     Topics  \\\n",
       "0  442,136 views  53,128 downloads  1,782 kernels  26 topics   \n",
       "1  396,214 views  46,367 downloads  1,459 kernels  75 topics   \n",
       "2  446,255 views  62,002 downloads  1,394 kernels  46 topics   \n",
       "3  187,877 views  26,309 downloads    608 kernels  11 topics   \n",
       "4  146,734 views  16,868 downloads     68 kernels  13 topics   \n",
       "\n",
       "                                                 URL  \\\n",
       "0     https://www.kaggle.com/mlg-ulb/creditcardfraud   \n",
       "1          https://www.kaggle.com/hugomathien/soccer   \n",
       "2    https://www.kaggle.com/tmdb/tmdb-movie-metadata   \n",
       "3               https://www.kaggle.com/START-UMD/gtd   \n",
       "4  https://www.kaggle.com/mczielinski/bitcoin-his...   \n",
       "\n",
       "                                         Description  \n",
       "0  The datasets contains transactions made by cre...  \n",
       "1  The ultimate Soccer database for data analysis...  \n",
       "2  Background\\nWhat can we say about the success ...  \n",
       "3  Context\\nInformation on more than 170,000 Terr...  \n",
       "4  Context\\nBitcoin is the longest running and mo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/kaggledatasets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the datasets contains transactions made by credit cards in september 2013 by european cardholders. this dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. the dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
      "it contains only numerical input variables which are the result of a pca transformation. unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. features v1, v2, ... v28 are the principal components obtained with pca, the only features which have not been transformed with pca are 'time' and 'amount'. feature 'time' contains the seconds elapsed between each transaction and the first transaction in the dataset. the feature 'amount' is the transaction amount, this feature can be used for example-dependant cost-senstive learning. feature 'class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
      "given the class imbalance ratio, we recommend measuring the accuracy using the area under the precision-recall curve (auprc). confusion matrix accuracy is not meaningful for unbalanced classification.\n",
      "the dataset has been collected and analysed during a research collaboration of worldline and the machine learning group (http://mlg.ulb.ac.be) of ulb (université libre de bruxelles) on big data mining and fraud detection. more details on current and past projects on related topics are available on http://mlg.ulb.ac.be/brufence and http://mlg.ulb.ac.be/artml\n",
      "please cite: andrea dal pozzolo, olivier caelen, reid a. johnson and gianluca bontempi. calibrating probability with undersampling for unbalanced classification. in symposium on computational intelligence and data mining (cidm), ieee, 2015\n",
      "the ultimate soccer database for data analysis and machine learning\n",
      "what you get:\n",
      "+25,000 matches\n",
      "+10,000 players\n",
      "11 european countries with their lead championship\n",
      "seasons 2008 to 2016\n",
      "players and teams' attributes* sourced from ea sports' fifa video game series, including the weekly updates\n",
      "team line up with squad formation (x, y coordinates)\n",
      "betting odds from up to 10 providers\n",
      "detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
      "*16th oct 2016: new table containing teams' attributes from fifa !\n",
      "original data source:\n",
      "you can easily find data about soccer matches but they are usually scattered across different websites. a thorough data collection and processing has been done to make your life easier. i must insist that you do not make any commercial use of the data. the data was sourced from:\n",
      "http://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events\n",
      "http://www.football-data.co.uk/ : betting odds. click here to understand the column naming system for betting odds:\n",
      "http://sofifa.com/ : players and teams attributes from ea sports fifa games. fifa series and all fifa assets property of ea sports.\n",
      "when you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. i have called those foreign keys \"api_id\".\n",
      "improving the dataset:\n",
      "you will notice that some players are missing from the lineup (null values). this is because i have not been able to source their attributes from fifa. this will be fixed overtime as the crawling algorithm is being improved. the dataset will also be expanded to include international games, national cups, champion's league and europa league. please ask me if you're after a specific tournament.\n",
      "please get in touch with me if you want to help improve this dataset.\n",
      "click here to access the project github\n",
      "important note for people interested in using the crawlers: since i first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. the existing script to crawl players ('player spider') will not work until i've updated it.\n",
      "exploring the data:\n",
      "now that's the fun part, there is a lot you can do with this dataset. i will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! here are some ideas for you:\n",
      "the holy grail... ... is obviously to predict the outcome of the game. the bookies use 3 classes (home win, draw, away win). they get it right about 53% of the time. this is also what i've achieved so far using my own svm. though it may sound high for such a random sport game, you've got to know that the home team wins about 46% of the time. so the base case (constantly predicting home win) has indeed 46% precision.\n",
      "probabilities vs odds\n",
      "when running a multi-class classifier like svm you could also output a probability estimate and compare it to the betting odds. have a look at your variance vs odds and see for what games you had very different predictions.\n",
      "explore and visualize features\n",
      "with access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into the beautiful game . who knows, guardiola himself may hire one of you some day!\n",
      "background\n",
      "what can we say about the success of a movie before it is released? are there certain companies (pixar?) that have found a consistent formula? given that major films costing over $100 million to produce can still flop, this question is more important than ever to the industry. film aficionados might have different interests. can we predict which films will be highly rated, whether or not they are a commercial success?\n",
      "this is a great place to start digging in to those questions, with data on the plot, cast, crew, budget, and revenues of several thousand films.\n",
      "data source transfer summary\n",
      "we (kaggle) have removed the original version of this dataset per a dmca takedown request from imdb. in order to minimize the impact, we're replacing it with a similar set of films and data fields from the movie database (tmdb) in accordance with their terms of use. the bad news is that kernels built on the old dataset will most likely no longer work.\n",
      "the good news is that:\n",
      "you can port your existing kernels over with a bit of editing. this kernel offers functions and examples for doing so. you can also find a general introduction to the new format here.\n",
      "the new dataset contains full credits for both the cast and the crew, rather than just the first three actors.\n",
      "actor and actresses are now listed in the order they appear in the credits. it's unclear what ordering the original dataset used; for the movies i spot checked it didn't line up with either the credits order or imdb's stars order.\n",
      "the revenues appear to be more current. for example, imdb's figures for avatar seem to be from 2010 and understate the film's global revenues by over $2 billion.\n",
      "some of the movies that we weren't able to port over (a couple of hundred) were just bad entries. for example, this imdb entry has basically no accurate information at all. it lists star wars episode vii as a documentary.\n",
      "data source transfer details\n",
      "several of the new columns contain json. you can save a bit of time by porting the load data functions from this kernel.\n",
      "even in simple fields like runtime may not be consistent across versions. for example, previous dataset shows the duration for avatar's extended cut while tmdb shows the time for the original version.\n",
      "there's now a separate file containing the full credits for both the cast and crew.\n",
      "all fields are filled out by users so don't expect them to agree on keywords, genres, ratings, or the like.\n",
      "your existing kernels will continue to render normally until they are re-run.\n",
      "if you are curious about how this dataset was prepared, the code to access tmdb's api is posted here.\n",
      "new columns:\n",
      "homepage\n",
      "id\n",
      "original_title\n",
      "overview\n",
      "popularity\n",
      "production_companies\n",
      "production_countries\n",
      "release_date\n",
      "spoken_languages\n",
      "status\n",
      "tagline\n",
      "vote_average\n",
      "lost columns:\n",
      "actor_1_facebook_likes\n",
      "actor_2_facebook_likes\n",
      "actor_3_facebook_likes\n",
      "aspect_ratio\n",
      "cast_total_facebook_likes\n",
      "color\n",
      "content_rating\n",
      "director_facebook_likes\n",
      "facenumber_in_poster\n",
      "movie_facebook_likes\n",
      "movie_imdb_link\n",
      "num_critic_for_reviews\n",
      "num_user_for_reviews\n",
      "open questions about the data\n",
      "there are some things we haven't had a chance to confirm about the new dataset. if you have any insights, please let us know in the forums!\n",
      "are the budgets and revenues all in us dollars? do they consistently show the global revenues?\n",
      "this dataset hasn't yet gone through a data quality analysis. can you find any obvious corrections? for example, in the imdb version it was necessary to treat values of zero in the budget field as missing. similar findings would be very helpful to your fellow kagglers! (it's probably a good idea to keep treating zeros as missing, with the caveat that missing budgets much more likely to have been from small budget films in the first place).\n",
      "inspiration\n",
      "can you categorize the films by type, such as animated or not? we don't have explicit labels for this, but it should be possible to build them from the crew's job titles.\n",
      "how sharp is the divide between major film studios and the independents? do those two groups fall naturally out of a clustering analysis or is something more complicated going on?\n",
      "acknowledgements\n",
      "this dataset was generated from the movie database api. this product uses the tmdb api but is not endorsed or certified by tmdb. their api also provides access to data on many additional movies, actors and actresses, crew members, and tv shows. you can try it for yourself here.\n",
      "context\n",
      "information on more than 170,000 terrorist attacks\n",
      "the global terrorism database (gtd) is an open-source database including information on terrorist attacks around the world from 1970 through 2016 (with annual updates planned for the future). the gtd includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 170,000 cases. the database is maintained by researchers at the national consortium for the study of terrorism and responses to terrorism (start), headquartered at the university of maryland. more information\n",
      "content\n",
      "geography: worldwide\n",
      "time period: 1970-2016, except 1993 (2017 in progress, publication expected june 2018)\n",
      "unit of analysis: attack\n",
      "variables: >100 variables on location, tactics, perpetrators, targets, and outcomes\n",
      "sources: unclassified media articles (note: please interpret changes over time with caution. global patterns are driven by diverse trends in particular regions, and data collection is influenced by fluctuations in access to media coverage over both time and place.)\n",
      "definition of terrorism:\n",
      "\"the threatened or actual use of illegal force and violence by a non-state actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation.\"\n",
      "see the gtd codebook for important details on data collection methodology, definitions, and coding schema.\n",
      "acknowledgements\n",
      "the global terrorism database is funded through start, by the us department of state (contract number: saqmma12m1292) and the us department of homeland security science and technology directorate’s office of university programs (award number 2012-st-061-cs0001, cstab 3.1). the coding decisions and classifications contained in the database are determined independently by start researchers and should not be interpreted as necessarily representing the official views or policies of the united states government.\n",
      "gtd team\n",
      "publications\n",
      "the gtd has been leveraged extensively in scholarly publications, reports, and media articles. putting terrorism in context: lessons from the global terrorism database, by gtd principal investigators lafree, dugan, and miller investigates patterns of terrorism and provides perspective on the challenges of data collection and analysis. the gtd's data collection manager, michael jensen, discusses important benefits and drawbacks of methodological advancements in data collection and coding.\n",
      "terms of use\n",
      "use of the data signifies your agreement to the following terms and conditions.\n",
      "definitions: within this section: \"gtd\" will refer to the global terrorism database produced by the national consortium for the study of terrorism and responses to terrorism. this includes the data and codebook, any auxiliary materials present, and the world wide web interface by which the data are presented. \"start\" will refer to the national consortium for the study of terrorism and responses to terrorism, a united states department of homeland security center of excellence based at the university of maryland. \"user\" denotes the individual or set of individuals who access the gtd, i.e. the data, codebook, any auxiliary materials, and the world wide web interface by which the data are presented. \"gtd representatives\" denotes any senior management staff of start, and any employee or representative of said organization whom senior management staff designate to represent start in dealings with the user.\n",
      "usage rights: pursuant to this agreement, start grants the user the non-exclusive, non-guaranteed right to search, browse, and view all contents of the gtd world wide web interface.\n",
      "authorship: all contents of the gtd were assembled by representatives of start and do not purport to reflect the official position or data collections of the department of homeland security or any other agency of the united states government.\n",
      "acknowledgement: all information sourced from the gtd should be acknowledged by the user and cited as follows: \"national consortium for the study of terrorism and responses to terrorism (start). (2017). global terrorism database [data file]. retrieved from https://www.kaggle.com/start-umd/gtd\"\n",
      "unauthorized publication of the data: no part of the gtd may be republished on any website or accessible for public download in any format without the express permission of a gtd staff member. in addition, no part of the gtd may be distributed for any commercial purpose, nor with the intent that the data be used in any commercial enterprise, without the express permission of a gtd staff member. start reserves the right to withhold this permission.\n",
      "penalties: penalties for failure to comply with the terms of this agreement may result in loss of access to the gtd and the forfeiture of user privileges, in addition to any other appropriate legal remedies.\n",
      "limitation of liability: although every reasonable effort has been made to check sources and verify facts, start cannot guarantee that accounts reported in the open literature are complete and accurate. start shall not be held liable for any loss or damage caused by errors or omissions or resulting from any use, misuse, or alteration of gtd data by the user. the user should not infer any additional actions or results beyond what is presented in a gtd entry and specifically, the user should not infer an individual associated with a particular incident was tried and convicted of terrorism or any other criminal offense. if new documentation about an event becomes available, an entry may be modified, as necessary and appropriate.\n",
      "termination of rights: the gtd developers reserve the right to remove access to the gtd website from any particular ip address or set of ip addresses, or to remove the database entirely from public access, at their discretion. in such an event, all user rights granted in this document are terminated.\n",
      "training\n",
      "start has released the first in a series of training modules designed to equip gtd users with the knowledge and tools to best leverage the database. this training module provides a general overview of the gtd, including the data collection process, uses of the gtd, and patterns of global terrorism. participants will learn basic data handling and how to generate summary statistics from the gtd using pivottables in microsoft excel.\n",
      "questions?\n",
      "find answers to frequently asked questions.\n",
      "contact the gtd staff at gtd@start.umd.edu.\n",
      "context\n",
      "bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous satoshi nakamoto. bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. transaction blocks contain a sha-256 cryptographic hash of previous transaction blocks, and are thus \"chained\" together, serving as an immutable record of all transactions that have ever occurred. as with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. happy (data) mining!\n",
      "content\n",
      "coincheckjpy_1-min_data_2014-10-31_to_2018-01-08.csv\n",
      "bitflyerjpy_1-min_data_2017-07-04_to_2018-01-08.csv\n",
      "coinbaseusd_1-min_data_2014-12-01_to_2018-01-08.csv\n",
      "bitstampusd_1-min_data_2012-01-01_to_2018-01-08.csv\n",
      "csv files for select bitcoin exchanges for the time period of jan 2012 to jan 2018, with minute to minute updates of ohlc (open, high, low, close), volume in btc and indicated currency, and weighted bitcoin price. timestamps are in unix time. timestamps without any trades or activity have their data fields populated with nans. if a timestamp is missing, or if there are jumps, this may be because the exchange (or its api) was down, the exchange (or its api) did not exist, or some other unforseen technical error in data reporting or gathering. all effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk.\n",
      "acknowledgements and inspiration\n",
      "the various exchange apis, for making it difficult or unintuitive enough to get ohlc and volume data at 1-min intervals that i set out on this data scraping project. satoshi nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. i'd also like to thank viewers like you! can't wait to see what code or insights you all have to share.\n",
      "i am a lowly ph.d. student who did this for fun in my meager spare time. if you find this data interesting and you can spare a coffee to fuel my science, send it my way and i'd be immensely grateful!\n",
      "1kmwmcqa8qn9zrdgfdkw8ehkbgugkbrcf\n",
      "context\n",
      "for the first time, kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. the survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.\n",
      "to share some of the initial insights from the survey, we’ve worked with the folks from the pudding to put together this interactive report. they’ve shared all of the kernels used in the report here.\n",
      "content\n",
      "the data includes 5 files:\n",
      "schema.csv: a csv file with survey schema. this schema includes the questions that correspond to each column name in both the multiplechoiceresponses.csv and freeformresponses.csv.\n",
      "multiplechoiceresponses.csv: respondents' answers to multiple choice and ranking questions. these are non-randomized and thus a single row does correspond to all of a single user's answers. -freeformresponses.csv: respondents' freeform answers to kaggle's survey questions. these responses are randomized within a column, so that reading across a single row does not give a single user's answers.\n",
      "conversionrates.csv: currency conversion rates (to usd) as accessed from the r package \"quantmod\" on september 14, 2017\n",
      "respondenttypereadme.txt: this is a schema for decoding the responses in the \"asked\" column of the schema.csv file.\n",
      "kernel awards in november\n",
      "in the month of november, we’re awarding $1000 a week for code and analyses shared on this dataset via kaggle kernels. read more about this month’s kaggle kernels awards and help us advance the state of machine learning and data science by exploring this one of a kind dataset.\n",
      "methodology\n",
      "this survey received 16,716 usable respondents from 171 countries and territories. if a country or territory received less than 50 respondents, we grouped them into a group named “other” for anonymity.\n",
      "we excluded respondents who were flagged by our survey system as “spam” or who did not answer the question regarding their employment status (this question was the first required question, so not answering it indicates that the respondent did not proceed past the 5th question in our survey).\n",
      "most of our respondents were found primarily through kaggle channels, like our email list, discussion forums and social media channels.\n",
      "the survey was live from august 7th to august 25th. the median response time for those who participated in the survey was 16.4 minutes. we allowed respondents to complete the survey at any time during that window.\n",
      "we received salary data by first asking respondents for their day-to-day currency, and then asking them to write in either their total compensation.\n",
      "we’ve provided a csv with an exchange rate to usd for you to calculate the salary in us dollars on your own.\n",
      "the question was optional\n",
      "not every question was shown to every respondent. in an attempt to ask relevant questions to each respondent, we generally asked work related questions to employed data scientists and learning related questions to students. there is a column in the schema.csv file called \"asked\" that describes who saw each question. you can learn more about the different segments we used in the schema.csv file and respondenttypereadme.txt in the data tab.\n",
      "to protect the respondents’ identity, the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. we do not provide a key to match up the multiple choice and free form responses. further, the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker.\n",
      "the iris dataset was used in r.a. fisher's classic 1936 paper, the use of multiple measurements in taxonomic problems, and can also be found on the uci machine learning repository.\n",
      "it includes three iris species with 50 samples each as well as some properties about each flower. one flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
      "the columns in this dataset are:\n",
      "id\n",
      "sepallengthcm\n",
      "sepalwidthcm\n",
      "petallengthcm\n",
      "petalwidthcm\n",
      "species\n",
      "the world development indicators from the world bank contain over a thousand annual indicators of economic development from hundreds of countries around the world.\n",
      "here's a list of the available indicators along with a list of the available countries.\n",
      "for example, this data includes the life expectancy at birth from many countries around the world:\n",
      "the dataset hosted here is a slightly transformed verion of the raw files available here to facilitate analytics.\n",
      "actually, i prepare this dataset for students on my deep learning and nlp course.\n",
      "but i am also very happy to see kagglers play around with it.\n",
      "have fun!\n",
      "description:\n",
      "there are two channels of data provided in this dataset:\n",
      "news data: i crawled historical news headlines from reddit worldnews channel (/r/worldnews). they are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date. (range: 2008-06-08 to 2016-07-01)\n",
      "stock data: dow jones industrial average (djia) is used to \"prove the concept\". (range: 2008-08-08 to 2016-07-01)\n",
      "i provided three data files in .csv format:\n",
      "redditnews.csv: two columns the first column is the \"date\", and second column is the \"news headlines\". all news are ranked from top to bottom based on how hot they are. hence, there are 25 lines for each date.\n",
      "djia_table.csv: downloaded directly from yahoo finance: check out the web page for more info.\n",
      "combined_news_djia.csv: to make things easier for my students, i provide this combined dataset with 27 columns. the first column is \"date\", the second is \"label\", and the following ones are news headlines ranging from \"top1\" to \"top25\".\n",
      "=========================================\n",
      "to my students:\n",
      "i made this a binary classification task. hence, there are only two labels:\n",
      "\"1\" when djia adj close value rose or stayed as the same;\n",
      "\"0\" when djia adj close value decreased.\n",
      "for task evaluation, please use data from 2008-08-08 to 2014-12-31 as training set, and test set is then the following two years data (from 2015-01-02 to 2016-07-01). this is roughly a 80%/20% split.\n",
      "and, of course, use auc as the evaluation metric.\n",
      "=========================================\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "to all kagglers:\n",
      "please upvote this dataset if you like this idea for market prediction.\n",
      "if you think you coded an amazing trading algorithm,\n",
      "friendly advice\n",
      "do play safe with your own money :)\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "feel free to contact me if there is any question~\n",
      "and, remember me when you become a millionaire :p\n",
      "this data set includes 721 pokemon, including their number, name, first and second type, and basic stats: hp, attack, defense, special attack, special defense, and speed. it has been of great use when teaching statistics to kids. with certain types you can also give a geeky introduction to machine learning.\n",
      "this are the raw attributes that are used for calculating how much damage an attack will do in the games. this dataset is about the pokemon games (not pokemon cards or pokemon go).\n",
      "the data as described by myles o'neill is:\n",
      "#: id for each pokemon\n",
      "name: name of each pokemon\n",
      "type 1: each pokemon has a type, this determines weakness/resistance to attacks\n",
      "type 2: some pokemon are dual type and have 2\n",
      "total: sum of all stats that come after this, a general guide to how strong a pokemon is\n",
      "hp: hit points, or health, defines how much damage a pokemon can withstand before fainting\n",
      "attack: the base modifier for normal attacks (eg. scratch, punch)\n",
      "defense: the base damage resistance against normal attacks\n",
      "sp atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)\n",
      "sp def: the base damage resistance against special attacks\n",
      "speed: determines which pokemon attacks first each round\n",
      "the data for this table has been acquired from several different sites, including:\n",
      "pokemon.com\n",
      "pokemondb\n",
      "bulbapeida\n",
      "one question has been answered with this database: the type of a pokemon cannot be inferred only by it's attack and deffence. it would be worthy to find which two variables can define the type of a pokemon, if any. two variables can be plotted in a 2d space, and used as an example for machine learning. this could mean the creation of a visual example any geeky machine learning class would love.\n",
      "these files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (current, late, fully paid, etc.) and latest payment information. the file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. the file is a matrix of about 890 thousand observations and 75 variables. a data dictionary is provided in a separate file. k\n",
      "context\n",
      "after watching somm (a documentary on master sommeliers) i wondered how i could create a predictive model to identify wines through blind tasting like a master sommelier would. the first step in this journey was gathering some data to train a model. i plan to use deep learning to predict the wine variety using words in the description/review. the model still won't be able to taste the wine, but theoretically it could identify the wine based on a description that a sommelier could give. if anyone has any ideas on how to accomplish this, please post them!\n",
      "content\n",
      "the data consists of 10 fields:\n",
      "points: the number of points wineenthusiast rated the wine on a scale of 1-100 (though they say they only post reviews for wines that score >=80)\n",
      "title: the title of the wine review, which often contains the vintage if you're interested in extracting that feature\n",
      "variety: the type of grapes used to make the wine (ie pinot noir)\n",
      "description: a few sentences from a sommelier describing the wine's taste, smell, look, feel, etc.\n",
      "country: the country that the wine is from\n",
      "province: the province or state that the wine is from\n",
      "region 1: the wine growing area in a province or state (ie napa)\n",
      "region 2: sometimes there are more specific regions specified within a wine growing area (ie rutherford inside the napa valley), but this value can sometimes be blank\n",
      "winery: the winery that made the wine\n",
      "designation: the vineyard within the winery where the grapes that made the wine are from\n",
      "price: the cost for a bottle of the wine\n",
      "taster name: name of the person who tasted and reviewed the wine\n",
      "taster twitter handle: twitter handle for the person who tasted and reviewed the wine\n",
      "acknowledgements\n",
      "the data was scraped from wineenthusiast during the week of june 15th, 2017. the code for the scraper can be found here if you have any more specific questions about data collection that i didn't address.\n",
      "update 11/24/2017 after feedback from users of the dataset i scraped the reviews again on november 22nd, 2017. this time around i collected the title of each review, which you can parse the year out of, the tasters name, and the taster's twitter handle. this should also fix the duplicate entry issue.\n",
      "inspiration\n",
      "i think that this dataset offers some great opportunities for sentiment analysis and other text related predictive models. my overall goal is to create a model that can identify the variety, winery, and location of a wine based on a description. if anyone has any ideas, breakthroughs, or other interesting insights/models please post them.\n",
      "some say climate change is the biggest threat of our age while others say it’s a myth based on dodgy science. we are turning some of the data over to you so you can form your own view.\n",
      "even more than with other data sets that kaggle has featured, there’s a huge amount of data cleaning and preparation that goes into putting together a long-time study of climate trends. early data was collected by technicians using mercury thermometers, where any variation in the visit time impacted measurements. in the 1940s, the construction of airports caused many weather stations to be moved. in the 1980s, there was a move to electronic thermometers that are said to have a cooling bias.\n",
      "given this complexity, there are a range of organizations that collate climate trends data. the three most cited land and ocean temperature data sets are noaa’s mlost, nasa’s gistemp and the uk’s hadcrut.\n",
      "we have repackaged the data from a newer compilation put together by the berkeley earth, which is affiliated with lawrence berkeley national laboratory. the berkeley earth surface temperature study combines 1.6 billion temperature reports from 16 pre-existing archives. it is nicely packaged and allows for slicing into interesting subsets (for example by country). they publish the source data and the code for the transformations they applied. they also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away.\n",
      "in this dataset, we have include several files:\n",
      "global land and ocean-and-land temperatures (globaltemperatures.csv):\n",
      "date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures\n",
      "landaveragetemperature: global average land temperature in celsius\n",
      "landaveragetemperatureuncertainty: the 95% confidence interval around the average\n",
      "landmaxtemperature: global average maximum land temperature in celsius\n",
      "landmaxtemperatureuncertainty: the 95% confidence interval around the maximum land temperature\n",
      "landmintemperature: global average minimum land temperature in celsius\n",
      "landmintemperatureuncertainty: the 95% confidence interval around the minimum land temperature\n",
      "landandoceanaveragetemperature: global average land and ocean temperature in celsius\n",
      "landandoceanaveragetemperatureuncertainty: the 95% confidence interval around the global average land and ocean temperature\n",
      "other files include:\n",
      "global average land temperature by country (globallandtemperaturesbycountry.csv)\n",
      "global average land temperature by state (globallandtemperaturesbystate.csv)\n",
      "global land temperatures by major city (globallandtemperaturesbymajorcity.csv)\n",
      "global land temperatures by city (globallandtemperaturesbycity.csv)\n",
      "the raw data comes from the berkeley earth data page.\n",
      "a food products database\n",
      "open food facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of information we can find on product labels.\n",
      "made by everyone\n",
      "open food facts is a non-profit association of volunteers. 5000+ contributors like you have added 100 000+ products from 150 countries using our android, iphone or windows phone app or their camera to scan barcodes and upload pictures of products and their labels.\n",
      "for everyone\n",
      "data about food is of public interest and has to be open. the complete database is published as open data and can be reused by anyone and for any use. check-out the cool reuses or make your own!\n",
      "dataset structure\n",
      "the dataset contains a single table, foodfacts, in csv form in foodfacts.csv and in sqlite form in database.sqlite.\n",
      "the columns in open food facts are as follows:\n",
      "code (text)\n",
      "url (text)\n",
      "creator (text)\n",
      "created_t (text)\n",
      "created_datetime (text)\n",
      "last_modified_t (text)\n",
      "last_modified_datetime (text)\n",
      "product_name (text)\n",
      "generic_name (text)\n",
      "quantity (text)\n",
      "packaging (text)\n",
      "packaging_tags (text)\n",
      "brands (text)\n",
      "brands_tags (text)\n",
      "categories (text)\n",
      "categories_tags (text)\n",
      "categories_en (text)\n",
      "origins (text)\n",
      "origins_tags (text)\n",
      "manufacturing_places (text)\n",
      "manufacturing_places_tags (text)\n",
      "labels (text)\n",
      "labels_tags (text)\n",
      "labels_en (text)\n",
      "emb_codes (text)\n",
      "emb_codes_tags (text)\n",
      "first_packaging_code_geo (text)\n",
      "cities (text)\n",
      "cities_tags (text)\n",
      "purchase_places (text)\n",
      "stores (text)\n",
      "countries (text)\n",
      "countries_tags (text)\n",
      "countries_en (text)\n",
      "ingredients_text (text)\n",
      "allergens (text)\n",
      "allergens_en (text)\n",
      "traces (text)\n",
      "traces_tags (text)\n",
      "traces_en (text)\n",
      "serving_size (text)\n",
      "no_nutriments (numeric)\n",
      "additives_n (numeric)\n",
      "additives (text)\n",
      "additives_tags (text)\n",
      "additives_en (text)\n",
      "ingredients_from_palm_oil_n (numeric)\n",
      "ingredients_from_palm_oil (numeric)\n",
      "ingredients_from_palm_oil_tags (text)\n",
      "ingredients_that_may_be_from_palm_oil_n (numeric)\n",
      "ingredients_that_may_be_from_palm_oil (numeric)\n",
      "ingredients_that_may_be_from_palm_oil_tags (text)\n",
      "nutrition_grade_uk (numeric)\n",
      "nutrition_grade_fr (text)\n",
      "pnns_groups_1 (text)\n",
      "pnns_groups_2 (text)\n",
      "states (text)\n",
      "states_tags (text)\n",
      "states_en (text)\n",
      "main_category (text)\n",
      "main_category_en (text)\n",
      "image_url (text)\n",
      "image_small_url (text)\n",
      "energy_100g (numeric)\n",
      "energy_from_fat_100g (numeric)\n",
      "fat_100g (numeric)\n",
      "saturated_fat_100g (numeric)\n",
      "butyric_acid_100g (numeric)\n",
      "caproic_acid_100g (numeric)\n",
      "caprylic_acid_100g (numeric)\n",
      "capric_acid_100g (numeric)\n",
      "lauric_acid_100g (numeric)\n",
      "myristic_acid_100g (numeric)\n",
      "palmitic_acid_100g (numeric)\n",
      "stearic_acid_100g (numeric)\n",
      "arachidic_acid_100g (numeric)\n",
      "behenic_acid_100g (numeric)\n",
      "lignoceric_acid_100g (numeric)\n",
      "cerotic_acid_100g (numeric)\n",
      "montanic_acid_100g (numeric)\n",
      "melissic_acid_100g (numeric)\n",
      "monounsaturated_fat_100g (numeric)\n",
      "polyunsaturated_fat_100g (numeric)\n",
      "omega_3_fat_100g (numeric)\n",
      "alpha_linolenic_acid_100g (numeric)\n",
      "eicosapentaenoic_acid_100g (numeric)\n",
      "docosahexaenoic_acid_100g (numeric)\n",
      "omega_6_fat_100g (numeric)\n",
      "linoleic_acid_100g (numeric)\n",
      "arachidonic_acid_100g (numeric)\n",
      "gamma_linolenic_acid_100g (numeric)\n",
      "dihomo_gamma_linolenic_acid_100g (numeric)\n",
      "omega_9_fat_100g (numeric)\n",
      "oleic_acid_100g (numeric)\n",
      "elaidic_acid_100g (numeric)\n",
      "gondoic_acid_100g (numeric)\n",
      "mead_acid_100g (numeric)\n",
      "erucic_acid_100g (numeric)\n",
      "nervonic_acid_100g (numeric)\n",
      "trans_fat_100g (numeric)\n",
      "cholesterol_100g (numeric)\n",
      "carbohydrates_100g (numeric)\n",
      "sugars_100g (numeric)\n",
      "sucrose_100g (numeric)\n",
      "glucose_100g (numeric)\n",
      "fructose_100g (numeric)\n",
      "lactose_100g (numeric)\n",
      "maltose_100g (numeric)\n",
      "maltodextrins_100g (numeric)\n",
      "starch_100g (numeric)\n",
      "polyols_100g (numeric)\n",
      "fiber_100g (numeric)\n",
      "proteins_100g (numeric)\n",
      "casein_100g (numeric)\n",
      "serum_proteins_100g (numeric)\n",
      "nucleotides_100g (numeric)\n",
      "salt_100g (numeric)\n",
      "sodium_100g (numeric)\n",
      "alcohol_100g (numeric)\n",
      "vitamin_a_100g (numeric)\n",
      "beta_carotene_100g (numeric)\n",
      "vitamin_d_100g (numeric)\n",
      "vitamin_e_100g (numeric)\n",
      "vitamin_k_100g (numeric)\n",
      "vitamin_c_100g (numeric)\n",
      "vitamin_b1_100g (numeric)\n",
      "vitamin_b2_100g (numeric)\n",
      "vitamin_pp_100g (numeric)\n",
      "vitamin_b6_100g (numeric)\n",
      "vitamin_b9_100g (numeric)\n",
      "vitamin_b12_100g (numeric)\n",
      "biotin_100g (numeric)\n",
      "pantothenic_acid_100g (numeric)\n",
      "silica_100g (numeric)\n",
      "bicarbonate_100g (numeric)\n",
      "potassium_100g (numeric)\n",
      "chloride_100g (numeric)\n",
      "calcium_100g (numeric)\n",
      "phosphorus_100g (numeric)\n",
      "iron_100g (numeric)\n",
      "magnesium_100g (numeric)\n",
      "zinc_100g (numeric)\n",
      "copper_100g (numeric)\n",
      "manganese_100g (numeric)\n",
      "fluoride_100g (numeric)\n",
      "selenium_100g (numeric)\n",
      "chromium_100g (numeric)\n",
      "molybdenum_100g (numeric)\n",
      "iodine_100g (numeric)\n",
      "caffeine_100g (numeric)\n",
      "taurine_100g (numeric)\n",
      "ph_100g (numeric)\n",
      "fruits_vegetables_nuts_100g (numeric)\n",
      "collagen_meat_protein_ratio_100g (numeric)\n",
      "cocoa_100g (numeric)\n",
      "chlorophyl_100g (numeric)\n",
      "carbon_footprint_100g (numeric)\n",
      "nutrition_score_fr_100g (numeric)\n",
      "nutrition_score_uk_100g (numeric)\n",
      "salary increase by type of college\n",
      "party school? liberal arts college? state school? you already know your starting salary will be different depending on what type of school you attend. but, increased earning power shows less disparity. ten years out, graduates of ivy league schools earned 99% more than they did at graduation. party school graduates saw an 85% increase. engineering school graduates fared worst, earning 76% more 10 years out of school. see where your school ranks.\n",
      "salaries by region\n",
      "attending college in the midwest leads to the lowest salary both at graduation and at mid-career, according to the payscale inc. survey. graduates of schools in the northeast and california fared best.\n",
      "salary increase by major\n",
      "your parents might have worried when you chose philosophy or international relations as a major. but a year-long survey of 1.2 million people with only a bachelor's degree by payscale inc. shows that graduates in these subjects earned 103.5% and 97.8% more, respectively, about 10 years post-commencement. majors that didn't show as much salary growth include nursing and information technology.\n",
      "all data was obtained from the wall street journal based on data from payscale, inc:\n",
      "salaries for colleges by type\n",
      "salaries for colleges by region\n",
      "degrees that pay you back\n",
      "context\n",
      "although this dataset was originally contributed to the uci machine learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. and how certain can your model be?\n",
      "content\n",
      "this dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the agaricus and lepiota family mushroom drawn from the audubon society field guide to north american mushrooms (1981). each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. this latter class was combined with the poisonous one. the guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for poisonous oak and ivy.\n",
      "time period: donated to uci ml 27 april 1987\n",
      "inspiration\n",
      "what types of machine learning models perform best on this dataset?\n",
      "which features are most indicative of a poisonous mushroom?\n",
      "acknowledgements\n",
      "this dataset was originally donated to the uci machine learning repository. you can learn more about past research using the data here.\n",
      "start a new kernel\n",
      "features are computed from a digitized image of a fine needle aspirate (fna) of a breast mass. they describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [k. p. bennett and o. l. mangasarian: \"robust linear programming discrimination of two linearly inseparable sets\", optimization methods and software 1, 1992, 23-34].\n",
      "this database is also available through the uw cs ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/wdbc/\n",
      "also can be found on uci machine learning repository: https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29\n",
      "attribute information:\n",
      "1) id number 2) diagnosis (m = malignant, b = benign) 3-32)\n",
      "ten real-valued features are computed for each cell nucleus:\n",
      "a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n",
      "the mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. for instance, field 3 is mean radius, field 13 is radius se, field 23 is worst radius.\n",
      "all feature values are recoded with four significant digits.\n",
      "missing attribute values: none\n",
      "class distribution: 357 benign, 212 malignant\n",
      "context\n",
      "this dataset consists of reviews of fine foods from amazon. the data span a period of more than 10 years, including all ~500,000 reviews up to october 2012. reviews include product and user information, ratings, and a plain text review. it also includes reviews from all other amazon categories.\n",
      "contents\n",
      "reviews.csv: pulled from the corresponding sqlite table named reviews in database.sqlite\n",
      "database.sqlite: contains the table 'reviews'\n",
      "\n",
      "data includes:\n",
      "- reviews from oct 1999 - oct 2012\n",
      "- 568,454 reviews\n",
      "- 256,059 users\n",
      "- 74,258 products\n",
      "- 260 users with > 50 reviews\n",
      "acknowledgements\n",
      "see this sqlite query for a quick sample of the dataset.\n",
      "if you publish articles based on this dataset, please cite the following paper:\n",
      "j. mcauley and j. leskovec. from amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. www, 2013.\n",
      "context\n",
      "the world happiness report is a landmark survey of the state of global happiness. the first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 update. the world happiness 2017, which ranks 155 countries by their happiness levels, was released at the united nations at an event celebrating international day of happiness on march 20th. the report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. leading experts across fields – economics, psychology, survey analysis, national statistics, health, public policy and more – describe how measurements of well-being can be used effectively to assess the progress of nations. the reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.\n",
      "content\n",
      "the happiness scores and rankings use data from the gallup world poll. the scores are based on answers to the main life evaluation question asked in the poll. this question, known as the cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. the scores are from nationally representative samples for the years 2013-2016 and use the gallup weights to make the estimates representative. the columns following the happiness score estimate the extent to which each of six factors – economic production, social support, life expectancy, freedom, absence of corruption, and generosity – contribute to making life evaluations higher in each country than they are in dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. they have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.\n",
      "inspiration\n",
      "what countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness? how did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports? did any country experience a significant increase or decrease in happiness?\n",
      "what is dystopia?\n",
      "dystopia is an imaginary country that has the world’s least-happy people. the purpose in establishing dystopia is to have a benchmark against which all countries can be favorably compared (no country performs more poorly than dystopia) in terms of each of the six key variables, thus allowing each sub-bar to be of positive width. the lowest scores observed for the six key variables, therefore, characterize dystopia. since life would be very unpleasant in a country with the world’s lowest incomes, lowest life expectancy, lowest generosity, most corruption, least freedom and least social support, it is referred to as “dystopia,” in contrast to utopia.\n",
      "what are the residuals?\n",
      "the residuals, or unexplained components, differ for each country, reflecting the extent to which the six variables either over- or under-explain average 2014-2016 life evaluations. these residuals have an average value of approximately zero over the whole set of countries. figure 2.2 shows the average residual for each country when the equation in table 2.1 is applied to average 2014- 2016 data for the six variables in that country. we combine these residuals with the estimate for life evaluations in dystopia so that the combined bar will always have positive values. as can be seen in figure 2.2, although some life evaluation residuals are quite large, occasionally exceeding one point on the scale from 0 to 10, they are always much smaller than the calculated value in dystopia, where the average life is rated at 1.85 on the 0 to 10 scale.\n",
      "what do the columns succeeding the happiness score(like family, generosity, etc.) describe?\n",
      "the following columns: gdp per capita, family, life expectancy, freedom, generosity, trust government corruption describe the extent to which these factors contribute in evaluating the happiness in each country. the dystopia residual metric actually is the dystopia happiness score(1.85) + the residual value or the unexplained value for each country as stated in the previous answer.\n",
      "if you add all these factors up, you get the happiness score so it might be un-reliable to model them to predict happiness scores.\n",
      "start a new kernel\n",
      "context\n",
      "fashion-mnist is a dataset of zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. each example is a 28x28 grayscale image, associated with a label from 10 classes. zalando intends fashion-mnist to serve as a direct drop-in replacement for the original mnist dataset for benchmarking machine learning algorithms. it shares the same image size and structure of training and testing splits.\n",
      "the original mnist dataset contains a lot of handwritten digits. members of the ai/ml/data science community love this dataset and use it as a benchmark to validate their algorithms. in fact, mnist is often the first dataset researchers try. \"if it doesn't work on mnist, it won't work at all\", they said. \"well, if it does work on mnist, it may still fail on others.\"\n",
      "zalando seeks to replace the original mnist dataset\n",
      "content\n",
      "each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. this pixel-value is an integer between 0 and 255. the training and test data sets have 785 columns. the first column consists of the class labels (see above), and represents the article of clothing. the rest of the columns contain the pixel-values of the associated image.\n",
      "to locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. the pixel is located on row i and column j of a 28 x 28 matrix.\n",
      "for example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
      "\n",
      "labels\n",
      "each training and test example is assigned to one of the following labels:\n",
      "0 t-shirt/top\n",
      "1 trouser\n",
      "2 pullover\n",
      "3 dress\n",
      "4 coat\n",
      "5 sandal\n",
      "6 shirt\n",
      "7 sneaker\n",
      "8 bag\n",
      "9 ankle boot\n",
      "\n",
      "tl;dr\n",
      "each row is a separate image\n",
      "column 1 is the class label.\n",
      "remaining columns are pixel numbers (784 total).\n",
      "each value is the darkness of the pixel (1 to 255)\n",
      "acknowledgements\n",
      "original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\n",
      "dataset was converted to csv with this script: https://pjreddie.com/projects/mnist-in-csv/\n",
      "license\n",
      "the mit license (mit) copyright © [2017] zalando se, https://tech.zalando.com\n",
      "permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “software”), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\n",
      "the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.\n",
      "the software is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.\n",
      "this contains data relevant for the 2016 us presidential election, including up-to-date primary results.\n",
      "exploration ideas\n",
      "what candidates within the republican party have results that are the most anti-correlated?\n",
      "which republican candidate is hillary clinton most correlated with based on county voting patterns? what about bernie sanders?\n",
      "what insights can you discover by mapping this data?\n",
      "do you have answers or other exploration ideas? add your ideas to this forum post and share your insights through kaggle scripts!\n",
      "do you think that we should augment this dataset with more data sources? submit a pull request to this repo, or let us know here!\n",
      "data description\n",
      "the 2016 us election dataset contains several main files and folders at the moment. you may download the entire archive via the \"download data\" link at the top of the page, or interact with the data in kaggle scripts through the ../input directory.\n",
      "original data sources\n",
      "primary results from cnn\n",
      "new hampshire county-level results\n",
      "county shapefiles\n",
      "county quickfacts\n",
      "overview\n",
      "game of thrones is a hit fantasy tv show based on the equally famous book series \"a song of fire and ice\" by george rr martin. the show is well known for its vastly complicated political landscape, large number of characters, and its frequent character deaths.\n",
      "data sources\n",
      "this dataset combines three sources of data, all of which are based on information from the book series.\n",
      "firstly, there is battles.csv which contains chris albon's \"the war of the five kings\" dataset, which can be found here: https://github.com/chrisalbon/war_of_the_five_kings_dataset . its a great collection of all of the battles in the series.\n",
      "secondly we have character-deaths.csv from erin pierce and ben kahle. this dataset was created as a part of their bayesian survival analysis which can be found here: http://allendowney.blogspot.com/2015/03/bayesian-survival-analysis-for-game-of.html\n",
      "finally we have a more comprehensive character dataset with character-predictions.csv. this comes from the team at a song of ice and data who scraped it from http://awoiaf.westeros.org/ . it also includes their predictions on which character will die, the methodology of which can be found here: https://got.show/machine-learning-algorithm-predicts-death-game-of-thrones\n",
      "what insights about the complicated political landscape of this fantasy world can you find in this data?\n",
      "of course, it goes without saying that this dataset contains spoilers ;)\n",
      "what influences love at first sight? (or, at least, love in the first four minutes?) this dataset was compiled by columbia business school professors ray fisman and sheena iyengar for their paper gender differences in mate selection: evidence from a speed dating experiment.\n",
      "data was gathered from participants in experimental speed dating events from 2002-2004. during the events, the attendees would have a four minute \"first date\" with every other participant of the opposite sex. at the end of their four minutes, participants were asked if they would like to see their date again. they were also asked to rate their date on six attributes: attractiveness, sincerity, intelligence, fun, ambition, and shared interests.\n",
      "the dataset also includes questionnaire data gathered from participants at different points in the process. these fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information. see the speed dating data key document below for details.\n",
      "for more analysis from iyengar and fisman, read racial preferences in dating.\n",
      "data exploration ideas\n",
      "what are the least desirable attributes in a male partner? does this differ for female partners?\n",
      "how important do people think attractiveness is in potential mate selection vs. its real impact?\n",
      "are shared interests more important than a shared racial background?\n",
      "can people accurately predict their own perceived value in the dating market?\n",
      "in terms of getting a second date, is it better to be someone's first speed date of the night or their last?\n",
      "context\n",
      "most publicly available football (soccer) statistics are limited to aggregated data such as goals, shots, fouls, cards. when assessing performance or building predictive models, this simple aggregation, without any context, can be misleading. for example, a team that produced 10 shots on target from long range has a lower chance of scoring than a club that produced the same amount of shots from inside the box. however, metrics derived from this simple count of shots will similarly asses the two teams.\n",
      "a football game generates much more events and it is very important and interesting to take into account the context in which those events were generated. this dataset should keep sports analytics enthusiasts awake for long hours as the number of questions that can be asked is huge.\n",
      "content\n",
      "this dataset is a result of a very tiresome effort of webscraping and integrating different data sources. the central element is the text commentary. all the events were derived by reverse engineering the text commentary, using regex. using this, i was able to derive 11 types of events, as well as the main player and secondary player involved in those events and many other statistics. in case i've missed extracting some useful information, you are gladly invited to do so and share your findings. the dataset provides a granular view of 9,074 games, totaling 941,009 events from the biggest 5 european football (soccer) leagues: england, spain, germany, italy, france from 2011/2012 season to 2016/2017 season as of 25.01.2017. there are games that have been played during these seasons for which i could not collect detailed data. overall, over 90% of the played games during these seasons have event data.\n",
      "the dataset is organized in 3 files:\n",
      "events.csv contains event data about each game. text commentary was scraped from: bbc.com, espn.com and onefootball.com\n",
      "ginf.csv - contains metadata and market odds about each game. odds were collected from oddsportal.com\n",
      "dictionary.txt contains a dictionary with the textual description of each categorical variable coded with integers\n",
      "past research\n",
      "i have used this data to:\n",
      "create predictive models for football games in order to bet on football outcomes.\n",
      "make visualizations about upcoming games\n",
      "build expected goals models and compare players\n",
      "inspiration\n",
      "there are tons of interesting questions a sports enthusiast can answer with this dataset. for example:\n",
      "what is the value of a shot? or what is the probability of a shot being a goal given it's location, shooter, league, assist method, gamestate, number of players on the pitch, time - known as expected goals (xg) models\n",
      "when are teams more likely to score?\n",
      "which teams are the best or sloppiest at holding the lead?\n",
      "which teams or players make the best use of set pieces?\n",
      "in which leagues is the referee more likely to give a card?\n",
      "how do players compare when they shoot with their week foot versus strong foot? or which players are ambidextrous?\n",
      "identify different styles of plays (shooting from long range vs shooting from the box, crossing the ball vs passing the ball, use of headers)\n",
      "which teams have a bias for attacking on a particular flank?\n",
      "and many many more...\n",
      "context\n",
      "zillow's economic research team collects, cleans and publishes housing and economic data from a variety of public and proprietary sources. public property record data filed with local municipalities -- including deeds, property facts, parcel information and transactional histories -- forms the backbone of our data products, and is fleshed out with proprietary data derived from property listings and user behavior on zillow.\n",
      "the large majority of zillow's aggregated housing market and economic data is made available for free download at zillow.com/data.\n",
      "content\n",
      "variable availability:\n",
      "zillow home value index (zhvi): a smoothed seasonally adjusted measure of the median estimated home value across a given region and housing type. a dollar denominated alternative to repeat-sales indices. find a more detailed methodology here: http://www.zillow.com/research/zhvi-methodology-6032/\n",
      "zillow rent index (zri): a smoothed seasonally adjusted measure of the median estimated market rate rent across a given region and housing type. a dollar denominated alternative to repeat-rent indices. find a more detailed methodology here: http://www.zillow.com/research/zillow-rent-index-methodology-2393/\n",
      "for-sale listing/inventory metrics: zillow provides many variables capturing current and historical for-sale listings availability, generally from 2012 to current. these variables include median list prices and inventory counts, both by various property types. variables capturing for-sale market competitiveness including share of listings with a price cut, median price cut size, age of inventory, and the days a listing spend on zillow before the sale is final.\n",
      "home sales metrics: zillow provides data on sold homes including median sale price by various housing types, sale counts (methodology here: http://www.zillow.com/research/home-sales-methodology-7733/), and a normalized view of sale volume referred to as turnover. the prevalence of foreclosures is also provided as ratio of the housing stock and the share of all sales in which the home was previously foreclosed upon.\n",
      "for-rent listing metrics: zillow provides median rents prices and median rent price per square foot by property type and bedroom count.\n",
      "housing type definitions:\n",
      "all homes: zillow defines all homes as single-family, condominium and co-operative homes with a county record. unless specified, all series cover this segment of the housing stock.\n",
      "condo/co-op: condominium and co-operative homes.\n",
      "multifamily 5+ units: units in buildings with 5 or more housing units, that are not a condominiums or co-ops.\n",
      "duplex/triplex: housing units in buildings with 2 or 3 housing units.\n",
      "tiers: by metro, we determine price tier cutoffs that divide the all homes housing stock into thirds using the full distribution of estimated home values. we then estimate real estate metrics within the property sets, bottom, middle, and top, defined by these cutoffs. when reported at the national level, all bottom tier homes defined at the metro level are pooled together to form the national bottom tier. the same holds for middle and top tier homes.\n",
      "regional availability:\n",
      "zillow metrics are reported for common us geographies including nation, state, metro (2013 census defined cbsas), county, city, zip code, and neighborhood.\n",
      "we provide a crosswalk between colloquial zillow region names and federally defined region names and linking variables such as county fips codes and cbsa codes. cities and neighborhoods do not match standard jurisdictional boundaries. zillow city boundaries reflect mailing address conventions and so are often visually similar to collections of zip codes. zillow neighborhood boundaries can be found here.\n",
      "suppression rules: to ensure reliability of reported values the zillow economic research team applies suppression rules triggered by low sample sizes and excessive volatility. these rules are customized to the metric and region type and explain most missingness found in the provided datasets.\n",
      "additional data products\n",
      "the following data products and more are available for free download exclusively at zillow.com/data:\n",
      "zillow home value forecast\n",
      "zillow rent forecast\n",
      "negative equity (the share of mortgaged properties worth less than mortgage balance)\n",
      "zillow home price expectations survey\n",
      "zillow housing aspirations report\n",
      "zillow rising sea levels research\n",
      "cash buyers time series\n",
      "buy vs. rent breakeven horizon\n",
      "mortgage affordability, rental affordability, price-to-income ratio\n",
      "conventional 30-year fixed mortgage rate, weekly time series\n",
      "jumbo 30-year fixed mortgage rates, weekly time series\n",
      "acknowledgements\n",
      "the mission of the zillow economic research team is to be the most open, authoritative source for timely and accurate housing data and unbiased insight. we aim to empower consumers, industry professionals, policy makers and researchers looking to better understand the housing market.\n",
      "to see more of our mission in action, we invite you to learn more about us and to check out our collection of research briefs, stories, data tools and past presentations at https://www.zillow.com/research/\n",
      "inspiration\n",
      "zillow, and the zillow economic research team, firmly believe that not only do data want to be free, data are going to be free. instead of simply publishing raw data, we believe in the power of pushing data up the ladder from raw data bits, to actionable information and finally to unique insight. we aim to answer questions of all kinds, even questions our users may not have known they had before coming to us. when done right, we firmly believe this process of turning data into insight can be transformational in people's lives.\n",
      "please join us on this journey, and we're excited to see what insights you can discover hidden amongst our data!\n",
      "this dataset contains house sale prices for king county, which includes seattle. it includes homes sold between may 2014 and may 2015.\n",
      "it's a great dataset for evaluating simple regression models.\n",
      "context\n",
      "these files contain metadata for all 45,000 movies listed in the full movielens dataset. the dataset consists of movies released on or before july 2017. data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, tmdb vote counts and vote averages.\n",
      "this dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. ratings are on a scale of 1-5 and have been obtained from the official grouplens website.\n",
      "content\n",
      "this dataset consists of the following files:\n",
      "movies_metadata.csv: the main movies metadata file. contains information on 45,000 movies featured in the full movielens dataset. features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.\n",
      "keywords.csv: contains the movie plot keywords for our movielens movies. available in the form of a stringified json object.\n",
      "credits.csv: consists of cast and crew information for all our movies. available in the form of a stringified json object.\n",
      "links.csv: the file that contains the tmdb and imdb ids of all the movies featured in the full movielens dataset.\n",
      "links_small.csv: contains the tmdb and imdb ids of a small subset of 9,000 movies of the full dataset.\n",
      "ratings_small.csv: the subset of 100,000 ratings from 700 users on 9,000 movies.\n",
      "the full movielens dataset consisting of 26 million ratings and 750,000 tag applications from 270,000 users on all the 45,000 movies in this dataset can be accessed here\n",
      "acknowledgements\n",
      "this dataset is an ensemble of data collected from tmdb and grouplens. the movie details, credits and keywords have been collected from the tmdb open api. this product uses the tmdb api but is not endorsed or certified by tmdb. their api also provides access to data on many additional movies, actors and actresses, crew members, and tv shows. you can try it for yourself here.\n",
      "the movie links and ratings have been obtained from the official grouplens website. the files are a part of the dataset available here\n",
      "inspiration\n",
      "this dataset was assembled as part of my second capstone project for springboard's data science career track. i wanted to perform an extensive eda on movie data to narrate the history and the story of cinema and use this metadata in combination with movielens ratings to build various types of recommender systems.\n",
      "both my notebooks are available as kernels with this dataset: the story of film and movie recommender systems\n",
      "some of the things you can do with this dataset: predicting movie revenue and/or movie success based on a certain metric. what movies tend to get higher vote counts and vote averages on tmdb? building content based and collaborative filtering based recommendation engines.\n",
      "context\n",
      "this dataset is a playground for fundamental and technical analysis. it is said that 30% of traffic on stocks is already generated by machines, can trading be fully automated? if not, there is still a lot to learn from historical data.\n",
      "content\n",
      "dataset consists of following files:\n",
      "prices.csv: raw, as-is daily prices. most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. there have been approx. 140 stock splits in that time, this set doesn't account for that.\n",
      "prices-split-adjusted.csv: same as prices, but there have been added adjustments for splits.\n",
      "securities.csv: general description of each company with division on sectors\n",
      "fundamentals.csv: metrics extracted from annual sec 10k fillings (2012-2016), should be enough to derive most of popular fundamental indicators.\n",
      "acknowledgements\n",
      "prices were fetched from yahoo finance, fundamentals are from nasdaq financials, extended by some fields from edgar sec databases.\n",
      "mining\n",
      "here is couple of things one could try out with this data:\n",
      "technical\n",
      "one day ahead prediction: rolling linear regression, arima, neural networks, lstm\n",
      "momentum/mean-reversion strategies\n",
      "security clustering, portfolio construction/hedging\n",
      "fundamental\n",
      "which company has biggest chance of being bankrupt? which one is undervalued (how prices behaved afterwards), what is return on investment?\n",
      "context\n",
      "in the last few days, i have been hearing a lot of buzz around cryptocurrencies. things like block chain, bitcoin, bitcoin cash, ethereum, ripple etc are constantly coming in the news articles i read. so i wanted to understand more about it and this post helped me get started. once the basics are done, the ds guy sleeping inside me (always lazy.!) woke up and started raising questions like\n",
      "how many such cryptocurrencies are there and what are their prices and valuations?\n",
      "why is there a sudden surge in the interest in recent days? is it due to the increase in the price in the last few days? etc.\n",
      "for getting answers to all these questions (and if possible to predict the future prices ;)), i started getting the data from coinmarketcap about the cryptocurrencies.\n",
      "update : bitcoin dataset\n",
      "so what next.? now that we have the price data, i wanted to dig a little more about the factors affecting the price of coins. i started of with bitcoin and there are quite a few parameters which affect the price of bitcoin. thanks to blockchain info, i was able to get quite a few parameters on once in two day basis.\n",
      "this will help understand the other factors related to bitcoin price and also help one make future predictions in a better way than just using the historical price.\n",
      "update2: ethereum dataset:\n",
      "this dataset has features related to ethereum. this is very similar to the bitcoin dataset and is available on a daily basis. data is taken from etherscan and the credits go to them for allowing us to use.\n",
      "content\n",
      "this dataset has the historical price information of some of the top cryptocurrencies by market capitalization. the currencies included are\n",
      "bitcoin\n",
      "ethereum\n",
      "ripple\n",
      "bitcoin cash\n",
      "bitconnect\n",
      "dash\n",
      "ethereum classic\n",
      "iota\n",
      "litecoin\n",
      "monero\n",
      "nem\n",
      "neo\n",
      "numeraire\n",
      "stratis\n",
      "waves\n",
      "in case if you are interested in the prices of some other currencies, please post in comments section and i will try to add them in the next version. i am planning to revise it once in a week.\n",
      "dataset has one csv file for each currency. price history is available on a daily basis from april 28, 2013. the columns in the csv file are\n",
      "date : date of observation\n",
      "open : opening price on the given day\n",
      "high : highest price on the given day\n",
      "low : lowest price on the given day\n",
      "close : closing price on the given day\n",
      "volume : volume of transactions on the given day\n",
      "market cap : market capitalization in usd\n",
      "bitcoin dataset (bitcoin_dataset.csv) :\n",
      "this dataset has the following features.\n",
      "date : date of observation\n",
      "btc_market_price : average usd market price across major bitcoin exchanges.\n",
      "btc_total_bitcoins : the total number of bitcoins that have already been mined.\n",
      "btc_market_cap : the total usd value of bitcoin supply in circulation.\n",
      "btc_trade_volume : the total usd value of trading volume on major bitcoin exchanges.\n",
      "btc_blocks_size : the total size of all block headers and transactions.\n",
      "btc_avg_block_size : the average block size in mb.\n",
      "btc_n_orphaned_blocks : the total number of blocks mined but ultimately not attached to the main bitcoin blockchain.\n",
      "btc_n_transactions_per_block : the average number of transactions per block.\n",
      "btc_median_confirmation_time : the median time for a transaction to be accepted into a mined block.\n",
      "btc_hash_rate : the estimated number of tera hashes per second the bitcoin network is performing.\n",
      "btc_difficulty : a relative measure of how difficult it is to find a new block.\n",
      "btc_miners_revenue : total value of coinbase block rewards and transaction fees paid to miners.\n",
      "btc_transaction_fees : the total value of all transaction fees paid to miners.\n",
      "btc_cost_per_transaction_percent : miners revenue as percentage of the transaction volume.\n",
      "btc_cost_per_transaction : miners revenue divided by the number of transactions.\n",
      "btc_n_unique_addresses : the total number of unique addresses used on the bitcoin blockchain.\n",
      "btc_n_transactions : the number of daily confirmed bitcoin transactions.\n",
      "btc_n_transactions_total : total number of transactions.\n",
      "btc_n_transactions_excluding_popular : the total number of bitcoin transactions, excluding the 100 most popular addresses.\n",
      "btc_n_transactions_excluding_chains_longer_than_100 : the total number of bitcoin transactions per day excluding long transaction chains.\n",
      "btc_output_volume : the total value of all transaction outputs per day.\n",
      "btc_estimated_transaction_volume : the total estimated value of transactions on the bitcoin blockchain.\n",
      "btc_estimated_transaction_volume_usd : the estimated transaction value in usd value.\n",
      "ethereum dataset (ethereum_dataset.csv):\n",
      "this dataset has the following features\n",
      "date(utc) : date of transaction\n",
      "unixtimestamp : unix timestamp\n",
      "eth_etherprice : price of ethereum\n",
      "eth_tx : number of transactions per day\n",
      "eth_address : cumulative address growth\n",
      "eth_supply : number of ethers in supply\n",
      "eth_marketcap : market cap in usd\n",
      "eth_hashrate : hash rate in gh/s\n",
      "eth_difficulty : difficulty level in th\n",
      "eth_blocks : number of blocks per day\n",
      "eth_uncles : number of uncles per day\n",
      "eth_blocksize : average block size in bytes\n",
      "eth_blocktime : average block time in seconds\n",
      "eth_gasprice : average gas price in wei\n",
      "eth_gaslimit : gas limit per day\n",
      "eth_gasused : total gas used per day\n",
      "eth_ethersupply : new ether supply per day\n",
      "eth_chaindatasize : chain data size in bytes\n",
      "eth_ens_register : ethereal name service (ens) registrations per day\n",
      "acknowledgements\n",
      "this data is taken from coinmarketcap and it is free to use the data.\n",
      "bitcoin dataset is obtained from blockchain info.\n",
      "ethereum dataset is obtained from etherscan.\n",
      "cover image : photo by thomas malama on unsplash\n",
      "inspiration\n",
      "some of the questions which could be inferred from this dataset are:\n",
      "how did the historical prices / market capitalizations of various currencies change over time?\n",
      "predicting the future price of the currencies\n",
      "which currencies are more volatile and which ones are more stable?\n",
      "how does the price fluctuations of currencies correlate with each other?\n",
      "seasonal trend in the price fluctuations\n",
      "bitcoin / ethereum dataset could be used to look at the following:\n",
      "factors affecting the bitcoin / ether price.\n",
      "directional prediction of bitcoin / ether price. (refer this paper for more inspiration)\n",
      "actual bitcoin price prediction.\n",
      "context\n",
      "these datasets contain information about all audio-video recordings of ted talks uploaded to the official ted.com website until september 21st, 2017. the ted main dataset contains information about all talks including number of views, number of comments, descriptions, speakers and titles. the ted transcripts dataset contains the transcripts for all talks available on ted.com.\n",
      "content (for the csv files)\n",
      "ted main dataset\n",
      "name: the official name of the ted talk. includes the title and the speaker.\n",
      "title: the title of the talk\n",
      "description: a blurb of what the talk is about.\n",
      "main_speaker: the first named speaker of the talk.\n",
      "speaker_occupation: the occupation of the main speaker.\n",
      "num_speaker: the number of speakers in the talk.\n",
      "duration: the duration of the talk in seconds.\n",
      "event: the ted/tedx event where the talk took place.\n",
      "film_date: the unix timestamp of the filming.\n",
      "published_date: the unix timestamp for the publication of the talk on ted.com\n",
      "comments: the number of first level comments made on the talk.\n",
      "tags: the themes associated with the talk.\n",
      "languages: the number of languages in which the talk is available.\n",
      "ratings: a stringified dictionary of the various ratings given to the talk (inspiring, fascinating, jaw dropping, etc.)\n",
      "related_talks: a list of dictionaries of recommended talks to watch next.\n",
      "url: the url of the talk.\n",
      "views: the number of views on the talk.\n",
      "ted transcripts dataset\n",
      "url: the url of the talk\n",
      "transcript: the official english transcript of the talk.\n",
      "acknowledgements\n",
      "the data has been scraped from the official ted website and is available under the creative commons license.\n",
      "inspiration\n",
      "i've always been fascinated by ted talks and the immense diversity of content that it provides for free. i was also thoroughly inspired by a ted talk that visually explored ted talks stats and i was motivated to do the same thing, albeit on a much less grander scale.\n",
      "some of the questions that can be answered with this dataset: 1. how is each ted talk related to every other ted talk? 2. which are the most viewed and most favorited talks of all time? are they mostly the same? what does this tell us? 3. what kind of topics attract the maximum discussion and debate (in the form of comments)? 4. which months are most popular among ted and tedx chapters? 5. which themes are most popular amongst tedsters?\n",
      "the health insurance marketplace public use files contain data on health and dental plans offered to individuals and small businesses through the us health insurance marketplace.\n",
      "exploration ideas\n",
      "to help get you started, here are some data exploration ideas:\n",
      "how do plan rates and benefits vary across states?\n",
      "how do plan benefits relate to plan rates?\n",
      "how do plan rates vary by age?\n",
      "how do plans vary across insurance network providers?\n",
      "see this forum thread for more ideas, and post there if you want to add your own ideas or answer some of the open questions!\n",
      "data description\n",
      "this data was originally prepared and released by the centers for medicare & medicaid services (cms). please read the cms disclaimer-user agreement before using this data.\n",
      "here, we've processed the data to facilitate analytics. this processed version has three components:\n",
      "1. original versions of the data\n",
      "the original versions of the 2014, 2015, 2016 data are available in the \"raw\" directory of the download and \"../input/raw\" on kaggle scripts. search for \"dictionaries\" on this page to find the data dictionaries describing the individual raw files.\n",
      "2. combined csv files that contain\n",
      "in the top level directory of the download (\"../input\" on kaggle scripts), there are six csv files that contain the combined at across all years:\n",
      "benefitscostsharing.csv\n",
      "businessrules.csv\n",
      "network.csv\n",
      "planattributes.csv\n",
      "rate.csv\n",
      "servicearea.csv\n",
      "additionally, there are two csv files that facilitate joining data across years:\n",
      "crosswalk2015.csv - joining 2014 and 2015 data\n",
      "crosswalk2016.csv - joining 2015 and 2016 data\n",
      "3. sqlite database\n",
      "the \"database.sqlite\" file contains tables corresponding to each of the processed csv files.\n",
      "the code to create the processed version of this data is available on github.\n",
      "context\n",
      "the myers briggs type indicator (or mbti for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n",
      "introversion (i) – extroversion (e)\n",
      "intuition (n) – sensing (s)\n",
      "thinking (t) – feeling (f)\n",
      "judging (j) – perceiving (p)\n",
      "(more can be learned about what these mean here)\n",
      "so for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an intp in the mbti system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.\n",
      "it is one of, if not the, the most popular personality test in the world. it is used in businesses, online, for fun, for research and lots more. a simple google search reveals all of the different ways the test has been used over time. it’s safe to say that this test is still very relevant in the world in terms of its use.\n",
      "from scientific or psychological perspective it is based on the work done on cognitive functions by carl jung i.e. jungian typology. this was a model of 8 distinct functions, thought processes or ways of thinking that were suggested to be present in the mind. later this work was transformed into several different personality systems to make it more accessible, the most popular of which is of course the mbti.\n",
      "recently, its use/validity has come into question because of unreliability in experiments surrounding it, among other reasons. but it is still clung to as being a very useful tool in a lot of areas, and the purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing, which overall explores the validity of the test in analysing, predicting or categorising behaviour.\n",
      "content\n",
      "this dataset contains over 8600 rows of data, on each row is a person’s:\n",
      "type (this persons 4 letter mbti code/type)\n",
      "a section of each of the last 50 things they have posted (each entry separated by \"|||\" (3 pipe characters))\n",
      "acknowledgements\n",
      "this data was collected through the personalitycafe forum, as it provides a large selection of people and their mbti personality type, as well as what they have written.\n",
      "inspiration\n",
      "some basic uses could include:\n",
      "use machine learning to evaluate the mbtis validity and ability to predict language styles and behaviour online.\n",
      "production of a machine learning algorithm that can attempt to determine a person’s personality type based on some text they have written.\n",
      "of all the universities in the world, which are the best?\n",
      "ranking universities is a difficult, political, and controversial practice. there are hundreds of different national and international university ranking systems, many of which disagree with each other. this dataset contains three global university rankings from very different places.\n",
      "university ranking data\n",
      "the times higher education world university ranking is widely regarded as one of the most influential and widely observed university measures. founded in the united kingdom in 2010, it has been criticized for its commercialization and for undermining non-english-instructing institutions.\n",
      "the academic ranking of world universities, also known as the shanghai ranking, is an equally influential ranking. it was founded in china in 2003 and has been criticized for focusing on raw research power and for undermining humanities and quality of instruction.\n",
      "the center for world university rankings, is a less well know listing that comes from saudi arabia, it was founded in 2012.\n",
      "how do these rankings compare to each other?\n",
      "are the various criticisms levied against these rankings fair or not?\n",
      "how does your alma mater fare against the world?\n",
      "supplementary data\n",
      "to further extend your analyses, we've also included two sets of supplementary data.\n",
      "the first of these is a set of data on educational attainment around the world. it comes from the world data bank and comprises information from the unesco institute for statistics and the barro-lee dataset. how does national educational attainment relate to the quality of each nation's universities?\n",
      "the second supplementary dataset contains information about public and private direct expenditure on education across nations. this data comes from the national center for education statistics. it represents expenditure as a percentage of gross domestic product. does spending more on education lead to better international university rankings?\n",
      "dataset information\n",
      "this dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace. you are also encouraged to analyze data from the ongoing 2016 survey found here.\n",
      "content\n",
      "this dataset contains the following data:\n",
      "timestamp\n",
      "age\n",
      "gender\n",
      "country\n",
      "state: if you live in the united states, which state or territory do you live in?\n",
      "self_employed: are you self-employed?\n",
      "family_history: do you have a family history of mental illness?\n",
      "treatment: have you sought treatment for a mental health condition?\n",
      "work_interfere: if you have a mental health condition, do you feel that it interferes with your work?\n",
      "no_employees: how many employees does your company or organization have?\n",
      "remote_work: do you work remotely (outside of an office) at least 50% of the time?\n",
      "tech_company: is your employer primarily a tech company/organization?\n",
      "benefits: does your employer provide mental health benefits?\n",
      "care_options: do you know the options for mental health care your employer provides?\n",
      "wellness_program: has your employer ever discussed mental health as part of an employee wellness program?\n",
      "seek_help: does your employer provide resources to learn more about mental health issues and how to seek help?\n",
      "anonymity: is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?\n",
      "leave: how easy is it for you to take medical leave for a mental health condition?\n",
      "mental_health_consequence: do you think that discussing a mental health issue with your employer would have negative consequences?\n",
      "phys_health_consequence: do you think that discussing a physical health issue with your employer would have negative consequences?\n",
      "coworkers: would you be willing to discuss a mental health issue with your coworkers?\n",
      "supervisor: would you be willing to discuss a mental health issue with your direct supervisor(s)?\n",
      "mental_health_interview: would you bring up a mental health issue with a potential employer in an interview?\n",
      "phys_health_interview: would you bring up a physical health issue with a potential employer in an interview?\n",
      "mental_vs_physical: do you feel that your employer takes mental health as seriously as physical health?\n",
      "obs_consequence: have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace?\n",
      "comments: any additional notes or comments\n",
      "inspiration\n",
      "some questions worth exploring:\n",
      "how does the frequency of mental health illness and attitudes towards mental health vary by geographic location?\n",
      "what are the strongest predictors of mental health illness or certain attitudes towards mental health in the workplace?\n",
      "acknowledgements\n",
      "the original dataset is from open sourcing mental illness and can be downloaded here.\n",
      "context\n",
      "high-quality financial data is expensive to acquire and is therefore rarely shared for free. here i provide the full historical daily price and volume data for all u.s.-based stocks and etfs trading on the nyse, nasdaq, and nyse mkt. it's one of the best datasets of its kind you can obtain.\n",
      "content\n",
      "the data (last updated 11/10/2017) is presented in csv format as follows: date, open, high, low, close, volume, openint. note that prices have been adjusted for dividends and splits.\n",
      "acknowledgements\n",
      "this dataset belongs to me. i’m sharing it here for free. you may do with it as you wish.\n",
      "inspiration\n",
      "many have tried, but most have failed, to predict the stock market's ups and downs. can you do any better?\n",
      "context\n",
      "a person makes a doctor appointment, receives all the instructions and no-show. who to blame? if this is help, don´t forget to upvote :) greatings!\n",
      "content\n",
      "300k medical appointments and its 15 variables (characteristics) of each. the most important one if the patient show-up or no-show the appointment. variable names are self-explanatory, if you have doubts, just let me know!\n",
      "scholarship variable means this concept = https://en.wikipedia.org/wiki/bolsa_fam%c3%adlia\n",
      "data dictionary\n",
      "patientid - identification of a patient appointmentid - identification of each appointment gender = male or female . female is the greater proportion, woman takes way more care of they health in comparison to man. datamarcacaoconsulta = the day of the actuall appointment, when they have to visit the doctor. dataagendamento = the day someone called or registered the appointment, this is before appointment of course. age = how old is the patient. neighbourhood = where the appointment takes place. scholarship = ture of false . observation, this is a broad topic, consider reading this article https://en.wikipedia.org/wiki/bolsa_fam%c3%adlia hipertension = true or false diabetes = true or false alcoholism = true or false handcap = true or false sms_received = 1 or more messages sent to the patient. no-show = true or false.\n",
      "inspiration\n",
      "what if that possible to predict someone to no-show an appointment?\n",
      "this data originally came from crowdflower's data for everyone library.\n",
      "as the original source says,\n",
      "a sentiment analysis job about the problems of each major u.s. airline. twitter data was scraped from february of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
      "the data we're providing on kaggle is a slightly reformatted version of the original source. it includes both a csv file and sqlite database. the code that does these transformations is available on github\n",
      "for example, it contains whether the sentiment of the tweets in this set was positive, neutral, or negative for six us airlines:\n",
      "voice gender\n",
      "gender recognition by voice and speech analysis\n",
      "this database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. the dataset consists of 3,168 recorded voice samples, collected from male and female speakers. the voice samples are pre-processed by acoustic analysis in r using the seewave and tuner packages, with an analyzed frequency range of 0hz-280hz (human vocal range).\n",
      "the dataset\n",
      "the following acoustic properties of each voice are measured and included within the csv:\n",
      "meanfreq: mean frequency (in khz)\n",
      "sd: standard deviation of frequency\n",
      "median: median frequency (in khz)\n",
      "q25: first quantile (in khz)\n",
      "q75: third quantile (in khz)\n",
      "iqr: interquantile range (in khz)\n",
      "skew: skewness (see note in specprop description)\n",
      "kurt: kurtosis (see note in specprop description)\n",
      "sp.ent: spectral entropy\n",
      "sfm: spectral flatness\n",
      "mode: mode frequency\n",
      "centroid: frequency centroid (see specprop)\n",
      "peakf: peak frequency (frequency with highest energy)\n",
      "meanfun: average of fundamental frequency measured across acoustic signal\n",
      "minfun: minimum fundamental frequency measured across acoustic signal\n",
      "maxfun: maximum fundamental frequency measured across acoustic signal\n",
      "meandom: average of dominant frequency measured across acoustic signal\n",
      "mindom: minimum of dominant frequency measured across acoustic signal\n",
      "maxdom: maximum of dominant frequency measured across acoustic signal\n",
      "dfrange: range of dominant frequency measured across acoustic signal\n",
      "modindx: modulation index. calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
      "label: male or female\n",
      "accuracy\n",
      "baseline (always predict male)\n",
      "50% / 50%\n",
      "logistic regression\n",
      "97% / 98%\n",
      "cart\n",
      "96% / 97%\n",
      "random forest\n",
      "100% / 98%\n",
      "svm\n",
      "100% / 99%\n",
      "xgboost\n",
      "100% / 99%\n",
      "research questions\n",
      "an original analysis of the data-set can be found in the following article:\n",
      "identifying the gender of a voice using machine learning\n",
      "the best model achieves 99% accuracy on the test set. according to a cart model, it appears that looking at the mean fundamental frequency might be enough to accurately classify a voice. however, some male voices use a higher frequency, even though their resonance differs from female voices, and may be incorrectly classified as female. to the human ear, there is apparently more than simple frequency, that determines a voice's gender.\n",
      "questions\n",
      "what other features differ between male and female voices?\n",
      "can we find a difference in resonance between male and female voices?\n",
      "can we identify falsetto from regular voices? (separate data-set likely needed for this)\n",
      "are there other interesting features in the data?\n",
      "cart diagram\n",
      "mean fundamental frequency appears to be an indicator of voice gender, with a threshold of 140hz separating male from female classifications.\n",
      "references\n",
      "the harvard-haskins database of regularly-timed speech\n",
      "telecommunications & signal processing laboratory (tsp) speech database at mcgill university, home\n",
      "voxforge speech corpus, home\n",
      "festvox cmu_arctic speech database at carnegie mellon university\n",
      "cryptocurrency market data\n",
      "historical cryptocurrency prices for all tokens!\n",
      "summary\n",
      "> observations: 649,051\n",
      "> variables: 13  \n",
      "> crypto tokens: 1,382  \n",
      "> start date: 28/04/2017  \n",
      "> end date: 03/01/2018  \n",
      "description\n",
      "all historic open, high, low, close, trading volume and market cap info for all cryptocurrencies.\n",
      "i've had to go over the code with a fine tooth comb to get it compatible with cran so there have been significant enhancements to how some of the field conversions have been undertaken and the data being cleaned. this should eliminate a few issues around number formatting or unexpected handling of scientific notations.\n",
      "data structure\n",
      "observations: 649,051    \n",
      "variables: 13    \n",
      "$ slug        <chr> \"bitcoin\", \"bitcoin\", \"bitcoin\", \"bitcoin\"...        \n",
      "$ symbol      <chr> \"btc\", \"btc\", \"btc\", \"btc\", \"btc\", \"btc\", ...    \n",
      "$ name        <chr> \"bitcoin\", \"bitcoin\", \"bitcoin\", \"bitcoin\"...    \n",
      "$ date        <date> 2013-04-28, 2013-04-29, 2013-04-30, 2013-...    \n",
      "$ ranknow     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...    \n",
      "$ open        <dbl> 135.30, 134.44, 144.00, 139.00, 116.38, 10...    \n",
      "$ high        <dbl> 135.98, 147.49, 146.93, 139.89, 125.60, 10...    \n",
      "$ low         <dbl> 132.10, 134.00, 134.05, 107.72, 92.28, 79...    \n",
      "$ close       <dbl> 134.21, 144.54, 139.00, 116.99, 105.21, 97...    \n",
      "$ volume      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    \n",
      "$ market      <dbl> 1500520000, 1491160000, 1597780000, 154282...    \n",
      "$ close_ratio <dbl> 0.5438, 0.7813, 0.3843, 0.2882, 0.3881, 0...    \n",
      "$ spread      <dbl> 3.88, 13.49, 12.88, 32.17, 33.32, 29.03, 2...    \n",
      "built with :heart: r\n",
      "i've ported my original kernel to generate this data, across into a r package which is awaiting being published on cran. run the below to go scrape all the historical tables of all the different cryptocurrencies listed on coinmarketcap and turn it into a data frame.\n",
      "you can install it via the github link below, or:\n",
      "devtools::install_github(\"jessevent/crypto\")  \n",
      "library(crypto)  \n",
      "will_i_get_rich <- getcoins()  \n",
      "authors\n",
      "jesse vent - package author - jessevent\n",
      "acknowledgments\n",
      "github - view my github repository for the full package.\n",
      "coinspot - invest $aud into crypto today!\n",
      "coinmarketcap - providing amazing data @coinmarketcap\n",
      "if this helps you become rich please consider making a donation!\n",
      "    erc-20: 0x375923bf82f0b728d23a5704261a6e16341fd860\n",
      "    xrp: rk59semlsujzewftxbfhwune6uhznjz2bk\n",
      "    ltc: lwpizmd2ceyqcdrzrs9tjsoutlwbffxwcj\n",
      "context:\n",
      "the data were obtained in a survey of students math and portuguese language courses in secondary school. it contains a lot of interesting social, gender and study information about students. you can use it for some eda or try to predict students final grade.\n",
      "content:\n",
      "attributes for both student-mat.csv (math course) and student-por.csv (portuguese language course) datasets:\n",
      "school - student's school (binary: 'gp' - gabriel pereira or 'ms' - mousinho da silveira)\n",
      "sex - student's sex (binary: 'f' - female or 'm' - male)\n",
      "age - student's age (numeric: from 15 to 22)\n",
      "address - student's home address type (binary: 'u' - urban or 'r' - rural)\n",
      "famsize - family size (binary: 'le3' - less or equal to 3 or 'gt3' - greater than 3)\n",
      "pstatus - parent's cohabitation status (binary: 't' - living together or 'a' - apart)\n",
      "medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
      "fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
      "mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
      "fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
      "reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
      "guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n",
      "traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
      "studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
      "failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
      "schoolsup - extra educational support (binary: yes or no)\n",
      "famsup - family educational support (binary: yes or no)\n",
      "paid - extra paid classes within the course subject (math or portuguese) (binary: yes or no)\n",
      "activities - extra-curricular activities (binary: yes or no)\n",
      "nursery - attended nursery school (binary: yes or no)\n",
      "higher - wants to take higher education (binary: yes or no)\n",
      "internet - internet access at home (binary: yes or no)\n",
      "romantic - with a romantic relationship (binary: yes or no)\n",
      "famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
      "freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
      "goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
      "dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
      "walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
      "health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
      "absences - number of school absences (numeric: from 0 to 93)\n",
      "these grades are related with the course subject, math or portuguese:\n",
      "g1 - first period grade (numeric: from 0 to 20)\n",
      "g2 - second period grade (numeric: from 0 to 20)\n",
      "g3 - final grade (numeric: from 0 to 20, output target)\n",
      "additional note: there are several (382) students that belong to both datasets . these students can be identified by searching for identical attributes that characterize each student, as shown in the annexed r file.\n",
      "source information\n",
      "p. cortez and a. silva. using data mining to predict secondary school student performance. in a. brito and j. teixeira eds., proceedings of 5th future business technology conference (fubutec 2008) pp. 5-12, porto, portugal, april, 2008, eurosis, isbn 978-9077381-39-7.\n",
      "fabio pagnotta, hossain mohammad amran. email:fabio.pagnotta@studenti.unicam.it, mohammadamra.hossain '@' studenti.unicam.it university of camerino\n",
      "https://archive.ics.uci.edu/ml/datasets/student+alcohol+consumption\n",
      "context\n",
      "h-1b visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the united states. for a foreign national to apply for h1-b visa, a us employer must offer them a job and submit a petition for a h-1b visa to the us immigration department. this is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.\n",
      "the following articles contain more information about the h-1b visa process:\n",
      "what is h1b lca ? why file it ? salary, processing times – dol\n",
      "h1b application process: step by step guide\n",
      "content\n",
      "this dataset contains five year's worth of h-1b petition data, with approximately 3 million records overall. the columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.\n",
      "for more information on individual columns, refer to the column metadata. a detailed description of the underlying raw dataset is available in an official data dictionary.\n",
      "acknowledgements\n",
      "the office of foreign labor certification (oflc) generates program data, including data about h1-b visas. the disclosure data updated annually and is available online.\n",
      "the raw data available is messy and not immediately suitable analysis. a set of data transformations were performed making the data more accessible for quick exploration. to learn more, refer to this blog post and to the complimentary r notebook.\n",
      "inspiration\n",
      "is the number of petitions with data engineer job title increasing over time?\n",
      "which part of the us has the most hardware engineer jobs?\n",
      "which industry has the most number of data scientist positions?\n",
      "which employers file the most petitions each year?\n",
      "introduction\n",
      "in 2013, students of the statistics class at fsev uk were asked to invite their friends to participate in this survey.\n",
      "the data file (responses.csv) consists of 1010 rows and 150 columns (139 integer and 11 categorical).\n",
      "for convenience, the original variable names were shortened in the data file. see the columns.csv file if you want to match the data with the original names.\n",
      "the data contain missing values.\n",
      "the survey was presented to participants in both electronic and written form.\n",
      "the original questionnaire was in slovak language and was later translated into english.\n",
      "all participants were of slovakian nationality, aged between 15-30.\n",
      "the variables can be split into the following groups:\n",
      "music preferences (19 items)\n",
      "movie preferences (12 items)\n",
      "hobbies & interests (32 items)\n",
      "phobias (10 items)\n",
      "health habits (3 items)\n",
      "personality traits, views on life, & opinions (57 items)\n",
      "spending habits (7 items)\n",
      "demographics (10 items)\n",
      "research questions\n",
      "many different techniques can be used to answer many questions, e.g.\n",
      "clustering: given the music preferences, do people make up any clusters of similar behavior?\n",
      "hypothesis testing: do women fear certain phenomena significantly more than men? do the left handed people have different interests than right handed?\n",
      "predictive modeling: can we predict spending habits of a person from his/her interests and movie or music preferences?\n",
      "dimension reduction: can we describe a large number of human interests by a smaller number of latent concepts?\n",
      "correlation analysis: are there any connections between music and movie preferences?\n",
      "visualization: how to effectively visualize a lot of variables in order to gain some meaningful insights from the data?\n",
      "(multivariate) outlier detection: small number of participants often cheats and randomly answers the questions. can you identify them? hint: local outlier factor may help.\n",
      "missing values analysis: are there any patterns in missing responses? what is the optimal way of imputing the values in surveys?\n",
      "recommendations: if some of user's interests are known, can we predict the other? or, if we know what a person listen, can we predict which kind of movies he/she might like?\n",
      "past research\n",
      "(in slovak) sleziak, p. - sabo, m.: gender differences in the prevalence of specific phobias. forum statisticum slovacum. 2014, vol. 10, no. 6. [differences (gender + whether people lived in village/town) in the prevalence of phobias.]\n",
      "sabo, miroslav. multivariate statistical methods with applications. diss. slovak university of technology in bratislava, 2014. [clustering of variables (music preferences, movie preferences, phobias) + clustering of people w.r.t. their interests.]\n",
      "questionnaire\n",
      "music preferences\n",
      "i enjoy listening to music.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i prefer.: slow paced music 1-2-3-4-5 fast paced music (integer)\n",
      "dance, disco, funk: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "folk music: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "country: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "classical: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "musicals: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "pop: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "rock: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "metal, hard rock: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "punk: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "hip hop, rap: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "reggae, ska: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "swing, jazz: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "rock n roll: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "alternative music: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "latin: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "techno, trance: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "opera: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "movie preferences\n",
      "i really enjoy watching movies.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "horror movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "thriller movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "comedies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "romantic movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "sci-fi movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "war movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "tales: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "cartoons: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "documentaries: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "western movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "action movies: don't enjoy at all 1-2-3-4-5 enjoy very much (integer)\n",
      "hobbies & interests\n",
      "history: not interested 1-2-3-4-5 very interested (integer)\n",
      "psychology: not interested 1-2-3-4-5 very interested (integer)\n",
      "politics: not interested 1-2-3-4-5 very interested (integer)\n",
      "mathematics: not interested 1-2-3-4-5 very interested (integer)\n",
      "physics: not interested 1-2-3-4-5 very interested (integer)\n",
      "internet: not interested 1-2-3-4-5 very interested (integer)\n",
      "pc software, hardware: not interested 1-2-3-4-5 very interested (integer)\n",
      "economy, management: not interested 1-2-3-4-5 very interested (integer)\n",
      "biology: not interested 1-2-3-4-5 very interested (integer)\n",
      "chemistry: not interested 1-2-3-4-5 very interested (integer)\n",
      "poetry reading: not interested 1-2-3-4-5 very interested (integer)\n",
      "geography: not interested 1-2-3-4-5 very interested (integer)\n",
      "foreign languages: not interested 1-2-3-4-5 very interested (integer)\n",
      "medicine: not interested 1-2-3-4-5 very interested (integer)\n",
      "law: not interested 1-2-3-4-5 very interested (integer)\n",
      "cars: not interested 1-2-3-4-5 very interested (integer)\n",
      "art: not interested 1-2-3-4-5 very interested (integer)\n",
      "religion: not interested 1-2-3-4-5 very interested (integer)\n",
      "outdoor activities: not interested 1-2-3-4-5 very interested (integer)\n",
      "dancing: not interested 1-2-3-4-5 very interested (integer)\n",
      "playing musical instruments: not interested 1-2-3-4-5 very interested (integer)\n",
      "poetry writing: not interested 1-2-3-4-5 very interested (integer)\n",
      "sport and leisure activities: not interested 1-2-3-4-5 very interested (integer)\n",
      "sport at competitive level: not interested 1-2-3-4-5 very interested (integer)\n",
      "gardening: not interested 1-2-3-4-5 very interested (integer)\n",
      "celebrity lifestyle: not interested 1-2-3-4-5 very interested (integer)\n",
      "shopping: not interested 1-2-3-4-5 very interested (integer)\n",
      "science and technology: not interested 1-2-3-4-5 very interested (integer)\n",
      "theatre: not interested 1-2-3-4-5 very interested (integer)\n",
      "socializing: not interested 1-2-3-4-5 very interested (integer)\n",
      "adrenaline sports: not interested 1-2-3-4-5 very interested (integer)\n",
      "pets: not interested 1-2-3-4-5 very interested (integer)\n",
      "phobias\n",
      "flying: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "thunder, lightning: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "darkness: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "heights: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "spiders: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "snakes: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "rats, mice: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "ageing: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "dangerous dogs: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "public speaking: not afraid at all 1-2-3-4-5 very afraid of (integer)\n",
      "health habits\n",
      "smoking habits: never smoked - tried smoking - former smoker - current smoker (categorical)\n",
      "drinking: never - social drinker - drink a lot (categorical)\n",
      "i live a very healthy lifestyle.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "personality traits, views on life & opinions\n",
      "i take notice of what goes on around me.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i try to do tasks as soon as possible and not leave them until last minute.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always make a list so i don't forget anything.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i often study or work even in my spare time.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i look at things from all different angles before i go ahead.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i believe that bad people will suffer one day and good people will be rewarded.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am reliable at work and always complete all tasks given to me.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always keep my promises.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i can fall for someone very quickly and then completely lose interest.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i would rather have lots of friends than lots of money.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always try to be the funniest one.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i can be two faced sometimes.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i damaged things in the past when angry.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i take my time to make decisions.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always try to vote in elections.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i often think about and regret the decisions i make.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i can tell if people listen to me or not when i talk to them.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am a hypochondriac.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am emphatetic person.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i eat because i have to. i don't enjoy food and eat as fast as i can.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i try to give as much as i can to other people at christmas.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i don't like seeing animals suffering.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i look after things i have borrowed from others.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i feel lonely in life.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i used to cheat at school.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i worry about my health.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i wish i could change the past because of the things i have done.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i believe in god.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always have good dreams.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always give to charity.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i have lots of friends.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "timekeeping.: i am often early. - i am always on time. - i am often running late. (categorical)\n",
      "do you lie to others?: never. - only to avoid hurting someone. - sometimes. - everytime it suits me. (categorical)\n",
      "i am very patient.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i can quickly adapt to a new environment.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "my moods change quickly.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am well mannered and i look after my appearance.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i enjoy meeting new people.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always let other people know about my achievements.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i think carefully before answering any important letters.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i enjoy childrens' company.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am not afraid to give my opinion if i feel strongly about something.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i can get angry very easily.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always make sure i connect with the right people.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i have to be well prepared before public speaking.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i will find a fault in myself if people don't like me.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i cry when i feel down or things don't go the right way.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am 100% happy with my life.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i am always full of life and energy.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i prefer big dangerous dogs to smaller, calmer dogs.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i believe all my personality traits are positive.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "if i find something the doesn't belong to me i will hand it in.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i find it very difficult to get up in the morning.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i have many different hobbies and interests.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i always listen to my parents' advice.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i enjoy taking part in surveys.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "how much time do you spend online?: no time at all - less than an hour a day - few hours a day - most of the day (categorical)\n",
      "spending habits\n",
      "i save all the money i can.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i enjoy going to large shopping centres.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i prefer branded clothing to non branded.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i spend a lot of money on partying and socializing.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i spend a lot of money on my appearance.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i spend a lot of money on gadgets.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "i will hapilly pay more money for good, quality or healthy food.: strongly disagree 1-2-3-4-5 strongly agree (integer)\n",
      "demographics\n",
      "age: (integer)\n",
      "height: (integer)\n",
      "weight: (integer)\n",
      "how many siblings do you have?: (integer)\n",
      "gender: female - male (categorical)\n",
      "i am: left handed - right handed (categorical)\n",
      "highest education achieved: currently a primary school pupil - primary school - secondary school - college/bachelor degree (categorical)\n",
      "i am the only child: no - yes (categorical)\n",
      "i spent most of my childhood in a: city - village (categorical)\n",
      "i lived most of my childhood in a: house/bungalow - block of flats (categorical)\n",
      "context\n",
      "youtube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. according to variety magazine, “to determine the year’s top-trending videos, youtube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). note that they’re not the most-viewed videos overall for the calendar year”. top performers on the youtube trending list are music videos (such as the famously virile “gangam style”), celebrity and/or reality tv performances, and the random dude-with-a-camera viral videos that youtube is well-known for.\n",
      "this dataset is a daily record of the top trending youtube videos.\n",
      "note that this dataset is a structurally improved version of this dataset.\n",
      "content\n",
      "this dataset includes several months (and counting) of data on daily trending youtube videos. data is included for the us, gb, de, ca, and fr regions (usa, great britain, germany, canada, and france, respectively), with up to 200 listed trending videos per day.\n",
      "each region’s data is in a separate file. data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.\n",
      "the data also includes a category_id field, which varies between regions. to retrieve the categories for a specific video, find it in the associated json. one such file is included for each of the five regions in the dataset.\n",
      "for more information on specific columns in the dataset refer to the column metadata.\n",
      "acknowledgements\n",
      "this dataset was collected using the youtube api.\n",
      "inspiration\n",
      "possible uses for this dataset could include:\n",
      "sentiment analysis in a variety of forms\n",
      "categorising youtube videos based on their comments and statistics.\n",
      "training ml algorithms like rnns to generate their own youtube comments.\n",
      "analysing what factors affect how popular a youtube video will be.\n",
      "statistical analysis over time .\n",
      "for further inspiration, see the kernels on this dataset!\n",
      "this dataset contains a list of video games with sales greater than 100,000 copies. it was generated by a scrape of vgchartz.com.\n",
      "fields include\n",
      "rank - ranking of overall sales\n",
      "name - the games name\n",
      "platform - platform of the games release (i.e. pc,ps4, etc.)\n",
      "year - year of the game's release\n",
      "genre - genre of the game\n",
      "publisher - publisher of the game\n",
      "na_sales - sales in north america (in millions)\n",
      "eu_sales - sales in europe (in millions)\n",
      "jp_sales - sales in japan (in millions)\n",
      "other_sales - sales in the rest of the world (in millions)\n",
      "global_sales - total worldwide sales.\n",
      "the script to scrape the data is available at https://github.com/gregorut/vgchartzscrape. it is based on beautifulsoup using python. there are 16,598 records. 2 records were dropped due to incomplete information.\n",
      "this dataset is the result of a crawl i did on http://ign.com/games/reviews .\n",
      "it contains 18625 lines with the fields like the release date, it's platform and ign's score. all the lines are fully filled.\n",
      "in 20 years, the gaming industry has grown and sophisticated. by exploring this dataset, one is able to find trends about the industry, compare consoles against eachother, search through the most popular genres and more.\n",
      "the dataset can also be a great place for beginners to start using python modules such as pandas and seaborn.\n",
      "you can find the crawl i used for the retrieval here\n",
      "throughout 2015, hillary clinton has been embroiled in controversy over the use of personal email accounts on non-government servers during her time as the united states secretary of state. some political experts and opponents maintain that clinton's use of personal email accounts to conduct secretary of state affairs is in violation of protocols and federal laws that ensure appropriate recordkeeping of government activity. hillary's campaign has provided their own four sentence summary of her email use here.\n",
      "there have been a number of freedom of information lawsuits filed over the state department's failure to fully release the emails sent and received on clinton's private accounts. on monday, august 31, the state department released nearly 7,000 pages of clinton's heavily redacted emails (its biggest release of emails to date).\n",
      "the documents were released by the state department as pdfs. we've cleaned and normalized the released documents and are hosting them for public analysis. kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.\n",
      "here's the code that creates this data release.\n",
      "uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. this is a fictional data set created by ibm data scientists.\n",
      "education 1 'below college' 2 'college' 3 'bachelor' 4 'master' 5 'doctor'\n",
      "environmentsatisfaction 1 'low' 2 'medium' 3 'high' 4 'very high'\n",
      "jobinvolvement\n",
      "1 'low' 2 'medium' 3 'high' 4 'very high'\n",
      "jobsatisfaction 1 'low' 2 'medium' 3 'high' 4 'very high'\n",
      "performancerating\n",
      "1 'low' 2 'good' 3 'excellent' 4 'outstanding'\n",
      "relationshipsatisfaction\n",
      "1 'low' 2 'medium' 3 'high' 4 'very high'\n",
      "worklifebalance 1 'bad' 2 'good' 3 'better' 4 'best'\n",
      "context\n",
      "chocolate is one of the most popular candies in the world. each year, residents of the united states collectively eat more than 2.8 billions pounds. however, not all chocolate bars are created equal! this dataset contains expert ratings of over 1,700 individual chocolate bars, along with information on their regional origin, percentage of cocoa, the variety of chocolate bean used and where the beans were grown.\n",
      "flavors of cacao rating system:\n",
      "5= elite (transcending beyond the ordinary limits)\n",
      "4= premium (superior flavor development, character and style)\n",
      "3= satisfactory(3.0) to praiseworthy(3.75) (well made with special qualities)\n",
      "2= disappointing (passable but contains at least one significant flaw)\n",
      "1= unpleasant (mostly unpalatable)\n",
      "each chocolate is evaluated from a combination of both objective qualities and subjective interpretation. a rating here only represents an experience with one bar from one batch. batch numbers, vintages and review dates are included in the database when known.\n",
      "the database is narrowly focused on plain dark chocolate with an aim of appreciating the flavors of the cacao when made into chocolate. the ratings do not reflect health benefits, social missions, or organic status.\n",
      "flavor is the most important component of the flavors of cacao ratings. diversity, balance, intensity and purity of flavors are all considered. it is possible for a straight forward single note chocolate to rate as high as a complex flavor profile that changes throughout. genetics, terroir, post harvest techniques, processing and storage can all be discussed when considering the flavor component.\n",
      "texture has a great impact on the overall experience and it is also possible for texture related issues to impact flavor. it is a good way to evaluate the makers vision, attention to detail and level of proficiency.\n",
      "aftermelt is the experience after the chocolate has melted. higher quality chocolate will linger and be long lasting and enjoyable. since the aftermelt is the last impression you get from the chocolate, it receives equal importance in the overall rating.\n",
      "overall opinion is really where the ratings reflect a subjective opinion. ideally it is my evaluation of whether or not the components above worked together and an opinion on the flavor development, character and style. it is also here where each chocolate can usually be summarized by the most prominent impressions that you would remember about each chocolate.\n",
      "acknowledgements\n",
      "these ratings were compiled by brady brelinski, founding member of the manhattan chocolate society. for up-to-date information, as well as additional content (including interviews with craft chocolate makers), please see his website: flavors of cacao\n",
      "inspiration\n",
      "where are the best cocoa beans grown?\n",
      "which countries produce the highest-rated bars?\n",
      "what’s the relationship between cocoa solids percentage and rating?\n",
      "uber tlc foil response\n",
      "this directory contains data on over 4.5 million uber pickups in new york city from april to september 2014, and 14.3 million more uber pickups from january to june 2015. trip-level data on 10 other for-hire vehicle (fhv) companies, as well as aggregated data for 329 fhv companies, is also included. all the files are as they were received on august 3, sept. 15 and sept. 22, 2015.\n",
      "fivethirtyeight obtained the data from the nyc taxi & limousine commission (tlc) by submitting a freedom of information law request on july 20, 2015. the tlc has sent us the data in batches as it continues to review trip data uber and other hfv companies have submitted to it. the tlc's correspondence with fivethirtyeight is included in the files tlc_letter.pdf, tlc_letter2.pdf and tlc_letter3.pdf. tlc records requests can be made here.\n",
      "this data was used for four fivethirtyeight stories: uber is serving new york’s outer boroughs more than taxis are, public transit should be uber’s new best friend, uber is taking millions of manhattan rides away from taxis, and is uber making nyc rush-hour traffic worse?.\n",
      "the data\n",
      "the dataset contains, roughly, four groups of files:\n",
      "uber trip data from 2014 (april - september), separated by month, with detailed location information\n",
      "uber trip data from 2015 (january - june), with less fine-grained location information\n",
      "non-uber fhv (for-hire vehicle) trips. the trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.\n",
      "aggregate ride and vehicle statistics for all fhv companies (and, occasionally, for taxi companies)\n",
      "uber trip data from 2014\n",
      "there are six files of raw data on uber pickups in new york city from april to september 2014. the files are separated by month and each has the following columns:\n",
      "date/time : the date and time of the uber pickup\n",
      "lat : the latitude of the uber pickup\n",
      "lon : the longitude of the uber pickup\n",
      "base : the tlc base company code affiliated with the uber pickup\n",
      "these files are named:\n",
      "uber-raw-data-apr14.csv\n",
      "uber-raw-data-aug14.csv\n",
      "uber-raw-data-jul14.csv\n",
      "uber-raw-data-jun14.csv\n",
      "uber-raw-data-may14.csv\n",
      "uber-raw-data-sep14.csv\n",
      "uber trip data from 2015\n",
      "also included is the file uber-raw-data-janjune-15.csv this file has the following columns:\n",
      "dispatching_base_num : the tlc base company code of the base that dispatched the uber\n",
      "pickup_date : the date and time of the uber pickup\n",
      "affiliated_base_num : the tlc base company code affiliated with the uber pickup\n",
      "locationid : the pickup location id affiliated with the uber pickup\n",
      "the base codes are for the following uber bases:\n",
      "b02512 : unter b02598 : hinter b02617 : weiter b02682 : schmecken b02764 : danach-ny b02765 : grun b02835 : dreist b02836 : drinnen\n",
      "for coarse-grained location information from these pickups, the file taxi-zone-lookup.csv shows the taxi zone (essentially, neighborhood) and borough for each locationid.\n",
      "non-uber flv trips\n",
      "the dataset also contains 10 files of raw data on pickups from 10 for-hire vehicle (fhv) companies. the trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.\n",
      "these files are named:\n",
      "american_b01362.csv\n",
      "diplo_b01196.csv\n",
      "highclass_b01717.csv\n",
      "skyline_b00111.csv\n",
      "carmel_b00256.csv\n",
      "federal_02216.csv\n",
      "lyft_b02510.csv\n",
      "dial7_b00887.csv\n",
      "firstclass_b01536.csv\n",
      "prestige_b01338.csv\n",
      "aggregate statistics\n",
      "there is also a file other-fhv-data-jan-aug-2015.csv containing daily pickup data for 329 fhv companies from january 2015 through august 2015.\n",
      "the file uber-jan-feb-foil.csv contains aggregated daily uber trip statistics in january and february 2015.\n",
      "the latest hot topic in the news is fake news and many are wondering what data scientists can do to detect it and stymie its viral spread. this dataset is only a first step in understanding and tackling this problem. it contains text and metadata scraped from 244 websites tagged as \"bullshit\" by the bs detector chrome extension by daniel sieradski.\n",
      "warning: i did not modify the list of news sources from the bs detector so as not to introduce my (useless) layer of bias; i'm not an authority on fake news. there may be sources whose inclusion you disagree with. it's up to you to decide how to work with the data and how you might contribute to \"improving it\". the labels of \"bs\" and \"junksci\", etc. do not constitute capital \"t\" truth. if there are other sources you would like to include, start a discussion. if there are sources you believe should not be included, start a discussion or write a kernel analyzing the data. or take the data and do something else productive with it. kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.\n",
      "contents\n",
      "the dataset contains text and metadata from 244 websites and represents 12,999 posts in total from the past 30 days. the data was pulled using the webhose.io api; because it's coming from their crawler, not all websites identified by the bs detector are present in this dataset. each website was labeled according to the bs detector as documented here. data sources that were missing a label were simply assigned a label of \"bs\". there are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\n",
      "fake news in the news\n",
      "for inspiration, i've included some (presumably non-fake) recent stories covering fake news in the news. this is a sensitive, nuanced topic and if there are other resources you'd like to see included here, please leave a suggestion. from defining fake, biased, and misleading news in the first place to deciding how to take action (a blacklist is not a good answer), there's a lot of information to consider beyond what can be neatly arranged in a csv file.\n",
      "how fake news spreads (nyt)\n",
      "we tracked down a fake-news creator in the suburbs. here's what we learned (npr)\n",
      "does facebook generate over half of its revenue from fake news? (forbes)\n",
      "fake news is not the only problem (points - medium)\n",
      "washington post disgracefully promotes a mccarthyite blacklist from a new, hidden, and very shady group (the intercept)\n",
      "improvements\n",
      "if you have suggestions for improvements or would like to contribute, please let me know. the most obvious extensions are to include data from \"real\" news sites and to address the bias in the current list. i'd be happy to include any contributions in future versions of the dataset.\n",
      "acknowledgements\n",
      "thanks to anthony for pointing me to daniel sieradski's bs detector. thank you to daniel nouri for encouraging me to add a disclaimer to the dataset's page.\n",
      "one way to understand how a city government works is by looking at who it employs and how its employees are compensated. this data contains the names, job title, and compensation for san francisco city employees on an annual basis from 2011 to 2014.\n",
      "exploration ideas\n",
      "to help get you started, here are some data exploration ideas:\n",
      "how have salaries changed over time between different groups of people?\n",
      "how are base pay, overtime pay, and benefits allocated between different groups?\n",
      "is there any evidence of pay discrimination based on gender in this dataset?\n",
      "how is budget allocated based on different groups and responsibilities?\n",
      "have other ideas you're curious for someone else to explore? post them in this forum thread.\n",
      "data description\n",
      "sf-salaries-release-*.zip (downloadable via the \"download data\" link in the header above) contains a csv table and a sqlite database (with the same data as the csv file). here's the code that creates this data release.\n",
      "the original source for this data is here. we've taken the raw files here and combined/normalized them into a single csv file as well as a sqlite database with an equivalently-defined table.\n",
      "context\n",
      "this dataset contains over 80,000 reports of ufo sightings over the last century.\n",
      "content\n",
      "there are two versions of this dataset: scrubbed and complete. the complete data includes entries where the location of the sighting was not found or blank (0.8146%) or have an erroneous or blank time (8.0237%). since the reports date back to the 20th century, some older data might be obscured. data contains city, state, time, description, and duration of each sighting.\n",
      "inspiration\n",
      "what areas of the country are most likely to have ufo sightings?\n",
      "are there any trends in ufo sightings over time? do they tend to be clustered or seasonal?\n",
      "do clusters of ufo sightings correlate with landmarks, such as airports or government research centers?\n",
      "what are the most common ufo descriptions?\n",
      "acknowledgement\n",
      "this dataset was scraped, geolocated, and time standardized from nuforc data by sigmond axel here.\n",
      "every year the cdc releases the country’s most detailed report on death in the united states under the national vital statistics systems. this mortality dataset is a record of every death in the country for 2005 through 2015, including detailed information about causes of death and the demographic background of the deceased.\n",
      "it's been said that \"statistics are human beings with the tears wiped off.\" this is especially true with this dataset. each death record represents somebody's loved one, often connected with a lifetime of memories and sometimes tragically too short.\n",
      "putting the sensitive nature of the topic aside, analyzing mortality data is essential to understanding the complex circumstances of death across the country. the us government uses this data to determine life expectancy and understand how death in the u.s. differs from the rest of the world. whether you’re looking for macro trends or analyzing unique circumstances, we challenge you to use this dataset to find your own answers to one of life’s great mysteries.\n",
      "overview\n",
      "this dataset is a collection of csv files each containing one year's worth of data and paired json files containing the code mappings, plus an icd 10 code set. the csvs were reformatted from their original fixed-width file formats using information extracted from the cdc's pdf manuals using this script. please note that this process may have introduced errors as the text extracted from the pdf is not a perfect match. if you have any questions or find errors in the preparation process, please leave a note in the forums. we hope to publish additional years of data using this method soon.\n",
      "a more detailed overview of the data can be found here. you'll find that the fields are consistent within this time window, but some of data codes change every few years. for example, the 113_cause_recode entry 069 only covers icd codes (i10,i12) in 2005, but by 2015 it covers (i10,i12,i15). when i post data from years prior to 2005, expect some of the fields themselves to change as well.\n",
      "all data comes from the cdc’s national vital statistics systems, with the exception of the icd10code, which are sourced from the world health organization.\n",
      "project ideas\n",
      "the cdc's mortality data was the basis of a widely publicized paper, by anne case and nobel prize winner angus deaton, arguing that middle-aged whites are dying at elevated rates. one of the criticisms against the paper is that it failed to properly account for the exact ages within the broad bins available through the cdc's wonder tool. what do these results look like with exact/not-binned age data?\n",
      "similarly, how sensitive are the mortality trends being discussed in the news to the choice of bin-widths?\n",
      "as noted above, the data preparation process could have introduced errors. can you find any discrepancies compared to the aggregate metrics on wonder? if so, please let me know in the forums!\n",
      "wonder is cited in numerous economics, sociology, and public health research papers. can you find any papers whose conclusions would be altered if they used the exact data available here rather than binned data from wonder?\n",
      "differences from the first version of the dataset\n",
      "this version of the dataset was prepared in a completely different many. this has allowed us to provide a much larger volume of data and ensure that codes are available for every field.\n",
      "we've replaced the batch of sql files with a single json per year. kaggle's platform currently offer's better support for json files, and this keeps the number of files manageable.\n",
      "a tutorial kernel providing a quick introduction to the new format is available here.\n",
      "lastly, i apologize if the transition has interrupted anyone's work! if need be, you can still download v1.\n",
      "this is the ball by ball data of all the ipl cricket matches till season 9.\n",
      "source: http://cricsheet.org/ (data is available on this website in the yaml format. this is converted to csv format by the contributors)\n",
      "the dataset contains 2 files: deliveries.csv and matches.csv. matches.csv contains details related to the match such as location, contesting teams, umpires, results, etc. deliveries.csv is the ball-by-ball data of all the ipl matches including data of the batting team, batsman, bowler, non-striker, runs scored, etc.\n",
      "research scope: predicting the winner of the next season of ipl based on past data, visualizations, perspectives, etc.\n",
      "context\n",
      "45 episodes across 4 seasons of monty python's flying circus - all of the scripts broken down into reusable bits.\n",
      "content\n",
      "the data attempts to create a structure around the flying circus scripts by breaking actions down into dialogue (someone is speaking) and direction (instructions for the actors). along with each action i have tried to allocate the episode number, episode name, recording date, air date, segment name, name of character and name of actor playing the character.\n",
      "acknowledgements\n",
      "the scripts are hosted in html at http://www.ibras.dk/montypython/justthewords.htm i have loaded all of the code that i wrote to scrape and process the data (warning: very messy) at https://github.com/allank/monty-python\n",
      "inspiration\n",
      "i scraped the data because i was looking at data sources for doing rnn to generate text based on an existing corpus. while the amount of data available in the flying circus scripts is probably not sufficient, there might be some interesting things to do with the data. for example, some markov chain generated dialogue lines:\n",
      "remember, buy whizzo butter and this dead crab. yeah, er, i, i personally think this is getting too silly. i don't like the sound of two bricks being bashed together.\n",
      "try your hand at automatically separating normal heartbeats from abnormal heartbeats and heart murmur with this machine learning challenge by peter bentley et al\n",
      "the data\n",
      "here's a brief overview of the format of this dataset as uploaded to kaggle. for a more detailed description, look at the description section below.\n",
      "the dataset is split into two sources, a and b: a was collected from the general public via an iphone app, and b was collected from a clinical trial in hospitals using a digital stethoscope.\n",
      "the goal of the task is to first (1) identify the locations of heart sounds from the audio, and (2) to classify the heart sounds into one of several categories (normal v. various non-normal heartbeat sounds).\n",
      "the csv files provided are: set_a.csv\n",
      "set_b.csv set_a_timing.csv\n",
      "the fields for set_a and set_b are as follows:\n",
      "dataset: a or b\n",
      "fname: the audio file\n",
      "label: either \"normal\", blank (for unlabelled data), or one of various categories of abnormal heartbeats\n",
      "sublabel: in set_b, some recordings are categorized as noisy, meaning they contain non-heart background noise; this field holds information on whether something is e.g. \"noisynormal\" or \"noisymurmur\"\n",
      "the file set_a_timing.csv contains gold-standard timing information for the \"normal\" recordings from set a. this file contains the following fields:\n",
      "fname: the audio file\n",
      "cycle: anywhere from 1 to 19; the heartbeat cycle that the time observation refers to\n",
      "sound: either s1 or s2; see below for what these mean\n",
      "location: the time location of this sound, in audio samples\n",
      "description\n",
      "the task, as described by the original authors\n",
      "task overview\n",
      "data has been gathered from two sources: (a) from the general public via the istethoscope pro iphone app, provided in dataset a, and (b) from a clinic trial in hospitals using the digital stethoscope digiscope, provided in dataset b.\n",
      "challenge 1 - heart sound segmentation\n",
      "the first challenge is to produce a method that can locate s1(lub) and s2(dub) sounds within audio data, segmenting the normal audio files in both datasets. to enable your machine learning method to learn we provide the exact location of s1 and s2 sounds for some of the audio files. you need to use them to identify and locate the s1 and s2 sounds of all the heartbeats in the unlabelled group. the locations of sounds are measured in audio samples for better precision. your method must use the same unit.\n",
      "challenge 2 - heart sound classification\n",
      "the task is to produce a method that can classify real heart audio (also known as “beat classification”) into one of four categories for dataset a:\n",
      "normal\n",
      "murmur\n",
      "extra heart sound\n",
      "artifact\n",
      "and three classes for dataset b:\n",
      "normal\n",
      "murmur\n",
      "extrasystole\n",
      "you may tackle either or both of these challenges. if you can solve the first challenge, the second will be considerably easier! the winner of each challenge will be the method best able to segment and/or classify two sets of unlabelled data into the correct categories after training on both datasets provided below.\n",
      "[obviously no longer applicable -- ed.]: the creator of the winning method will receive a wifi 32gb ipad as the prize, awarded at a workshop at aistats 2012.\n",
      "the audio files are of varying lengths, between 1 second and 30 seconds (some have been clipped to reduce excessive noise and provide the salient fragment of the sound). most information in heart sounds is contained in the low frequency components, with noise in the higher frequencies. it is common to apply a low-pass filter at 195 hz. fast fourier transforms are also likely to provide useful information about volume and frequency over time. more domain-specific knowledge about the difference between the categories of sounds is provided below.\n",
      "normal category\n",
      "in the normal category there are normal, healthy heart sounds. these may contain noise in the final second of the recording as the device is removed from the body. they may contain a variety of background noises (from traffic to radios). they may also contain occasional random noise corresponding to breathing, or brushing the microphone against clothing or skin. a normal heart sound has a clear “lub dub, lub dub” pattern, with the time from “lub” to “dub” shorter than the time from “dub” to the next “lub” (when the heart rate is less than 140 beats per minute). note the temporal description of “lub” and “dub” locations over time in the following illustration:\n",
      "…lub……….dub……………. lub……….dub……………. lub……….dub……………. lub……….dub…\n",
      "in medicine we call the lub sound \"s1\" and the dub sound \"s2\". most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute. however, note that since the data may have been collected from children or adults in calm or excited states, the heart rates in the data may vary from 40 to 140 beats or higher per minute. dataset b also contains noisy_normal data - normal data which includes a substantial amount of background noise or distortion. you may choose to use this or ignore it, however the test set will include some equally noisy examples.\n",
      "murmur category\n",
      "heart murmurs sound as though there is a “whooshing, roaring, rumbling, or turbulent fluid” noise in one of two temporal locations: (1) between “lub” and “dub”, or (2) between “dub” and “lub”. they can be a symptom of many heart disorders, some serious. there will still be a “lub” and a “dub”. one of the things that confuses non-medically trained people is that murmurs happen between lub and dub or between dub and lub; not on lub and not on dub. below, you can find an asterisk* at the locations a murmur may be.\n",
      "…lub..*...dub……………. lub..*..dub ……………. lub..*..dub ……………. lub..*..dub … or …lub……….dub…*….lub………. dub…*….lub ………. dub…**….lub ……….dub…\n",
      "dataset b also contains noisy_murmur data - murmur data which includes a substantial amount of background noise or distortion. you may choose to use this or ignore it, however the test set will include some equally noisy examples\n",
      "extra heart sound category (dataset a)\n",
      "extra heart sounds can be identified because there is an additional sound, e.g. a “lub-lub dub” or a “lub dub-dub”. an extra heart sound may not be a sign of disease. however, in some situations it is an important sign of disease, which if detected early could help a person. the extra heart sound is important to be able to detect as it cannot be detected by ultrasound very well. below, note the temporal description of the extra heart sounds:\n",
      "…lub.lub……….dub………..………. lub. lub……….dub…………….lub.lub……..…….dub……. or …lub………. dub.dub………………….lub.……….dub.dub………………….lub……..…….dub. dub……\n",
      "artifact category (dataset a)\n",
      "in the artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. there are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 hz. this category is the most different from the others. it is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.\n",
      "extrasystole category (dataset b)\n",
      "extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a “lub-lub dub” or a “lub dub-dub”. (this is not the same as an extra heart sound as the event is not regularly occuring.) an extrasystole may not be a sign of disease. it can happen normally in an adult and can be very common in children. however, in some situations extrasystoles can be caused by heart diseases. if these diseases are detected earlier, then treatment is likely to be more effective. below, note the temporal description of the extra heart sounds: …........lub……….dub………..………. lub. ………..……….dub…………….lub.lub……..…….dub……. or …lub………. dub......………………….lub.…………………dub.dub………………….lub……..…….dub.……\n",
      "acknowledgments\n",
      "please use the following citation if the data is used:\n",
      "@misc{pascal-chsc-2011, author = \"bentley, p. and nordehn, g. and coimbra, m. and mannor, s.\", title = \"the {pascal} {c}lassifying {h}eart {s}ounds {c}hallenge 2011 {(chsc2011)} {r}esults\", howpublished = \"http://www.peterjbentley.com/heartchallenge/index.html\"}\n",
      "live feed\n",
      "please comment on the discussion above \"live feed\" and i will share details with you in a message. hope we won't exceed server limitations.\n",
      "context\n",
      "i have been recording available different types of data on soccer matches since 2012, live 24/7. the whole database contains more than 350,000 soccer matches held all around the world from over 27,000 teams of more than 180 countries. an all-in-one package including servers, algorithms and its database are now under the \"analyst masters\" research platform. the app is also free for everyone to get its predictions on android play store . how could it become useful for a data scientist?\n",
      "did you know that,\n",
      "more than 1000 soccer matches are played in a week?\n",
      "the average profit of the stock market from its beginning to now has been less than 10% a year? but you can earn at least 10% on a single match in 2 hours and get your profit in cash\n",
      "it is one of the very rare datasets that you do not need to prove to other companies your method is the most accurate one and get the prize :) . on the other hand you do not have to classify every data point to be rewarded. just tune or focus to correctly classify only 1% of matches and there you go! let me give you a simple hint how easily it can become a classification problem rather than a time series prediction:\n",
      "example 1: who wins based on the number of wins in a head 2 head history?\n",
      "q) consider two teams midtjylland and randers from denmark. they have played against each other for very long time. midtjyland has won randers over 8 times in the past 10 matches in a 4 year time span. forget any other complicated algorithm and simply predict who wins this match?\n",
      "a) that is easy! however, i am also gathering a lot more information than just their history. you can check their head-to-head history and the odds you could get for predicting this match is \"1.73\" check here.\n",
      "example 2: number of goals based on their history?\n",
      "q) consider two teams \"san martin s.j.\" and \"rosario central\" from argentina. their odds for wining \"team 1 (home)\", \"draw\" and \"team 2 (away)\" is [3.16, 3.2, 2.25] respectively. they rank 22 and 13 in their league. they have recently won 45%,35% of their matches in their past 14 matches. their average head to head goals in their last 7 matches were 1.3 full time (f) and 0.3 until half-time (ht). how many goals do you think they score in their match? (note that a safe side of number of goals in soccer betting is over 0.5 goals in ht, under 1.5 goals in ht, over 1.5 goals in ft and under 3.5 goals in ft). which one do you choose?\n",
      "a) for sure under 1.5 goals in ht (you get 35%) and under 3.5 goals in ft (you get 30%) . bingo you get 65% in a single match in 2 hours\n",
      "example 3: based on the money placed for betting on teams who wins the match?\n",
      "q) \"memmingen\" and \"munich 1860\" are well known in germany. one of our reliable sources of data is the ratio of money placed on betting from 10 hours before the match until it starts. assume that the ratio of bets on \"munich 1860\" to \"memmingen\" are recorded every hour as below, which team do you think will win?\n",
      "[bets in $ on munich 1860]/[bets in $ on memmingen] : {1.01, 1.02, 1.04, 1.1, 1.2, 1.4, 1.58, 2.3, 2.6, 2.8}\n",
      "a) in 10 hours the amount of money placed on wining munich 1860 vs memmingen increased from 1.01 to 2.8, who is the winner? easy again, munich 1860 that gives you 160% as stated here.\n",
      "try the dataset and inspect every strategy you may come up with, as i gave you three reliable examples above. just perform well enough to predict 15 matches correctly in a row, start with $1000 and you are a millionaire. if you can't be that accurate use the kelly criterion to divide your whole money into smaller stakes.\n",
      "let me do the math for you, if you can only get 90% accuracy on 1% of data points (10 out of 1000 matches a week) and your average profit on each match is only 20%. you earn (9*20% = 180%) and lose 100% for your error in 10 predictions. your net profit would be 80% in a week or approximately 12% in a day. if you risk only 33% of your whole money on each match then the daily net profit becomes 4%. i guess you can easily calculate how fast you can progress @ 4% daily accumulative profit.\n",
      "for sure one needs a live data feed to predict the outcome before the match. if everything goes well and enough users are interested i will open the live feed of data for you in a shared folder of dropbox saved in csv.\n",
      "content\n",
      "here is what the dataset contains for 'n' matches:\n",
      "names6.csv\n",
      ": team names as the order of \"home-away\" separated using \"/\" ; size : (n x 1)\n",
      "results6.csv*\n",
      ": scores recorded during the match, every 2 rows show scores for one match ; size : (2n x 14)\n",
      "fresults6.csv*\n",
      ": final scores after full-time ; size : (n x 2)\n",
      "odds6.csv\n",
      ": odds in the order of: home-draw-away ; size : (n x 3)\n",
      "dollars6.csv*\n",
      ": ratio of the money spent on teams at 15 minutes intervals ; size : (n x 76)\n",
      "ranks6.csv\n",
      ": their ranks in the league at the day of the match irrespectively ; size : (n x 2)\n",
      "winrate6.csv\n",
      ": their winrate in the last (maximum 14) matches in 2017 irrespectively ; size : (n x 2)\n",
      "country6.csv\n",
      ": their country as some countries are difficult to analyze e.g. belarussia ; size : (n x 1)\n",
      "wins6.csv\n",
      ": number of wins in their last (maximum 6) head to head matches ; size : (n x 1)\n",
      "ft_ht6.csv\n",
      ": average of total goals in their last (maximum 6) head to head matches ft and ht ; size : (n x 2)\n",
      "*. recorded every 15 minutes\n",
      "try to predict the match as examples above using the given data in the zip file. i will upload the respective data from mid of august to mid september later on.\n",
      "acknowledgements\n",
      "i developed various data scrappers and classifiers running on multiple servers worldwide and never published a paper due to their sensitivity. you may refer to this database by mentioning the \"analyst masters\" research package.\n",
      "inspiration\n",
      "10 years ago i invented the world's first home-size cooking robot in my father's basement but in the end after cooking for us for 2 years it ended up in nothing. so, you as a data scientist can earn money using this live data stream for yourself if you can perform accurately without outperforming others in the competition just get an acceptable accuracy and you are good to go :)\n",
      "for more information on the overall platform and its live, pre-match and in-play analysis read at www.analystmasters.com or download the app for free to get easy predictions at 5% profit per week. more details on how the app operates is available at https://youtu.be/fqlu0yeyqc0\n",
      "context\n",
      "this dataset is a subset of yelp's businesses, reviews, and user data. it was originally put together for the yelp dataset challenge which is a chance for students to conduct research or analysis on yelp's data and share their discoveries. in the dataset you'll find information about businesses across 11 metropolitan areas in four countries.\n",
      "content\n",
      "this dataset contains seven csv files. the original json files can be found in yelp_academic_dataset.zip.\n",
      "\n",
      "you may find this documentation helpful:\n",
      "https://www.yelp.com/dataset/documentation/json\n",
      "in total, there are :\n",
      "5,200,000 user reviews\n",
      "information on 174,000 businesses\n",
      "the data spans 11 metropolitan areas\n",
      "acknowledgements\n",
      "the dataset was converted from json to csv format and we thank the team of the yelp dataset challenge for creating this dataset.\n",
      "by downloading this dataset, you agree to the yelp dataset terms of use.\n",
      "inspiration\n",
      "natural language processing & sentiment analysis\n",
      "what's in a review? is it positive or negative? yelp's reviews contain a lot of metadata that can be mined and used to infer meaning, business attributes, and sentiment.\n",
      "graph mining\n",
      "we recently launched our local graph but can you take the graph further? how do user's relationships define their usage patterns? where are the trend setters eating before it becomes popular?\n",
      "if you want to download and experiment with the scrapy script, you can do so from forum data science this page is a forum for data scientist i started, in hope , that you will participate and maybe even improve the scrapy script.\n",
      "over 370000 used cars scraped with scrapy from ebay-kleinanzeigen. the content of the data is in german, so one has to translate it first if one can not speak german. those fields are included: autos.csv:\n",
      "datecrawled : when this ad was first crawled, all field-values are taken from this date\n",
      "name : \"name\" of the car\n",
      "seller : private or dealer\n",
      "offertype\n",
      "price : the price on the ad to sell the car\n",
      "abtest\n",
      "vehicletype\n",
      "yearofregistration : at which year the car was first registered\n",
      "gearbox\n",
      "powerps : power of the car in ps\n",
      "model\n",
      "kilometer : how many kilometers the car has driven\n",
      "monthofregistration : at which month the car was first registered\n",
      "fueltype\n",
      "brand\n",
      "notrepaireddamage : if the car has a damage which is not repaired yet\n",
      "datecreated : the date for which the ad at ebay was created\n",
      "nrofpictures : number of pictures in the ad (unfortunately this field contains everywhere a 0 and is thus useless (bug in crawler!) )\n",
      "postalcode\n",
      "lastseenonline : when the crawler saw this ad last online\n",
      "the fields lastseen and datecreated could be used to estimate how long a car will be at least online before it is sold.\n",
      "brought to you by orges leka\n",
      "regression on average price per year based on this dataset\n",
      "table of value loss of an average used car per year\n",
      "the second file is produced in mysql from the first one through the query:\n",
      "select \n",
      " count(*) as count, \n",
      " kilometer, \n",
      " yearofregistration, \n",
      "20*round(powerps/20) as powerps, \n",
      "min(price) as minprice, \n",
      "max(price) as maxprice, \n",
      "avg(price) as avgpreis, \n",
      "sqrt(variance(price)) as sdpreis from items where \n",
      "     yearofregistration > 1990 and yearofregistration < 2016 \n",
      "    and price > 100 and price < 100000 \n",
      "    and powerps < 600 and powerps > 0 \n",
      " group by yearofregistration, round(powerps/20),kilometer \n",
      "having count > 10 \n",
      "into outfile '/tmp/cnt_km_year_powerps_minprice_maxprice_avgprice_sdprice.csv' \n",
      "fields terminated by ',' lines terminated by '\\n';\n",
      "happy coding!\n",
      "context\n",
      "motivated by gregory smith's web scrape of vgchartz video games sales, this data set simply extends the number of variables with another web scrape from metacritic. unfortunately, there are missing observations as metacritic only covers a subset of the platforms. also, a game may not have all the observations of the additional variables discussed below. complete cases are ~ 6,900\n",
      "content\n",
      "alongside the fields: name, platform, year_of_release, genre, publisher, na_sales, eu_sales, jp_sales, other_sales, global_sales, we have:-\n",
      "critic_score - aggregate score compiled by metacritic staff\n",
      "critic_count - the number of critics used in coming up with the critic_score\n",
      "user_score - score by metacritic's subscribers\n",
      "user_count - number of users who gave the user_score\n",
      "developer - party responsible for creating the game\n",
      "rating - the esrb ratings\n",
      "acknowledgements\n",
      "this repository, https://github.com/wtamu-cisresearch/scraper, after a few adjustments worked extremely well!\n",
      "inspiration\n",
      "it would be interesting to see any machine learning techniques or continued data visualizations applied on this data set.\n",
      "us social security applications are a great way to track trends in how babies born in the us are named.\n",
      "data.gov releases two datasets that are helplful for this: one at the national level and another at the state level. note that only names with at least 5 babies born in the same year (/ state) are included in this dataset for privacy.\n",
      "i've taken the raw files here and combined/normalized them into two csv files (one for each dataset) as well as a sqlite database with two equivalently-defined tables. the code that did these transformations is available here.\n",
      "new to data exploration in r? take the free, interactive datacamp course, \"data exploration with kaggle scripts,\" to learn the basics of visualizing data with ggplot. you'll also create your first kaggle scripts along the way.\n",
      "content\n",
      "the murder accountability project is the most complete database of homicides in the united states currently available. this dataset includes murders from the fbi's supplementary homicide report from 1976 to the present and freedom of information act data on more than 22,000 homicides that were not reported to the justice department. this dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used.\n",
      "acknowledgements\n",
      "the data was compiled and made available by the murder accountability project, founded by thomas hargrove.\n",
      "overview\n",
      "this dataset contains 50000 ranked ladder matches from the dota 2 data dump created by opendota. it was inspired by the dota 2 matches data published here by joe ramir. this is an update and improved version of that dataset. i have kept the same image and a similar title.\n",
      "dota 2 is a popular moba available as free to play, and can take up thousands of hours of your life. the number of games in this dataset are played about every hour. if you like the data there are an additional 2-3 million matches easily available for download.\n",
      "the aim of this dataset is to enable the exploration of player behavior, skill estimation, or anything you find interesting. the intent is to create an accessible, and easy to use resource, which can be expanded and modified if needed. as such i am open to a wide variety of suggestions as to what additions or changes to make.\n",
      "help getting started\n",
      "if there is some aspect of this data you would like to explore but seems difficult to get figure out how to work with please feel free to request some starter code in one of the following two kernels discussion section. i usually check kaggle every day or so. if you post a request about the current data i will try to get something working.\n",
      "python https://www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/misc-howtos-dota-requests-welcome/\n",
      "r https://www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/howtos-request-welcome/\n",
      "whats currently available\n",
      "see https://github.com/odota/core/wiki/json-data-dump for documentaion on data. i have found a few undocumented areas in the data, including the objectives information. player_slot can be used to combine most of the data, and it is available in most of the tables. additionally all tables include match_id, and some have account_id to make it easier to look at an individual players matches. match_id, and account_id have been reencoded to save a little space. i can upload tables to allow conversion if needed.\n",
      "matches: contains top level information about each match. see https://wiki.teamfortress.com/wiki/webapi/getmatchdetails#tower_status%22tower_status_dire%22:%202047) for interpreting tower and barracks status. cluster can link matches to geographic region.\n",
      "players: individual players are identified by account_id but there is an option to play anonymously and roughly one third of the account_id are not available. anonymous users have the value of 0 for account_id. contains totals for kills, deaths, denies, etc. player action counts are available, and are indicated by variable names beginning with unit_order_. counts for reasons for acquiring or losing gold, and gaining experience, have prefixes gold_, and xp_.\n",
      "player_time: contains last hits, experience, and gold sampled at one minute interval for all players in all matches. the column names indicate the player_slot. for instance xp_t_1 indicates that this column has experience sums for the player in slot one.\n",
      "teamfights: start and stop time of teamfights, as well as last death time. teamfights appear to be all battles with three or more deaths. as such this does not include all battles for the entire match.\n",
      "teamfights_players : additional information provided for each player in each teamfight. player_slot can be used to link this back to players.csv\n",
      "objectives: gives information on all the objectives completed, by which player and at what time.\n",
      "chat: all chat for the 50k matches. there is plenty of profanity, and good natured trolling.\n",
      "test_labels: match_id and radiant_win(as integer 1 or 0)\n",
      "test_player: full player and match table with hero_id, player_slot, match_id, and account_id\n",
      "nov 5th update\n",
      "added several additional tables. none of the previously uploaded data was altered. i plan to add several kernels in the next week going over how to use the data, and performing some eda. many improvements to the player rating method i used are possible for those interested in mmr.\n",
      "player_ratings contains match counts, win counts, and trueskill rating, calculated on 900k matches which occurred prior to other uploaded data. trueskill ratings have two components, mu, which can be interpreted as the skill, with higher value being better, and sigma which is the uncertainty of the rating.\n",
      "match_outcomes data for ~900k matches used to calculate player ratings. use this to improve on the ratings i uploaded.\n",
      "purchase_log item purchase times\n",
      "ability_upgrade ability upgrade times and levels\n",
      "cluster_region allows the mapping cluster found in match.csv to geographic region.\n",
      "patch_dates release dates for various patches, use start_time from match.csv to determine which patch a match was played in.\n",
      "ability_ids use with ability_upgrades.csv to get the names of upgraded abilities\n",
      "item_ids use with purchase_log.csv to get the names of purchased items\n",
      "kernel showing how player skill was computed: contains several resources on trueskill rating system.\n",
      "past research\n",
      "there seem to be some efforts to establish indicators for skillfull play based on specific parts of gameplay. opendota has many statistics, and some analysis for specific benchmarks at different times in the game. dotabuff has a lot of information i have not explored it deeply. this is an area to gather more information.\n",
      "some possible directions of investigation\n",
      "insight from domain experts would also be useful to help clarify what problems are interesting to work on. some initial task ideas\n",
      "predict match outcomes based on aggregates for individual players using only account_id as prior information\n",
      "add hero id to this and see if there is a differences in performance\n",
      "estimate player skill based on a sample of in game play(this might need an external mmr source or different definition skill)\n",
      "create improved indicators of skillful play based game actions to help players target areas for improvement\n",
      "all of these areas have been worked on, but i am not aware of the most up to date research on dota2 gameplay.\n",
      "i plan on setting up several different predictive tasks in the upcoming weeks. a test set of an additional 50 to 100 thousand matches with just hero_id, and account_id included along with outcome of the match.\n",
      "the current dataset seems pretty small for modeling individual players. i would prefer to have a wide range of features instead of a larger dataset for the moment.\n",
      "dataset idea for anyone interested in creating their own dota 2 dataset. it would be useful to have a few full matches available to work on. they would need to be extracted from the .dem replay file to something easily parsed by r and python as available in kernels. given the size of a full match data only a few matches would be needed. there are files available from opendota' s website(check for replays). looking at fine grained match details would potentially allow for the creation of better high level parsed data. i think it would be a lot of work just to get a handle on working with full match data so a sample would be good to have.\n",
      "acknowledgements\n",
      "orginal kaggle dataset on dota2 matches by joe ramir i also borrowed the image and some of the content for these acknowledgements from the above, thanks!.\n",
      "image source\n",
      "data download source created by yasp\n",
      "description of original dataset creation: https://github.com/yasp-dota/yasp/issues/924\n",
      "yasp's license\n",
      "\"license: cc by-sa 4.0\"\n",
      "\"terms: we ask that you attribute yasp.co if you create or publish anything related to our data. also, please seed for as long as possible.\"\n",
      "yasp is now known as opendota here are links to their website and github page\n",
      "https://www.opendota.com/ the data is used to for this site and its a easy way to get familier with it\n",
      "https://github.com/odota/core check here for info especially this wiki page which gives details on the schema.\n",
      "analysis of the public dataset: \"airplane crashes and fatalities since 1908\" (full history of airplane crashes throughout the world, from 1908-present) hosted by open data by socrata available at:\n",
      "https://opendata.socrata.com/government/airplane-crashes-and-fatalities-since-1908/q2te-8cvq\n",
      "questions\n",
      "yearly how many planes crashed? how many people were on board? how many survived? how many died?\n",
      "highest number of crashes by operator and type of aircrafts.\n",
      "‘summary’ field has the details about the crashes. find the reasons of the crash and categorize them in different clusters i.e fire, shot down, weather (for the ‘blanks’ in the data category can be unknown) you are open to make clusters of your choice but they should not exceed 7.\n",
      "find the number of crashed aircrafts and number of deaths against each category from above step.\n",
      "find any interesting trends/behaviors that you encounter when you analyze the dataset.\n",
      "my solution\n",
      "the following bar charts display the answers requested by point 1. of the assignment, in particular:\n",
      "the planes crashed per year\n",
      "people aboard per year during crashes\n",
      "people dead per year during crashes\n",
      "people survived per year during crashes\n",
      "the following answers regard point 2 of the assignment\n",
      "highest number of crashes by operator: aeroflot with 179 crashes\n",
      "by type of aircraft: douglas dc-3 with 334 crashes\n",
      "i have identified 7 clusters using k-means clustering technique on a matrix obtained by a text corpus created by using text analysis (plain text, remove punctuation, to lower, etc.) the following table summarize for each cluster the number of crashes and death.\n",
      "cluster 1: 258 crashes, 6368 deaths\n",
      "cluster 2: 500 crashes, 9408 deaths\n",
      "cluster 3: 211 crashes, 3513 deaths\n",
      "cluster 4: 1014 crashes, 14790 deaths\n",
      "cluster 5: 2749 crashes, 58826 deaths\n",
      "cluster 6: 195 crashes, 4439 deaths\n",
      "cluster 7: 341 crashes, 8135 deaths\n",
      "the following picture shows clusters using the first 2 principal components:\n",
      "for each clusters i will summarize the most used words and i will try to identify the causes of the crash\n",
      "cluster 1 (258) aircraft, crashed, plane, shortly, taking. no many information about this cluster can be deducted using text analysis\n",
      "cluster 2 (500) aircraft, airport, altitude, crashed, crew, due, engine, failed, failure, fire, flight, landing, lost, pilot, plane, runway, takeoff, taking. engine failure on the runway after landing or takeoff\n",
      "cluster 3 (211): aircraft, crashed, fog crash caused by fog\n",
      "cluster 4 (1014): aircraft, airport, attempting, cargo, crashed, fire, land, landing, miles, pilot, plane, route, runway, struck, takeoff struck a cargo during landing or takeoff\n",
      "cluster 5 (2749): accident, aircraft, airport, altitude, approach, attempting, cargo, conditions, control, crashed, crew, due, engine, failed, failure, feet, fire, flight, flying, fog, ground, killed, land, landing, lost, low, miles, mountain, pilot. plane, poor, route, runway, short, shortly, struck, takeoff, taking, weather\n",
      "struck a cargo due to engine failure or bad weather conditions mainly fog\n",
      "cluster 6 (195): aircraft, crashed, engine, failure, fire, flight, left, pilot, plane, runway\n",
      "engine failure on the runway\n",
      "cluster 7 (341): accident, aircraft, altitude, cargo, control, crashed, crew, due, engine, failure, flight, landing, loss, lost, pilot, plane, takeoff\n",
      "engine failure during landing or takeoff\n",
      "better solutions are welcome. thanks.\n",
      "context\n",
      "mass shootings in the united states of america (1966-2017) the us has witnessed 398 mass shootings in last 50 years that resulted in 1,996 deaths and 2,488 injured. the latest and the worst mass shooting of october 2, 2017 killed 58 and injured 515 so far. the number of people injured in this attack is more than the number of people injured in all mass shootings of 2015 and 2016 combined. the average number of mass shootings per year is 7 for the last 50 years that would claim 39 lives and 48 injured per year.\n",
      "content\n",
      "geography: united states of america\n",
      "time period: 1966-2017\n",
      "unit of analysis: mass shooting attack\n",
      "dataset: the dataset contains detailed information of 398 mass shootings in the united states of america that killed 1996 and injured 2488 people.\n",
      "variables: the dataset contains serial no, title, location, date, summary, fatalities, injured, total victims, mental health issue, race, gender, and lat-long information.\n",
      "acknowledgements\n",
      "i’ve consulted several public datasets and web pages to compile this data. some of the major data sources include wikipedia, mother jones, stanford, usa today and other web sources.\n",
      "inspiration\n",
      "with a broken heart, i like to call the attention of my fellow kagglers to use machine learning and data sciences to help me explore these ideas:\n",
      "• how many people got killed and injured per year?\n",
      "• visualize mass shootings on the u.s map\n",
      "• is there any correlation between shooter and his/her race, gender\n",
      "• any correlation with calendar dates? do we have more deadly days, weeks or months on average\n",
      "• what cities and states are more prone to such attacks\n",
      "• can you find and combine any other external datasets to enrich the analysis, for example, gun ownership by state\n",
      "• any other pattern you see that can help in prediction, crowd safety or in-depth analysis of the event\n",
      "• how many shooters have some kind of mental health problem? can we compare that shooter with general population with same condition\n",
      "mass shootings dataset ver 3\n",
      "this is the new version of mass shootings dataset. i've added eight new variables:\n",
      "incident area (where the incident took place),\n",
      "open/close location (inside a building or open space)\n",
      "target (possible target audience or company),\n",
      "cause (terrorism, hate crime, fun (for no obvious reason etc.)\n",
      "policeman killed (how many on duty officers got killed)\n",
      "age (age of the shooter)\n",
      "employed (y/n)\n",
      "employed at (employer name)\n",
      "age, employed and employed at (3 variables) contain shooter details\n",
      "mass shootings dataset ver 4\n",
      "quite a few missing values have been added\n",
      "mass shootings dataset ver 5\n",
      "three more recent mass shootings have been added including the texas church shooting of november 5, 2017\n",
      "i hope it will help create more visualization and extract patterns.\n",
      "keep coding!\n",
      "context\n",
      "part backorders is a common supply chain problem. working to identify parts at risk of backorder before the event occurs so the business has time to react.\n",
      "content\n",
      "training data file contains the historical data for the 8 weeks prior to the week we are trying to predict. the data was taken as weekly snapshots at the start of each week. columns are defined as follows:\n",
      "sku - random id for the product\n",
      "national_inv - current inventory level for the part\n",
      "lead_time - transit time for product (if available)\n",
      "in_transit_qty - amount of product in transit from source\n",
      "forecast_3_month - forecast sales for the next 3 months\n",
      "forecast_6_month - forecast sales for the next 6 months\n",
      "forecast_9_month - forecast sales for the next 9 months\n",
      "sales_1_month - sales quantity for the prior 1 month time period\n",
      "sales_3_month - sales quantity for the prior 3 month time period\n",
      "sales_6_month - sales quantity for the prior 6 month time period\n",
      "sales_9_month - sales quantity for the prior 9 month time period\n",
      "min_bank - minimum recommend amount to stock\n",
      "potential_issue - source issue for part identified\n",
      "pieces_past_due - parts overdue from source\n",
      "perf_6_month_avg - source performance for prior 6 month period\n",
      "perf_12_month_avg - source performance for prior 12 month period\n",
      "local_bo_qty - amount of stock orders overdue\n",
      "deck_risk - part risk flag\n",
      "oe_constraint - part risk flag\n",
      "ppap_risk - part risk flag\n",
      "stop_auto_buy - part risk flag\n",
      "rev_stop - part risk flag\n",
      "went_on_backorder - product actually went on backorder. this is the target value.\n",
      "context\n",
      "the sms spam collection is a set of sms tagged messages that have been collected for sms spam research. it contains one set of sms messages in english of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
      "content\n",
      "the files contain one message per line. each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
      "acknowledgements\n",
      "the original dataset can be found here. the creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
      "inspiration\n",
      "can you use this dataset to build a prediction model that will accurately classify which texts are spam?\n",
      "context\n",
      "this dataset deals with pollution in the u.s. pollution in the u.s. has been well documented by the u.s. epa but it is a pain to download all the data and arrange them in a format that interests data scientists. hence i gathered four major pollutants (nitrogen dioxide, sulphur dioxide, carbon monoxide and ozone) for every day from 2000 - 2016 and place them neatly in a csv file.\n",
      "content\n",
      "there is a total of 28 fields. the four pollutants (no2, o3, so2 and o3) each has 5 specific columns. observations totaled to over 1.4 million. this kernel provides a good introduction to this dataset!\n",
      "for observations on specific columns visit the column metadata on the data tab.\n",
      "acknowledgements\n",
      "all the data is scraped from the database of u.s. epa : https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html\n",
      "inspiration\n",
      "i did a related project with some of my friends in college, and decided to open source our dataset so that data scientists don't need to re-scrape the u.s. epa site for historical pollution data.\n",
      "context\n",
      "the u.s. department of transportation's (dot) bureau of transportation statistics tracks the on-time performance of domestic flights operated by large air carriers. summary information on the number of on-time, delayed, canceled, and diverted flights is published in dot's monthly air travel consumer report and in this dataset of 2015 flight delays and cancellations.\n",
      "acknowledgements\n",
      "the flight delay and cancellation data was collected and published by the dot's bureau of transportation statistics.\n",
      "the dataset you can play with.\n",
      "context\n",
      "dataset for people who love data science and have grown up playing fifa.\n",
      "content\n",
      "every player featuring in fifa 18\n",
      "70+ attributes\n",
      "player and flag images\n",
      "playing position data\n",
      "attributes based on actual data of the latest ea's fifa 18 game\n",
      "attributes include on all player style statistics like dribbling, aggression, gk skills etc.\n",
      "player personal data like nationality, photo, club, age, wage, salary etc.\n",
      "upcoming update will include :\n",
      "team (national and club) data\n",
      "player images in zip folder\n",
      "betting odds\n",
      "the dataset contains all the statistics and playing attributes of all the players in the full version of fifa 18.\n",
      "data source\n",
      "the data is scraped from the website https://sofifa.com by extracting the player personal data and player ids and then the playing and style statistics.\n",
      "github project\n",
      "possible explorations\n",
      "make your dream team\n",
      "analyse which club or national team has the best-rated players\n",
      "assess the strength of a team at a particular position\n",
      "analyse the team with the best dribbling speed\n",
      "co-relate between age and overall rating\n",
      "co-relate between age and nationality\n",
      "co-relate between age and potential\n",
      "could prove of immense value to fantasy premier league enthusiasts.\n",
      "these are just basic examples, sky is the limit.\n",
      "acknowledgements\n",
      "the data has been crawled from the https://sofifa.com website.\n",
      "inspiration\n",
      "several insights and correlations between player value, wage, age, and performance can be derived from the dataset. furthermore, how do the players in this dataset compare against themselves in last year's dataset?\n",
      "contributing\n",
      "changes and improvement suggestions are welcome. feel free to comment new additions that you think are useful or drop a pr on the github project.\n",
      "we aren't saying this dataset is the rosetta stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on kaggle community and competition activity.\n",
      "strategizing to become a master? wondering who, where, and what goes in to a winning team? deciding between evaluation metrics for your next data science project? we hope the scripts published here will enrich and entertain kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.\n",
      "this data (available through kaggle scripts as csv files and a sqlite database) contains the tables listed below.\n",
      "note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. over time, we'll add more of the tables that we can release publicly to it.\n",
      "the dataset for people who double on fifa and data science\n",
      "content\n",
      "17,000+ players\n",
      "50+ attributes per player ranging from ball skills aggression etc.\n",
      "player's attributes sourced from ea sports' fifa video game series, including the weekly updates\n",
      "players from all around the globe\n",
      "urls to their homepage\n",
      "club logos\n",
      "player images male and female\n",
      "national and club team data\n",
      "weekly updates would include :\n",
      "real life data (match events etc.)\n",
      "the fifa generated player dataset\n",
      "betting odds\n",
      "growth\n",
      "data source\n",
      "data was scraped from https://www.fifaindex.com/ first by getting player profile url set (as stored in playernames.csv) and then scraping the individual pages for their attributes\n",
      "improvements\n",
      "you may have noticed that for a lot of players, their national details are absent (team and kit number) even though the nationality is listed. this may be attributed to the missing data on fifa sites.\n",
      "github project\n",
      "there is much more than just 50 attributes by which fifa decides what happens to players over time, how they perform under pressure, how they grow etc. this data obviously would be well hidden by the organisation and thus would be tough to find\n",
      "important note for people interested in using the scraping: the site is not uniform and thus the scraping script requires considering a lot of corner cases (i.e. interchanged position of different attributes). also the script contains proxy preferences which may be removed if not required.\n",
      "exploring the data\n",
      "for starters you can become a scout:\n",
      "create attribute dependent or overall best teams\n",
      "create the fastest/slowest teams\n",
      "see which areas of the world provide which attributes (like africa : stamina, pace)\n",
      "see which players are the best at each position\n",
      "see which outfield players can play a better role at some other position\n",
      "see which youngsters have attributes which can be developed\n",
      "and that is just the beginning. this is the playground.. literally!\n",
      "data description\n",
      "the file fulldata.csv contains attributes describing the in game play style and also some of the real statistics such as nationality etc.\n",
      "the file playernames.csv contains urls for different players from their profiles on fifaindex.com. append the urls after the base url fifaindex.com.\n",
      "the compressed file pictures.zip contains pictures for top 1000 players in fifa 17.\n",
      "the compressed file pictures_f.zip contains pictures for top 139 female players in fifa 17.\n",
      "the compressed file clubpictures.zip contains pictures for emblems of some major clubs in fifa 17.\n",
      "inspiration\n",
      "i am a huge fifa fanatic. while playing career mode i realised that i picked great young players early on every single time and since a lot of digital learning relies on how our brain works, i thought scouting great qualities in players would be something that can be worked on. since then i started working on scraping the website and here is the data. i hope we can build something on it.\n",
      "with access to players attributes you can become the best scout in the world. go for it!\n",
      "context\n",
      "netflix held the netflix prize open competition for the best algorithm to predict user ratings for films. the grand prize was $1,000,000 and was won by bellkor's pragmatic chaos team. this is the dataset that was used in that competition.\n",
      "content\n",
      "this comes directly from the readme:\n",
      "training dataset file description\n",
      "the file \"training_set.tar\" is a tar of a directory containing 17770 files, one per movie. the first line of each file contains the movie id followed by a colon. each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\n",
      "customerid,rating,date\n",
      "movieids range from 1 to 17770 sequentially.\n",
      "customerids range from 1 to 2649429, with gaps. there are 480189 users.\n",
      "ratings are on a five star (integral) scale from 1 to 5.\n",
      "dates have the format yyyy-mm-dd.\n",
      "movies file description\n",
      "movie information in \"movie_titles.txt\" is in the following format:\n",
      "movieid,yearofrelease,title\n",
      "movieid do not correspond to actual netflix movie ids or imdb movie ids.\n",
      "yearofrelease can range from 1890 to 2005 and may correspond to the release of corresponding dvd, not necessarily its theaterical release.\n",
      "title is the netflix movie title and may not correspond to titles used on other sites. titles are in english.\n",
      "qualifying and prediction dataset file description\n",
      "the qualifying dataset for the netflix prize is contained in the text file \"qualifying.txt\". it consists of lines indicating a movie id, followed by a colon, and then customer ids and rating dates, one per line for that movie id. the movie and customer ids are contained in the training set. of course the ratings are withheld. there are no empty lines in the file.\n",
      "movieid1:\n",
      "customerid11,date11\n",
      "customerid12,date12\n",
      "...\n",
      "movieid2:\n",
      "customerid21,date21\n",
      "customerid22,date22\n",
      "for the netflix prize, your program must predict the all ratings the customers gave the movies in the qualifying dataset based on the information in the training dataset.\n",
      "the format of your submitted prediction file follows the movie and customer id, date order of the qualifying dataset. however, your predicted rating takes the place of the corresponding customer id (and date), one per line.\n",
      "for example, if the qualifying dataset looked like:\n",
      "111:\n",
      "3245,2005-12-19\n",
      "5666,2005-12-23\n",
      "6789,2005-03-14\n",
      "225:\n",
      "1234,2005-05-26\n",
      "3456,2005-11-07\n",
      "then a prediction file should look something like:\n",
      "111:\n",
      "3.0\n",
      "3.4\n",
      "4.0\n",
      "225:\n",
      "1.0\n",
      "2.0\n",
      "which predicts that customer 3245 would have rated movie 111 3.0 stars on the 19th of decemeber, 2005, that customer 5666 would have rated it slightly higher at 3.4 stars on the 23rd of decemeber, 2005, etc.\n",
      "you must make predictions for all customers for all movies in the qualifying dataset.\n",
      "the probe dataset file description\n",
      "to allow you to test your system before you submit a prediction set based on the qualifying dataset, we have provided a probe dataset in the file \"probe.txt\". this text file contains lines indicating a movie id, followed by a colon, and then customer ids, one per line for that movie id.\n",
      "movieid1:\n",
      "customerid11\n",
      "customerid12\n",
      "...\n",
      "movieid2:\n",
      "customerid21\n",
      "customerid22\n",
      "like the qualifying dataset, the movie and customer id pairs are contained in the training set. however, unlike the qualifying dataset, the ratings (and dates) for each pair are contained in the training dataset.\n",
      "if you wish, you may calculate the rmse of your predictions against those ratings and compare your rmse against the cinematch rmse on the same data. see http://www.netflixprize.com/faq#probe for that value.\n",
      "acknowledgements\n",
      "the training data came in 17,000+ files. in the interest of keeping files together and file sizes as low as possible, i combined them into four text files: combined_data_(1,2,3,4).txt\n",
      "the contest was originally hosted at http://netflixprize.com/index.html\n",
      "the dataset was downloaded from https://archive.org/download/nf_prize_dataset.tar\n",
      "inspiration\n",
      "this is a fun dataset to work with. you can read about the winning algorithm by bellkor's pragmatic chaos here\n",
      "the enron email dataset contains approximately 500,000 emails generated by employees of the enron corporation. it was obtained by the federal energy regulatory commission during its investigation of enron's collapse.\n",
      "this is the may 7, 2015 version of dataset, as published at https://www.cs.cmu.edu/~./enron/\n",
      "data on shots taken during the 2014-2015 season, who took the shot, where on the floor was the shot taken from, who was the nearest defender, how far away was the nearest defender, time on the shot clock, and much more. the column titles are generally self-explanatory.\n",
      "useful for evaluating who the best shooter is, who the best defender is, the hot-hand hypothesis, etc.\n",
      "scraped from nba's rest api.\n",
      "context\n",
      "there is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets.\n",
      "we present a synthetic dataset generated using the simulator called paysim as an approach to such a problem. paysim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods.\n",
      "content\n",
      "paysim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an african country. the original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\n",
      "this synthetic dataset is scaled down 1/4 of the original dataset and it is created just for kaggle.\n",
      "headers\n",
      "this is a sample of 1 row with headers explanation:\n",
      "1,payment,1060.31,c429214117,1089.0,28.69,m1591654462,0.0,0.0,0,0\n",
      "step - maps a unit of time in the real world. in this case 1 step is 1 hour of time. total steps 744 (30 days simulation).\n",
      "type - cash-in, cash-out, debit, payment and transfer.\n",
      "amount - amount of the transaction in local currency.\n",
      "nameorig - customer who started the transaction\n",
      "oldbalanceorg - initial balance before the transaction\n",
      "newbalanceorig - new balance after the transaction\n",
      "namedest - customer who is the recipient of the transaction\n",
      "oldbalancedest - initial balance recipient before the transaction. note that there is not information for customers that start with m (merchants).\n",
      "newbalancedest - new balance recipient after the transaction. note that there is not information for customers that start with m (merchants).\n",
      "isfraud - this is the transactions made by the fraudulent agents inside the simulation. in this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n",
      "isflaggedfraud - the business model aims to control massive transfers from one account to another and flags illegal attempts. an illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.\n",
      "past research\n",
      "there are 5 similar files that contain the run of 5 different scenarios. these files are better explained at my phd thesis chapter 7 (phd thesis available here http://urn.kb.se/resolve?urn=urn:nbn:se:bth-12932).\n",
      "we ran paysim several times using random seeds for 744 steps, representing each hour of one month of real time, which matches the original logs. each run took around 45 minutes on an i7 intel processor with 16gb of ram. the final result of a run contains approximately 24 million of financial records divided into the 5 types of categories: cash-in, cash-out, debit, payment and transfer.\n",
      "acknowledgements\n",
      "this work is part of the research project ”scalable resource-efficient systems for big data analytics” funded by the knowledge foundation (grant: 20140032) in sweden.\n",
      "please refer to this dataset using the following citations:\n",
      "paysim first paper of the simulator:\n",
      "e. a. lopez-rojas , a. elmir, and s. axelsson. \"paysim: a financial mobile money simulator for fraud detection\". in: the 28th european modeling and simulation symposium-emss, larnaca, cyprus. 2016\n",
      "youtube faces dataset with facial keypoints\n",
      "this dataset is a processed version of the youtube faces dataset, that basically contained short videos of celebrities that are publicly available and were downloaded from youtube. there are multiple videos of each celebrity (up to 6 videos per celebrity). i've cropped the original videos around the faces, plus kept only consecutive frames of up to 240 frames for each original video. this is done also for reasons of disk space, but mainly to make the dataset easier to use.\n",
      "additionally, for this kaggle version of the dataset i've extracted facial keypoints for each frame of each video using this amazing 2d and 3d face alignment library that was recently published. please check out this video demonstrating the library. it's performance is really amazing, and i feel i'm quite qualified to say that after manually curating many thousands of individual frames and their corresponding keypoints. i removed all videos with extremely bad keypoints labeling. the end result of my curation process is approximately 2800 videos. right now only 1293 of those videos are uploaded due to dataset size limitations (10gb), but since overall this totals into 155,560 single image frames, i think this is more than enough to do a lot of interesting kernels as well as potentially very interesting research.\n",
      "context\n",
      "kaggle datasets platform and its integration with kernels is really amazing, but it's yet to have a videos dataset (at least that i'm aware of). videos are special in the fact that they contain rich spatial patterns (in this case images of human faces) and rich temporal patterns (in this case how the faces move in time).\n",
      "i was also inspired by the face images with marked landmark points dataset uploaded by drguillermo and decided to create and share a dataset that would be similar but would also add something extra.\n",
      "acknowledgements\n",
      "if you use the youtube faces dataset, or refer to its results, please cite the following paper:\n",
      "lior wolf, tal hassner and itay maoz\n",
      "face recognition in unconstrained videos with matched background similarity.\n",
      "ieee conf. on computer vision and pattern recognition (cvpr), 2011. (pdf)\n",
      "if you use the 2d or 3d keypoints, or refer to its results, please cite the following paper:\n",
      "adrian bulat and georgios tzimiropoulos.\n",
      "how far are we from solving the 2d & 3d face alignment problem?\n",
      "(and a dataset of 230,000 3d facial landmarks), arxiv, 2017. (pdf)\n",
      "also, i would like to thank gil levi for pointing out youtube faces to me a few years back.\n",
      "inspiration\n",
      "the youtube faces dataset was originally intended to be used for face recognition across videos, i.e. given two videos, are those videos of the same person or not?\n",
      "i think it can be used to serve many additional goals, especially when combined with the keypoints information. for example, can we build a face movement model and predict what facial expression will come next?\n",
      "this dataset can also be used to test transfer learning between other face datasets (like face images with marked landmark points that i mentioned earlier), or even other types of faces like cat or dog faces (like here or here). also, using the pre-trained keras models might be useful (example kernel).\n",
      "have fun!\n",
      "dataset information\n",
      "this dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in taiwan from april 2005 to september 2005.\n",
      "content\n",
      "there are 25 variables:\n",
      "id: id of each client\n",
      "limit_bal: amount of given credit in nt dollars (includes individual and family/supplementary credit\n",
      "sex: gender (1=male, 2=female)\n",
      "education: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
      "marriage: marital status (1=married, 2=single, 3=others)\n",
      "age: age in years\n",
      "pay_0: repayment status in september, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
      "pay_2: repayment status in august, 2005 (scale same as above)\n",
      "pay_3: repayment status in july, 2005 (scale same as above)\n",
      "pay_4: repayment status in june, 2005 (scale same as above)\n",
      "pay_5: repayment status in may, 2005 (scale same as above)\n",
      "pay_6: repayment status in april, 2005 (scale same as above)\n",
      "bill_amt1: amount of bill statement in september, 2005 (nt dollar)\n",
      "bill_amt2: amount of bill statement in august, 2005 (nt dollar)\n",
      "bill_amt3: amount of bill statement in july, 2005 (nt dollar)\n",
      "bill_amt4: amount of bill statement in june, 2005 (nt dollar)\n",
      "bill_amt5: amount of bill statement in may, 2005 (nt dollar)\n",
      "bill_amt6: amount of bill statement in april, 2005 (nt dollar)\n",
      "pay_amt1: amount of previous payment in september, 2005 (nt dollar)\n",
      "pay_amt2: amount of previous payment in august, 2005 (nt dollar)\n",
      "pay_amt3: amount of previous payment in july, 2005 (nt dollar)\n",
      "pay_amt4: amount of previous payment in june, 2005 (nt dollar)\n",
      "pay_amt5: amount of previous payment in may, 2005 (nt dollar)\n",
      "pay_amt6: amount of previous payment in april, 2005 (nt dollar)\n",
      "default.payment.next.month: default payment (1=yes, 0=no)\n",
      "inspiration\n",
      "some ideas for exploration:\n",
      "how does the probability of default payment vary by categories of different demographic variables?\n",
      "which variables are the strongest predictors of default payment?\n",
      "acknowledgements\n",
      "any publications based on this dataset should acknowledge the following:\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "the original dataset can be found here at the uci machine learning repository.\n",
      "emergency (911) calls: fire, traffic, ems for montgomery county, pa\n",
      "you can get a quick introduction to this dataset with this kernel: dataset walk-through\n",
      "acknowledgements: data provided by montcoalert.org\n",
      "this dataset is originally from the national institute of diabetes and digestive and kidney diseases. the objective is to predict based on diagnostic measurements whether a patient has diabetes.\n",
      "dataset information\n",
      "several constraints were placed on the selection of these instances from a larger database. in particular, all patients here are females at least 21 years old of pima indian heritage.\n",
      "relevant papers\n",
      "smith, j.w., everhart, j.e., dickson, w.c., knowler, w.c., & johannes, r.s. (1988). using the adap learning algorithm to forecast the onset of diabetes mellitus. in proceedings of the symposium on computer applications and medical care (pp. 261--265). ieee computer society press.\n",
      "the search for new earths\n",
      "github\n",
      "the data describe the change in flux (light intensity) of several thousand stars. each star has a binary label of 2 or 1. 2 indicated that that the star is confirmed to have at least one exoplanet in orbit; some observations are in fact multi-planet systems.\n",
      "as you can imagine, planets themselves do not emit light, but the stars that they orbit do. if said star is watched over several months or years, there may be a regular 'dimming' of the flux (the light intensity). this is evidence that there may be an orbiting body around the star; such a star could be considered to be a 'candidate' system. further study of our candidate system, for example by a satellite that captures light at a different wavelength, could solidify the belief that the candidate can in fact be 'confirmed'.\n",
      "in the above diagram, a star is orbited by a blue planet. at t = 1, the starlight intensity drops because it is partially obscured by the planet, given our position. the starlight rises back to its original value at t = 2. the graph in each box shows the measured flux (light intensity) at each time interval.\n",
      "description\n",
      "trainset:\n",
      "5087 rows or observations.\n",
      "3198 columns or features.\n",
      "column 1 is the label vector. columns 2 - 3198 are the flux values over time.\n",
      "37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.\n",
      "testset:\n",
      "570 rows or observations.\n",
      "3198 columns or features.\n",
      "column 1 is the label vector. columns 2 - 3198 are the flux values over time.\n",
      "5 confirmed exoplanet-stars and 565 non-exoplanet-stars.\n",
      "acknowledgements\n",
      "the data presented here are cleaned and are derived from observations made by the nasa kepler space telescope. the mission is ongoing - for instance data from campaign 12 was released on 8th march 2017. over 99% of this dataset originates from campaign 3. to boost the number of exoplanet-stars in the dataset, confirmed exoplanets from other campaigns were also included.\n",
      "to be clear, all observations from campaign 3 are included. and in addition to this, confirmed exoplanet-stars from other campaigns are also included.\n",
      "the datasets were prepared late-summer 2016.\n",
      "campaign 3 was used because 'it was felt' that this campaign is unlikely to contain any undiscovered (i.e. wrongly labelled) exoplanets.\n",
      "nasa open-sources the original kepler mission data and it is hosted at the mikulski archive. after being beamed down to earth, nasa applies de-noising algorithms to remove artefacts generated by the telescope. the data - in the .fits format - is stored online. and with the help of a seasoned astrophysicist, anyone with an internet connection can embark on a search to find and retrieve the datafiles from the archive.\n",
      "the cover image is copyright © 2011 by dan lessmann\n",
      "recently reddit released an enormous dataset containing all ~1.7 billion of their publicly available comments. the full dataset is an unwieldy 1+ terabyte uncompressed, so we've decided to host a small portion of the comments here for kagglers to explore. (you don't even need to leave your browser!)\n",
      "you can find all the comments from may 2015 on scripts for your natural language processing pleasure. what had redditors laughing, bickering, and nsfw-ing this spring?\n",
      "who knows? top visualizations may just end up on reddit.\n",
      "data description\n",
      "the database has one table, may2015, with the following fields:\n",
      "created_utc\n",
      "ups\n",
      "subreddit_id\n",
      "link_id\n",
      "name\n",
      "score_hidden\n",
      "author_flair_css_class\n",
      "author_flair_text\n",
      "subreddit\n",
      "id\n",
      "removal_reason\n",
      "gilded\n",
      "downs\n",
      "archived\n",
      "author\n",
      "score\n",
      "retrieved_on\n",
      "body\n",
      "distinguished\n",
      "edited\n",
      "controversiality\n",
      "parent_id\n",
      "context\n",
      "blockchain technology, first implemented by satoshi nakamoto in 2009 as a core component of bitcoin, is a distributed, public ledger recording transactions. its usage allows secure peer-to-peer communication by linking blocks containing hash pointers to a previous block, a timestamp, and transaction data. bitcoin is a decentralized digital currency (cryptocurrency) which leverages the blockchain to store transactions in a distributed manner in order to mitigate against flaws in the financial industry.\n",
      "nearly ten years after its inception, bitcoin and other cryptocurrencies experienced an explosion in popular awareness. the value of bitcoin, on the other hand, has experienced more volatility. meanwhile, as use cases of bitcoin and blockchain grow, mature, and expand, hype and controversy have swirled.\n",
      "content\n",
      "in this dataset, you will have access to information about blockchain blocks and transactions. all historical data are in the bigquery-public-data:bitcoin_blockchain dataset. it’s updated it every 10 minutes. the data can be joined with historical prices in kernels. see available similar datasets here: https://www.kaggle.com/datasets?search=bitcoin.\n",
      "querying bigquery tables\n",
      "you can use the bigquery python client library to query tables in this dataset in kernels. note that methods available in kernels are limited to querying data. tables are at bigquery-public-data.bitcoin_blockchain.[tablename]. fork this kernel to get started.\n",
      "method & acknowledgements\n",
      "allen day (twitter | medium), google cloud developer advocate & colin bookman, google cloud customer engineer retrieve data from the bitcoin network using a custom client available on github that they built with the bitcoinj java library. historical data from the origin block to 2018-01-31 were loaded in bulk to two bigquery tables, blocks_raw and transactions. these tables contain fresh data, as they are now appended when new blocks are broadcast to the bitcoin network. for additional information visit the google cloud big data and machine learning blog post \"bitcoin in bigquery: blockchain analytics on public data\".\n",
      "photo by andre francois on unsplash.\n",
      "inspiration\n",
      "how many bitcoins are sent each day?\n",
      "how many addresses receive bitcoin each day?\n",
      "compare transaction volume to historical prices by joining with other available data sources\n",
      "every year, stack overflow conducts a massive survey of people on the site, covering all sorts of information like programming languages, salary, code style and various other information. this year, they amassed more than 64,000 responses fielded from 213 countries.\n",
      "data\n",
      "the data is made up of two files:\n",
      "1. survey_results_public.csv - csv file with main survey results, one respondent per row and one column per answer\n",
      "2. survey_results_schema.csv - csv file with survey schema, i.e., the questions that correspond to each column name m\n",
      "acknowledgements\n",
      "data is directly taken from stackoverflow and licensed under the odbl license.\n",
      "it's no secret that us university students often graduate with debt repayment obligations that far outstrip their employment and income prospects. while it's understood that students from elite colleges tend to earn more than graduates from less prestigious universities, the finer relationships between future income and university attendance are quite murky. in an effort to make educational investments less speculative, the us department of education has matched information from the student financial aid system with federal tax returns to create the college scorecard dataset.\n",
      "kaggle is hosting the college scorecard dataset in order to facilitate shared learning and collaboration. insights from this dataset can help make the returns on higher education more transparent and, in turn, more fair.\n",
      "data description\n",
      "here's a script showing an exploratory overview of some of the data.\n",
      "college-scorecard-release-*.zip contains a compressed version of the same data available through kaggle scripts.\n",
      "it consists of three components:\n",
      "all the raw data files released in version 1.40 of the college scorecard data\n",
      "scorecard.csv, a single csv file with all the years data combined. in it, we've converted categorical variables represented by integer keys in the original data to their labels and added a year column\n",
      "database.sqlite, a sqlite database containing a single scorecard table that contains the same information as scorecard.csv\n",
      "new to data exploration in r? take the free, interactive datacamp course, \"data exploration with kaggle scripts,\" to learn the basics of visualizing data with ggplot. you'll also create your first kaggle scripts along the way.\n",
      "context\n",
      "this data set contains information on user preference data from 73,516 users on 12,294 anime. each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings.\n",
      "content\n",
      "anime.csv\n",
      "anime_id - myanimelist.net's unique id identifying an anime.\n",
      "name - full name of anime.\n",
      "genre - comma separated list of genres for this anime.\n",
      "type - movie, tv, ova, etc.\n",
      "episodes - how many episodes in this show. (1 if movie).\n",
      "rating - average rating out of 10 for this anime.\n",
      "members - number of community members that are in this anime's \"group\".\n",
      "rating.csv\n",
      "user_id - non identifiable randomly generated user id.\n",
      "anime_id - the anime that this user has rated.\n",
      "rating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).\n",
      "acknowledgements\n",
      "thanks to myanimelist.net api for providing anime data and user ratings.\n",
      "inspiration\n",
      "building a better anime recommendation system based only on user viewing history.\n",
      "about\n",
      "a dataset containing the attributes and the ratings for around 94,000 among board games and expansions as get from boardgamegeek.\n",
      "resources\n",
      "the same data and the scripts used to crawl it from boardgamegeek are available in the form of r package on github.\n",
      "description\n",
      "we collected eeg signal data from 10 college students while they watched mooc video clips. we extracted online education videos that are assumed not to be confusing for college students, such as videos of the introduction of basic algebra or geometry. we also prepare videos that are expected to confuse a typical college student if a student is not familiar with the video topics like quantum mechanics, and stem cell research. we prepared 20 videos, 10 in each category. each video was about 2 minutes long. we chopped the two-minute clip in the middle of a topic to make the videos more confusing. the students wore a single-channel wireless mindset that measured activity over the frontal lobe. the mindset measures the voltage between an electrode resting on the forehead and two electrodes (one ground and one reference) each in contact with an ear. after each session, the student rated his/her confusion level on a scale of 1-7, where one corresponded to the least confusing and seven corresponded to the most confusing. these labels if further normalized into labels of whether the students are confused or not. this label is offered as self-labelled confusion in addition to our predefined label of confusion.\n",
      "data information:\n",
      "-----data.csv\n",
      "column 1: subject id\n",
      "column 2: video id\n",
      "column 3: attention (proprietary measure of mental focus)\n",
      "column 4: mediation (proprietary measure of calmness)\n",
      "column 5: raw (raw eeg signal)\n",
      "column 6: delta (1-3 hz of power spectrum)\n",
      "column 7: theta (4-7 hz of power spectrum)\n",
      "column 8: alpha 1 (lower 8-11 hz of power spectrum)\n",
      "column 9: alpha 2 (higher 8-11 hz of power spectrum)\n",
      "column 10: beta 1 (lower 12-29 hz of power spectrum)\n",
      "column 11: beta 2 (higher 12-29 hz of power spectrum)\n",
      "column 12: gamma 1 (lower 30-100 hz of power spectrum)\n",
      "column 13: gamma 2 (higher 30-100 hz of power spectrum)\n",
      "column 14: predefined label (whether the subject is expected to be confused)\n",
      "column 15: user-defined label (whether the subject is actually confused)\n",
      "-----subject demographic\n",
      "column 1: subject id\n",
      "column 2: age\n",
      "column 3: ethnicity (categorized according to https://en.wikipedia.org/wiki/list_of_contemporary_ethnic_groups)\n",
      "column 4: gender\n",
      "-----video data\n",
      "each video lasts roughly two-minute long, we remove the first 30 seconds and last 30 seconds, only collect the eeg data during the middle 1 minute.\n",
      "format\n",
      "these data are collected from ten students, each watching ten videos.\n",
      "therefore, it can be seen as only 100 data points for these 12000+ rows. if you look at this way, then each data point consists of 120+ rows, which is sampled every 0.5 seconds (so each data point is a one minute video). signals with higher frequency are reported as the mean value during each 0.5 second.\n",
      "reference:\n",
      "wang, h., li, y., hu, x., yang, y., meng, z., & chang, k. m. (2013, june). using eeg to improve massive open online courses feedback interaction. in aied workshops. [pdf]\n",
      "data collection\n",
      "the data is collected from a software that we implemented ourselves. check haohanwang/bioimaging for the source code.\n",
      "inspiration\n",
      "this dataset is an extremely challenging data set to perform binary classification. 65% of prediction accuracy is quite decent according to our experience.\n",
      "it is an interesting data set to carry out the variable selection (causal inference) task that may help further research. past research has indicated that theta signal is correlated with confusion level.\n",
      "it is also an interesting data set for confounding factors correction model because we offer two labels (subject id and video id) that could profoundly confound the results.\n",
      "warning\n",
      "the data for subject 3 might be corrupted.\n",
      "other resources\n",
      "promotion video\n",
      "source code of data collection software\n",
      "contact\n",
      "haohan wang\n",
      "baffled why your team traded for that 34-year-old pitcher? convinced you can create a new and improved version of war? wondering what made the 1907 cubs great and if can they do it again?\n",
      "the history of baseball is a reformatted version of the famous lahman’s baseball database. it contains major league baseball’s complete batting and pitching statistics from 1871 to 2015, plus fielding statistics, standings, team stats, park stats, player demographics, managerial records, awards, post-season data, and more.\n",
      "scripts, kaggle’s free, in-browser analytics tool, makes it easy to share detailed sabermetrics, predict the next hall of fame inductee, illustrate how speed scores runs, or publish a definitive analysis on why the los angeles dodgers will never win another world series.\n",
      "we have more ideas for analysis than games in a season, but here are a few we’d really love to see:\n",
      "is there a most error-prone position?\n",
      "when do players at different positions peak?\n",
      "are the best performers selected for all-star game?\n",
      "how many walks does it take for a starting pitcher to get pulled?\n",
      "do players with a high ground into double play (gidp) have a lower batting average?\n",
      "which players are the most likely to choke during the post-season?\n",
      "why should or shouldn’t the national league adopt the designated hitter rule?\n",
      "see the full sqlite schema.\n",
      "context\n",
      "this dataset contains all noise complaints calls that were received by the city police with complaint type \"loud music/party\" in 2016. the data contains the time of the call, time of the police response, coordinates and part of the city.\n",
      "this data should help match taxi rides from \"new york city taxi trip duration\" competition to the night rides of partygoers.\n",
      "content\n",
      "the new york city hotline receives non-urgent community concerns, which are made public by the city through nyc open data portal. the full dataset contains a variety of complaints ranging from illegal parking to customer complaints. this dataset focuses on noise complaints that were collected in 2016 and indicate ongoing party in a given neighborhood.\n",
      "parties_in_nyc.csv:\n",
      "columns:\n",
      "created date - time of the call\n",
      "closed date - time when ticket was closed by police\n",
      "location type - type of the location\n",
      "incident zip - zip code of the location\n",
      "city - name of the city (almost the same as the borough field)\n",
      "borough - administrative division of the city\n",
      "latitude - latitude of the location\n",
      "longitude - longitude of the location\n",
      "\n",
      "test_parties and train_parties:\n",
      "columns:\n",
      "id - id of the ride\n",
      "num_complaints - number of noise complaints about ongoing parties within ~500 meters and within 2 hours of pickup place and time\n",
      "acknowledgements\n",
      "https://opendata.cityofnewyork.us/ - nyc open data portal contains many other interesting datasets photo by yvette de wit on unsplash\n",
      "inspiration\n",
      "after a fun night out in the city majority of people are too exhausted to travel by public transport, so they catch a cab to their home. i hope this data will help the community to find the patterns in the data that will lead to better solutions.\n",
      "context: this data set contains published itraq proteome profiling of 77 breast cancer samples generated by the clinical proteomic tumor analysis consortium (nci/nih). it contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.\n",
      "content:\n",
      "file: 77_cancer_proteomes_cptac_itraq.csv\n",
      "refseq_accession_number: refseq protein id (each protein has a unique id in a refseq database)\n",
      "gene_symbol: a symbol unique to each gene (every protein is encoded by some gene)\n",
      "gene_name: a full name of that gene remaining columns: log2 itraq ratios for each sample (protein expression data, most important), three last columns are from healthy individuals\n",
      "file: clinical_data_breast_cancer.csv\n",
      "first column \"complete tcga id\" is used to match the sample ids in the main cancer proteomes file (see example script). all other columns have self-explanatory names, contain data about the cancer classification of a given sample using different methods. 'pam50 mrna' classification is being used in the example script.\n",
      "file: pam50_proteins.csv\n",
      "contains the list of genes and proteins used by the pam50 classification system. the column refseqproteinid contains the protein ids that can be matched with the ids in the main protein expression data set.\n",
      "past research: the original study: http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning)\n",
      "in brief: the data were used to assess how the mutations in the dna are affecting the protein expression landscape in breast cancer. genes in our dna are first transcribed into rna molecules which then are translated into proteins. changing the information content of dna has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, dna repair, enzymatic reactions and signaling etc. they performed k-means clustering on the protein data to divide the breast cancer patients into sub-types, each having unique protein expression signature. they found that the best clustering was achieved using 3 clusters (original pam50 gene set yields four different subtypes using rna data).\n",
      "inspiration:\n",
      "this is an interesting study and i myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that i am performing as a part of my phd. however, i though that the kaggle community (or at least that part with biomedical interests) would enjoy playing with it. i added a simple k-means clustering example for that data with some comments, the same approach as used in the original paper. one thing is that there is a panel of genes, the pam50 which is used to classify breast cancers into subtypes. this panel was originally based on the rna expression data which is (in my opinion) not as robust as the measurement of mrna's final product, the protein. perhaps using this data set, someone could find a different set of proteins (they all have unique np_/xp_ identifiers) that would divide the data set even more robustly? perhaps into a higher numbers of clusters with very distinct protein expression signatures?\n",
      "example k-means analysis script: http://pastebin.com/a0wj41dp\n",
      "context\n",
      "these are the lyrics for 57650 songs. they can be used for natural language processing purposes, such as clustering of the words with similar meanings or predicting artist by the song. the dataset can be expanded with some more features for more advanced research like sentiment analysis. the data is not modified, only slightly cleaned, which gives a lot of freedom to devise your own applications.\n",
      "mining\n",
      "i have mined this dataset as a corpus for my nlp studies. however, before performing any transformation to bag-of-words or bag-of-n-grams, i decided to share the data. the data has been acquired from lyricsfreak through scraping. then i did some very basic work on removing inconvenient data: non-english lyrics, extremely short and extremely long lyrics, lyrics with non-ascii symbols. however, there's still work to be done in terms of data preparation.\n",
      "content\n",
      "the dataset contains 4 columns:\n",
      "artist\n",
      "song name\n",
      "link to a webpage with the song (for reference). this is to be concatenated with http://www.lyricsfreak.com to form a real url.\n",
      "lyrics of the song, unmodified.\n",
      "acknowledgements\n",
      "i would like to acknowledge lyricsfreak, which is the direct source of the data.\n",
      "context\n",
      "i'm a crowdfunding enthusiast and i'm watching kickstarter since its early days. right now i just collect data and the only app i've made is this twitter bot which tweet any project reaching some milestone: @bloomwatcher . i have a lot of other ideas, but sadly not enough time to develop them... but i hope you can!\n",
      "content\n",
      "you'll find most useful data for project analysis. columns are self explanatory except:\n",
      "usd_pledged: conversion in us dollars of the pledged column (conversion done by kickstarter).\n",
      "usd pledge real: conversion in us dollars of the pledged column (conversion from fixer.io api).\n",
      "usd goal real: conversion in us dollars of the goal column (conversion from fixer.io api).\n",
      "acknowledgements\n",
      "data are collected from kickstarter platform\n",
      "usd conversion (usd_pledged_real and usd_goal_real columns) were generated from convert ks pledges to usd script done by tonyplaysguitar\n",
      "inspiration\n",
      "i hope to see great projects, and why not a model to predict if a project will be successful before it is released? :)\n",
      "students' academic performance dataset (xapi-edu-data)\n",
      "data set characteristics: multivariate\n",
      "number of instances: 480\n",
      "area: e-learning, education, predictive models, educational data mining\n",
      "attribute characteristics: integer/categorical\n",
      "number of attributes: 16\n",
      "date: 2016-11-8\n",
      "associated tasks: classification\n",
      "missing values? no\n",
      "file formats: xapi-edu-data.csv\n",
      "source:\n",
      "elaf abu amrieh, thair hamtini, and ibrahim aljarah, the university of jordan, amman, jordan, http://www.ibrahimaljarah.com www.ju.edu.jo\n",
      "dataset information:\n",
      "this is an educational data set which is collected from learning management system (lms) called kalboard 360. kalboard 360 is a multi-agent lms, which has been designed to facilitate learning through the use of leading-edge technology. such system provides users with a synchronous access to educational resources from any device with internet connection.\n",
      "the data is collected using a learner activity tracker tool, which called experience api (xapi). the xapi is a component of the training and learning architecture (tla) that enables to monitor learning progress and learner’s actions like reading an article or watching a training video. the experience api helps the learning activity providers to determine the learner, activity and objects that describe a learning experience. the dataset consists of 480 student records and 16 features. the features are classified into three major categories: (1) demographic features such as gender and nationality. (2) academic background features such as educational stage, grade level and section. (3) behavioral features such as raised hand on class, opening resources, answering survey by parents, and school satisfaction.\n",
      "the dataset consists of 305 males and 175 females. the students come from different origins such as 179 students are from kuwait, 172 students are from jordan, 28 students from palestine, 22 students are from iraq, 17 students from lebanon, 12 students from tunis, 11 students from saudi arabia, 9 students from egypt, 7 students from syria, 6 students from usa, iran and libya, 4 students from morocco and one student from venezuela.\n",
      "the dataset is collected through two educational semesters: 245 student records are collected during the first semester and 235 student records are collected during the second semester.\n",
      "the data set includes also the school attendance feature such as the students are classified into two categories based on their absence days: 191 students exceed 7 absence days and 289 students their absence days under 7.\n",
      "this dataset includes also a new category of features; this feature is parent parturition in the educational process. parent participation feature have two sub features: parent answering survey and parent school satisfaction. there are 270 of the parents answered survey and 210 are not, 292 of the parents are satisfied from the school and 188 are not.\n",
      "(see the related papers for more details).\n",
      "attributes\n",
      "1 gender - student's gender (nominal: 'male' or 'female’)\n",
      "2 nationality- student's nationality (nominal:’ kuwait’,’ lebanon’,’ egypt’,’ saudiarabia’,’ usa’,’ jordan’,’ venezuela’,’ iran’,’ tunis’,’ morocco’,’ syria’,’ palestine’,’ iraq’,’ lybia’)\n",
      "3 place of birth- student's place of birth (nominal:’ kuwait’,’ lebanon’,’ egypt’,’ saudiarabia’,’ usa’,’ jordan’,’ venezuela’,’ iran’,’ tunis’,’ morocco’,’ syria’,’ palestine’,’ iraq’,’ lybia’)\n",
      "4 educational stages- educational level student belongs (nominal: ‘lowerlevel’,’middleschool’,’highschool’)\n",
      "5 grade levels- grade student belongs (nominal: ‘g-01’, ‘g-02’, ‘g-03’, ‘g-04’, ‘g-05’, ‘g-06’, ‘g-07’, ‘g-08’, ‘g-09’, ‘g-10’, ‘g-11’, ‘g-12 ‘)\n",
      "6 section id- classroom student belongs (nominal:’a’,’b’,’c’)\n",
      "7 topic- course topic (nominal:’ english’,’ spanish’, ‘french’,’ arabic’,’ it’,’ math’,’ chemistry’, ‘biology’, ‘science’,’ history’,’ quran’,’ geology’)\n",
      "8 semester- school year semester (nominal:’ first’,’ second’)\n",
      "9 parent responsible for student (nominal:’mom’,’father’)\n",
      "10 raised hand- how many times the student raises his/her hand on classroom (numeric:0-100)\n",
      "11- visited resources- how many times the student visits a course content(numeric:0-100)\n",
      "12 viewing announcements-how many times the student checks the new announcements(numeric:0-100)\n",
      "13 discussion groups- how many times the student participate on discussion groups (numeric:0-100)\n",
      "14 parent answering survey- parent answered the surveys which are provided from school or not (nominal:’yes’,’no’)\n",
      "15 parent school satisfaction- the degree of parent satisfaction from school(nominal:’yes’,’no’)\n",
      "16 student absence days-the number of absence days for each student (nominal: above-7, under-7)\n",
      "the students are classified into three numerical intervals based on their total grade/mark:\n",
      "low-level: interval includes values from 0 to 69,\n",
      "middle-level: interval includes values from 70 to 89,\n",
      "high-level: interval includes values from 90-100.\n",
      "relevant papers:\n",
      "amrieh, e. a., hamtini, t., & aljarah, i. (2016). mining educational data to predict student’s academic performance using ensemble methods. international journal of database theory and application, 9(8), 119-136.\n",
      "amrieh, e. a., hamtini, t., & aljarah, i. (2015, november). preprocessing and analyzing educational data set using x-api for improving student's performance. in applied electrical engineering and computing technologies (aeect), 2015 ieee jordan conference on (pp. 1-5). ieee.\n",
      "citation request:\n",
      "please include these citations if you plan to use this dataset:\n",
      "amrieh, e. a., hamtini, t., & aljarah, i. (2016). mining educational data to predict student’s academic performance using ensemble methods. international journal of database theory and application, 9(8), 119-136.\n",
      "amrieh, e. a., hamtini, t., & aljarah, i. (2015, november). preprocessing and analyzing educational data set using x-api for improving student's performance. in applied electrical engineering and computing technologies (aeect), 2015 ieee jordan conference on (pp. 1-5). ieee.\n",
      "context\n",
      "stock market data can be interesting to analyze and as a further incentive, strong predictive models can have large financial payoff. the amount of financial data on the web is seemingly endless. a large and well structured dataset on a wide array of companies can be hard to come by. here i provide a dataset with historical stock prices (last 5 years) for all companies currently found on the s&p 500 index.\n",
      "the script i used to acquire all of these .csv files can be found in this github repository in the future if you wish for a more up to date dataset, this can be used to acquire new versions of the .csv files.\n",
      "feb 2018 note: i have just updated the dataset to include data up to feb 2018. i have also accounted for changes in the stocks on the s&p 500 index (rip whole foods etc. etc.).\n",
      "content\n",
      "the data is presented in a couple of formats to suit different individual's needs or computational limitations. i have included files containing 5 years of stock data (in the all_stocks_5yr.csv and corresponding folder).\n",
      "the folder individual_stocks_5yr contains files of data for individual stocks, labelled by their stock ticker name. the all_stocks_5yr.csv contains the same data, presented in a merged .csv file. depending on the intended use (graphing, modelling etc.) the user may prefer one of these given formats.\n",
      "all the files have the following columns: date - in format: yy-mm-dd\n",
      "open - price of the stock at market open (this is nyse data so all in usd)\n",
      "high - highest price reached in the day\n",
      "low close - lowest price reached in the day\n",
      "volume - number of shares traded\n",
      "name - the stock's ticker name\n",
      "acknowledgements\n",
      "due to volatility in google finance, for the newest version i have switched over to acquiring the data from the investor's exchange api, the simple script i use to do this is found here. special thanks to kaggle, github, pandas_datareader and the market.\n",
      "inspiration\n",
      "this dataset lends itself to a some very interesting visualizations. one can look at simple things like how prices change over time, graph an compare multiple stocks at once, or generate and graph new metrics from the data provided. from these data informative stock stats such as volatility and moving averages can be easily calculated. the million dollar question is: can you develop a model that can beat the market and allow you to make statistically informed trades!\n",
      "cervical cancer risk factors for biopsy: this dataset is obtained from uci repository and kindly acknowledged!\n",
      "this file contains a list of risk factors for cervical cancer leading to a biopsy examination!\n",
      "about 11,000 new cases of invasive cervical cancer are diagnosed each year in the u.s. however, the number of new cervical cancer cases has been declining steadily over the past decades. although it is the most preventable type of cancer, each year cervical cancer kills about 4,000 women in the u.s. and about 300,000 women worldwide. in the united states, cervical cancer mortality rates plunged by 74% from 1955 - 1992 thanks to increased screening and early detection with the pap test. age fifty percent of cervical cancer diagnoses occur in women ages 35 - 54, and about 20% occur in women over 65 years of age. the median age of diagnosis is 48 years. about 15% of women develop cervical cancer between the ages of 20 - 30. cervical cancer is extremely rare in women younger than age 20. however, many young women become infected with multiple types of human papilloma virus, which then can increase their risk of getting cervical cancer in the future. young women with early abnormal changes who do not have regular examinations are at high risk for localized cancer by the time they are age 40, and for invasive cancer by age 50. socioeconomic and ethnic factors although the rate of cervical cancer has declined among both caucasian and african-american women over the past decades, it remains much more prevalent in african-americans -- whose death rates are twice as high as caucasian women. hispanic american women have more than twice the risk of invasive cervical cancer as caucasian women, also due to a lower rate of screening. these differences, however, are almost certainly due to social and economic differences. numerous studies report that high poverty levels are linked with low screening rates. in addition, lack of health insurance, limited transportation, and language difficulties hinder a poor woman’s access to screening services. high sexual activity human papilloma virus (hpv) is the main risk factor for cervical cancer. in adults, the most important risk factor for hpv is sexual activity with an infected person. women most at risk for cervical cancer are those with a history of multiple sexual partners, sexual intercourse at age 17 years or younger, or both. a woman who has never been sexually active has a very low risk for developing cervical cancer. sexual activity with multiple partners increases the likelihood of many other sexually transmitted infections (chlamydia, gonorrhea, syphilis).studies have found an association between chlamydia and cervical cancer risk, including the possibility that chlamydia may prolong hpv infection. family history women have a higher risk of cervical cancer if they have a first-degree relative (mother, sister) who has had cervical cancer. use of oral contraceptives studies have reported a strong association between cervical cancer and long-term use of oral contraception (oc). women who take birth control pills for more than 5 - 10 years appear to have a much higher risk hpv infection (up to four times higher) than those who do not use ocs. (women taking ocs for fewer than 5 years do not have a significantly higher risk.) the reasons for this risk from oc use are not entirely clear. women who use ocs may be less likely to use a diaphragm, condoms, or other methods that offer some protection against sexual transmitted diseases, including hpv. some research also suggests that the hormones in ocs might help the virus enter the genetic material of cervical cells. having many children studies indicate that having many children increases the risk for developing cervical cancer, particularly in women infected with hpv. smoking smoking is associated with a higher risk for precancerous changes (dysplasia) in the cervix and for progression to invasive cervical cancer, especially for women infected with hpv. immunosuppression women with weak immune systems, (such as those with hiv / aids), are more susceptible to acquiring hpv. immunocompromised patients are also at higher risk for having cervical precancer develop rapidly into invasive cancer. diethylstilbestrol (des) from 1938 - 1971, diethylstilbestrol (des), an estrogen-related drug, was widely prescribed to pregnant women to help prevent miscarriages. the daughters of these women face a higher risk for cervical cancer. des is no longer prsecribed.\n",
      "this dataset contains us stocks fundamental data, such as income statement, balance sheet and cash flows.\n",
      "12,129 companies\n",
      "8,526 unique indicators\n",
      "~20 indicators comparable across most companies\n",
      "five years of data, yearly\n",
      "the data is provided by http://usfundamentals.com.\n",
      "context\n",
      "ray kroc wanted to build a restaurant system that would be famous for providing food of consistently high quality and uniform methods of preparation. he wanted to serve burgers, buns, fries and beverages that tasted just the same in alaska as they did in alabama. to achieve this, he chose a unique path: persuading both franchisees and suppliers to buy into his vision, working not for mcdonald’s but for themselves, together with mcdonald’s. many of mcdonald’s most famous menu items – like the big mac, filet-o-fish, and egg mcmuffin – were created by franchisees.\n",
      "content\n",
      "this dataset provides a nutrition analysis of every menu item on the us mcdonald's menu, including breakfast, beef burgers, chicken and fish sandwiches, fries, salads, soda, coffee and tea, milkshakes, and desserts.\n",
      "acknowledgements\n",
      "the menu items and nutrition facts were scraped from the mcdonald's website.\n",
      "inspiration\n",
      "how many calories does the average mcdonald's value meal contain? how much do beverages, like soda or coffee, contribute to the overall caloric intake? does ordered grilled chicken instead of crispy increase a sandwich's nutritional value? what about ordering egg whites instead of whole eggs? what is the least number of items could you order from the menu to meet one day's nutritional requirements?\n",
      "start a new kernel\n",
      "crime data for philadelphia\n",
      "to get started quickly, take a look at philly data crime walk-through.\n",
      "data was provided by opendataphilly\n",
      "the human activity recognition database was built from the recordings of 30 study participants performing activities of daily living (adl) while carrying a waist-mounted smartphone with embedded inertial sensors. the objective is to classify activities into one of the six activities performed.\n",
      "description of experiment\n",
      "the experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. each person performed six activities (walking, walking_upstairs, walking_downstairs, sitting, standing, laying) wearing a smartphone (samsung galaxy s ii) on the waist. using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50hz. the experiments have been video-recorded to label the data manually. the obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n",
      "the sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). the sensor acceleration signal, which has gravitational and body motion components, was separated using a butterworth low-pass filter into body acceleration and gravity. the gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 hz cutoff frequency was used. from each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n",
      "attribute information\n",
      "for each record in the dataset the following is provided:\n",
      "triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n",
      "triaxial angular velocity from the gyroscope.\n",
      "a 561-feature vector with time and frequency domain variables.\n",
      "its activity label.\n",
      "an identifier of the subject who carried out the experiment.\n",
      "relevant papers\n",
      "davide anguita, alessandro ghio, luca oneto, xavier parra and jorge l. reyes-ortiz. human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. international workshop of ambient assisted living (iwaal 2012). vitoria-gasteiz, spain. dec 2012\n",
      "davide anguita, alessandro ghio, luca oneto, xavier parra, jorge l. reyes-ortiz. energy efficient smartphone-based activity recognition using fixed-point arithmetic. journal of universal computer science. special issue in ambient assisted living: home care. volume 19, issue 9. may 2013\n",
      "davide anguita, alessandro ghio, luca oneto, xavier parra and jorge l. reyes-ortiz. human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. 4th international workshop of ambient assited living, iwaal 2012, vitoria-gasteiz, spain, december 3-5, 2012. proceedings. lecture notes in computer science 2012, pp 216-223.\n",
      "jorge luis reyes-ortiz, alessandro ghio, xavier parra-llanas, davide anguita, joan cabestany, andreu català. human activity and motion disorder recognition: towards smarter interactive cognitive environments. 21st european symposium on artificial neural networks, computational intelligence and machine learning, esann 2013. bruges, belgium 24-26 april 2013.\n",
      "citation\n",
      "davide anguita, alessandro ghio, luca oneto, xavier parra and jorge l. reyes-ortiz. a public domain dataset for human activity recognition using smartphones. 21st european symposium on artificial neural networks, computational intelligence and machine learning, esann 2013. bruges, belgium 24-26 april 2013.\n",
      "context\n",
      "the ethereum blockchain gives a revolutionary way of decentralized applications and provides its own cryptocurrency. ethereum is a decentralized platform that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third party interference. these apps run on a custom built blockchain, an enormously powerful shared global infrastructure that can move value around and represent the ownership of property. this enables developers to create markets, store registries of debts or promises, move funds in accordance with instructions given long in the past (like a will or a futures contract) and many other things that have not been invented yet, all without a middle man or counterparty risk.\n",
      "content\n",
      "what you may see in the csvs are just numbers, but there is more to this. numbers make machine learning easy. i've labeled each column, the first in all of them is the day; it may look weird but it makes sense if you look closely.\n",
      "note:\n",
      "timestamp format\n",
      "how to convert timestamp in python:\n",
      "import datetime as dt\n",
      "# the (would-be) timestamp value is below\n",
      "timestamp = 1339521878.04 \n",
      "# technechly you would iterate through and change them all if you were graphing\n",
      "timevalue = dt.datetime.fromtimestamp(timestamp)\n",
      "#year, month, day, hour, minute, second\n",
      "print(timevalue.strftime('%y-%m-%d %h:%m:%s'))\n",
      "acknowledgements\n",
      "mr. vitalik buterin. co-founder of ethereum and as a co-founder of bitcoin magazine.\n",
      "hit a brother up\n",
      "0x767e8b211f70c5b8b4caa38c2efe05bf8eac0da7\n",
      "will be updating every month with new ethereum history!\n",
      "context\n",
      "typically e-commerce datasets are proprietary and consequently hard to find among publicly available data. however, the uci machine learning repository has made this dataset containing actual transactions from 2010 and 2011. the dataset is maintained on their site, where it can be found by the title \"online retail\".\n",
      "content\n",
      "\"this is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a uk-based and registered non-store online retail.the company mainly sells unique all-occasion gifts. many customers of the company are wholesalers.\"\n",
      "acknowledgements\n",
      "per the uci machine learning repository, this data was made available by dr daqing chen, director: public analytics group. chend '@' lsbu.ac.uk, school of engineering, london south bank university, london se1 0aa, uk.\n",
      "image from stocksnap.io.\n",
      "inspiration\n",
      "analyses for this dataset could include time series, clustering, classification and more.\n",
      "introduction\n",
      "data and code behind the stories and interactives at fivethirtyeight. there are 80 datasets here, included in one place. this allows you to make comparisons and utilize multiple datasets.\n",
      "airline-safety\n",
      "alcohol-consumption\n",
      "avengers\n",
      "bad-drivers\n",
      "bechdel\n",
      "biopics\n",
      "births\n",
      "bob-ross\n",
      "buster-posey-mvp\n",
      "classic-rock\n",
      "college-majors\n",
      "comic-characters\n",
      "comma-survey-data\n",
      "congress-age\n",
      "cousin-marriage\n",
      "... for the complete list, see the zip files listed here\n",
      "python\n",
      "if you're planning to use python, you might want to take a look at how to read datasets, which helps navigate the zipped datasets. again, there are multiple, 80 datasets, in this dataset.\n",
      "r\n",
      "if you're planning to use r, it's included in the kernels. no need to unzip files. for example, if you wanted to load \"bechdel\", you could use the following commands.\n",
      "library(fivethirtyeight)\n",
      "data(package = \"fivethirtyeight\")\n",
      "head(bechdel) \n",
      "reference r quick start, for an example.\n",
      "references\n",
      "the following is fivethirtyeight's public repository on github.\n",
      "https://github.com/fivethirtyeight/data\n",
      "the cran package is maintained at the following link:\n",
      "https://github.com/rudeboybert/fivethirtyeight\n",
      "license\n",
      "copyright (c) 2014 espn internet ventures\n",
      "permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"software\"), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\n",
      "the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.\n",
      "the software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.\n",
      "github is how people build software and is home to the largest community of open source developers in the world, with over 12 million people contributing to 31 million projects on github since 2008.\n",
      "this 3tb+ dataset comprises the largest released source of github activity to date. it contains a full snapshot of the content of more than 2.8 million open source github repositories including more than 145 million unique commits, over 2 billion different file paths, and the contents of the latest revision for 163 million files, all of which are searchable with regular expressions.\n",
      "querying bigquery tables\n",
      "you can use the bigquery python client library to query tables in this dataset in kernels. note that methods available in kernels are limited to querying data. tables are at bigquery-public-data.github_repos.[tablename]. fork this kernel to get started to learn how to safely manage analyzing large bigquery datasets.\n",
      "acknowledgements\n",
      "this dataset was made available per github's terms of service.\n",
      "inspiration\n",
      "this is the perfect dataset for fighting language wars.\n",
      "can you identify any signals that predict which packages or languages will become popular, in advance of their mass adoption?\n",
      "this dataset contains the characters, locations, episode details, and script lines for approximately 600 simpsons episodes, dating back to 1989.\n",
      "inspiration and credit for gathering the data goes to todd schneider:\n",
      "http://toddwschneider.com/posts/the-simpsons-by-the-data/\n",
      "https://github.com/toddwschneider/flim-springfield\n",
      "context\n",
      "in november, the united states will elect a new president. before then, three presidential debates will take place between hillary clinton and donald trump as well as one vice presidential debate between tim kaine and mike pence. while you can watch the debates live, why not also read deeply into the candidates' responses with text analytics?\n",
      "you can now answer any questions you have about the platforms of our presidential hopefuls or their speaking skills.\n",
      "which candidate is the most given to loquaciousness?\n",
      "how many times does clinton get interrupted?\n",
      "who gets the most audience applause?\n",
      "when is positive sentiment at its highest during the candidates' word play?\n",
      "content & acknowledgements\n",
      "for consistency, full transcripts of the debates were all taken from the washington post who made annotated transcripts available following each debate:\n",
      "first debate taking place september 26th, 2016 was obtained from the washington post.\n",
      "the vice presidential debate from october 4th, 2016 was similarly obtained here.\n",
      "the \"town hall\" presidential debate on october 9th is found here.\n",
      "the final presidential debate taking place on october 19th is found here.\n",
      "please make any dataset suggestions or requests on the forum. word cloud from debate visualization by jane yu.\n",
      "310 observations, 13 attributes (12 numeric predictors, 1 binary class attribute - no demographics)\n",
      "lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. typical sources of low back pain include:\n",
      "the large nerve roots in the low back that go to the legs may be irritated\n",
      "the smaller nerves that supply the low back may be irritated\n",
      "the large paired lower back muscles (erector spinae) may be strained\n",
      "the bones, ligaments or joints may be damaged\n",
      "an intervertebral disc may be degenerating\n",
      "an irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.\n",
      "while lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. a simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\n",
      "this data set is about to identify a person is abnormal or normal using collected physical spine details/data.\n",
      "content\n",
      "the data-set contains aggregate individual statistics for 67 nba seasons. from basic box-score attributes such as points, assists, rebounds etc., to more advanced money-ball like features such as value over replacement.\n",
      "acknowledgements\n",
      "the data was scraped from basketball-reference take a look in their glossary for a detailed column description glossary\n",
      "welcome to weedle's cave. will you be able to predict the outcome of future matches?\n",
      "to do it you will have the pokemon characteristics and the results of previous combats.\n",
      "three files are available. the first one contains the pokemon characteristics (the first column being the id of the pokemon). the second one contains information about previous combats. the first two columns contain the ids of the combatants and the third one the id of the winner. important: the pokemon in the first columns attacks first.\n",
      "the goal is to develop a machine learning model able to predict the result of future pokemon combats.\n",
      "if you have any questions, please email: t7pokemonchallenge@intelygenz.com\n",
      "disclaimer\n",
      "in intelygenz we are against animal abuse. no animal real or imaginary should be forced to fight against other. freedom for the pokemons !\n",
      "this dataset contains a list of 2410 us craft beers and 510 us breweries. the beers and breweries are linked together with an \"id\". this data was collected in january 2017 on craftcans.com. the dataset is an a tidy format and values have been cleaned up for your enjoyment.\n",
      "if you are interested in learning more about how this dataset was acquired, i wrote an extensive blogpost about it (http://www.jeannicholashould.com/python-web-scraping-tutorial-for-craft-beers.html).\n",
      "enjoy!\n",
      "context\n",
      "starbucks started as a roaster and retailer of whole bean and ground coffee, tea and spices with a single store in seattle’s pike place market in 1971. the company now operates more than 24,000 retail stores in 70 countries.\n",
      "content\n",
      "this dataset includes a record for every starbucks or subsidiary store location currently in operation as of february 2017.\n",
      "acknowledgements\n",
      "this data was scraped from the starbucks store locator webpage by github user chrismeller.\n",
      "inspiration\n",
      "what city or country has the highest number of starbucks stores per capita? what two starbucks locations are the closest in proximity to one another? what location on earth is farthest from a starbucks? how has starbucks expanded overseas?\n",
      "context\n",
      "the holy quran is the central text for 1.5 billion muslims around the world. it literally means \"the recitation.\" it is undoubtedly the finest work in arabic literature and revealed by allah (god) to his messenger prophet muhammed (peace be upon him) through angel gabriel. it was revealed verbally from december 22, 609 (ad) to 632 ad (when prophet muhammed (peace be upon him) died)\n",
      "the book is divided into 30 parts, 114 chapters and 6,000+ verses.\n",
      "there has been a lot of questions and comments on the text of this holy book given the contemporary geo-political situation of the world, wars in the middle east and afghanistan and the ongoing terrorism.\n",
      "i have put this dataset together to call my fellow data scientists to run their nlp algorithms and kernels to find and explore the sacred text by them selves.\n",
      "content\n",
      "the data contains complete holy quran in following 21 languages (so data scientists from different parts of the world can work with it). the original text was revealed in arabic. other 20 files are the translations of the original text.\n",
      "arabic (original book by god)\n",
      "english (transalation by yusuf ali)\n",
      "persian (makarim sheerazi)\n",
      "urdu (jalandhari)\n",
      "turkish (y. n. ozturk)\n",
      "portuguese (el. hayek)\n",
      "dutch (keyzer)\n",
      "norwegian (einar berg)\n",
      "italian (piccardo)\n",
      "french (hamidullah)\n",
      "german (zaidan)\n",
      "swedish (rashad kalifa)\n",
      "indonesia (bhasha indoenisan)\n",
      "bangla\n",
      "chinese/madarin\n",
      "japanese\n",
      "malay\n",
      "malayalam\n",
      "russian\n",
      "tamil\n",
      "uzbek\n",
      "inspiration\n",
      "here are some ideas to explore:\n",
      "can we make a word cloud for each chapter\n",
      "can we make a word cloud of the whole book and find out the frequency of each word\n",
      "can we describe or annotate subjects in each chapter and verse\n",
      "can we find how many times the quran has mentioned humans, women, humility, heaven or hell\n",
      "can we compare the text with other famous books and see the correlation\n",
      "can we compare the text with laws in multiple countries to see the resemblance\n",
      "any other ideas you can think of\n",
      "i am looking forward to see your work and ideas and will keep adding more ideas to explore\n",
      "welcome on board to learn the finest text on earth with data sciences and machine learning!\n",
      "updates\n",
      "complete verse-by-verse dataset has been shared. a good contribution by zohaib ali - https://www.kaggle.com/zohaib1111 (nov 20, 2017)\n",
      "what's in the deep-nlp dataset?\n",
      "sheet_1.csv contains 80 user responses, in the response_text column, to a therapy chatbot. bot said: 'describe a time when you have acted as a resource for someone else'.  user responded. if a response is 'not flagged', the user can continue talking to the bot. if it is 'flagged', the user is referred to help.\n",
      "sheet_2.csv contains 125 resumes, in the resume_text column. resumes were queried from indeed.com with keyword 'data scientist', location 'vermont'. if a resume is 'not flagged', the applicant can submit a modified resume version at a later date. if it is 'flagged', the applicant is invited to interview.\n",
      "what do i do with this?\n",
      "classify new resumes/responses as flagged or not flagged.\n",
      "there are two sets of data here - resumes and responses. split the data into a train set and a test set to test the accuracy of your classifier. bonus points for using the same classifier for both problems.\n",
      "good luck.\n",
      "acknowledgements\n",
      "thank you to parsa ghaffari (aylien), without whom these visuals (cover photo is in parsa ghaffari's excellent linkedin article on english, spanish and german postive v. negative sentiment analysis) would not exist.\n",
      "there is a 'deep natural language processing' kernel. i will update it. i hope you find it useful.\n",
      "you can use any of the code in that kernel anywhere, on or off kaggle. ping me at @_samputnam for questions.\n",
      "because of memory limitations,data format change csv -> db (sqlite format)\n",
      "this dataset includes yearly and monthly versions of japan's international trading data (segmented by country , the type of good and local custom ).\n",
      "japan trade statistics is searchable here\n",
      "context\n",
      "this dataset contains all stories and comments from hacker news from its launch in 2006. each story contains a story id, the author that made the post, when it was written, and the number of points the story received. hacker news is a social news website focusing on computer science and entrepreneurship. it is run by paul graham's investment fund and startup incubator, y combinator. in general, content that can be submitted is defined as \"anything that gratifies one's intellectual curiosity\".\n",
      "content\n",
      "each story contains a story id, the author that made the post, when it was written, and the number of points the story received.\n",
      "please note that the text field includes profanity. all texts are the author’s own, do not necessarily reflect the positions of kaggle or hacker news, and are presented without endorsement.\n",
      "querying bigquery tables\n",
      "you can use the bigquery python client library to query tables in this dataset in kernels. note that methods available in kernels are limited to querying data. tables are at bigquery-public-data.hacker_news.[tablename]. fork this kernel to get started.\n",
      "acknowledgements\n",
      "this dataset was kindly made publicly available by hacker news under the mit license.\n",
      "inspiration\n",
      "recent studies have found that many forums tend to be dominated by a very small fraction of users. is this true of hacker news?\n",
      "hacker news has received complaints that the site is biased towards y combinator startups. do the data support this?\n",
      "is the amount of coverage by hacker news predictive of a startup’s success?\n",
      "an outbreak of the zika virus, an infection transmitted mostly by the aedes species mosquito (ae. aegypti and ae. albopictus), has been sweeping across the americas and the pacific since mid-2015. although first isolated in 1947 in uganda, a lack of previous research has challenged the scientific community to quickly understand its devastating effects as the epidemic continues to spread.\n",
      "all countries & territories with active zika virus transmission\n",
      "the data\n",
      "this dataset shares publicly available data related to the ongoing zika epidemic. it is being provided as a resource to the scientific community engaged in the public health response. the data provided here is not official and should be considered provisional and non-exhaustive. the data in reports may change over time, reflecting delays in reporting or changes in classifications. and while accurate representation of the reported data is the objective in the machine readable files shared here, that accuracy is not guaranteed. before using any of these data, it is advisable to review the original reports and sources, which are provided whenever possible along with further information on the cdc zika epidemic github repo.\n",
      "the dataset includes the following fields:\n",
      "report_date - the report date is the date that the report was published. the date should be specified in standard iso format (yyyy-mm-dd).\n",
      "location - a location is specified for each observation following the specific names specified in the country place name database. this may be any place with a 'location_type' as listed below, e.g. city, state, country, etc. it should be specified at up to three hierarchical levels in the following format: [country]-[state/province]-[county/municipality/city], always beginning with the country name. if the data is for a particular city, e.g. salvador, it should be specified: brazil-bahia-salvador.\n",
      "location_type - a location code is included indicating: city, district, municipality, county, state, province, or country. if there is need for an additional 'location_type', open an issue to create a new 'location_type'.\n",
      "data_field - the data field is a short description of what data is represented in the row and is related to a specific definition defined by the report from which it comes.\n",
      "data_field_code - this code is defined in the country data guide. it includes a two letter country code (iso-3166 alpha-2, list), followed by a 4-digit number corresponding to a specific report type and data type.\n",
      "time_period - optional. if the data pertains to a specific period of time, for example an epidemiological week, that number should be indicated here and the type of time period in the 'time_period_type', otherwise it should be na.\n",
      "time_period_type - required only if 'time_period' is specified. types will also be specified in the country data guide. otherwise should be na.\n",
      "value - the observation indicated for the specific 'report_date', 'location', 'data_field' and when appropriate, 'time_period'.\n",
      "unit - the unit of measurement for the 'data_field'. this should conform to the 'data_field' unit options as described in the country-specific data guide.\n",
      "if you find the data useful, please support data sharing by referencing this dataset and the original data source. if you're interested in contributing to the zika project from github, you can read more here. the source for the zika virus structure is available here.\n",
      "update 28/11/2017 - last few weeks clearance levels starting to decrease (i may just be seeing a pattern i want to see.. maybe i'm just evil). anyway, can any of you magicians make any sense of it?\n",
      "melbourne is currently experiencing a housing bubble (some experts say it may burst soon). maybe someone can find a trend or give a prediction? which suburbs are the best to buy in? which ones are value for money? where's the expensive side of town? and more importantly where should i buy a 2 bedroom unit?\n",
      "content & acknowledgements\n",
      "this data was scraped from publicly available results posted every week from domain.com.au, i've cleaned it as best i can, now it's up to you to make data analysis magic. the dataset includes address, type of real estate, suburb, method of selling, rooms, price, real estate agent, date of sale and distance from c.b.d.\n",
      "....now with extra data including including property size, land size and council area, you may need to change your code!\n",
      "some key details\n",
      "suburb: suburb\n",
      "address: address\n",
      "rooms: number of rooms\n",
      "price: price in dollars\n",
      "method: s - property sold; sp - property sold prior; pi - property passed in; pn - sold prior not disclosed; sn - sold not disclosed; nb - no bid; vb - vendor bid; w - withdrawn prior to auction; sa - sold after auction; ss - sold after auction price not disclosed. n/a - price or highest bid not available.\n",
      "type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n",
      "sellerg: real estate agent\n",
      "date: date sold\n",
      "distance: distance from cbd\n",
      "regionname: general region (west, north west, north, north east ...etc)\n",
      "propertycount: number of properties that exist in the suburb.\n",
      "bedroom2 : scraped # of bedrooms (from different source)\n",
      "bathroom: number of bathrooms\n",
      "car: number of carspots\n",
      "landsize: land size\n",
      "buildingarea: building size\n",
      "yearbuilt: year the house was built\n",
      "councilarea: governing council for the area\n",
      "lattitude: self explanitory\n",
      "longtitude: self explanitory\n",
      "context\n",
      "this dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the city of chicago from 2001 to present, minus the most recent seven days. data is extracted from the chicago police department's clear (citizen law enforcement analysis and reporting) system. in order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified. should you have questions about this dataset, you may contact the research & development division of the chicago police department at 312.745.6071 or rdanalysis@chicagopolice.org. disclaimer: these crimes may be based upon preliminary information supplied to the police department by the reporting parties that have not been verified. the preliminary crime classifications may be changed at a later date based upon additional investigation and there is always the possibility of mechanical or human error. therefore, the chicago police department does not guarantee (either expressed or implied) the accuracy, completeness, timeliness, or correct sequencing of the information and the information should not be used for comparison purposes over time. the chicago police department will not be responsible for any error or omission, or for the use of, or the results obtained from the use of this information. all data visualizations on maps should be considered approximate and attempts to derive specific addresses are strictly prohibited. the chicago police department is not responsible for the content of any off-site pages that are referenced by or that reference this web page other than an official city of chicago or chicago police department web page. the user specifically acknowledges that the chicago police department is not responsible for any defamatory, offensive, misleading, or illegal conduct of other users, links, or third parties and that the risk of injury from the foregoing rests entirely with the user. the unauthorized use of the words \"chicago police department,\" \"chicago police,\" or any colorable imitation of these words or the unauthorized use of the chicago police department logo is unlawful. this web page does not, in any way, authorize such use. data are updated daily. the dataset contains more than 6,000,000 records/rows of data and cannot be viewed in full in microsoft excel. to access a list of chicago police department - illinois uniform crime reporting (iucr) codes, go to http://data.cityofchicago.org/public-safety/chicago-police-department-illinois-uniform-crime-r/c7ck-438e\n",
      "content\n",
      "id - unique identifier for the record.\n",
      "case number - the chicago police department rd number (records division number), which is unique to the incident.\n",
      "date - date when the incident occurred. this is sometimes a best estimate.\n",
      "block - the partially redacted address where the incident occurred, placing it on the same block as the actual address.\n",
      "iucr - the illinois unifrom crime reporting code. this is directly linked to the primary type and description. see the list of iucr codes at https://data.cityofchicago.org/d/c7ck-438e.\n",
      "primary type - the primary description of the iucr code.\n",
      "description - the secondary description of the iucr code, a subcategory of the primary description.\n",
      "location description - description of the location where the incident occurred.\n",
      "arrest - indicates whether an arrest was made.\n",
      "domestic - indicates whether the incident was domestic-related as defined by the illinois domestic violence act.\n",
      "beat - indicates the beat where the incident occurred. a beat is the smallest police geographic area – each beat has a dedicated police beat car. three to five beats make up a police sector, and three sectors make up a police district. the chicago police department has 22 police districts. see the beats at https://data.cityofchicago.org/d/aerh-rz74.\n",
      "district - indicates the police district where the incident occurred. see the districts at https://data.cityofchicago.org/d/fthy-xz3r.\n",
      "ward - the ward (city council district) where the incident occurred. see the wards at https://data.cityofchicago.org/d/sp34-6z76.\n",
      "community area - indicates the community area where the incident occurred. chicago has 77 community areas. see the community areas at https://data.cityofchicago.org/d/cauq-8yn6.\n",
      "fbi code - indicates the crime classification as outlined in the fbi's national incident-based reporting system (nibrs). see the chicago police department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.\n",
      "x coordinate - the x coordinate of the location where the incident occurred in state plane illinois east nad 1983 projection. this location is shifted from the actual location for partial redaction but falls on the same block.\n",
      "y coordinate - the y coordinate of the location where the incident occurred in state plane illinois east nad 1983 projection. this location is shifted from the actual location for partial redaction but falls on the same block.\n",
      "year - year the incident occurred.\n",
      "updated on - date and time the record was last updated.\n",
      "latitude - the latitude of the location where the incident occurred. this location is shifted from the actual location for partial redaction but falls on the same block.\n",
      "longitude - the longitude of the location where the incident occurred. this location is shifted from the actual location for partial redaction but falls on the same block.\n",
      "location - the location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. this location is shifted from the actual location for partial redaction but falls on the same block.\n",
      "acknowledgements\n",
      "i really want to say thank you to the city of chicago and the chicago police department for making this comprehensive data set available to everyone!\n",
      "inspiration\n",
      "how has crime changed over the years? is it possible to predict where or when a crime will be committed? which areas of the city have evolved over this time span?\n",
      "overview\n",
      "pokemongo is a mobile augmented reality game developed by niantic inc. for ios, android, and apple watch devices. it was initially released in selected countries in july 2016. in the game, players use a mobile device's gps capability to locate, capture, battle, and train virtual creatures, called pokémon, who appear on the screen as if they were in the same real-world location as the player.\n",
      "dataset\n",
      "dataset consists of roughly 293,000 pokemon sightings (historical appearances of pokemon), having coordinates, time, weather, population density, distance to pokestops/ gyms etc. as features. the target is to train a machine learning algorithm so that it can predict where pokemon appear in future. so, can you predict'em all?)\n",
      "feature description\n",
      "pokemonid - the identifier of a pokemon, should be deleted to not affect predictions. (numeric; ranges between 1 and 151)\n",
      "latitude, longitude - coordinates of a sighting (numeric)\n",
      "appearedlocaltime - exact time of a sighting in format yyyy-mm-dd't'hh-mm-ss.ms'z' (nominal)\n",
      "cellid 90-5850m - geographic position projected on a s2 cell, with cell sizes ranging from 90 to 5850m (numeric)\n",
      "appearedtimeofday - time of the day of a sighting (night, evening, afternoon, morning)\n",
      "appearedhour/appearedminute - local hour/minute of a sighting (numeric)\n",
      "appeareddayofweek - week day of a sighting (monday, tuesday, wednesday, thursday, friday, saturday, sunday)\n",
      "appearedday/appearedmonth/appearedyear - day/month/year of a sighting (numeric)\n",
      "terraintype - terrain where pokemon appeared described with help of glcf modis land cover (numeric)\n",
      "closetowater - did pokemon appear close (100m or less) to water (boolean, same source as above)\n",
      "city - the city of a sighting (nominal)\n",
      "continent (not always parsed right) - the continent of a sighting (nominal)\n",
      "weather - weather type during a sighting (foggy clear, partlycloudy, mostlycloudy, overcast, rain, breezyandovercast, lightrain, drizzle, breezyandpartlycloudy, heavyrain, breezyandmostlycloudy, breezy, windy, windyandfoggy, humid, dry, windyandpartlycloudy, dryandmostlycloudy, dryandpartlycloudy, drizzleandbreezy, lightrainandbreezy, humidandpartlycloudy, humidandovercast, rainandwindy) // source for all weather features\n",
      "temperature - temperature in celsius at the location of a sighting (numeric)\n",
      "windspeed - speed of the wind in km/h at the location of a sighting (numeric)\n",
      "windbearing - wind direction (numeric)\n",
      "pressure - atmospheric pressure in bar at the location of a sighting (numeric)\n",
      "weathericon - a compact representation of the weather at the location of a sighting (fog, clear-night, partly-cloudy-night, partly-cloudy-day, cloudy, clear-day, rain, wind)\n",
      "sunriseminutesmidnight-sunsetminutesbefore - time of appearance relatively to sunrise/sunset source\n",
      "population density - what is the population density per square km of a sighting (numeric, source)\n",
      "urban-rural - how urban is location where pokemon appeared (boolean, built on population density, <200 for rural, >=200 and <400 for midurban, >=400 and <800 for suburban, >800 for urban)\n",
      "gymdistancekm, pokestopdistancekm - how far is the nearest gym/pokestop in km from a sighting? (numeric, extracted from this dataset)\n",
      "gymin100m-pokestopin5000m - is there a gym/pokestop in 100/200/etc meters? (boolean)\n",
      "cooc 1-cooc 151 - co-occurrence with any other pokemon (pokemon ids range between 1 and 151) within 100m distance and within the last 24 hours (boolean)\n",
      "class - says which pokemonid it is, to be predicted.\n",
      "data dump\n",
      "all pokemon sightings (in json file, without features) can be found in discussion \"datadump\"\n",
      "in the mid-eighteenth to nineteenth centuries, navigating the open ocean was an imprecise and often dangerous feat. in order to calculate their daily progress and avoid running/sailing into the unknown, a ship's crew kept a detailed logbook with data on winds, waves, and any remarkable weather.\n",
      "handwritten in archived logbooks, these rich datasets were nearly impossible to study until the european union funded their digitization in 2001. you can visit the eu project website for detailed information on the countries and ships included.\n",
      "we're hosting the full 1750-1850 dataset on kaggle to promote the exploration of this unique and invaluable climatology resource.\n",
      "data description\n",
      "this data comes from the climatological database for the world's oceans 1750-1850 (cliwoc), version 1.5 data release.\n",
      "the primary data file is cliwoc15.csv. the columns in this table are described on this page (scroll down to the table that starts with \"field abbreviation\"). it includes 280,280 observational records of ship locations weather data, and other associated information.\n",
      "the ancillary data files are described on the above site.\n",
      "context:\n",
      "this data publication contains a spatial database of wildfires that occurred in the united states from 1992 to 2015. it is the third update of a publication originally generated to support the national fire program analysis (fpa) system. the wildfire records were acquired from the reporting systems of federal, state, and local fire organizations. the following core data elements were required for records to be included in this data publication: discovery date, final fire size, and a point location at least as precise as public land survey system (plss) section (1-square mile grid). the data were transformed to conform, when possible, to the data standards of the national wildfire coordinating group (nwcg). basic error-checking was performed and redundant records were identified and removed, to the degree possible. the resulting product, referred to as the fire program analysis fire-occurrence database (fpa fod), includes 1.88 million geo-referenced wildfire records, representing a total of 140 million acres burned during the 24-year period.\n",
      "content:\n",
      "this dataset is an sqlite database that contains the following information:\n",
      "fires: table including wildfire data for the period of 1992-2015 compiled from us federal, state, and local reporting systems.\n",
      "fod_id = global unique identifier.\n",
      "fpa_id = unique identifier that contains information necessary to track back to the original record in the source dataset.\n",
      "source_system_type = type of source database or system that the record was drawn from (federal, nonfederal, or interagency).\n",
      "source_system = name of or other identifier for source database or system that the record was drawn from. see table 1 in short (2014), or \\supplements\\fpa_fod_source_list.pdf, for a list of sources and their identifier.\n",
      "nwcg_reporting_agency = active national wildlife coordinating group (nwcg) unit identifier for the agency preparing the fire report (bia = bureau of indian affairs, blm = bureau of land management, bor = bureau of reclamation, dod = department of defense, doe = department of energy, fs = forest service, fws = fish and wildlife service, ia = interagency organization, nps = national park service, st/c&l = state, county, or local organization, and tribe = tribal organization).\n",
      "nwcg_reporting_unit_id = active nwcg unit identifier for the unit preparing the fire report.\n",
      "nwcg_reporting_unit_name = active nwcg unit name for the unit preparing the fire report.\n",
      "source_reporting_unit = code for the agency unit preparing the fire report, based on code/name in the source dataset.\n",
      "source_reporting_unit_name = name of reporting agency unit preparing the fire report, based on code/name in the source dataset.\n",
      "local_fire_report_id = number or code that uniquely identifies an incident report for a particular reporting unit and a particular calendar year.\n",
      "local_incident_id = number or code that uniquely identifies an incident for a particular local fire management organization within a particular calendar year.\n",
      "fire_code = code used within the interagency wildland fire community to track and compile cost information for emergency fire suppression (https://www.firecode.gov/).\n",
      "fire_name = name of the incident, from the fire report (primary) or ics-209 report (secondary).\n",
      "ics_209_incident_number = incident (event) identifier, from the ics-209 report.\n",
      "ics_209_name = name of the incident, from the ics-209 report.\n",
      "mtbs_id = incident identifier, from the mtbs perimeter dataset.\n",
      "mtbs_fire_name = name of the incident, from the mtbs perimeter dataset.\n",
      "complex_name = name of the complex under which the fire was ultimately managed, when discernible.\n",
      "fire_year = calendar year in which the fire was discovered or confirmed to exist.\n",
      "discovery_date = date on which the fire was discovered or confirmed to exist.\n",
      "discovery_doy = day of year on which the fire was discovered or confirmed to exist.\n",
      "discovery_time = time of day that the fire was discovered or confirmed to exist.\n",
      "stat_cause_code = code for the (statistical) cause of the fire.\n",
      "stat_cause_descr = description of the (statistical) cause of the fire.\n",
      "cont_date = date on which the fire was declared contained or otherwise controlled (mm/dd/yyyy where mm=month, dd=day, and yyyy=year).\n",
      "cont_doy = day of year on which the fire was declared contained or otherwise controlled.\n",
      "cont_time = time of day that the fire was declared contained or otherwise controlled (hhmm where hh=hour, mm=minutes).\n",
      "fire_size = estimate of acres within the final perimeter of the fire.\n",
      "fire_size_class = code for fire size based on the number of acres within the final fire perimeter expenditures (a=greater than 0 but less than or equal to 0.25 acres, b=0.26-9.9 acres, c=10.0-99.9 acres, d=100-299 acres, e=300 to 999 acres, f=1000 to 4999 acres, and g=5000+ acres).\n",
      "latitude = latitude (nad83) for point location of the fire (decimal degrees).\n",
      "longitude = longitude (nad83) for point location of the fire (decimal degrees).\n",
      "owner_code = code for primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident.\n",
      "owner_descr = name of primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident.\n",
      "state = two-letter alphabetic code for the state in which the fire burned (or originated), based on the nominal designation in the fire report.\n",
      "county = county, or equivalent, in which the fire burned (or originated), based on nominal designation in the fire report.\n",
      "fips_code = three-digit code from the federal information process standards (fips) publication 6-4 for representation of counties and equivalent entities.\n",
      "fips_name = county name from the fips publication 6-4 for representation of counties and equivalent entities.\n",
      "nwcg_unitidactive_20170109: look-up table containing all nwcg identifiers for agency units that were active (i.e., valid) as of 9 january 2017, when the list was downloaded from https://www.nifc.blm.gov/unit_id/publish.html and used as the source of values available to populate the following fields in the fires table: nwcg_reporting_agency, nwcg_reporting_unit_id, and nwcg_reporting_unit_name.\n",
      "unitid = nwcg unit id.\n",
      "geographicarea = two-letter code for the geographic area in which the unit is located (na=national, in=international, ak=alaska, ca=california, ea=eastern area, gb=great basin, nr=northern rockies, nw=northwest, rm=rocky mountain, sa=southern area, and sw=southwest).\n",
      "gacc = seven or eight-letter code for the geographic area coordination center in which the unit is located or primarily affiliated with (cambcifc=canadian interagency forest fire centre, usakcc=alaska interagency coordination center, uscaoncc=northern california area coordination center, uscaoscc=southern california coordination center, uscormcc=rocky mountain area coordination center, usgasac=southern area coordination center, usidnic=national interagency coordination center, usmtnrc=northern rockies coordination center, usnmswc=southwest area coordination center, usornwc=northwest area coordination center, usutgbc=western great basin coordination center, uswieacc=eastern area coordination center).\n",
      "wildlandrole = role of the unit within the wildland fire community.\n",
      "unittype = type of unit (e.g., federal, state, local).\n",
      "department = department (or state/territory) to which the unit belongs (ak=alaska, al=alabama, ar=arkansas, az=arizona, ca=california, co=colorado, ct=connecticut, de=delaware, dhs=department of homeland security, doc= department of commerce, dod=department of defense, doe=department of energy, doi= department of interior, dol=department of labor, fl=florida, ga=georgia, ia=iowa, ia/gc=non-departmental agencies, id=idaho, il=illinois, in=indiana, ks=kansas, ky=kentucky, la=louisiana, ma=massachusetts, md=maryland, me=maine, mi=michigan, mn=minnesota, mo=missouri, ms=mississippi, mt=montana, nc=north carolina, ne=nebraska, ng=non-government, nh=new hampshire, nj=new jersey, nm=new mexico, nv=nevada, ny=new york, oh=ohio, ok=oklahoma, or=oregon, pa=pennsylvania, pr=puerto rico, ri=rhode island, sc=south carolina, sd=south dakota, st/l=state or local government, tn=tennessee, tribe=tribe, tx=texas, usda=department of agriculture, ut=utah, va=virginia, vi=u. s. virgin islands, vt=vermont, wa=washington, wi=wisconsin, wv=west virginia, wy=wyoming).\n",
      "agency = agency or bureau to which the unit belongs (ag=air guard, anc=alaska native corporation, bia=bureau of indian affairs, blm=bureau of land management, boem=bureau of ocean energy management, bor=bureau of reclamation, bsee=bureau of safety and environmental enforcement, c&l=county & local, cdf=california department of forestry & fire protection, dc=department of corrections, dfe=division of forest environment, dff=division of forestry fire & state lands, dfl=division of forests and land, dfr=division of forest resources, dl=department of lands, dnr=department of natural resources, dnrc=department of natural resources and conservation, dnrf=department of natural resources forest service, doa=department of agriculture, doc=department of conservation, doe=department of energy, dof=department of forestry, dvf=division of forestry, dwf=division of wildland fire, epa=environmental protection agency, fc=forestry commission, fema=federal emergency management agency, ffc=bureau of forest fire control, ffp=forest fire protection, ffs=forest fire service, fr=forest rangers, fs=forest service, fws=fish & wildlife service, hq=headquarters, jc=job corps, nbc=national business center, ng=national guard, nnsa=national nuclear security administration, nps=national park service, nws=national weather service, oes=office of emergency services, pri=private, sf=state forestry, sfs=state forest service, sp=state parks, tnc=the nature conservancy, usa=united states army, usace=united states army corps of engineers, usaf=united states air force, usgs=united states geological survey, usn=united states navy).\n",
      "parent = agency subgroup to which the unit belongs (a concatenation of state and unit from this report - https://www.nifc.blm.gov/unit_id/publish/unitidreport.rtf).\n",
      "country = country in which the unit is located (e.g. us = united states).\n",
      "state = two-letter code for the state in which the unit is located (or primarily affiliated).\n",
      "code = unit code (follows state code to create unitid).\n",
      "name = unit name.\n",
      "acknowledgements:\n",
      "these data were collected using funding from the u.s. government and can be used without additional permissions or fees. if you use these data in a publication, presentation, or other research product please use the following citation:\n",
      "short, karen c. 2017. spatial wildfire occurrence data for the united states, 1992-2015 [fpa_fod_20170508]. 4th edition. fort collins, co: forest service research data archive. https://doi.org/10.2737/rds-2013-0009.4\n",
      "inspiration:\n",
      "have wildfires become more or less frequent over time?\n",
      "what counties are the most and least fire-prone?\n",
      "given the size, location and date, can you predict the cause of a fire wildfire?\n",
      "updated: https://www.kaggle.com/datasnaek/youtube-new\n",
      "plan\n",
      "data collected from the (up to) 200 listed trending youtube videos every day in the us and the uk.\n",
      "description\n",
      "the dataset includes data gathered from videos on youtube that are contained within the trending category each day.\n",
      "there are two kinds of data files, one includes comments and one includes video statistics. they are linked by the unique video_id field.\n",
      "the headers in the video file are:\n",
      "video_id (common id field to both comment and video csv files)\n",
      "title\n",
      "channel_title\n",
      "category_id (can be looked up using the included json files, but varies per region so use the appropriate json file for the csv file's country)\n",
      "tags (separated by | character, [none] is displayed if there are no tags)\n",
      "views\n",
      "likes\n",
      "dislikes\n",
      "thumbnail_link\n",
      "date (formatted like so: [day].[month])\n",
      "the headers in the comments file are:\n",
      "video_id (common id field to both comment and video csv files)\n",
      "comment_text\n",
      "likes\n",
      "replies\n",
      "extra info: the youtube api is not effective at formatting comments by relevance, although it claims to do so. as a result, the most relevant comments do not align with the top comments at all, they aren't even sorted by likes or replies.\n",
      "inspiration\n",
      "possible uses for this dataset could include:\n",
      "sentiment analysis in a variety of forms\n",
      "categorising youtube videos based on their comments and statistics.\n",
      "training ml algorithms to generate their own youtube comments.\n",
      "analysing what factors affect how popular a youtube video will be.\n",
      "although there are likely many more possibilities, including analysis of changes over time etc.\n",
      "the american community survey is an ongoing survey from the us census bureau. in this survey, approximately 3.5 million households per year are asked detailed questions about who they are and how they live. many topics are covered, including ancestry, education, work, transportation, internet use, and residency.\n",
      "the responses reveal a fascinating, granular snapshot into the lives of many americans.\n",
      "we''re publishing this data on scripts to make it easy for you to explore this rich dataset, share your work, and collaborate with other data scientists. no data download or local environment needed! we''ve also added shapefiles to simplify publishing maps.\n",
      "what surprising insights can you find in this data? we look forward to seeing and sharing what you discover on scripts!\n",
      "data description\n",
      "here''s a data dictionary.\n",
      "there are two types of survey data provided, housing and population.\n",
      "for the housing data, each row is a housing unit, and the characteristics are properties like rented vs. owned, age of home, etc.\n",
      "for the population data, each row is a person and the characteristics are properties like age, gender, whether they work, method/length of commute, etc.\n",
      "each data set is divided in two pieces, \"a\" and \"b\" (where \"a\" contains states 1 to 25 and \"b\" contains states 26 to 50).\n",
      "both data sets have weights associated with them. weights are included to account for the fact that individuals are not sampled with equal probably (people who have a greater chance of being sampled have a lower weight to reflect this).\n",
      "weight variable for the housing data: wgtp\n",
      "weight variable for the population data: pwgtp\n",
      "in kaggle scripts, these files can be accessed at:\n",
      "../input/pums/ss13husa.csv (housing, a)\n",
      "../input/pums/ss13husb.csv (housing, b)\n",
      "../input/pums/ss13pusa.csv (population, a)\n",
      "../input/pums/ss13pusb.csv (population, b)\n",
      "you can download the data from the census website:\n",
      "housing\n",
      "population\n",
      "in scripts, they are accessed at:\n",
      "../input/shapefiles/pums/tl_2013_[state]_puma10.[extension].\n",
      "the shapefiles can also be downloaded here.\n",
      "datacamp and kaggle have teamed up to bring you the basics of data exploration with kaggle scripts. take the free, interactive course here and start building your data science portfolio.\n",
      "we scraped over 17,000 tweets from 100+ pro-isis fanboys from all over the world since the november 2015 paris attacks. we are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. in order to maximize our impact, we need assistance in quickly analyzing message frames.\n",
      "the dataset includes the following:\n",
      "name\n",
      "username\n",
      "description\n",
      "location\n",
      "number of followers at the time the tweet was downloaded\n",
      "number of statuses by the user when the tweet was downloaded\n",
      "date and timestamp of the tweet\n",
      "the tweet itself\n",
      "based on this data, here are some useful ways of deriving insights and analysis:\n",
      "social network cluster analysis: who are the major players in the pro-isis twitter network? ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers.\n",
      "keyword analysis: which keywords derived from the name, username, description, location, and tweets were the most commonly used by isis fanboys? examples include: \"baqiyah\", \"dabiq\", \"wilayat\", \"amaq\"\n",
      "data categorization of links: which websites are pro-isis fanboys linking to? categories include: mainstream media, altermedia, jihadist websites, image upload, video upload,\n",
      "sentiment analysis: which clergy do pro-isis fanboys quote the most and which ones do they hate the most? search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why. examples of clergy they like the most: \"anwar awlaki\", \"ahmad jibril\", \"ibn taymiyyah\", \"abdul wahhab\". examples of clergy that they hate the most: \"hamza yusuf\", \"suhaib webb\", \"yaser qadhi\", \"nouman ali khan\", \"yaqoubi\".\n",
      "timeline view: visualize all the tweets over a timeline and identify peak moments\n",
      "further reading: \"isis has a twitter strategy and it is terrifying [infographic]\"\n",
      "about fifth tribe\n",
      "fifth tribe is a digital agency based out of dc that serves businesses, non-profits, and government agencies. we provide our clients with product development, branding, web/mobile development, and digital marketing services. our client list includes oxfam, ernst and young, kaiser permanente, aetna innovation health, the u.s. air force, and the u.s. peace corps. along with goldman sachs international and ibm, we serve on the private sector committee of the board of the global community engagement and resilience fund (gcerf), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. in december 2014, we won the anti-isis \"hedaya hack\" organized by affinis labs and hosted at the \"global countering violent extremism (cve) expo \" in abu dhabi. since then, we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools.\n",
      "context\n",
      "i created this dataset to investigate the claim that 2016 had an unnaturally large number of celebrity deaths.\n",
      "content\n",
      "points listed by name, age, cause of death and reason for fame\n",
      "acknowledgements\n",
      "lifted from: https://en.wikipedia.org/wiki/deaths_in_2016 for all years\n",
      "variation of hospital charges in the various hospitals in the us for the top 100 diagnoses.\n",
      "the dataset is owned by the us government. it is freely available on data.gov the dataset keeps getting updated periodically here\n",
      "this dataset will show you how price for the same diagnosis and the same treatment and in the same city can vary differently across different providers. it might help you or your loved one find a better hospital for your treatment. you can also analyze to detect fraud among providers.\n",
      "crowdfunding has become one of the main sources of initial capital for small businesses and start-up companies that are looking to launch their first products. websites like kickstarter and indiegogo provide a platform for millions of creators to present their innovative ideas to the public. this is a win-win situation where creators could accumulate initial fund while the public get access to cutting-edge prototypical products that are not available in the market yet.\n",
      "at any given point, indiegogo has around 10,000 live campaigns while kickstarter has 6,000. it has become increasingly difficult for projects to stand out of the crowd. of course, advertisements via various channels are by far the most important factor to a successful campaign. however, for creators with a smaller budget, this leaves them wonder,\n",
      "\"how do we increase the probability of success of our campaign starting from the very moment we create our project on these websites?\"\n",
      "data sources\n",
      "all of my raw data are scraped from kickstarter.com.\n",
      "first 4000 live projects that are currently campaigning on kickstarter (live.csv)\n",
      "last updated: 2016-10-29 5pm pdt\n",
      "amt.pledged: amount pledged (float)\n",
      "blurb: project blurb (string)\n",
      "by: project creator (string)\n",
      "country: abbreviated country code (string of length 2)\n",
      "currency: currency type of amt.pledged (string of length 3)\n",
      "end.time: campaign end time (string \"yyyy-mm-ddthh:mm:ss-tzd\")\n",
      "location: mostly city (string)\n",
      "pecentage.funded: unit % (int)\n",
      "state: mostly us states (string of length 2) and others (string)\n",
      "title: project title (string)\n",
      "type: type of location (string: county/island/localadmin/suburb/town/zip)\n",
      "url: project url after domain (string)\n",
      "top 4000 most backed projects ever on kickstarter (most_backed.csv)\n",
      "last updated: 2016-10-30 10pm pdt\n",
      "amt.pledged\n",
      "blurb\n",
      "by\n",
      "category: project category (string)\n",
      "currency\n",
      "goal: original pledge goal (float)\n",
      "location\n",
      "num.backers: total number of backers (int)\n",
      "num.backers.tier: number of backers corresponds to the pledge amount in pledge.tier (int[len(pledge.tier)])\n",
      "pledge.tier: pledge tiers in usd (float[])\n",
      "title\n",
      "url\n",
      "see more at http://datapolymath.paperplane.io/\n",
      "the u.s. department of transportation's (dot) bureau of transportation statistics (bts) tracks the on-time performance of domestic flights operated by large air carriers. summary information on the number of on-time, delayed, canceled and diverted flights appears in dot's monthly air travel consumer report, published about 30 days after the month's end, as well as in summary tables posted on this website. bts began collecting details on the causes of flight delays in june 2003. summary statistics and raw data are made available to the public at the time the air travel consumer report is released.\n",
      "this version of the dataset was compiled from the statistical computing statistical graphics 2009 data expo and is also available here.\n",
      "league of legends competitive matches between 2015-2017. the matches include the nalcs, eulcs, lck, lms, and cblol leagues as well as the world championship and mid-season invitational tournaments.\n",
      "context\n",
      "this contains data of news headlines published over a period of 15 years. from the reputable australian news source abc (australian broadcasting corp.)\n",
      "site: http://www.abc.net.au/\n",
      "prepared by rohit kulkarni\n",
      "content\n",
      "format: csv rows: 1,103,665\n",
      "column 1: publish_date (yyyymmdd format)\n",
      "column 2: headline_text (ascii, lowercase)\n",
      "start date: 2003-02-19 end date: 2017-12-31\n",
      "acknowledgements\n",
      "special thanks to the java jsoup library.\n",
      "this dataset is free to use with citation:\n",
      "rohit kulkarni (2017), a million news headlines [csv data file], doi:10.7910/dvn/sybgzl, retrieved from: [this url]\n",
      "inspiration\n",
      "i look at this news dataset as a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on australia.\n",
      "this includes the entire corpus of articles published by the abc website in the given time range. with a volume of 200 articles per day and a good focus on international news, we can be fairly certain that every event of significance has been captured here.\n",
      "digging into the keywords, one can see all the important episodes shaping the last decade and how they evolved over time. ex: financial crisis, iraq war, multiple us elections, ecological disasters, terrorism, famous people, australian crimes etc.\n",
      "similar work\n",
      "your kernals can be reused with minimal changes across all these datasets\n",
      "3m clickbait headlines for 6 years: examine the examiner\n",
      "1.3m global headlines from 20k sources over 1 week: global news week\n",
      "2.6m news headlines from india from 2001-2017: headlines of india\n",
      "context\n",
      "sudoku is a popular number puzzle that requires you to fill blanks in a 9x9 grid with digits so that each column, each row, and each of the nine 3×3 subgrids contains all of the digits from 1 to 9. sudoku-solving has gained much attention from various fields. as a deep learning researcher, i was inclined to investigate the possibilities of neural networks solving sudoku. this dataset was prepared for that.\n",
      "content\n",
      "there are dozens of source codes to generate sudoku games available. i picked one of them, and ran the code. it took approximately 6 hours to generate 1 million games ( + solutions).\n",
      "a sudoku puzzle is represented as a 9x9 python numpy array. the blanks were replaced with 0's. you can easily load and explore the data by running this.\n",
      "import numpy as np\n",
      "quizzes = np.load('sudoku_quizzes.npy') # shape = (1000000, 9, 9)\n",
      "solutions = np.load('sudoku_solutions.npy') # shape = (1000000, 9, 9)\n",
      "for quiz, solution in zip(quizzes[:10], solutions[:10]):\n",
      "    print(quiz)\n",
      "    print(solution)\n",
      "** updates for version 3. **\n",
      "i converted numpy arrays to csv so they are easily accessible, irrespective of language. in each line, a sudoku quiz and its corresponding solution are separated by a comma. you can restore the csv file content to numpy arrays if needed as follows:\n",
      "import numpy as np\n",
      "quizzes = np.zeros((1000000, 81), np.int32)\n",
      "solutions = np.zeros((1000000, 81), np.int32)\n",
      "for i, line in enumerate(open('sudoku.csv', 'r').read().splitlines()[1:]):\n",
      "    quiz, solution = line.split(\",\")\n",
      "    for j, q_s in enumerate(zip(quiz, solution)):\n",
      "        q, s = q_s\n",
      "        quizzes[i, j] = q\n",
      "        solutions[i, j] = s\n",
      "quizzes = quizzes.reshape((-1, 9, 9))\n",
      "solutions = solutions.reshape((-1, 9, 9))\n",
      "acknowledgements\n",
      "i'm grateful to arel cordero, who wrote and shared this great sudoku generation code. https://www.ocf.berkeley.edu/~arel/sudoku/main.html.\n",
      "inspiration\n",
      "check https://github.com/kyubyong/sudoku to see if cnns can crack sudoku puzzles.\n",
      "also, reinforcement learning can be a promising alternative to this task.\n",
      "feel free to challenge sudoku puzzles.\n",
      "u.s. opiate prescriptions\n",
      "accidental death by fatal drug overdose is a rising trend in the united states. what can you do to help?\n",
      "this dataset contains summaries of prescription records for 250 common opioid and non-opioid drugs written by 25,000 unique licensed medical professionals in 2014 in the united states for citizens covered under class d medicare as well as some metadata about the doctors themselves. this is a small subset of data that was sourced from cms.gov. the full dataset contains almost 24 million prescription instances in long format. i have cleaned and compiled this data here in a format with 1 row per prescriber and limited the approximately 1 million total unique prescribers down to 25,000 to keep it manageable. if you are interested in more data, you can get the script i used to assemble the dataset here and run it yourself. the main data is in prescriber-info.csv. there is also opioids.csv that contains the names of all opioid drugs included in the data and overdoses.csv that contains information on opioid related drug overdose fatalities.\n",
      "the increase in overdose fatalities is a well-known problem, and the search for possible solutions is an ongoing effort. my primary interest in this dataset is detecting sources of significant quantities of opiate prescriptions. however, there is plenty of other studies to perform, and i am interested to see what other kagglers will come up with, or if they can improve the model i have already built.\n",
      "the data consists of the following characteristics for each prescriber\n",
      "npi – unique national provider identifier number\n",
      "gender - (m/f)\n",
      "state - u.s. state by abbreviation\n",
      "credentials - set of initials indicative of medical degree\n",
      "specialty - description of type of medicinal practice\n",
      "a long list of drugs with numeric values indicating the total number of prescriptions written for the year by that individual\n",
      "opioid.prescriber - a boolean label indicating whether or not that individual prescribed opiate drugs more than 10 times in the year\n",
      "each week the cfpb sends thousands of consumers’ complaints about financial products and services to companies for response. those complaints are published here after the company responds or after 15 days, whichever comes first. by adding their voice, consumers help improve the financial marketplace.\n",
      "context\n",
      "the demonetization of ₹500 and ₹1000 banknotes was a step taken by the government of india on 8 november 2016, ceasing the usage of all ₹500 and ₹1000 banknotes of the mahatma gandhi series as a form of legal tender in india from 9 november 2016.\n",
      "the announcement was made by the prime minister of india narendra modi in an unscheduled live televised address to the nation at 20:15 indian standard time (ist) the same day. in the announcement, modi declared circulation of all ₹500 and ₹1000 banknotes of the mahatma gandhi series as invalid and announced the issuance of new ₹500 and ₹2000 banknotes of the mahatma gandhi new series in exchange for the old banknotes.\n",
      "content\n",
      "the data contains 6000 most recent tweets on #demonetization. there are 6000 rows(one for each tweet) and 14 columns.\n",
      "metadata:\n",
      "text (tweets)\n",
      "favorited\n",
      "favoritecount\n",
      "replytosn\n",
      "created\n",
      "truncated\n",
      "replytosid\n",
      "id\n",
      "replytouid\n",
      "statussource\n",
      "screenname\n",
      "retweetcount\n",
      "isretweet\n",
      "retweeted\n",
      "acknowledgement\n",
      "the data was collected using the \"twitter\" package in r using the twitter api.\n",
      "past research\n",
      "i have performed my own analysis on the data. i only did a sentiment analysis and formed a word cloud.\n",
      "click here to see the analysis on github\n",
      "inspiration\n",
      "what percentage of tweets are negative, positive or neutral ?\n",
      "what are the most famous/re-tweeted tweets ?\n",
      "our first glimpse at planets outside of the solar system we call home came in 1992 when several terrestrial-mass planets were detected orbiting the pulsar psr b1257+12. in this dataset, you can become a space explorer too by analyzing the characteristics of all discovered exoplanets (plus some familiar faces like mars, saturn, and even earth). data fields include planet and host star attributes, discovery methods, and (of course) date of discovery.\n",
      "data was originally collected and continues to be updated by hanno rein at the open exoplanet catalogue github repository. if you discover any new exoplanets, please submit a pull request there.\n",
      "constants\n",
      "jupiter mass: 1.8991766e+27 kg\n",
      "solar mass: 1.9891e+30 kg\n",
      "jupiter radius: 69911000 m\n",
      "solar radius: 6.96e+08 m\n",
      "license\n",
      "the database is licensed under an mit license. if you use it for a scientific publication, please include a reference to the open exoplanet catalogue on github or to this arxiv paper.\n",
      "context\n",
      "this classic dataset contains the prices and other attributes of almost 54,000 diamonds. it's a great dataset for beginners learning to work with data analysis and visualization.\n",
      "content\n",
      "price price in us dollars (\\$326--\\$18,823)\n",
      "carat weight of the diamond (0.2--5.01)\n",
      "cut quality of the cut (fair, good, very good, premium, ideal)\n",
      "color diamond colour, from j (worst) to d (best)\n",
      "clarity a measurement of how clear the diamond is (i1 (worst), si2, si1, vs2, vs1, vvs2, vvs1, if (best))\n",
      "x length in mm (0--10.74)\n",
      "y width in mm (0--58.9)\n",
      "z depth in mm (0--31.8)\n",
      "depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
      "table width of top of diamond relative to widest point (43--95)\n",
      "context\n",
      "the two datasets are related to red and white variants of the portuguese \"vinho verde\" wine. for more details, consult the reference [cortez et al., 2009]. due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
      "these datasets can be viewed as classification or regression tasks. the classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\n",
      "this dataset is also available from the uci machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , i just shared it to kaggle for convenience. (if i am mistaken and the public license type disallowed me from doing so, i will take this down if requested.)\n",
      "content\n",
      "for more information, read [cortez et al., 2009].\n",
      "input variables (based on physicochemical tests):\n",
      "1 - fixed acidity\n",
      "2 - volatile acidity\n",
      "3 - citric acid\n",
      "4 - residual sugar\n",
      "5 - chlorides\n",
      "6 - free sulfur dioxide\n",
      "7 - total sulfur dioxide\n",
      "8 - density\n",
      "9 - ph\n",
      "10 - sulphates\n",
      "11 - alcohol\n",
      "output variable (based on sensory data):\n",
      "12 - quality (score between 0 and 10)\n",
      "tips\n",
      "what might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'. this allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the roc curve and the auc value. without doing any kind of feature engineering or overfitting you should be able to get an auc of .88 (without even using random forest algorithm)\n",
      "knime is a great tool (gui) that can be used for this.\n",
      "1 - file reader (for csv) to linear correlation node and to interactive histogram for basic eda.\n",
      "2- file reader to 'rule engine node' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:\n",
      "- $quality$ > 6.5 => \"good\"\n",
      "- true => \"bad\"\n",
      "3- rule engine node output to input of column filter node to filter out your original 10point feature (this prevent leaking)\n",
      "4- column filter node output to input of partitioning node (your standard train/tes split, e.g. 75%/25%, choose 'random' or 'stratified')\n",
      "5- partitioning node train data split output to input of train data split to input decision tree learner node and\n",
      "6- partitioning node test data split output to input decision tree predictor node\n",
      "7- decision tree learner node output to input decision tree node input\n",
      "8- decision tree output to input roc node.. (here you can evaluate your model base on auc value)\n",
      "inspiration\n",
      "use machine learning to determine which physiochemical properties make a wine 'good'!\n",
      "acknowledgements\n",
      "this dataset is also available from the uci machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , i just shared it to kaggle for convenience. (i am mistaken and the public license type disallowed me from doing so, i will take this down at first request. i am not the owner of this dataset.\n",
      "please include this citation if you plan to use this database: p. cortez, a. cerdeira, f. almeida, t. matos and j. reis. modeling wine preferences by data mining from physicochemical properties. in decision support systems, elsevier, 47(4):547-553, 2009.\n",
      "relevant publication\n",
      "p. cortez, a. cerdeira, f. almeida, t. matos and j. reis. modeling wine preferences by data mining from physicochemical properties. in decision support systems, elsevier, 47(4):547-553, 2009.\n",
      "context\n",
      "kaggle has more and more computer vision challenges. although kernel resources were increased recently we still can not train useful cnns without gpu. the other main problem is that kernels can't use network connection to download pretrained keras model weights. this dataset helps you to apply your favorite pretrained model in the kaggle kernel environment.\n",
      "happy data exploration and transfer learning!\n",
      "content\n",
      "model (top-1 accuracy | top -5 accuracy)\n",
      "xception (0.790 | 0.945)\n",
      "vgg16 (0.715 | 0.901)\n",
      "vgg19 (0.727 | 0.910)\n",
      "resnet50 (0.759 | 0.929)\n",
      "inceptionv3 (0.788 | 0.944)\n",
      "inceptionresnetv2 (0.804 | 0.953) (could not upload due to 500 mb limit)\n",
      "for more information see https://keras.io/applications/\n",
      "acknowledgements\n",
      "thanks to françois chollet for collecting these models and for the awesome keras.\n",
      "context\n",
      "the national earthquake information center (neic) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. the neic compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. the neic is the national data center and archive for earthquake information.\n",
      "content\n",
      "this dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported magnitude 5.5 or higher since 1965.\n",
      "start a new kernel\n",
      "context\n",
      "free code camp is an open source community where you learn to code and build projects for nonprofits. codenewbie.org is the most supportive community of people learning to code. together, we surveyed more than 15,000 people who are actively learning to code. we reached them through the twitter accounts and email lists of various organizations that help people learn to code. our goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background. we've written in depth about this dataset.\n",
      "in may 2017 we just released an even bigger open dataset with our 2017 survey results.\n",
      "this data set was used to train a crowdflower ai gender predictor. you can read all about the project here. contributors were asked to simply view a twitter profile and judge whether the user was a male, a female, or a brand (non-individual). the dataset contains 20,000 rows, each with a user name, a random tweet, account profile and image, location, and even link and sidebar color.\n",
      "inspiration\n",
      "here are a few questions you might try to answer with this dataset:\n",
      "how well do words in tweets and profiles predict user gender?\n",
      "what are the words that strongly predict male or female gender?\n",
      "how well do stylistic factors (like link color and sidebar color) predict user gender?\n",
      "acknowledgments\n",
      "data was provided by the data for everyone library on crowdflower.\n",
      "our data for everyone library is a collection of our favorite open data jobs that have come through our platform. they're available free of charge for the community, forever.\n",
      "the data\n",
      "the dataset contains the following fields:\n",
      "_unit_id: a unique id for user\n",
      "_golden: whether the user was included in the gold standard for the model; true or false\n",
      "_unit_state: state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations)\n",
      "_trusted_judgments: number of trusted judgments (int); always 3 for non-golden, and what may be a unique id for gold standard observations\n",
      "_last_judgment_at: date and time of last contributor judgment; blank for gold standard observations\n",
      "gender: one of male, female, or brand (for non-human profiles)\n",
      "gender:confidence: a float representing confidence in the provided gender\n",
      "profile_yn: \"no\" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it\n",
      "profile_yn:confidence: confidence in the existence/non-existence of the profile\n",
      "created: date and time when the profile was created\n",
      "description: the user's profile description\n",
      "fav_number: number of tweets the user has favorited\n",
      "gender_gold: if the profile is golden, what is the gender?\n",
      "link_color: the link color on the profile, as a hex value\n",
      "name: the user's name\n",
      "profile_yn_gold: whether the profile y/n value is golden\n",
      "profileimage: a link to the profile image\n",
      "retweet_count: number of times the user has retweeted (or possibly, been retweeted)\n",
      "sidebar_color: color of the profile sidebar, as a hex value\n",
      "text: text of a random one of the user's tweets\n",
      "tweet_coord: if the user has location turned on, the coordinates as a string with the format \"[latitude, longitude]\"\n",
      "tweet_count: number of tweets that the user has posted\n",
      "tweet_created: when the random tweet (in the text column) was created\n",
      "tweet_id: the tweet id of the random tweet\n",
      "tweet_location: location of the tweet; seems to not be particularly normalized\n",
      "user_timezone: the timezone of the user\n",
      "overview\n",
      "the dataset is designed to allow for different methods to be tested for examining the trends in ct image data associated with using contrast and patient age. the basic idea is to identify image textures, statistical patterns and features correlating strongly with these traits and possibly build simple tools for automatically classifying these images when they have been misclassified (or finding outliers which could be suspicious cases, bad measurements, or poorly calibrated machines)\n",
      "data\n",
      "the data are a tiny subset of images from the cancer imaging archive. they consist of the middle slice of all ct images taken where valid age, modality, and contrast tags could be found. this results in 475 series from 69 different patients.\n",
      "tcia archive link - https://wiki.cancerimagingarchive.net/display/public/tcga-luad\n",
      "license\n",
      "http://creativecommons.org/licenses/by/3.0/\n",
      "after the publication embargo period ends these collections are freely available to browse, download, and use for commercial, scientific and educational purposes as outlined in the creative commons attribution 3.0 unported license. questions may be directed to help@cancerimagingarchive.net. please be sure to acknowledge both this data set and tcia in publications by including the following citations in your work:\n",
      "data citation\n",
      "albertina, b., watson, m., holback, c., jarosz, r., kirk, s., lee, y., … lemmerman, j. (2016). radiology data from the cancer genome atlas lung adenocarcinoma [tcga-luad] collection. the cancer imaging archive. http://doi.org/10.7937/k9/tcia.2016.jgnihep5\n",
      "tcia citation\n",
      "clark k, vendt b, smith k, freymann j, kirby j, koppel p, moore s, phillips s, maffitt d, pringle m, tarbox l, prior f. the cancer imaging archive (tcia): maintaining and operating a public information repository, journal of digital imaging, volume 26, number 6, december, 2013, pp 1045-1057. (paper)\n",
      "context\n",
      "well, basically what happened was i was looking for a semi-definite easy to read list of international football matches and couldn't find anything decent. so i took it upon myself to collect it for my own use. i might as well share it.\n",
      "content\n",
      "this dataset includes 38,759 results of international football matches starting from the very first official match in 1972 up to 2018. the matches range from world cup to baltic cup to regular friendly matches. the matches are strictly men's full internationals and the data does not include olympic games or matches where at least one of the teams was the nation's b-team, u-23 or a league select team.\n",
      "results.csv includes the following columns:\n",
      "date\n",
      "home_team\n",
      "away_team\n",
      "home_score\n",
      "away_score\n",
      "tournament\n",
      "city\n",
      "country\n",
      "acknowledgements\n",
      "the data is gathered from several sources including but not limited to wikipedia, fifa.com, rsssf.com and individual football associations' websites.\n",
      "inspiration\n",
      "some directions to take when exploring the data:\n",
      "which teams dominated different eras of football\n",
      "who is the best team of all time\n",
      "what trends have there been in international football throughout the ages - home advantage, total goals scored, distribution of teams' strength etc\n",
      "can we say anything about geopolitics from football fixtures - how has the number of countries changed, which teams like to play each other\n",
      "which countries host the most matches where they themselves are not participating in\n",
      "how much, if at all, does hosting a major tournament help a country's chances in said tournament\n",
      "which teams are the most active in playing friendlies and friendly tournaments - does it help or harm them\n",
      "do you dare to make any predictions for 2018 world cup based on this data?\n",
      "and so on...\n",
      "the world's your oyster, my friend.\n",
      "context\n",
      "music streaming is ubiquitous. currently, spotify plays an important part on that. this dataset enable us to explore how artists and songs' popularity varies in time.\n",
      "content\n",
      "this dataset contains the daily ranking of the 200 most listened songs in 53 countries from 2017 and 2018 by spotify users. it contains more than 2 million rows, which comprises 6629 artists, 18598 songs for a total count of one hundred five billion streams count.\n",
      "the data spans from 1st january 2017 to 9th january 2018 and will be kept up-to-date on following versions. it has been collected from spotify's regional chart data.\n",
      "inspiration\n",
      "can you predict what is the rank position or the number of streams a song will have in the future?\n",
      "how long does songs \"resist\" on the top 3, 5, 10, 20 ranking?\n",
      "what are the signs of a song that gets into the top rank to stay?\n",
      "do continents share same top ranking artists or songs?\n",
      "are people listening to the very same top ranking songs on countries far away from each other?\n",
      "how long time does a top ranking song takes to get into the ranking of neighbor countries?\n",
      "example\n",
      "to start out, you can take a look into a simple kernel i have made in order to read the data, filter data from a song, plot is temporal tendency per country than make a simple forecast of the its streams count here.\n",
      "crawler\n",
      "the crawler used to collect this data can be found here.\n",
      "context\n",
      "to better follow the energy consumption, the government wants energy suppliers to install smart meters in every home in england, wales and scotland. there are more than 26 million homes for the energy suppliers to get to, with the goal of every home having a smart meter by 2020.\n",
      "this roll out of meter is lead by the european union who asked all member governments to look at smart meters as part of measures to upgrade our energy supply and tackle climate change. after an initial study, the british government decided to adopt smart meters as part of their plan to update our ageing energy system.\n",
      "in this dataset, you will find a refactorised version of the data from the london data store, that contains the energy consumption readings for a sample of 5,567 london households that took part in the uk power networks led low carbon london project between november 2011 and february 2014. the data from the smart meters seems associated only to the electrical consumption.\n",
      "there is infomations on the acorn classification details that you can find in this report or the website of caci.\n",
      "i added weather data for london area, i used the darksky api to collect this data.\n",
      "content\n",
      "there is 19 files in this dataset :\n",
      "informations_households.csv : this file that contains all the information on the households in the panel (their acorn group, their tariff) and in which block.csv.gz file their data are stored\n",
      "halfhourly_dataset.zip: zip file that contains the block files with the half-hourly smart meter measurement\n",
      "daily_dataset.zip: zip file that contains the block files with the daily information like the number of measures, minimum, maximum, mean, median, sum and std.\n",
      "acorn_details.csv : details on the acorn groups and their profile of the people in the group, it's come from this xlsx spreadsheet.the first three columns are the attributes studied, the acorn-x is the index of the attribute. at a national scale, the index is 100 if for one column the value is 150 it means that there are 1.5 times more people with this attribute in the acorn group than at the national scale. you can find an explanation on the caci website\n",
      "weather_daily_darksky.csv : that contains the daily data from darksky api. you can find more details about the parameters in the documentation of the api\n",
      "weather_hourly_darksky.csv : that contains the hourly data from darksky api. you can find more details about the parameters in the documentation of the api\n",
      "acknowledgements\n",
      "all the big work of data collection has been done by the uk power networks for the smart meter data.\n",
      "the details related at the acorn group are provided by the caci.\n",
      "the weather data are from darksky.\n",
      "inspiration\n",
      "for me some ideas to analyze the data:\n",
      "segmentation of the consumption daily pattern\n",
      "disaggregation of the electricity load curve\n",
      "cross the consumption result and the acorn information\n",
      "forecast the electricity consumption of a household, i wrote an article on this subject\n",
      "what if i add electrical heating system ? an ev battery system ?\n",
      "forecast at a global scale (london consumption)\n",
      "context:\n",
      "lego is a popular brand of toy building bricks. they are often sold in sets with in order to build a specific object. each set contains a number of parts in different shapes, sizes and colors. this database contains information on which parts are included in different lego sets. it was originally compiled to help people who owned some lego sets already figure out what other sets they could build with the pieces they had.\n",
      "content:\n",
      "this dataset contains the lego parts/sets/colors and inventories of every official lego set in the rebrickable database. these files are current as of july 2017. if you need it to be more recent data, you can use rebrickable’s api which provides up to date data, and additional features.\n",
      "acknowledgements:\n",
      "this dataset was compiled by rebrickable, which is a website to help identify what lego sets can be built given bricks and pieces from other lego sets. you can use these files for any purpose.\n",
      "inspiration:\n",
      "this is a very rich dataset that offers lots of rooms for exploration, especially since the “sets” file includes the year in which a set was first released.\n",
      "how have the size of sets changed over time?\n",
      "what colors are associated with witch themes? could you predict which theme a set is from just by the bricks it contains?\n",
      "what sets have the most-used pieces in them? what sets have the rarest pieces in them?\n",
      "have the colors of legos included in sets changed over time?\n",
      "context\n",
      "the uk government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. it's a huge picture of a country undergoing change.\n",
      "note that all the contained accident data comes from police reports, so this data does not include minor incidents.\n",
      "content\n",
      "uktrafficaadf.csv tracks how much traffic there was on all major roads in the given time period (2000 through 2016). aadt, the core statistic included in this file, stands for \"average annual daily flow\", and is a measure of how activity a road segment based on how many vehicle trips traverse it. the aadt page on wikipedia is a good reference on the subject.\n",
      "accidents data is split across three csv files: accidents_2005_to_2007.csv, accidents_2009_to_2011.csv, and accidents_2012_to_2014.csv. these three files together constitute 1.6 million traffic accidents. the total time period is 2005 through 2014, but 2008 is missing.\n",
      "a data dictionary for the raw dataset at large is available from the uk department of transport website here. for descriptions of individual columns, see the column metadata.\n",
      "acknowledgements\n",
      "the license for this dataset is the open givernment licence used by all data on data.gov.uk (here). the raw datasets are available from the uk department of transport website here.\n",
      "inspiration\n",
      "how has changing traffic flow impacted accidents?\n",
      "can we predict accident rates over time? what might improve accident rates?\n",
      "plot interactive maps of changing trends, e.g. how has london has changed for cyclists? busiest roads in the nation?\n",
      "which areas never change and why? identify infrastructure needs, failings and successes.\n",
      "how have rural and urban areas differed (see roadcategory)? how about the differences between england, scotland, and wales?\n",
      "the uk government also like to look at miles driven. you can do this by multiplying the aadf by the corresponding length of road (link length) and by the number of days in the years. what does this tell you about uk roads?\n",
      "this data was extracted from the 1994 census bureau database by ronny kohavi and barry becker (data mining and visualization, silicon graphics). a set of reasonably clean records was extracted using the following conditions: ((aage>16) && (agi>100) && (afnlwgt>1) && (hrswk>0)). the prediction task is to determine whether a person makes over $50k a year.\n",
      "description of fnlwgt (final weight)\n",
      "the weights on the current population survey (cps) files are controlled to independent estimates of the civilian noninstitutional population of the us. these are prepared monthly for us by population division here at the census bureau. we use 3 sets of controls. these are:\n",
      "a single cell estimate of the population 16+ for each state.\n",
      "controls for hispanic origin by age and sex.\n",
      "controls by race, age and sex.\n",
      "we use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. the term estimate refers to population totals derived from cps by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. people with similar demographic characteristics should have similar weights. there is one important caveat to remember about this statement. that is that since the cps sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.\n",
      "relevant papers\n",
      "ron kohavi, \"scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid\", proceedings of the second international conference on knowledge discovery and data mining, 1996. (pdf)\n",
      "context: daily horse racing (thoroughbred) information that has(is) being actively collected and aggregated from a variety of sources. years covered are just 2016, country is irrelevant to the dataset.\n",
      "acknowledgements: this data has(is) being actively collected and aggregated from a variety of sources, all in the public domain.\n",
      "past research: none of merit, data is used currently to influence some betting decisions but no solid machine learning model(s) have been developed.\n",
      "have thrown various versions of the data into:\n",
      "google prediction\n",
      "amazon machine learning\n",
      "azure machine learning\n",
      "watson analytics\n",
      "as a way to learn how these systems work.\n",
      "inspiration: probably one of the hardest things to do is pick stocks and horses. i have been involved in the stocks and horses industry for many years and through publishing previous libraries and software i have met many interesting people and also one of my long term clients/friends.\n",
      "i am currently trying enhance my software development skills by learning data science / machine learning.\n",
      "i have a done a few tutorials and i am hoping that by publishing this data i can learn and collaborate with members of the kaggle community.\n",
      "content:\n",
      "markets.csv\n",
      "id\n",
      "start_time\n",
      "what time did the race start, datetime in utc\n",
      "venue_id\n",
      "race_number\n",
      "distance(m)\n",
      "condition_id\n",
      "track condition, see conditions.csv\n",
      "weather_id\n",
      "weather on day, see weathers.csv\n",
      "total_pool_win_one\n",
      "rough $ amount wagered across all runners for win market\n",
      "total_pool_place_one\n",
      "rough $ amount wagered across all runners for place market\n",
      "total_pool_win_two\n",
      "total_pool_place_two\n",
      "total_pool_win_three\n",
      "total_pool_place_three\n",
      "runners.csv\n",
      "id\n",
      "collected\n",
      "what time was this row created/data collected, datetime in utc\n",
      "market_id\n",
      "position\n",
      "this is the field we want to predict!!!!\n",
      "will either be 1,2,3,4,5,6 etc or 0/null if the horse was scratched or failed to finish\n",
      "if all positions for a market_id are null it means we were unable to match up the positional data for this market\n",
      "place_paid\n",
      "will either be 1/0 or null\n",
      "if you see a race that only has 2 booleans of 1 it means that the race only paid out places on the first two positions\n",
      "margin\n",
      "if the runner didnt win, how many lengths behind the 1st place was it\n",
      "horse_id\n",
      "see horses.csv\n",
      "trainer_id\n",
      "rider_id\n",
      "see riders.csv\n",
      "handicap_weight\n",
      "number\n",
      "barrier\n",
      "blinkers\n",
      "emergency\n",
      "did it come into the race at the last minute\n",
      "form_rating_one\n",
      "form_rating_two\n",
      "form_rating_three\n",
      "last_five_starts\n",
      "favourite_odds_win\n",
      "from one of the odds sources, will it win - true/false\n",
      "favourite_odds_place\n",
      "from one of the odds sources, will it win - true/false\n",
      "favourite_pool_win\n",
      "favourite_pool_place\n",
      "tip_one_win\n",
      "from a tipster, will it win - true/false\n",
      "tip_one_place\n",
      "from a tipster, will it place - true/false\n",
      "tip_two_win\n",
      "tip_two_place\n",
      "tip_three_win\n",
      "tip_three_place\n",
      "tip_four_win\n",
      "tip_four_place\n",
      "tip_five_win\n",
      "tip_five_place\n",
      "tip_six_win\n",
      "tip_six_place\n",
      "tip_seven_win\n",
      "tip_seven_place\n",
      "tip_eight_win\n",
      "tip_eight_place\n",
      "tip_nine_win\n",
      "tip_nine_place\n",
      "odds.csv (collected for every runner 10 minutes out from race start until race starts)\n",
      "runner_id\n",
      "collected\n",
      "what time was this row created/data collected, datetime in utc\n",
      "odds_one_win\n",
      "from odds source, win odds\n",
      "odds_one_win_wagered\n",
      "from odds source, rough $ amount wagered on win\n",
      "odds_one_place\n",
      "from odds source, place odds\n",
      "odds_one_place_wagered\n",
      "from odds source, rough $ amount wagered on place\n",
      "odds_two_win\n",
      "odds_two_win_wagered\n",
      "odds_two_place\n",
      "odds_two_place_wagered\n",
      "odds_three_win\n",
      "odds_three_win_wagered\n",
      "odds_three_place\n",
      "odds_three_place_wagered\n",
      "odds_four_win\n",
      "odds_four_win_wagered\n",
      "odds_four_place\n",
      "odds_four_place_wagered\n",
      "forms.csv\n",
      "collected\n",
      "what time was this row created/data collected, datetime in utc\n",
      "market_id\n",
      "horse_id\n",
      "runner_number\n",
      "last_twenty_starts\n",
      "e.g. f9x726x753x92222x35\n",
      "f = failed to finish, 7 = finished 7th, 6 = finished 6th, 7 = finished 7th, x = runner was scratched\n",
      "class_level_id\n",
      "1 = eq (in same class as other horses)\n",
      "2 = up (up in class)\n",
      "3 = dn (down in class)\n",
      "field_strength\n",
      "days_since_last_run\n",
      "runs_since_spell\n",
      "overall_starts\n",
      "overall_wins\n",
      "overall_places\n",
      "track_starts\n",
      "track_wins\n",
      "track_places\n",
      "firm_starts\n",
      "firm_wins\n",
      "firm_places\n",
      "good_starts\n",
      "good_wins\n",
      "good_places\n",
      "dead_starts\n",
      "dead_wins\n",
      "dead_places\n",
      "slow_starts\n",
      "slow_wins\n",
      "slow_places\n",
      "soft_starts\n",
      "soft_wins\n",
      "soft_places\n",
      "heavy_starts\n",
      "heavy_wins\n",
      "heavy_places\n",
      "distance_starts\n",
      "distance_wins\n",
      "distance_places\n",
      "class_same_starts\n",
      "class_same_wins\n",
      "class_same_places\n",
      "class_stronger_starts\n",
      "class_stronger_wins\n",
      "class_stronger_places\n",
      "first_up_starts\n",
      "first_up_wins\n",
      "first_up_places\n",
      "second_up_starts\n",
      "second_up_wins\n",
      "second_up_places\n",
      "track_distance_starts\n",
      "track_distance_wins\n",
      "track_distance_places\n",
      "conditions.csv\n",
      "id\n",
      "name\n",
      "weathers.csv\n",
      "id\n",
      "name\n",
      "riders.csv (jockeys)\n",
      "id\n",
      "sex\n",
      "horses.csv\n",
      "id\n",
      "age\n",
      "sex_id\n",
      "see horse_sexes.csv\n",
      "sire_id\n",
      "not related to horses.id, there is another table called horse_sires that is not present here\n",
      "dam_id\n",
      "not related to horses.id, there is another table called horse_dams that is not present here\n",
      "prize_money\n",
      "total aggregate prize money\n",
      "horse_sexes.csv\n",
      "id\n",
      "name\n",
      "outline\n",
      "it was reported that an estimated 4292,000 new cancer cases and 2814,000 cancer deaths would occur in china in 2015. chen, w., etc. (2016), cancer statistics in china, 2015.\n",
      "small molecules play an non-trivial role in cancer chemotherapy. here i focus on inhibitors of 8 protein kinases(name: abbr):\n",
      "cyclin-dependent kinase 2: cdk2\n",
      "epidermal growth factor receptor erbb1: egfr_erbb1\n",
      "glycogen synthase kinase-3 beta: gsk3b\n",
      "hepatocyte growth factor receptor: hgfr\n",
      "map kinase p38 alpha: map_k_p38a\n",
      "tyrosine-protein kinase lck: tpk_lck\n",
      "tyrosine-protein kinase src: tpk_src\n",
      "vascular endothelial growth factor receptor 2: vegfr2\n",
      "for each protein kinase, several thousand inhibitors are collected from chembl database, in which molecules with ic50 lower than 10 um are usually considered as inhibitors, otherwise non-inhibitors.\n",
      "challenge\n",
      "based on those labeled molecules, build your model, and try to make the right prediction.\n",
      "additionally, more than 70,000 small molecules are generated from pubchem database. and you can screen these molecules to find out potential inhibitors. p.s. the majority of these molecules are non-inhibitors.\n",
      "datasets(hdf5 version)\n",
      "there are 8 protein kinase files and 1 pubchem negative samples file. taking \"cdk2.h5\" as an example:\n",
      "import h5py\n",
      "from scipy import sparse\n",
      "hf = h5py.file(\"../input/cdk2.h5\", \"r\")\n",
      "ids = hf[\"chembl_id\"].value # the name of each molecules\n",
      "ap = sparse.csr_matrix((hf[\"ap\"][\"data\"], hf[\"ap\"][\"indices\"], hf[\"ap\"][\"indptr\"]), shape=[len(hf[\"ap\"][\"indptr\"]) - 1, 2039])\n",
      "mg = sparse.csr_matrix((hf[\"mg\"][\"data\"], hf[\"mg\"][\"indices\"], hf[\"mg\"][\"indptr\"]), shape=[len(hf[\"mg\"][\"indptr\"]) - 1, 2039])\n",
      "tt = sparse.csr_matrix((hf[\"tt\"][\"data\"], hf[\"tt\"][\"indices\"], hf[\"tt\"][\"indptr\"]), shape=[len(hf[\"tt\"][\"indptr\"]) - 1, 2039])\n",
      "features = sparse.hstack([ap, mg, tt]).toarray() # the samples' features, each row is a sample, and each sample has 3*2039 features\n",
      "labels = hf[\"label\"].value # the label of each molecule\n",
      "food choices and preferences of college students\n",
      "this dataset includes information on food choices, nutrition, preferences, childhood favorites, and other information from college students. there are 126 responses from students. data is raw and uncleaned. cleaning is in the process and as soon as that is done, additional versions of the data will be posted. acknowledgements\n",
      "thank you to all the students of mercyhurst university who agreed to participate in this survey.\n",
      "inspiration\n",
      "how important is nutrition information for today's college kids? is their taste in food defined by their food preferences when they were children? are kids of parents who cook more likely to make better food choices than others? are these kids likely to have a different taste compared to others? there a number of open ended questions included in this dataset such as: what is your favorite comfort food? what is your favorite cuisine? that could work well for natural language processing\n",
      "context\n",
      "this dataset is an extension of that found here. it contains several extra fields and is pre-cleaned to a much greater extent. after talking with the creator of the original dataset, he and i agreed that merging our work would require making breaking changes to the original, and that this should be published as a new dataset.\n",
      "content\n",
      "185 fields for every player in fifa 18.\n",
      "player info such as age, club, league, nationality, salary and physical attributes\n",
      "all playing attributes, such as finishing and dribbling\n",
      "special attributes like skill moves and international reputation\n",
      "traits and specialities\n",
      "overall, potential, and ratings for each position\n",
      "differences\n",
      "here are the columns in this dataset that aren't in the original:\n",
      "birth_date\n",
      "eur_release_clause\n",
      "height_cm\n",
      "weight_kg\n",
      "body_type\n",
      "real_face\n",
      "league\n",
      "headline attributes: pac, sho, pas, dri, def, and phy. these are what appear on ultimate team cards\n",
      "international_reputation\n",
      "skill_moves\n",
      "weak_foot\n",
      "work_rate_att\n",
      "work_rate_def\n",
      "preferred_foot\n",
      "all traits and specialities as dummy variables\n",
      "all position preferences as dummy variables\n",
      "acknowledgements\n",
      "credit goes to aman shrivastava for building the original dataset. and thanks of course to https://sofifa.com for not banning my ip when i scraped over 18000 pages to get this data.\n",
      "inspiration\n",
      "what insights can this data give us, not only into fifa 18 but into real-world football? the kernels on last year's dataset are a good place to find ideas.\n",
      "contributing\n",
      "contributions to the github project are more than welcome. do let me know if you think of ways to improve either the code or the dataset!\n",
      "this data originally came from crowdflower's data for everyone library.\n",
      "as the original source says,\n",
      "we looked through tens of thousands of tweets about the early august gop debate in ohio and asked contributors to do both sentiment analysis and data categorization. contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. we've removed the non-relevant messages from the uploaded dataset.\n",
      "the data we're providing on kaggle is a slightly reformatted version of the original source. it includes both a csv file and sqlite database. the code that does these transformations is available on github\n",
      "neural information processing systems (nips) is one of the top machine learning conferences in the world. it covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.\n",
      "this year, kaggle is hosting the nips 2015 paper dataset to facilitate and showcase exploratory analytics on the nips data. we've extracted the paper text from the raw pdf files and are releasing that both in csv files and as a sqlite database. here's a quick script that gives an overview of what's included in the data.\n",
      "we encourage you to explore this data and share what you find through kaggle scripts!\n",
      "data description\n",
      "overview of the data in kaggle scripts.\n",
      "nips-2015-papers-release-*.zip (downloadable from the link above) contains the below files/folders. all this data's available through kaggle scripts as well, and you can create a new script to immediately start exploring the data in r, python, julia, or sqlite.\n",
      "this dataset is available in two formats: three csv files and a single sqlite database (consisting of three tables with content identical to the csv files).\n",
      "you can see the code used to create this dataset on github.\n",
      "papers.csv\n",
      "this file contains one row for each of the 403 nips papers from this year's conference. it includes the following fields\n",
      "id - unique identifier for the paper (equivalent to the one in nips's system)\n",
      "title - title of the paper\n",
      "eventtype - whether it's a poster, oral, or spotlight presentation\n",
      "pdfname - filename for the pdf document\n",
      "abstract - text for the abstract (scraped from the nips website)\n",
      "papertext - raw text from the pdf document (created using the tool pdftotext)\n",
      "authors.csv\n",
      "this file contains id's and names for each of the authors on this year's nips papers.\n",
      "id - unique identifier for the author (equivalent to the one in nips's system)\n",
      "name - author's name\n",
      "paperauthors.csv\n",
      "this file links papers to their corresponding authors.\n",
      "id - unique identifier\n",
      "paperid - id for the paper\n",
      "authorid - id for the author\n",
      "database.sqlite\n",
      "this sqlite database contains the tables with equivalent data and formatting as the papers.csv, authors.csv, and paperauthors.csv files.\n",
      "pdfs\n",
      "this folder contains the raw pdf files for each of the papers.\n",
      "this dataset contains data for the entire collection of cards for hearthstone, the popular online card game by blizzard. launching to the public on march 11, 2011 after being under development for almost 5 years, hearthstone has gained popularity as a freemium game, launching into esports across the globe, and the source of many twitch channels.\n",
      "the data in this dataset was extracted from hearthstonejson.com, and the documentation for all the data can be found on the cards.json documentation page.\n",
      "the original data was extracted from the actual card data files used in the game, so all of the data should be here, enabling explorations like:\n",
      "card strengths and weaknesses\n",
      "card strengths relative to cost and rarity\n",
      "comparisons across player classes, bosses, and sets\n",
      "whether a set of optimal cards can be determined per class\n",
      "the cards can be explored in one of four ways:\n",
      "cards.json: the raw json pulled from hearthstonejson.com\n",
      "cards_flat.csv: a flat csv containing a row for each card, and any n:m data stored as arrays in single fields\n",
      "database.sqlite: a sqlite database containing relational data of the cards\n",
      "cards.csv, mechanics.csv, dust_costs.csv, play_requirements.csv, and entourages.csv: the normalized data in csv format.\n",
      "this dataset will be updated as new releases and expansions are made to hearthstone.\n",
      "currently, any localized string values are in en-us, but i may look into adding other languages if the demand seems to be there.\n",
      "introduction\n",
      "the lack of publicly available national football league (nfl) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics. while clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; pitchf/x data in baseball; the nba api for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the nfl. to solve this issue, a group of carnegie mellon university statistical researchers led by recent graduate, maksim horowitz, built and released nflscrapr an r package which uses an api maintained by the nfl to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels. these datasets allow for the advancement of nfl research in the public domain by allowing analysts to develop from a common source in order to create reproducible nfl research, similar to what is being done currently in other professional sports.\n",
      "2015 nfl play-by-play dataset\n",
      "the dataset made available on kaggle contains all the regular season plays from the 2015-2016 nfl season. the dataset contain 46,129 rows and 63 columns. each play is broken down into great detail containing information on; game situation, players involved and results. detailed information about the dataset can be found in the nflscrapr documentation.\n",
      "downloading and installing nflscrapr:\n",
      "use the following code in your r console:\n",
      "# must install the devtools package using the below code\n",
      "install.packages('devtools')\n",
      "library(devtools)\n",
      "# for now you must install nflscrapr from github\n",
      "if (!is.element(\"nflscrapr\", installed.packages())) {\n",
      "    # print installing nflscrapr\n",
      "    devtools::install_github(repo = \"maksimhorowitz/nflscrapr\")\n",
      "}\n",
      "\n",
      "library(nflscrapr)\n",
      "context\n",
      "with the rise of the popularity of machine learning, this is a good opportunity to share a wide database of the even more popular video-game pokémon by nintendo, game freak, and creatures, originally released in 1996.\n",
      "pokémon started as a role playing game (rpg), but due to its increasing popularity, its owners ended up producing many tv series, manga comics, and so on, as well as other types of video-games (like the famous pokémon go!).\n",
      "this dataset is focused on the stats and features of the pokémon in the rpgs. until now (08/01/2017) seven generations of pokémon have been published. all in all, this dataset does not include the data corresponding to the last generation, since 1) i created the databased when the seventh generation was not released yet, and 2) this database is a modification+extension of the database \"721 pokemon with stats\" by alberto barradas (https://www.kaggle.com/abcsds/pokemon), which does not include (of course) the latest generation either.\n",
      "content\n",
      "this database includes 21 variables per each of the 721 pokémon of the first six generations, plus the pokémon id and its name. these variables are briefly described next:\n",
      "number. pokémon id in the pokédex.\n",
      "name. name of the pokémon.\n",
      "type_1. primary type.\n",
      "type_2. second type, in case the pokémon has it.\n",
      "total. sum of all the base stats (health points, attack, defense, special attack, special defense, and speed).\n",
      "hp. base health points.\n",
      "attack. base attack.\n",
      "defense. base defense.\n",
      "sp_atk. base special attack.\n",
      "sp_def. base special defense.\n",
      "speed. base speed.\n",
      "generation. number of the generation when the pokémon was introduced.\n",
      "islegendary. boolean that indicates whether the pokémon is legendary or not.\n",
      "color. color of the pokémon according to the pokédex.\n",
      "hasgender. boolean that indicates if the pokémon can be classified as female or male.\n",
      "pr_male. in case the pokémon has gender, the probability of its being male. the probability of being female is, of course, 1 minus this value.\n",
      "egg_group_1. egg group of the pokémon.\n",
      "egg_group_2. second egg group of the pokémon, in case it has two.\n",
      "hasmegaevolution. boolean that indicates whether the pokémon is able to mega-evolve or not.\n",
      "height_m. height of the pokémon, in meters.\n",
      "weight_kg. weight of the pokémon, in kilograms.\n",
      "catch_rate. catch rate.\n",
      "body_style. body style of the pokémon according to the pokédex.\n",
      "notes\n",
      "please note that many pokémon are multi-form, and also some of them can mega-evolve. i wanted to keep the structure of the dataset as simple and general as possible, as well as the number variable (the id of the pokémon) unique. hence, in the cases of the multi-form pokémon, or the ones capable of mega-evolve, i just chose one of the forms, the one i (and my brother) considered the standard and/or the most common. the specific choice for each of this pokémon are shown below:\n",
      "mega-evolutions are not considered as pokémon.\n",
      "kyogre, groudon. primal forms not considered.\n",
      "deoxis. only normal form considered.\n",
      "wormadam. only plant form considered.\n",
      "rotom. only normal form considered, the one with types electric and ghost.\n",
      "giratina. origin form considered.\n",
      "shaymin. land form considered.\n",
      "darmanitan. standard mode considered.\n",
      "tornadus, thundurus, landorus. incarnate form considered.\n",
      "kyurem. normal form considered, not white or black forms.\n",
      "meloetta. aria form considered.\n",
      "mewstic. both female and male forms are equal in the considered variables.\n",
      "aegislash. shield form considered.\n",
      "pumpkaboo, gourgeist. average size considered.\n",
      "zygarde. 50% form considered.\n",
      "hoopa. confined form considered.\n",
      "acknowledgements\n",
      "as said at the beginning, this database was based on the kaggle database \"721 pokemon with stats\" by alberto barradas (https://www.kaggle.com/abcsds/pokemon). the other resources i mainly used are listed below:\n",
      "wikidex (http://es.pokemon.wikia.com/wiki/wikidex).\n",
      "bulbapedia, the community driven pokémon encyclopedia (http://bulbapedia.bulbagarden.net/wiki/main_page).\n",
      "smogon university (http://www.smogon.com/).\n",
      "possible future work\n",
      "this dataset can be used with different objectives, such as, pokémon clustering, trying to find relations or dependencies between the variables, and also for supervised classification purposes, where the class could be the primary type, but also many of the other variables.\n",
      "author\n",
      "asier lópez zorrilla\n",
      "context\n",
      "satellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. new commercial imagery providers, such as planet and blacksky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.\n",
      "this flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.\n",
      "the aim of this dataset is to help address the difficult task of detecting the location of large ships in satellite images. automating this process can be applied to many issues including monitoring port activity levels and supply chain analysis.\n",
      "continusouly updates will be made to this dataset as new planet imagery released. current images were collected as late as september 2017.\n",
      "content\n",
      "the dataset consists of image chips extracted from planet satellite imagery collected over the san franciso bay area. it includes 2800 80x80 rgb images labeled with either a \"ship\" or \"no-ship\" classification. image chips were derived from planetscope full-frame visual scene products, which are orthorectified to a 3 meter pixel size.\n",
      "provided is a zipped directory shipsnet.7z that contains the entire dataset as .png image chips. each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png\n",
      "label: valued 1 or 0, representing the \"ship\" class and \"no-ship\" class, respectively.\n",
      "scene id: the unique identifier of the planetscope visual scene the image chip was extracted from. the scene id can be used with the planet api to discover and download the entire scene.\n",
      "longitude_latitude: the longitude and latitude coordinates of the image center point, with values separated by a single underscore.\n",
      "the dataset is also distributed as a json formatted text file shipsnet.json. the loaded object contains data, label, scene_ids, and location lists.\n",
      "the pixel value data for each 80x80 rgb image is stored as a list of 19200 integers within the data list. the first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. the image is stored in row-major order, so that the first 80 entries of the array are the red channel values of the first row of the image.\n",
      "the list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.\n",
      "class labels\n",
      "the \"ship\" class includes 700 images. images in this class are near-centered on the body of a single ship. ships of different ship sizes, orientations, and atmospheric collection conditions are included. example images from this class are shown below.\n",
      "the \"no-ship\" class includes 2100 images. a third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an ship. the next third are \"partial ships\" that contain only a portion of an ship, but not enough to meet the full definition of the \"ship\" class. the last third are images that have previously been mislabeled by machine learning models, typically caused by bright pixels or strong linear features. example images from this class are shown below.\n",
      "acknowledgements\n",
      "satellite imagery used to build this dataset is made available through planet's open california dataset, which is openly licensed. as such, this dataset is also available under the same cc-by-sa license. users can sign up for a free planet account to search, view, and download thier imagery and gain access to their api.\n",
      "the aqs data mart is a database containing all of the information from aqs. it has every measured value the epa has collected via the national ambient air monitoring program. it also includes the associated aggregate values calculated by epa (8-hour, daily, annual, etc.). the aqs data mart is a copy of aqs made once per week and made accessible to the public through web-based applications. the intended users of the data mart are air quality data analysts in the regulatory, academic, and health research communities. it is intended for those who need to download large volumes of detailed technical data stored at epa and does not provide any interactive analytical tools. it serves as the back-end database for several agency interactive tools that could not fully function without it: airdata, aircompare, the remote sensing information gateway, the map monitoring sites kml page, etc.\n",
      "aqs must maintain constant readiness to accept data and meet high data integrity requirements, thus is limited in the number of users and queries to which it can respond. the data mart, as a read only copy, can allow wider access.\n",
      "the most commonly requested aggregation levels of data (and key metrics in each) are:\n",
      "sample values (2.4 billion values back as far as 1957, national consistency begins in 1980, data for 500 substances routinely collected) the sample value converted to standard units of measure (generally 1-hour averages as reported to epa, sometimes 24-hour averages) local standard time (lst) and gmt timestamps measurement method measurement uncertainty, where known any exceptional events affecting the data naaqs averages naaqs average values (8-hour averages for ozone and co, 24-hour averages for pm2.5) daily summary values (each monitor has the following calculated each day) observation count observation per cent (of expected observations) arithmetic mean of observations max observation and time of max aqi (air quality index) where applicable number of observations > standard where applicable annual summary values (each monitor has the following calculated each year) observation count and per cent valid days required observation count null observation count exceptional values count arithmetic mean and standard deviation 1st - 4th maximum (highest) observations percentiles (99, 98, 95, 90, 75, 50) number of observations > standard site and monitor information fips state code (the first 5 items on this list make up the aqs monitor identifier) fips county code site number (unique within the county) parameter code (what is measured) poc (parameter occurrence code) to distinguish from different samplers at the same site latitude longitude measurement method information owner / operator / data-submitter information monitoring network to which the monitor belongs exemptions from regulatory requirements operational dates city and cbsa where the monitor is located quality assurance information various data fields related to the 19 different qa assessments possible\n",
      "querying bigquery tables\n",
      "you can use the bigquery python client library to query tables in this dataset in kernels. note that methods available in kernels are limited to querying data. tables are at bigquery-public-data.epa_historical_air_quality.[tablename]. fork this kernel to get started.\n",
      "acknowledgements\n",
      "data provided by the us environmental protection agency air quality system data mart.\n",
      "context\n",
      "quora's first public dataset is related to the problem of identifying duplicate questions. at quora, an important product principle is that there should be a single question page for each logically distinct question. for example, the queries “what is the most populous state in the usa?” and “which state in the united states has the most people?” should not exist separately on quora because the intent behind both is identical. having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.\n",
      "the dataset is based on actual data from quora and will give anyone the opportunity to train and test models of semantic equivalence.\n",
      "content\n",
      "there are over 400,000 lines of potential question duplicate pairs. each line contains ids for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.\n",
      "acknowledgements\n",
      "for more information on this dataset, check out quora's first dataset release page.\n",
      "license\n",
      "this data is subject to quora's terms of service, allowing for non-commercial use.\n",
      "version 5 description\n",
      "tldr\n",
      "the directory 1-1-16_5-31-17_weather contains 1663 files (one for each of the 1663 stations in japan)\n",
      "as its name implies, the data is from the same date window as the competition's date_info file\n",
      "if a station file's name ends with four underscores and a date, the date indicates when the station was terminated\n",
      "please, read on...\n",
      "context\n",
      "this dataset contains the dataset from the recruit restaurant visitor forecasting competition (active from 11-28-17 to 2-6-18).\n",
      "the focus is on using data about reservations made at various restaurants throughout japan, along with restaurant location and genre information to predict the actual number of visitors a restaurant will have on a given day.\n",
      "this dataset augments the above with the addition of information about the weather at various locations in japan over time to produce an exciting, multi-faceted dataset that deals with time, geography, weather, and delicious food.\n",
      "thank you for your interest in this dataset! please let me know if you have any suggestions, questions or problems!\n",
      "content\n",
      "the core of the dataset comprises the files in the following directory:\n",
      "1-1-16_5-31-17_weather (1663 .csv files):\n",
      "this directory contains translated weather data for the time period denoted by the directory’s name (from 1-1-16 through 5-31-17).\n",
      "each .csv file in this directory is of shape (517, 15), and is named according to the id values in the below weather_stations file.\n",
      "there are a few reasons why there may seem to be a lot of null values:\n",
      "the primary reason is that different types of stations/sensors are used, and some just don't capture as much data as others\n",
      "questionable data is sometimes removed by the agency\n",
      "if the station was terminated, its values are null\n",
      "these are the features for all translated weather files (i won’t hazard a description for the features that aren’t already self-explanatory because i’m no meteorologist, and i’d hate for you to get that impression):\n",
      "calendar_date - the observation date, formatted thusly \"yyyy-mm-dd\"\n",
      "avg_temperature\n",
      "high_temperature\n",
      "low_temperature\n",
      "precipitation\n",
      "hours_sunlight\n",
      "solar_radiation\n",
      "deepest_snowfall\n",
      "total_snowfall\n",
      "avg_wind_speed\n",
      "avg_vapor_pressure\n",
      "avg_local_pressure\n",
      "avg_humidity\n",
      "avg_sea_pressure\n",
      "cloud_cover\n",
      "this dataset adds the following .csv files regarding weather stations, and their relations to the competition data:\n",
      "weather_stations.csv (1663, 8):\n",
      "this file contains the location and termination dates for 1,663 weather stations in japan.\n",
      "id - the join of a station’s prefecture, first_name, and second_name, with \"__\" (double underscores)\n",
      "note: if date_terminated is not null, id will end with four underscores and the date_terminated\n",
      "prefecture - the prefecture in which this station is located (see note 1)\n",
      "first_name - the first name given to specify a location (see note 2)\n",
      "second_name - the second name given to specify a location (see note 2)\n",
      "latitude - latitude of the station, converted from degrees, minutes, seconds to decimal degrees for consistency\n",
      "longitude - longitude of the station, converted from degrees, minutes, seconds to decimal degrees for consistency\n",
      "altitude - altitude of the station\n",
      "date_terminated - if the station was terminated, the date of its termination (formatted thusly \"yyyy-mm-dd\") else null\n",
      "nearby_active_stations.csv (62, 8):\n",
      "this file is a subset of weather_stations.csv (above) selected via the following criteria:\n",
      "    1) the station was not terminated, and\n",
      "    2) the station was the closest station to at least one store in air_store_info or hpg_store_info\n",
      "as you can see, there is a lot of overlap here, because while the weather stations seem to generally be scattered throughout japan, the store locations tend to be clustered around several areas.\n",
      "column names and descriptions are identical to those of weather_stations.\n",
      "feature_manifest.csv (1663, 15):\n",
      "this file contains information about each station's \"coverage\" of each weather feature.\n",
      "values of 0.0 for any of the below features except id mean that station collected no data on that feature.\n",
      "values of 1.0 for any of the below features except id mean that station collected data on that feature for every day.\n",
      "id - the id of this weather station\n",
      "avg_temperature - ratio of non-null values for this feature at this station\n",
      "high_temperature - ratio of non-null values for this feature at this station\n",
      "low_temperature - ratio of non-null values for this feature at this station\n",
      "precipitation - ratio of non-null values for this feature at this station\n",
      "hours_sunlight - ratio of non-null values for this feature at this station\n",
      "solar_radiation - ratio of non-null values for this feature at this station\n",
      "deepest_snowfall - ratio of non-null values for this feature at this station\n",
      "total_snowfall - ratio of non-null values for this feature at this station\n",
      "avg_wind_speed - ratio of non-null values for this feature at this station\n",
      "avg_vapor_pressure - ratio of non-null values for this feature at this station\n",
      "avg_local_pressure - ratio of non-null values for this feature at this station\n",
      "avg_humidity - ratio of non-null values for this feature at this station\n",
      "avg_sea_pressure - ratio of non-null values for this feature at this station\n",
      "cloud_cover - ratio of non-null values for this feature at this station\n",
      "air_station_distances.csv (1663, 111):\n",
      "this file contains the vincenty distance from every weather station to every unique latitude/longitude pair in the air system.\n",
      "station_id - the id of this weather station\n",
      "station_latitude - station latitude (in decimal degrees)\n",
      "station_longitude - station longitude (in decimal degrees)\n",
      "<<unique air coordinate pairs>> - the remaining 108 columns are stringified versions of all unique air coordinate pairs\n",
      "they are formatted thusly: (##.###, ##.###), where the first float is the store latitude and the second is the store longitude (see note 3)\n",
      "ex) the first of these columns is: (34.6951242, 135.1978525)\n",
      "hpg_station_distances.csv (1663, 132):\n",
      "this file contains the vincenty distance from every weather station to every unique latitude/longitude pair in the hpg system.\n",
      "station_id - the id of this weather station\n",
      "station_latitude - station latitude (in decimal degrees)\n",
      "station_longitude - station longitude (in decimal degrees)\n",
      "<<unique hpg coordinate pairs>> - the remaining 129 columns are stringified versions of all unique hpg coordinate pairs\n",
      "they are formatted thusly: (##.###, ##.###), where the first float is the store latitude and the second is the store longitude (see note 3)\n",
      "ex) the first of these columns is: (35.6436746642265, 139.668220854814)\n",
      "air_store_info_with_nearest_active_station.csv (829, 12):\n",
      "hpg_store_info_with_nearest_active_station.csv (4690, 12):\n",
      "these two files are supplemented versions of air_store_info and hpg_store_info, and they contain the original competition data for the store_info file specified in the file’s name, plus the following features:\n",
      "latitude_str - a stringified version of this store's latitude for lookup in air_station_distances and hpg_station_distances\n",
      "longitude_str - a stringified version of this store's longitude for lookup in air_station_distances and hpg_station_distances\n",
      "station_id - the id of the weather station nearest to this store\n",
      "station_latitude - the latitude (in decimal degrees) of the weather station nearest to this store\n",
      "station_longitude - the longitude (in decimal degrees) of the weather station nearest to this store\n",
      "station_vincenty - the vincenty distance between this store and the station to which it is closest\n",
      "station_great_circle - the great circle distance between this store and the station to which it is closest\n",
      "other available data\n",
      "date ranges outside of the one used for this competition\n",
      "records seem to go fairly far back, but of course the data we can actually get will be subject to the activation and termination dates of each individual station\n",
      "observation periods other than daily values\n",
      "hourly, every “x” days, monthly, seasonal\n",
      "option to compare data with average from the past 30 years\n",
      "other features:\n",
      "day’s maximum instantaneous wind speed\n",
      "day’s maximum wind direction\n",
      "day’s minimum relative humidity\n",
      "day’s minimum sea pressure\n",
      "option to show the time at which the observation period’s maximum and minimum values occurred\n",
      "acknowledgements\n",
      "all weather data contained herein came from the japan meteorological agency\n",
      "all data pertaining to restaurants, reservations, and visitors came from the recruit restaurant visitor forecasting competition\n",
      "disclaimers\n",
      "the site has one station (that i know of) that i did not include in the list of all stations. that station is located in the antarctic, and i didn’t include it because the antarctic is not located in japan\n",
      "i have not tested the feasibility of actually gathering and processing the “other available data” listed above\n",
      "below is a list of my qualifications for processing weather data for japan, from a site written in japanese:\n",
      "1\n",
      "2\n",
      "3\n",
      "look upon the list and see that it is empty. i’m just a guy that knows how to get stuff done and enjoys a challenge\n",
      "again, read all translations with several grains of salt\n",
      "plans/ideas to expand this dataset (tell me if these interest you)\n",
      "getting data from previous years to get a better idea of how the weather should be on a given day (next version)\n",
      "averaging data from multiple stations to maybe create some representation of the weather for that prefecture/area\n",
      "for stores that are particularly far from their nearest station, doing a similar sort of averaging as above to attempt to minimize any risk posed by using weather data too far from the target store\n",
      "notes\n",
      "1) the site separates the prefecture \"hokkaido\" into its 14 subprefectures:\n",
      "therefore, when you might expect prefecture to be \"hokkaido\", you will actually see \"hokkaido_<subprefecture>\"\n",
      "i.e., the concatenation of the three strings \"hokkaido\", \"_\" (single underscore), and the subprefecture name\n",
      "2) formatting of first_name and last_name:\n",
      "spaces were replaced by \"-\" (single dash)\n",
      "if a value was not provided or could not be translated, the value is \"none\"\n",
      "i would suggest you don’t rely too heavily on these features, as they are the result of several different translation apis, which were rarely in agreement on the translation. these features are provided more as a convenience, and in an effort to give you as much data as possible\n",
      "3) column formatting in air_station_distances.csv and hpg_station_distances.csv:\n",
      "these files show the coordinates with the precision shown in the original air_store_info.csv and hpg_store_info.csv files; however, pandas, doesn't read these values in with the same precision\n",
      "if you plan on using these files, you must use the latitude_str and longitude_str features in the appropriate ..._store_info_with_nearest_active_station file to look up coordinates\n",
      "for more information, see this discussion post\n",
      "context\n",
      "i tried to gather as many lyrics as i could. i ran my code on a a free ec2 instance and ran out of storage space. i have attached the code below so if any one wants to try out it and get all lyrics, please do.\n",
      "content\n",
      "there are around 380,000+ lyrics in the data set from a lot of different artists from a lot of different genres arranged by year. structure is artist/year/song. every artist folder has a genre.txt that tells what is the genre of the musician. find the crawler here.\n",
      "acknowledgements\n",
      "i would like to thank shruti jasoria, sjasoria on github for writing the multi-threaded version.\n",
      "inspiration\n",
      "i wanted to find out what genre and what artist abuses what substance. do rapstars like cocaine or liquor? if liquor then what liquor? does eminem prefer hennesy over jack daniels? do rockstars love pot?\n",
      "context\n",
      "this data set includes customers who have paid off their loans, who have been past due and put into collection without paying back their loan and interests, and who have paid off only after they were put in collection. the financial product is a bullet loan that customers should pay off all of their loan debt in just one time by the end of the term, instead of an installment schedule. of course, they could pay off earlier than their pay schedule.\n",
      "content\n",
      "loan_id a unique loan number assigned to each loan customers\n",
      "loan_status whether a loan is paid off, in collection, new customer yet to payoff, or paid off after the collection efforts\n",
      "principal basic principal loan amount at the origination\n",
      "terms can be weekly (7 days), biweekly, and monthly payoff schedule\n",
      "effective_date when the loan got originated and took effects\n",
      "due_date since it’s one-time payoff schedule, each loan has one single due date\n",
      "paidoff_time the actual time a customer pays off the loan\n",
      "pastdue_days how many days a loan has been past due\n",
      "age, education, gender a customer’s basic demographic information\n",
      "context\n",
      "this dataset contains information on all 802 pokemon from all seven generations of pokemon. the information contained in this dataset include base stats, performance against other types, height, weight, classification, egg steps, experience points, abilities, etc. the information was scraped from http://serebii.net/\n",
      "content\n",
      "name: the english name of the pokemon\n",
      "japanese_name: the original japanese name of the pokemon\n",
      "pokedex_number: the entry number of the pokemon in the national pokedex\n",
      "percentage_male: the percentage of the species that are male. blank if the pokemon is genderless.\n",
      "type1: the primary type of the pokemon\n",
      "type2: the secondary type of the pokemon\n",
      "classification: the classification of the pokemon as described by the sun and moon pokedex\n",
      "height_m: height of the pokemon in metres\n",
      "weight_kg: the weight of the pokemon in kilograms\n",
      "capture_rate: capture rate of the pokemon\n",
      "base_egg_steps: the number of steps required to hatch an egg of the pokemon\n",
      "abilities: a stringified list of abilities that the pokemon is capable of having\n",
      "experience_growth: the experience growth of the pokemon\n",
      "base_happiness: base happiness of the pokemon\n",
      "against_?: eighteen features that denote the amount of damage taken against an attack of a particular type\n",
      "hp: the base hp of the pokemon\n",
      "attack: the base attack of the pokemon\n",
      "defense: the base defense of the pokemon\n",
      "sp_attack: the base special attack of the pokemon\n",
      "sp_defense: the base special defense of the pokemon\n",
      "speed: the base speed of the pokemon\n",
      "generation: the numbered generation which the pokemon was first introduced\n",
      "is_legendary: denotes if the pokemon is legendary.\n",
      "acknowledgements\n",
      "the data was scraped from http://serebii.net/.\n",
      "inspiration\n",
      "pokemon holds a very special place in my heart as it is probably the only video game i have judiciously followed for more than 10 years. with this dataset, i wanted to be able to answer the following questions:\n",
      "is it possible to build a classifier to identify legendary pokemon?\n",
      "how does height and weight of a pokemon correlate with its various base stats?\n",
      "what factors influence the experience growth and egg steps? are these quantities correlated?\n",
      "which type is the strongest overall? which is the weakest?\n",
      "which type is the most likely to be a legendary pokemon?\n",
      "can you build a pokemon dream team? a team of 6 pokemon that inflicts the most damage while remaining relatively impervious to any other team of 6 pokemon.\n",
      "this dataset consists of a few million amazon customer reviews (input text) and star ratings (output labels) for learning how to train fasttext for sentiment analysis.\n",
      "the idea here is a dataset is more than a toy - real business data on a reasonable scale - but can be trained in minutes on a modest laptop.\n",
      "content\n",
      "the fasttext supervised learning tutorial requires data in the following format:\n",
      "__label__<x> __label__<y> ... <text>\n",
      "where x and y are the class names. no quotes, all on one line.\n",
      "in this case, the classes are __label__1 and __label__2, and there is only one class per row.\n",
      "__label__1 corresponds to 1- and 2-star reviews, and __label__2 corresponds to 4- and 5-star reviews.\n",
      "(3-star reviews i.e. reviews with neutral sentiment were not included in the original),\n",
      "the review titles, followed by ':' and a space, are prepended to the text.\n",
      "most of the reviews are in english, but there are a few in other languages, like spanish.\n",
      "source\n",
      "the data was lifted from xiang zhang's google drive dir, but it was in .csv format, not suitable for fasttext.\n",
      "training and testing\n",
      "follow the basic instructions at fasttext supervised learning tutorial to set up the directory.\n",
      "to train:\n",
      "./fasttext supervised -input train.ft.txt -output model_amzn\n",
      "this should take a few minutes.\n",
      "to test:\n",
      "./fasttext test model_amzn.bin test.ft.txt\n",
      "expect precision and recall of 0.916 if all is in order.\n",
      "you can also train and test in python, see kernel.\n",
      "context\n",
      "census of india is a rich database which can tell stories of over a billion indians. it is important not only for research point of view, but commercially as well for the organizations that want to understand india's complex yet strongly knitted heterogeneity. however, nowhere on the web, there exists a single database that combines the district- wise information of all the variables (most include no more than 4-5 out of over 50 variables!). extracting and using data from census of india 2001 is quite a laborious task since all data is made available in scattered pdfs district wise. individual pdfs can be extracted from http://www.censusindia.gov.in/(s(ogvuk1y2e5sueoyc5eyc0g55))/tables_published/basic_data_sheet.aspx.\n",
      "content\n",
      "this database has been extracted from census of 2001 and includes data of 590 districts, having around 80 variables each.\n",
      "in case of confusion regarding the context of the variable, refer to the following pdf and you will be able to make sense out of it: http://censusindia.gov.in/dist_file/datasheet-2923.pdf\n",
      "all the extraction work can be found @ https://github.com/preetskhalsa97/census2001auto the final csv can be found at finalcsv/all.csv\n",
      "the subtle hack that was used to automate extraction to a great extent was the the urls of all the pdfs were same except the four digits (that were respective state and district codes).\n",
      "a few abbreviations used for states:\n",
      "an- andaman and nicobar cg- chhattisgarh d_d- daman and diu d_n_h- dadra and nagar haveli jk- jammu and kashmir mp- madhya pradesh tn- tamil nadu up- uttar pradesh wb- west bengal\n",
      "a few variables for clarification: growth..1991...2001- population growth from 1991 to 2001 x0..4 years- people in age group 0 to 4 years sc1- scheduled class with highest population\n",
      "acknowledgements\n",
      "inspiration\n",
      "this is a massive dataset which can be used to explain the interplay between education, caste, development, gender and much more. it really can explain a lot about india and propel data driven research. happy number crunching!\n",
      "context\n",
      "interested in the indian startup ecosystem just like me? wanted to know what type of startups are getting funded in the last few years? wanted to know who are the important investors? wanted to know the hot fields that get a lot of funding these days? this dataset is a chance to explore the indian start up scene. deep dive into funding data and derive insights into the future!\n",
      "content\n",
      "this dataset has funding information of the indian startups from january 2015 to august 2017. it includes columns with the date funded, the city the startup is based out of, the names of the funders, and the amount invested (in usd).\n",
      "for more information on the values of individual fields, check out the column metadata.\n",
      "acknowledgements\n",
      "thanks to trak.in who are generous enough to share the data publicly for free.\n",
      "inspiration\n",
      "possible questions which could be answered are:\n",
      "how does the funding ecosystem change with time?\n",
      "do cities play a major role in funding?\n",
      "which industries are favored by investors for funding?\n",
      "who are the important investors in the indian ecosystem?\n",
      "how much funds does startups generally get in india?\n",
      "context\n",
      "the dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (item_properties.сsv) and a file, which describes category tree (category_tree.сsv). the data has been collected from a real-world ecommerce website. it is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues. the purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback.\n",
      "content\n",
      "the behaviour data, i.e. events like clicks, add to carts, transactions, represent interactions that were collected over a period of 4.5 months. a visitor can make three types of events, namely “view”, “addtocart” or “transaction”. in total there are 2 756 101 events including 2 664 312 views, 69 332 add to carts and 22 457 transactions produced by 1 407 580 unique visitors. for about 90% of events corresponding properties can be found in the “item_properties.csv” file.\n",
      "for example:\n",
      "“1439694000000,1,view,100,” means visitorid = 1, clicked the item with id = 100 at 1439694000000 (unix timestamp)\n",
      "“1439694000000,2,transaction,1000,234” means visitorid = 2 purchased the item with id = 1000 in transaction with id = 234 at 1439694000000 (unix timestamp)\n",
      "the file with item properties (item_properties.csv) includes 20 275 902 rows, i.e. different properties, describing 417 053 unique items. file is divided into 2 files due to file size limitations. since the property of an item can vary in time (e.g., price changes over time), every row in the file has corresponding timestamp. in other words, the file consists of concatenated snapshots for every week in the file with the behaviour data. however, if a property of an item is constant over the observed period, only a single snapshot value will be present in the file. for example, we have three properties for single item and 4 weekly snapshots, like below:\n",
      "timestamp,itemid,property,value\n",
      "1439694000000,1,100,1000\n",
      "1439695000000,1,100,1000\n",
      "1439696000000,1,100,1000\n",
      "1439697000000,1,100,1000\n",
      "1439694000000,1,200,1000\n",
      "1439695000000,1,200,1100\n",
      "1439696000000,1,200,1200\n",
      "1439697000000,1,200,1300\n",
      "1439694000000,1,300,1000\n",
      "1439695000000,1,300,1000\n",
      "1439696000000,1,300,1100\n",
      "1439697000000,1,300,1100\n",
      "after snapshot merge it would looks like:\n",
      "1439694000000,1,100,1000\n",
      "1439694000000,1,200,1000\n",
      "1439695000000,1,200,1100\n",
      "1439696000000,1,200,1200\n",
      "1439697000000,1,200,1300\n",
      "1439694000000,1,300,1000\n",
      "1439696000000,1,300,1100\n",
      "because property=100 is constant over time, property=200 has different values for all snapshots, property=300 has been changed once.\n",
      "item properties file contain timestamp column because all of them are time dependent, since properties may change over time, e.g. price, category, etc. initially, this file consisted of snapshots for every week in the events file and contained over 200 millions rows. we have merged consecutive constant property values, so it's changed from snapshot form to change log form. thus, constant values would appear only once in the file. this action has significantly reduced the number of rows in 10 times.\n",
      "all values in the “item_properties.csv” file excluding \"categoryid\" and \"available\" properties were hashed. value of the \"categoryid\" property contains item category identifier. value of the \"available\" property contains availability of the item, i.e. 1 means the item was available, otherwise 0. all numerical values were marked with \"n\" char at the beginning, and have 3 digits precision after decimal point, e.g., \"5\" will become \"n5.000\", \"-3.67584\" will become \"n-3.675\". all words in text values were normalized (stemming procedure: https://en.wikipedia.org/wiki/stemming) and hashed, numbers were processed as above, e.g. text \"hello world 2017!\" will become \"24214 44214 n2017.000\"\n",
      "the category tree file has 1669 rows. every row in the file specifies a child categoryid and the corresponding parent. for example:\n",
      "line “100,200” means that categoryid=1 has parent with categoryid=200\n",
      "line “300,” means that categoryid hasn’t parent in the tree\n",
      "acknowledgements\n",
      "retail rocket (retailrocket.io) helps web shoppers make better shopping decisions by providing personalized real-time recommendations through multiple channels with over 100mm unique monthly users and 1000+ retail partners over the world.\n",
      "inspiration\n",
      "how to use item properties and category tree data to improve collaborative filtering model?\n",
      "recurrent neural networks with top-k gains for session-based recommendations https://github.com/hidasib/gru4rec and paper https://arxiv.org/abs/1706.03847\n",
      "https://www.researchgate.net/publication/280538158_application_of_kullback-leibler_divergence_for_short-term_user_interest_detection\n",
      "https://pdfs.semanticscholar.org/66dc/1724c4ed1e74fe6b22e636b52031a33c8ebe.pdf https://www.slideshare.net/lukaslerche/adaptation-and-evaluation-of-recommendationsfor-shortterm-shopping-goals adaptation and evaluation of recommendations for short-term shopping goals\n",
      "tasks\n",
      "task 1\n",
      "when a customer comes to an e-commerce site, he looks for a product with particular properties: price range, vendor, product type and etc. these properties are implicit, so it's hard to determine them through clicks log.\n",
      "try to create an algorithm which predicts properties of items in \"addtocart\" event by using data from \"view\" events for any visitor in the published log.\n",
      "task 2\n",
      "description:\n",
      "process of analyzing ecommerce data include very important part of data cleaning. researchers noticed that in some cases browsing data include up to 40% of abnormal traffic.\n",
      "firstly, abnormal users add a lot of noise into data and make recommendation system less effective. in order to increase efficiency of recommendation system, abnormal users should be removed from the raw data.\n",
      "secondly, abnormal users add bias to results of split tests, so this type of users should be removed also from split test data.\n",
      "goals:\n",
      "the main goal is to find abnormal users of e-shop.\n",
      "subgoals:\n",
      "generate features\n",
      "build a model\n",
      "create a metric that helps to evaluate quality of the model\n",
      "context\n",
      "this data set contains combined on-court performance data for nba players in the 2016-2017 season, alongside salary, twitter engagement, and wikipedia traffic data.\n",
      "further information can be found in a series of articles for ibm developerworks: \"explore valuation and attendance using data science and machine learning\" and \"exploring the individual nba players\".\n",
      "acknowledgement\n",
      "data sources include espn, basketball-reference, twitter, five-thirtyeight, and wikipedia. the source code for this dataset (in python and r) can be found on github. links to more writing can be found at noahgift.com.\n",
      "inspiration\n",
      "do nba fans know more about who the best players are, or do owners?\n",
      "what is the true worth of the social media presence of athletes in the nba?\n",
      "context\n",
      "a high-quality, dataset of images containing fruits. the following fruits are included: apples - (different varieties: golden, golden-red, granny smith, red, red delicious), apricot, avocado, avocado ripe, banana (yellow, red), cactus fruit, carambula, cherry, clementine, cocos, dates, granadilla, grape (pink, white, white2), grapefruit (pink, white), guava, huckleberry, kiwi, kaki, kumsquats, lemon (normal, meyer), lime, litchi, mandarine, mango, maracuja, nectarine, orange, papaya, passion fruit, peach, pepino, pear (different varieties, abate, monster, williams), pineapple, pitahaya red, plum, pomegranate, quince, raspberry, salak, strawberry, tamarillo, tangelo.\n",
      "dataset properties\n",
      "training set size: 28736 images.\n",
      "validation set size: 9673 images.\n",
      "number of classes: 60 (fruits).\n",
      "image size: 100x100 pixels.\n",
      "filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg). \"r\" stands for rotated fruit. \"100\" comes from image size (100x100 pixels).\n",
      "different varieties of the same fruit (apple for instance) are shown having different labels.\n",
      "content\n",
      "fruits were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.\n",
      "a logitech c920 camera was used for filming the fruits. this is one of the best webcams available.\n",
      "behind the fruits we placed a white sheet of paper as background.\n",
      "however due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. this algorithm is of flood fill type: we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. we repeat the previous step until no more pixels can be marked.\n",
      "all marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object.\n",
      "the maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.\n",
      "how to cite\n",
      "horea muresan, mihai oltean, fruit recognition from images using deep learning, technical report, babes-bolyai university, 2017\n",
      "alternate download\n",
      "this dataset is also available for download from github: fruits-360 dataset\n",
      "history\n",
      "fruits were filmed at the dates given below:\n",
      "2017.02.25 - apple (golden).\n",
      "2017.02.28 - apple (red-yellow, red, golden2), kiwi, pear, grapefruit, lemon, orange, strawberry, banana.\n",
      "2017.03.05 - apple (golden3, braeburn, granny smith, red2).\n",
      "2017.03.07 - apple (red3).\n",
      "2017.05.10 - plum, peach, peach flat, apricot, nectarine, pomegranate.\n",
      "2017.05.27 - avocado, papaya, grape, cherrie.\n",
      "2017.12.25 - carambula, cactus fruit, granadilla, kaki, kumsquats, passion fruit, avocado ripe, quince.\n",
      "2017.12.28 - clementine, cocos, mango, lime, litchi.\n",
      "2017.12.31 - apple red delicious, pear monster, grape white.\n",
      "2018.01.14 - ananas, grapefruit pink, mandarine, pineapple, tangelo.\n",
      "2018.01.19 - huckleberry, raspberry.\n",
      "2018.01.26 - dates, maracuja, salak, tamarillo.\n",
      "2018.02.05 - guava, grape white 2, lemon meyer\n",
      "2018.02.07 - banana red, pepino, pitahaya red.\n",
      "2018.02.08 - pear abate, pear williams.\n",
      "context\n",
      "in 2012, the massachusetts institute of technology (mit) and harvard university launched open online courses on edx, a non-profit learning platform co-founded by the two institutions. four years later, what have we learned about these online “classrooms” and the global community of learners who take them?\n",
      "content\n",
      "this report provides data on 290 harvard and mit online courses, 250 thousand certifications, 4.5 million participants, and 28 million participant hours on the edx platform since 2012.\n",
      "acknowledgements\n",
      "isaac chuang, a professor at mit, and andrew ho, a professor at harvard university, published this data as an appendix to their paper \"harvardx and mitx: four years of open online courses\".\n",
      "this version of the dataset is obsolete. it contains duplicate ratings (same user_id,book_id), as reported by philipp spachtholz in his illustrious notebook.\n",
      "the current version has duplicates removed, and more ratings (six million), sorted by time. book and user ids are the same.\n",
      "it is available at https://github.com/zygmuntz/goodbooks-10k.\n",
      "there have been good datasets for movies (netflix, movielens) and music (million songs) recommendation, but not for books. that is, until now.\n",
      "this dataset contains ratings for ten thousand popular books. as to the source, let's say that these ratings were found on the internet. generally, there are 100 reviews for each book, although some have less - fewer - ratings. ratings go from one to five.\n",
      "both book ids and user ids are contiguous. for books, they are 1-10000, for users, 1-53424. all users have made at least two ratings. median number of ratings per user is 8.\n",
      "there are also books marked to read by the users, book metadata (author, year, etc.) and tags.\n",
      "contents\n",
      "ratings.csv contains ratings and looks like that:\n",
      "book_id,user_id,rating\n",
      "1,314,5\n",
      "1,439,3\n",
      "1,588,5\n",
      "1,1169,4\n",
      "1,1185,4\n",
      "to_read.csv provides ids of the books marked \"to read\" by each user, as user_id,book_id pairs.\n",
      "books.csv has metadata for each book (goodreads ids, authors, title, average rating, etc.).\n",
      "the metadata have been extracted from goodreads xml files, available in the third version of this dataset as books_xml.tar.gz. the archive contains 10000 xml files. one of them is available as sample_book.xml. to make the download smaller, these files are absent from the current version. download version 3 if you want them.\n",
      "book_tags.csv contains tags/shelves/genres assigned by users to books. tags in this file are represented by their ids.\n",
      "tags.csv translates tag ids to names.\n",
      "see the notebook for some basic stats of the dataset.\n",
      "goodreads ids\n",
      "each book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.\n",
      "you can use the goodreads book and work ids to create urls as follows:\n",
      "https://www.goodreads.com/book/show/2767052\n",
      "https://www.goodreads.com/work/editions/2792775\n",
      "context\n",
      "i created this data set merging the census 2011 of indian cities with population more than 1 lac and city wise number of graduates from the census 2011, to create a visualization of where the future cities of india stands today, i will try to add more columns [ fertility rate, religion distribution, health standards, number of schools, mortality rate ] in the future, hope people will contribute.\n",
      "content\n",
      "data of 500 cities with population more than 1 lac by census 2011\n",
      "'name_of_city'                  : name of the city \n",
      "'state_code'                    : state code of the city\n",
      "'state_name'                    : state name of the city\n",
      "'dist_code'                     : district code where the city belongs ( 99 means multiple district ) \n",
      "'population_total'              : total population\n",
      "'population_male'               : male population \n",
      "'population_female'             : female population\n",
      "'0-6_population_total'          : 0-6 age total population\n",
      "'0-6_population_male'           : 0-6 age male population\n",
      "'0-6_population_female'         : 0-6 age female population\n",
      "'literates_total'               : total literates\n",
      "'literates_male'                : male literates\n",
      "'literates_female'              : female literates \n",
      "'sex_ratio'                     : sex ratio \n",
      "'child_sex_ratio'               : sex ratio in 0-6\n",
      "'effective_literacy_rate_total' : literacy rate over age 7 \n",
      "'effective_literacy_rate_male'  : male literacy rate over age 7 \n",
      "'effective_literacy_rate_female': female literacy rate over age 7 \n",
      "'location'                      : lat,lng\n",
      "'total_graduates'               : total number of graduates\n",
      "'male_graduates'                : male graduates \n",
      "'female_graduates'              : female graduates\n",
      "acknowledgements\n",
      "census 2011\n",
      "http://censusindia.gov.in/2011-prov-results/paper2/data_files/india2/table_2_pr_cities_1lakh_and_above.xls\n",
      "google geocoder for location fetching.\n",
      "graduation data census 2011\n",
      "http://www.censusindia.gov.in/2011census/c-series/ddwct-0000c-08.xlsx\n",
      "inspiration\n",
      "what story do the top 500 cities of india tell to the world? i wrote a post in my blog about the dataset .\n",
      "introduction\n",
      "the lack of publicly available national football league (nfl) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics. while clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; pitchf/x data in baseball; the basketball reference for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the nfl. to solve this issue, a group of carnegie mellon university statistical researchers including maksim horowitz, ron yurko, and sam ventura, built and released nflscrapr an r package which uses an api maintained by the nfl to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels. using the data outputted by the package, the trio went on to develop reproducible methods for building expected point and win probability models for the nfl. the outputs of these models are included in this dataset and can be accessed using the nflscrapr package.\n",
      "content\n",
      "the dataset made available on kaggle contains all the regular season plays from the 2009-2016 nfl seasons. the dataset has 356,768 rows and 100 columns. each play is broken down into great detail containing information on: game situation, players involved, results, and advanced metrics such as expected point and win probability values. detailed information about the dataset can be found at the following web page, along with more nfl data: https://github.com/ryurko/nflscrapr-data.\n",
      "acknowledgements\n",
      "this dataset was compiled by ron yurko, sam ventura, and myself. special shout-out to ron for improving our current expected points and win probability models and compiling this dataset. all three of us are proud founders of the carnegie mellon sports analytics club.\n",
      "inspiration\n",
      "this dataset is meant to both grow and bring together the community of sports analytics by providing clean and easily accessible nfl data that has never been availabe on this scale for free.\n",
      "feel free to check and recommend my medium post part 1 on a classification model and part 2 on a detection model (faster r-cnn) about this dataset and what i am doing with it.\n",
      "you can also find the related github repo here .\n",
      "context\n",
      "as a big simpsons fan, i have watched a lot (and still watching) of the simpson episodes -multiple times each- over the years. i wanted to build a neural network which can recognize characters\n",
      "content\n",
      "i am still building this dataset (labeling pictures), i will upload new versions of this dataset. please check the files there are descriptions and explanations.\n",
      "file simpson-set.tar.gz : this is an image dataset: 20 folders (one for each character) with 400-2000 pictures in each folder.\n",
      "file simpson-test-set.zip. : preview of the image dataset\n",
      "file weights.best.h5 : weights computed, in order to predict in kernels.\n",
      "file annotation.txt : annotation file for bounding boxes for each character\n",
      "help me to build this dataset\n",
      "if someone wants to contribute and make this dataset bigger and more relevant, any help will be appreciated.\n",
      "acknowledgements\n",
      "data is directly taken and labeled from tv show episodes.\n",
      "context\n",
      "this is a glass identification data set from uci. it contains 10 attributes including id. the response is glass type(discrete 7 values)\n",
      "content\n",
      "attribute information:\n",
      "id number: 1 to 214 (removed from csv file)\n",
      "ri: refractive index\n",
      "na: sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n",
      "mg: magnesium\n",
      "al: aluminum\n",
      "si: silicon\n",
      "k: potassium\n",
      "ca: calcium\n",
      "ba: barium\n",
      "fe: iron\n",
      "type of glass: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps\n",
      "acknowledgements\n",
      "https://archive.ics.uci.edu/ml/datasets/glass+identification source:\n",
      "creator: b. german central research establishment home office forensic science service aldermaston, reading, berkshire rg7 4pn\n",
      "donor: vina spiehler, ph.d., dabft diagnostic products corporation (213) 776-0180 (ext 3014)\n",
      "inspiration\n",
      "data exploration of this dataset reveals two important characteristics : 1) the variables are highly corelated with each other including the response variables: so which kind of ml algorithm is most suitable for this dataset random forest , knn or other? also since dataset is too small is there any chance of applying pca or it should be completely avoided?\n",
      "2) highly skewed data: is scaling sufficient or are there any other techniques which should be applied to normalize data? like box-cox power transformation?\n",
      "context\n",
      "full text of all questions and answers from stack overflow that are tagged with the python tag. useful for natural language processing and community analysis. see also the dataset of r questions.\n",
      "content\n",
      "this dataset is organized as three tables:\n",
      "questions contains the title, body, creation date, score, and owner id for each python question.\n",
      "answers contains the body, creation date, score, and owner id for each of the answers to these questions. the parentid column links back to the questions table.\n",
      "tags contains the tags on each question besides the python tag.\n",
      "questions may be deleted by the user who posted them. they can also be closed by community vote, if the question is deemed off-topic for instance. such questions are not included in this dataset.\n",
      "the dataset contains questions all questions asked between august 2, 2008 and ocotober 19, 2016.\n",
      "license\n",
      "all stack overflow user contributions are licensed under cc-by-sa 3.0 with attribution required.\n",
      "context\n",
      "the national park service publishes a database of animal and plant species identified in individual national parks and verified by evidence — observations, vouchers, or reports that document the presence of a species in a park. all park species records are available to the public on the national park species portal; exceptions are made for sensitive, threatened, or endangered species when widespread distribution of information could pose a risk to the species in the park.\n",
      "content\n",
      "national park species lists provide information on the presence and status of species in our national parks. these species lists are works in progress and the absence of a species from a list does not necessarily mean the species is absent from a park. the time and effort spent on species inventories varies from park to park, which may result in data gaps. species taxonomy changes over time and reflects regional variations or preferences; therefore, records may be listed under a different species name.\n",
      "each park species record includes a species id, park name, taxonomic information, scientific name, one or more common names, record status, occurrence (verification of species presence in park), nativeness (species native or foreign to park), abundance (presence and visibility of species in park), seasonality (season and nature of presence in park), and conservation status (species classification according to us fish & wildlife service). taxonomic classes have been translated from latin to english for species categorization; order, family, and scientific name (genus, species, subspecies) are in latin.\n",
      "acknowledgements\n",
      "the national park service species list database is managed and updated by staff at individual national parks and the systemwide inventory and monitoring department.\n",
      "context\n",
      "formula one (also formula 1 or f1 and officially the fia formula one world championship) is the highest class of single-seat auto racing that is sanctioned by the fédération internationale de l'automobile (fia). the fia formula one world championship has been one of the premier forms of racing around the world since its inaugural season in 1950.\n",
      "content\n",
      "this dataset contains data from 1950 all the way through the 2017 season, and consists of tables describing constructors, race drivers, lap times, pit stops and more.\n",
      "acknowledgements\n",
      "the data was downloaded from http://ergast.com/mrd/ at the conclusion of the 2017 season. the data was originally gathered and published to the public domain by chris newell.\n",
      "inspiration\n",
      "i think this dataset offers an exciting insight into a $ billion industry, enjoyed by hundreds of millions of viewers all over the world. so please, explore and enjoy!\n",
      "this is the dataset used in the roam blog post prescription-based prediction. it is derived from a variety of us open health datasets, but the bulk of the data points come from the medicare part d dataset and the national provider identifier dataset.\n",
      "the prescription vector for each doctor tells a rich story about that doctor's attributes, including specialty, gender, age, and region. there are 239,930 doctors in the dataset.\n",
      "the file is in jsonl format (one json record per line):\n",
      "{\n",
      "    'provider_variables': \n",
      "        {\n",
      "            'brand_name_rx_count': int,\n",
      "            'gender': 'm' or 'f',\n",
      "            'generic_rx_count': int,\n",
      "            'region': 'south' or 'midwest' or 'northeast' or 'west',\n",
      "            'settlement_type': 'non-urban' or 'urban'\n",
      "            'specialty': str\n",
      "            'years_practicing': int\n",
      "        },\n",
      "     'npi': str,\n",
      "     'cms_prescription_counts':\n",
      "        {\n",
      "            `drug_name`: int, \n",
      "            `drug_name`: int, \n",
      "            ...\n",
      "        }\n",
      "}\n",
      "the brand/generic classifications behind brand_name_rx_count and generic_rx_count are defined heuristically. for more details, see the blog post or go directly to the associated code.\n",
      "the meteoritical society collects data on meteorites that have fallen to earth from outer space. this dataset includes the location, mass, composition, and fall year for over 45,000 meteorites that have struck our planet.\n",
      "notes on missing or incorrect data points:\n",
      "a few entries here contain date information that was incorrectly parsed into the nasa database. as a spot check: any date that is before 860 ce or after 2016 are incorrect; these should actually be bce years. there may be other errors and we are looking for a way to identify them.\n",
      "a few entries have latitude and longitude of 0n/0e (off the western coast of africa, where it would be quite difficult to recover meteorites). many of these were actually discovered in antarctica, but exact coordinates were not given. 0n/0e locations should probably be treated as na.\n",
      "the starter kernel for this dataset has a quick way to filter out these observations using dplyr in r, provided here for convenience:\n",
      "meteorites.geo <- meteorites.all %>%\n",
      "filter(year>=860 & year<=2016) %>% # filter out weird years\n",
      "filter(reclong<=180 & reclong>=-180 & (reclat!=0 | reclong!=0)) # filter out weird locations\n",
      "the data\n",
      "note that a few column names start with \"rec\" (e.g., recclass, reclat, reclon). these are the recommended values of these variables, according to the meteoritical society. in some cases, there were historical reclassification of a meteorite, or small changes in the data on where it was recovered; this dataset gives the currently recommended values.\n",
      "the dataset contains the following variables:\n",
      "name: the name of the meteorite (typically a location, often modified with a number, year, composition, etc)\n",
      "id: a unique identifier for the meteorite\n",
      "nametype: one of:\n",
      "-- valid: a typical meteorite\n",
      "-- relict: a meteorite that has been highly degraded by weather on earth\n",
      "recclass: the class of the meteorite; one of a large number of classes based on physical, chemical, and other characteristics (see the wikipedia article on meteorite classification for a primer)\n",
      "mass: the mass of the meteorite, in grams\n",
      "fall: whether the meteorite was seen falling, or was discovered after its impact; one of:\n",
      "-- fell: the meteorite's fall was observed\n",
      "-- found: the meteorite's fall was not observed\n",
      "year: the year the meteorite fell, or the year it was found (depending on the value of fell)\n",
      "reclat: the latitude of the meteorite's landing\n",
      "reclong: the longitude of the meteorite's landing\n",
      "geolocation: a parentheses-enclose, comma-separated tuple that combines reclat and reclong\n",
      "what can we do with this data?\n",
      "here are a couple of thoughts on questions to ask and ways to look at this data:\n",
      "how does the geographical distribution of observed falls differ from that of found meteorites? -- this would be great overlaid on a cartogram or alongside a high-resolution population density map\n",
      "are there any geographical differences or differences over time in the class of meteorites that have fallen to earth?\n",
      "acknowledgements\n",
      "this dataset was downloaded from nasa's data portal, and is based on the meteoritical society's meteoritical bulletin database (this latter database provides additional information such as meteorite images, links to primary sources, etc.).\n",
      "neural information processing systems (nips) is one of the top machine learning conferences in the world. it covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.\n",
      "this dataset includes the title, authors, abstracts, and extracted text for all nips papers to date (ranging from the first 1987 conference to the current 2016 conference). i've extracted the paper text from the raw pdf files and are releasing that both in csv files and as a sqlite database. the code to scrape and create this dataset is on github. here's a quick rmarkdown exploratory overview of what's in the data. we encourage you to explore this data and share what you find through kaggle kernels!\n",
      "context\n",
      "i created this data set to help with the new york city taxi trip duration playground. i used osrm to extract information about the fastest routes for each data point.\n",
      "i think it will be very useful for anyone doing work in that competition. please try it out and tell me what you want me to add. i intend to improve and add features.\n",
      "content\n",
      "starting_street\n",
      "the street where the taxi-trip starts. in version 1 this field contained a lot of empty values. that has been dealt with.\n",
      "end_street\n",
      "the street where the taxi-trip ends. in version 1 this field contained a lot of empty values. that has been dealt with.\n",
      "total_distance\n",
      "the total distance is measured between the pickup coordinates and the drop-off coordinates in train.csv and test.csv. the unit is meters.\n",
      "total_travel_time\n",
      "the total travel time for that data point in seconds\n",
      "number_of_steps\n",
      "the number of steps on that trip. one step consists of some driving and an action the taxi needs to perform. it can be something like a turn or going on to a highway. see step_maneuvers for more information.\n",
      "street_for_each_step\n",
      "a list of streets where each step occurs. multiple steps can be performed on the same street. therefore there might the same street might occur multiple times.\n",
      "(the values are stored as a string separated by '|')\n",
      "distance_per_step\n",
      "the distance for each step.\n",
      "(the values are stored as a string separated by '|')\n",
      "travel_time_per_step\n",
      "the travel time for each step\n",
      "(the values are stored as a string separated by '|')\n",
      "step_maneuvers\n",
      "the action (or maneuver) performed in each step. the possible maneuvers are:\n",
      "turn: a basic turn\n",
      "new name: no turn is taken/possible, but the road name changes.\n",
      "depart: the trip starts\n",
      "arrive: the trip ends\n",
      "merge: merge onto a street (e.g. getting on the highway from a ramp)\n",
      "on ramp: entering a highway (direction given my modifier )\n",
      "off ramp: exiting a highway\n",
      "fork: the road forks\n",
      "end of road: the road ends in a t intersection\n",
      "continue: turn to stay on the same road\n",
      "roundabout: a roundabout\n",
      "rotary a traffic circle (a bigger roundabout)\n",
      "roundabout turn: a small roundabout that can be treated as a regular turn\n",
      "(the values are stored as a string separated by '|')\n",
      "step_direction\n",
      "the direction for each action (or maneuver)\n",
      "step_location_list\n",
      "the coordinates for each action (or maneuver)\n",
      "nih chest x-ray dataset\n",
      "national institutes of health chest x-ray dataset\n",
      "chest x-ray exams are one of the most frequent and cost-effective medical imaging examinations available. however, clinical diagnosis of a chest x-ray can be challenging and sometimes more difficult than diagnosis via chest ct imaging. the lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (cad) in real world medical sites with chest x-rays. one major hurdle in creating large x-ray image datasets is the lack resources for labeling so many images. prior to the release of this dataset, openi was the largest publicly available source of chest x-ray images with 4,143 images available.\n",
      "this nih chest x-ray dataset is comprised of 112,120 x-ray images with disease labels from 30,805 unique patients. to create these labels, the authors used natural language processing to text-mine disease classifications from the associated radiological reports. the labels are expected to be >90% accurate and suitable for weakly-supervised learning. the original radiology reports are not publicly available but you can find more details on the labeling process in this open access paper: \"chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.\" (wang et al.)\n",
      "link to paper\n",
      "\n",
      "data limitations:\n",
      "the image labels are nlp extracted so there could be some erroneous labels but the nlp labeling accuracy is estimated to be >90%.\n",
      "very limited numbers of disease region bounding boxes (see bbox_list_2017.csv)\n",
      "chest x-ray radiology reports are not anticipated to be publicly shared. parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation\n",
      "\n",
      "file contents\n",
      "image format: 112,120 total images with size 1024 x 1024\n",
      "images_001.zip: contains 4999 images\n",
      "images_002.zip: contains 10,000 images\n",
      "images_003.zip: contains 10,000 images\n",
      "images_004.zip: contains 10,000 images\n",
      "images_005.zip: contains 10,000 images\n",
      "images_006.zip: contains 10,000 images\n",
      "images_007.zip: contains 10,000 images\n",
      "images_008.zip: contains 10,000 images\n",
      "images_009.zip: contains 10,000 images\n",
      "images_010.zip: contains 10,000 images\n",
      "images_011.zip: contains 10,000 images\n",
      "images_012.zip: contains 7,121 images\n",
      "readme_chestxray.pdf: original readme file\n",
      "bbox_list_2017.csv: bounding box coordinates. note: start at x,y, extend horizontally w pixels, and vertically h pixels\n",
      "image index: file name\n",
      "finding label: disease type (class label)\n",
      "bbox x\n",
      "bbox y\n",
      "bbox w\n",
      "bbox h\n",
      "data_entry_2017.csv: class labels and patient data for the entire dataset\n",
      "image index: file name\n",
      "finding labels: disease type (class label)\n",
      "follow-up #\n",
      "patient id\n",
      "patient age\n",
      "patient gender\n",
      "view position: x-ray orientation\n",
      "originalimagewidth\n",
      "originalimageheight\n",
      "originalimagepixelspacing_x\n",
      "originalimagepixelspacing_y\n",
      "\n",
      "class descriptions\n",
      "there are 15 classes (14 diseases, and one for \"no findings\"). images can be classified as \"no findings\" or one or more disease classes:\n",
      "atelectasis\n",
      "consolidation\n",
      "infiltration\n",
      "pneumothorax\n",
      "edema\n",
      "emphysema\n",
      "fibrosis\n",
      "effusion\n",
      "pneumonia\n",
      "pleural_thickening\n",
      "cardiomegaly\n",
      "nodule mass\n",
      "hernia\n",
      "\n",
      "full dataset content\n",
      "there are 12 zip files in total and range from ~2 gb to 4 gb in size. additionally, we randomly sampled 5% of these images and created a smaller dataset for use in kernels. the random sample contains 5606 x-ray images and class labels.\n",
      "sample: sample.zip\n",
      "\n",
      "modifications to original data\n",
      "original tar archives were converted to zip archives to be compatible with the kaggle platform\n",
      "csv headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory\n",
      "\n",
      "citations\n",
      "wang x, peng y, lu l, lu z, bagheri m, summers rm. chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. ieee cvpr 2017, chestx-ray8_hospital-scale_chest_cvpr_2017_paper.pdf\n",
      "nih news release: nih clinical center provides one of the largest publicly available chest x-ray datasets to scientific community\n",
      "original source files and documents: https://nihcc.app.box.com/v/chestxray-nihcc/folder/36938765345\n",
      "\n",
      "acknowledgements\n",
      "this work was supported by the intramural research program of the nclinical center (clinicalcenter.nih.gov) and national library of medicine (www.nlm.nih.gov).\n",
      "context:\n",
      "a permanent labor certification issued by the department of labor (dol) allows an employer to hire a foreign worker to work permanently in the united states. in most instances, before the u.s. employer can submit an immigration petition to the department of homeland security's u.s. citizenship and immigration services (uscis), the employer must obtain a certified labor certification application from the dol's employment and training administration (eta). the dol must certify to the uscis that there are not sufficient u.s. workers able, willing, qualified and available to accept the job opportunity in the area of intended employment and that employment of the foreign worker will not adversely affect the wages and working conditions of similarly employed u.s. workers.\n",
      "content:\n",
      "data covers 2012-2017 and includes information on employer, position, wage offered, job posting history, employee education and past visa history, associated lawyers, and final decision.\n",
      "acknowledgements:\n",
      "this data was collected and distributed by the us department of labor.\n",
      "inspiration:\n",
      "can you predict visa decisions based on employee/employer/wage?\n",
      "how does this data compare to h1b decisions in this dataset?\n",
      "league of legends ranked matches\n",
      "data about 184070 league of legends ranked solo games, spanning across several years\n",
      "content\n",
      "matches\n",
      "player and team stats\n",
      "bans\n",
      "acknowledgements\n",
      "i found this data on a sql database and exported it to csv. all data belongs ultimately to riot games and their data policies applies. these files are presented only as a simpler way to obtain a large dataset without stressing the riot api and are in no way associated with riot games. the data is provided as-is without any warranty on its correctness. if your algorithm catches fire, don't blame me or riot. if you are rito and are opposed to sharing this data here, contact me and it will be removed immediately.\n",
      "possible questions\n",
      "can we predict the winner given the teams?\n",
      "can ranked matchmaking be assumed to be unbiased (or adjusted for red-side advantage)?\n",
      "does the region affect significantly win rates?\n",
      "can we compare the data in relation to competitive data (also available on kaggle)?\n",
      "can we assess information on the different metas?\n",
      "context\n",
      "steam is the world's most popular pc gaming hub, with over 6,000 games and a community of millions of gamers. with a massive collection that includes everything from aaa blockbusters to small indie titles, great discovery tools are a highly valuable asset for steam. how can we make them better?\n",
      "content\n",
      "this dataset is a list of user behaviors, with columns: user-id, game-title, behavior-name, value. the behaviors included are 'purchase' and 'play'. the value indicates the degree to which the behavior was performed - in the case of 'purchase' the value is always 1, and in the case of 'play' the value represents the number of hours the user has played the game.\n",
      "acknowledgements\n",
      "this dataset is generated entirely from public steam data, so we want to thank steam for building such an awesome platform and community!\n",
      "inspiration\n",
      "the dataset is formatted to be compatible with tamber. build a tamber engine and take it for a spin!\n",
      "combine our collaborative filter's results with your favorite machine learning techniques with ensemble learning, or make tamber do battle with something else you've built.\n",
      "have fun, the tamber team\n",
      "the bay area bike share enables quick, easy, and affordable bike trips around the san francisco bay area. they make regular open data releases (this dataset is a transformed version of the data from this link), plus maintain a real-time api.\n",
      "exploration ideas\n",
      "how does weather impact bike trips?\n",
      "how do bike trip patterns vary by time of day and the day of the week?\n",
      "500 actual skus from an outdoor apparel brand's product catalog. it's somewhat rare to get real item level data in a real-world format. very useful for testing things like recommendation engines. in fact...maybe i'll publish some code along with this :)\n",
      "context\n",
      "stars mostly form in clusters and associations rather than in isolation. milky way star clusters are easily observable with small telescopes, and in some cases even with the naked eye. depending on a variety of conditions, star clusters may dissolve quickly or be very long lived. the dynamical evolution of star clusters is a topic of very active research in astrophysics. some popular models of star clusters are the so-called direct n-body simulations [1, 2], where every star is represented by a point particle that interacts gravitationally with every other particle. this kind of simulation is computationally expensive, as it scales as o(n^2) where n is the number of particles in the simulated cluster. in the following, the words \"particle\" and \"star\" are used interchangeably.\n",
      "content\n",
      "this dataset contains the positions and velocities of simulated stars (particles) in a direct n-body simulation of a star cluster. in the cluster there are initially 64000 stars distributed in position-velocity space according to a king model [3]. each .csv file named c_xxxx.csv corresponds to a snapshot of the simulation at time t = xxxx. for example, c_0000.csv contains the initial conditions (positions and velocities of stars at time t=0). times are measured in standard n-body units [4]. this is a system of units where g = m = −4e = 1 (g is the gravitational constant, m the total mass of the cluster, and e its total energy).\n",
      "x, y, z columns 1, 2, and 3 of each file are the x, y, z positions of the stars. they are also expressed in standard n-body units [4]. you can switch to units of the median radius of the cluster by finding the cluster center and calculating the median distance of stars from it, and then dividing x, y, and z by this number. in general, the median radius changes in time. the initial conditions are approximately spherically symmetric (you can check) so there is no particular physical meaning attached to the choice of x, y, and z.\n",
      "vx, vy, vz columns 4, 5, and 6 contain the x, y, and z velocity, also in n-body units. a scale velocity for the stars can be obtained by taking the standard deviation of velocity along one direction (e.g. z). you may check that the ratio between the typical radius (see above) and the typical velocity is of order unity.\n",
      "m column 7 is the mass of each star. for this simulation this is identically 1.5625e-05, i.e. 1/64000. the total mass of the cluster is initially 1. more realistic simulations (coming soon) have a spectrum of different masses and live stelar evolution, that results in changes in the mass of stars. this simulation is a pure n-body problem instead.\n",
      "star id number the id numbers of each particle are listed in the last column (8) of the files under the header \"id\". the ids are unique and can be used to trace the position and velocity of a star across all files. there are initially 64000 particles. at end of the simulation there are 63970. this is because some particles escape the cluster.\n",
      "acknowledgements\n",
      "this simulation was run on a center for galaxy evolution research (cger) workstation at yonsei university (seoul, korea), using the nbody6 software (https://www.ast.cam.ac.uk/~sverre/web/pages/nbody.htm).\n",
      "inspiration\n",
      "some stars hover around the center of the cluster, while some other get kicked out to the cluster outskirts or even leave the cluster altogether. can we predict where a star will be at any given time based on its initial position and velocity? can we predict its velocity?\n",
      "how correlated are the motions of stars? can we predict the velocity of a given star based on the velocity of its neighbours?\n",
      "the size of the cluster can be measured by defining a center (see below) and finding the median distance of stars from it. this is called the three-dimensional effective radius. can we predict how it evolves over time? what are its properties as a time series? what can we say about other quantiles of the radius?\n",
      "how to define the cluster center? just as the mode of a kde of the distribution of stars? how does it move over time and how to quantify the properties of its fluctuations? is the cluster symmetric around this center?\n",
      "some stars leave the cluster: over time they exchange energy in close encounters with other stars and reach the escape velocity. this can be seen by comparing later snapshots with the initial one: some ids are missing and there is overall a lower number of stars. can we predict which stars are more likely to escape? when will a given star escape?\n",
      "references\n",
      "[1] heggie, d., hut, p. 2003, the gravitational million-body problem: a multidisciplinary approach to star cluster dynamics ~ cambridge university press, 2003\n",
      "[2] aarseth, s.~j. 2003, gravitational n-body simulations - cambridge university press, 2003\n",
      "[3] king, i. 1966, aj, 71, 64\n",
      "[4] heggie, d. c., mathieu, r. d. 1986, lecture notes in physics, vol. 267, the use of supercomputers in stellar dynamics, berlin, springer\n",
      "context\n",
      "this is a table of shark attack incidents compiled by the global shark attack file: please see their website for more details on where this data comes from.\n",
      "acknowledgements\n",
      "this data was downloaded with permission from the global shark attack file's website\n",
      "context\n",
      "pakistan drone attacks (2004-2016)\n",
      "the united states has targeted militants in the federally administered tribal areas [fata] and the province of khyber pakhtunkhwa [kpk] in pakistan via its predator and reaper drone strikes since year 2004. pakistan body count (www.pakistanbodycount.org) is the oldest and most accurate running tally of drone strikes in pakistan. the given database (pakistandroneattacks.csv) has been populated by using majority of the data from pakistan body count, and building up on it by canvassing open source newspapers, media reports, think tank analyses, and personal contacts in media and law enforcement agencies. we provide a count of the people killed and injured in drone strikes, including the ones who died later in hospitals or homes due to injuries caused or aggravated by drone strikes, making it the most authentic source for drone related data in this region.\n",
      "we will keep releasing the updates every quarter at this page.\n",
      "content\n",
      "geography: pakistan\n",
      "time period: 2004-2016\n",
      "unit of analysis: attack\n",
      "dataset: the dataset contains detailed information of 397 drone attacks in pakistan that killed an estimated 3,558 and injured 1,333 people including 2,539 civilians.\n",
      "variables: the dataset contains serial no, incident day & date, approximate time of the attack, specific location, city, province, number of people killed who claimed to be from al-qaeeda, number of people killed who claimed to be from taliban, minimum and maximum count of foreigners killed, minimum and maximum count of civilians killed, minimum and maximum count of civilians injured, special mention (more details) and comments about the attack, longitude and latitude of the location. sources: unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases.\n",
      "acknowledgements & references\n",
      "pakistan body count has been leveraged extensively in scholarly publications, reports, media articles and books. the website and the dataset has been collected and curated by the founder zeeshan-ul-hassan usmani. users are allowed to use, copy, distribute and cite the dataset as follows: “zeeshan-ul-hassan usmani, pakistan body count, drone attacks dataset, kaggle dataset repository, jan 25, 2017.”\n",
      "past research\n",
      "zeeshan-ul-hassan usmani and hira bashir, “the impact of drone strikes in pakistan”, cost of war project, brown university, december 16, 2014\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "• how many people got killed and injured per year in last 12 years?\n",
      "• how many attacks involved killing of actual terrorists from al-qaeeda and taliban?\n",
      "• how many attacks involved women and children?\n",
      "• visualize drone attacks on timeline\n",
      "• find out any correlation with number of drone attacks with specific date and time, for example, do we have more drone attacks in september?\n",
      "• find out any correlation with drone attacks and major global events (us funding to pakistan and/or afghanistan, friendly talks with terrorist outfits by local or foreign government?)\n",
      "• the number of drone attacks in bush vs obama tenure?\n",
      "• the number of drone attacks versus the global increase/decrease in terrorism?\n",
      "• correlation between number of drone strikes and suicide bombings in pakistan\n",
      "questions?\n",
      "for detailed visit www.pakistanbodycount.org\n",
      "or contact pakistan body count staff at info@pakistanbodycount.org\n",
      "south park cartoon lines\n",
      "+70k lines, annotated with season, episode and speaker\n",
      "it is interesting to practice nlp with ml techniques in order to guess who is speaking. later there will be file with pre-proccesed data to train\n",
      "quick start\n",
      "for a quick introduction to this dataset, take a look at the kernel traffic fatalities getting started.\n",
      "see the fatality analysis reporting system fars user’s manual for understanding the column abbreviations and possible values.\n",
      "also, see the following reference\n",
      "original source of this data containing all files can be obtained here\n",
      "below are the files released by the (nhtsa) national highway traffic safety administration, in their original format. additional files can be found in the extra folder. reference traffic fatalities getting started. for how to access this extra folder with contents.\n",
      "data compared to 2014\n",
      "a few interesting notes about this data compared to 2014\n",
      "pedalcyclist fatalities increased by 89 (12.2 percent)\n",
      "motorcyclist fatalities increased by 382 (8.3-percent increase)\n",
      "alcohol-impaired driving fatalities increased by 3.2 percent, from 9,943 in 2014 to 10,265 in 2015\n",
      "vehicle miles traveled (vmt) increased by 3.5 percent from 2014 to 2015, the largest increase since 1992, nearly 25 years ago.\n",
      "see traffic safety facts for more detail on the above findings.\n",
      "overview\n",
      "the world of warcraft avatar history dataset is a collection of records that detail information about player characters in the game over time. it includes information about their character level, race, class, location, and social guild. the kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the 'horde' faction of players in the game from a single game server).\n",
      "full dataset source and information: http://mmnet.iis.sinica.edu.tw/dl/wowah/\n",
      "code used to clean the data: https://github.com/myles-oneill/wowah-parser\n",
      "ideas for using the dataset\n",
      "from the perspective of game system designers, players' behavior is one of the most important factors they must consider when designing game systems. to gain a fundamental understanding of the game play behavior of online gamers, exploring users' game play time provides a good starting point. this is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network qos on users' behavior. it can even help us predict players' loyalty to specific games.\n",
      "open questions\n",
      "understand user gameplay behavior (game sessions, movement, leveling)\n",
      "understand user interactions (guilds)\n",
      "predict players unsubscribing from the game based on activity\n",
      "what are the most popular zones in wow, what level players tend to inhabit each?\n",
      "wrath of the lich king\n",
      "an expansion to world of warcraft, \"wrath of the lich king\" (wotlk) was released on november 13, 2008. it introduced new zones for players to go to, a new character class (the death knight), and a new level cap of 80 (up from 70 previously). this event intersects nicely with the dataset and is probably interesting to investigate.\n",
      "map\n",
      "this dataset doesn't include a shapefile (if you know of one that exists, let me know!) to show where the zones the dataset talks about are. here is a list of zones an information from this version of the game, including their recommended levels: http://wowwiki.wikia.com/wiki/zones_by_level_(original) .\n",
      "update (version 3): dmi3kno has generously put together some supplementary zone information files which have now been included in this dataset. some notes about the files:\n",
      "note that some zone names contain chinese characters. unicode names are preserved as a key to the original dataset. what this addition will allow is to understand properties of the zones a bit better - their relative location to each other, competititive properties, type of gameplay and, hopefully, their contribution to character leveling. location coordinates contain some redundant (and possibly duplicate) records as they are collected from different sources. working with uncleaned location coordinate data will allow users to demonstrate their data wrangling skills (both working with strings and spatial data).\n",
      "context\n",
      "pitchfork is a music-centric online magazine. it was started in 1995 and grew out of independent music reviewing into a general publication format, but is still famed for its variety music reviews. i scraped over 18,000 pitchfork reviews (going back to january 1999). initially, this was done to satisfy a few of my own curiosities, but i bet kagglers can come up with some really interesting analyses!\n",
      "content\n",
      "this dataset is provided as a sqlite database with the following tables: artists, content, genres, labels, reviews, years. for column-level information on specific tables, refer to the metadata tab.\n",
      "inspiration\n",
      "do review scores for individual artists generally improve over time, or go down?\n",
      "how has pitchfork's review genre selection changed over time?\n",
      "who are the most highly rated artists? the least highly rated artists?\n",
      "acknowledgements\n",
      "gotta love beautiful soup!\n",
      "context\n",
      "the challenge - one challenge of modeling retail data is the need to make decisions based on limited history. holidays and select major events come once a year, and so does the chance to see how strategic decisions impacted the bottom line. in addition, markdowns are known to affect sales – the challenge is to predict which departments will be affected and to what extent.\n",
      "content\n",
      "you are provided with historical sales data for 45 stores located in different regions - each store contains a number of departments. the company also runs several promotional markdown events throughout the year. these markdowns precede prominent holidays, the four largest of which are the super bowl, labor day, thanksgiving, and christmas. the weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks.\n",
      "within the excel sheet, there are 3 tabs – stores, features and sales\n",
      "stores\n",
      "anonymized information about the 45 stores, indicating the type and size of store\n",
      "features\n",
      "contains additional data related to the store, department, and regional activity for the given dates.\n",
      "store - the store number\n",
      "date - the week\n",
      "temperature - average temperature in the region\n",
      "fuel_price - cost of fuel in the region\n",
      "markdown1-5 - anonymized data related to promotional markdowns. markdown data is only available after nov 2011, and is not available for all stores all the time. any missing value is marked with an na\n",
      "cpi - the consumer price index\n",
      "unemployment - the unemployment rate\n",
      "isholiday - whether the week is a special holiday week\n",
      "sales\n",
      "historical sales data, which covers to 2010-02-05 to 2012-11-01. within this tab you will find the following fields:\n",
      "store - the store number\n",
      "dept - the department number\n",
      "date - the week\n",
      "weekly_sales -  sales for the given department in the given store\n",
      "isholiday - whether the week is a special holiday week\n",
      "the task\n",
      "predict the department-wide sales for each store for the following year\n",
      "model the effects of markdowns on holiday weeks\n",
      "provide recommended actions based on the insights drawn, with prioritization placed on largest business impact\n",
      "context\n",
      "healthstats provides key health, nutrition and population statistics gathered from a variety of international sources. themes include population dynamics, nutrition, reproductive health, health financing, medical resources and usage, immunization, infectious diseases, hiv/aids, daly, population projections and lending. healthstats also includes health, nutrition and population statistics by wealth quintiles.\n",
      "content\n",
      "this dataset includes 345 indicators, such as immunization rates, malnutrition prevalence, and vitamin a supplementation rates across 263 countries around the world. data was collected on a yearly basis from 1960-2016.\n",
      "inspiration\n",
      "in your opinion, what are some of the more surprising indicators? are there any you would consider adding?\n",
      "is there a relationship between condom use and rates of children born with hiv? how do these rates compare over time?\n",
      "which countries have the highest consumption of iodized salt? has this indicator changed over time, and if so, in which countries? are there any other indicators that seem to correlate with this one?\n",
      "acknowledgements\n",
      "data was acquired from the world bank, and can be accessed in multiple formats here.\n",
      "context\n",
      "this dataset is meant to be used with other datasets that have features like country and city but no latitude/longitude. it is simply a list of cities in the world. being able to put cities on a map will help people tell their stories more effectively. another way to think about it is that you can use this make more pretty graphs!\n",
      "content\n",
      "fields:\n",
      "city\n",
      "region\n",
      "country\n",
      "population\n",
      "latitude\n",
      "longitude\n",
      "acknowledgements\n",
      "these data come from maxmind.com and have not been altered. the original source can be found by clicking here\n",
      "additionally, the maxmind sharing license has been included.\n",
      "inspiration\n",
      "i wanted to analyze a dataset and make a map, but i was only given a city name without any latitude or longotude coordinates. i found this dataset very helpful and i hoe you do too!\n",
      "the customer support on twitter dataset is a large, modern corpus of tweets and replies to aid innovation in natural language understanding and conversational models, and for study of modern customer support practices and impact.\n",
      "context\n",
      "natural language remains the densest encoding of human experience we have, and innovation in nlp has accelerated to power understanding of that data, but the datasets driving this innovation don't match the real language in use today. the customer support on twitter dataset offers a large corpus of modern english (mostly) conversations between consumers and customer support agents on twitter, and has three important advantages over other conversational text datasets:\n",
      "focused - consumers contact customer support to have a specific problem solved, and the manifold of problems to be discussed is relatively small, especially compared to unconstrained conversational datasets like the reddit corpus.\n",
      "natural - consumers in this dataset come from a much broader segment than those in the ubuntu dialogue corpus and have much more natural and recent use of typed text than the cornell movie dialogs corpus.\n",
      "succinct - twitter's brevity causes more natural responses from support agents (rather than scripted), and to-the-point descriptions of problems and solutions. also, its convenient in allowing for a relatively low message limit size for recurrent nets.\n",
      "inspiration\n",
      "the size and breadth of this dataset inspires many interesting questions:\n",
      "can we predict company responses? given the bounded set of subjects handled by each company, the answer seems like yes!\n",
      "do requests get stale? how quickly do the best companies respond, compared to the worst?\n",
      "can we learn high quality dense embeddings or representations of similarity for topical clustering?\n",
      "how does tone affect the customer support conversation? does saying sorry help?\n",
      "can we help companies identify new problems, or ones most affecting their customers?\n",
      "content\n",
      "the dataset is a csv, where each row is a tweet. the different columns are described below. every conversation included has at least one request from a consumer and at least one response from a company. which user ids are company user ids can be calculated using the inbound field.\n",
      "tweet_id\n",
      "a unique, anonymized id for the tweet. referenced by response_tweet_id and in_response_to_tweet_id.\n",
      "author_id\n",
      "a unique, anonymized user id. @s in the dataset have been replaced with their associated anonymized user id.\n",
      "inbound\n",
      "whether the tweet is \"inbound\" to a company doing customer support on twitter. this feature is useful when re-organizing data for training conversational models.\n",
      "created_at\n",
      "date and time when the tweet was sent.\n",
      "text\n",
      "tweet content. sensitive information like phone numbers and email addresses are replaced with mask values like __email__.\n",
      "response_tweet_id\n",
      "ids of tweets that are responses to this tweet, comma-separated.\n",
      "in_response_to_tweet_id\n",
      "id of the tweet this tweet is in response to, if any.\n",
      "contributing\n",
      "know of other brands the dataset should include? found something that needs to be fixed? start a discussion, or email me directly at $firstname@$lastname.com!\n",
      "acknowledgements\n",
      "a huge thank you to my friends who helped bootstrap the list of companies that do customer support on twitter! there are many rocks that would have been left un-turned were it not for your suggestions!\n",
      "relevant resources\n",
      "nltk - casual_tokenize for social media text tokenizing, vader sentiment analysis for social media text\n",
      "scikit learn - bow count vectorizer, multinomial naive bayes classifier\n",
      "topic modeling via phrase detection with gensim\n",
      "facebook research - fasttext text classifier\n",
      "context\n",
      "recent growing interest in cryptocurrencies, specifically as a speculative investment vehicle, has sparked global conversation over the past 12 months. although this data is available across various sites, there is a lack of understanding as to what is driving the exponential rise of many individual currencies. this data set is intended to be a starting point for a detailed analysis into what is driving price action, and what can be done to predict future movement.\n",
      "content\n",
      "consolidated financial information for the top 200 cryptocurrencies by marketcap. pulled from coinmarketcap.com. attributes include:\n",
      "currency name (e.g. bitcoin)\n",
      "date\n",
      "open\n",
      "high\n",
      "low\n",
      "close\n",
      "volume\n",
      "marketcap\n",
      "inspiration\n",
      "for the past few months i have been searching for a reliable source for historical price information related to cryptocurrencies. i wasn't able to find anything that i could use to my liking, so i built my own data set.\n",
      "i've written a small script that scrapes historical price information for the top 200 coins by market cap as listed on coinmarketcap.com.\n",
      "i plan to run some basic analysis on it to answer questions that i have a \"gut\" feeling about, but no quantitative evidence (yet!).\n",
      "questions such as:\n",
      "what is the correlation between bitcoin and alt coin prices?\n",
      "what is the average age of the top 10 coins by market cap?\n",
      "what day of the week is best to buy/sell?\n",
      "which coins in the top two hundred are less than 6 months old?\n",
      "which currencies are the most volatile?\n",
      "what the hell happens when we go to bed and asia starts trading?\n",
      "feel free to use this for your own purposes! i just ask that you share your results with the group when complete. happy hunting!\n",
      "on february 11th 2016 ligo-virgo collaboration gave the announce of the discovery of gravitational waves, just 100 years after the einstein’s paper on their prediction. the ligo scientific collaboration (lsc) and the virgo collaboration prepared a web page to inform the broader community about a confirmed astrophysical event observed by the gravitational-wave detectors, and to make the data around that time available for others to analyze: https://losc.ligo.org/events/gw150914/\n",
      "you can find much more information on the losc web site, and a good starting tutorial at the following link:\n",
      "https://losc.ligo.org/tutorial00/\n",
      "these data sets contain 32 secs of data sampled at 4096hz an 16384hz around the gw event detected on 14/09/2015.\n",
      "longer sets of data can be downloaded here\n",
      "https://losc.ligo.org/s/events/gw150914/h-h1_losc_4_v1-1126257414-4096.hdf5\n",
      "https://losc.ligo.org/s/events/gw150914/l-l1_losc_4_v1-1126257414-4096.hdf5\n",
      "https://losc.ligo.org/s/events/gw150914/h-h1_losc_16_v1-1126257414-4096.hdf5\n",
      "https://losc.ligo.org/s/events/gw150914/l-l1_losc_16_v1-1126257414-4096.hdf5\n",
      "how to acknowledge use of this data: if your research used data from one of the data releases, please cite as:\n",
      "ligo scientific collaboration, \"ligo open science center release of s5\", 2014, doi 10.7935/k5wd3xhr\n",
      "ligo scientific collaboration, \"ligo open science center release of s6\", 2015, doi 10.7935/k5rn35sd\n",
      "ligo scientific collaboration, \"ligo open science center release of gw150914\", 2016, doi10.7935/k5mw2f23\n",
      "and please include the statement \"this research has made use of data, software and/or web tools obtained from the ligo open science center (https://losc.ligo.org), a service of ligo laboratory and the ligo scientific collaboration. ligo is funded by the u.s. national science foundation.\"\n",
      "if you would also like to cite a published paper, m vallisneri et al. \"the ligo open science center\", proceedings of the 10th lisa symposium, university of florida, gainesville, may 18-23, 2014; also arxiv:1410.4839\n",
      "publications we request that you let the losc team know if you publish (or intend to publish) a paper using data released from this site. if you would like, we may be able to review your work prior to publication, as we do for our colleagues in the ligo scientific collaboration. credits losc development: the losc team and the ligo scientific collaboration\n",
      "the data products made available through the losc web service are created and maintained by ligo lab and the ligo scientific collaboration. the development of this web page was a team effort, with all members of the losc team making contributions in most areas. in addition to the team members listed below, a large number of individuals in the ligo scientific collaboration have contributed content and advice. the losc team includes:\n",
      "alan weinstein: losc director\n",
      "roy williams: losc developer, web services and data base architecture\n",
      "jonah kanner: losc developer, tutorials, documentation, data set curation\n",
      "michele vallisneri: losc developer, data quality curation\n",
      "branson stephens: losc developer, event database and web site architecture\n",
      "please send any comments, questions, or concerns to: losc@ligo.org\n",
      "drosophila melanogaster\n",
      "drosophila melanogaster, the common fruit fly, is a model organism which has been extensively used in entymological research. it is one of the most studied organisms in biological research, particularly in genetics and developmental biology.\n",
      "when its not being used for scientific research, d. melanogaster is a common pest in homes, restaurants, and anywhere else that serves food. they are not to be confused with tephritidae flys (also known as fruit flys).\n",
      "https://en.wikipedia.org/wiki/drosophila_melanogaster\n",
      "about the genome\n",
      "this genome was first sequenced in 2000. it contains four pairs of chromosomes (2,3,4 and x/y). more than 60% of the genome appears to be functional non-protein-coding dna.\n",
      "the genome is maintained and frequently updated at flybase. this dataset is sourced from the ucsc genome bioinformatics download page. it uses the august 2014 version of the d. melanogaster genome (dm6, bdgp release 6 + iso1 mt). http://hgdownload.soe.ucsc.edu/downloads.html#fruitfly\n",
      "files were modified by kaggle to be a better fit for analysis on scripts. this primarily involved turning files into csv format, with a header row, as well as converting the genome itself from 2bit format into a fasta sequence file.\n",
      "bioinformatics\n",
      "genomic analysis can be daunting to data scientists who haven't had much experience with bioinformatics before. we have tried to give basic explanations to each of the files in this dataset, as well as links to further reading on the biological basis for each. if you haven't had the chance to study much biology before, some light reading (ie wikipedia) on the following topics may be helpful to understand the nuances of the data provided here: genetics, genomics (sequencing/genome assembly), chromosomes, dna, rna (mrna/mirna), genes, alleles, exons, introns, transcription, translation, peptides, proteins, gene regulation, mutation, phylogenetics, and snps.\n",
      "of course, if you've got some idea of the basics already - don't be afraid to jump right in!\n",
      "learning bioinformatics\n",
      "there are a lot of great resources for learning bioinformatics on the web. one cool site is rosalind - a platform that gives you bioinformatic coding challenges to complete. you can use kaggle scripts on this dataset to easily complete the challenges on rosalind (and see myles' solutions here if you get stuck). we have set up biopython on kaggle's docker image which is a great library to help you with your analyses. check out their tutorial here and we've also created a python notebook with some of the tutorial applied to this dataset as a reference.\n",
      "files in this dataset\n",
      "drosophila melanogaster genome\n",
      "genome.fa\n",
      "the assembled genome itself is presented here in fasta format. each chromosome is a different sequence of nucleotides. repeats from repeatmasker and tandem repeats finder (with period of 12 or less) are show in lower case; non-repeating sequence is shown in upper case.\n",
      "meta information\n",
      "there are 3 additional files with meta information about the genome.\n",
      "meta-cpg-island-ext-unmasked.csv\n",
      "this file contains descriptive information about cpg islands in the genome.\n",
      "https://en.wikipedia.org/wiki/cpg_site\n",
      "meta-cytoband.csv\n",
      "this file describes the positions of cytogenic bands on each chromosome.\n",
      "https://en.wikipedia.org/wiki/cytogenetics\n",
      "meta-simple-repeat.csv\n",
      "this file describes simple tandem repeats in the genome.\n",
      "https://en.wikipedia.org/wiki/repeated_sequence_(dna) https://en.wikipedia.org/wiki/tandem_repeat\n",
      "drosophila melanogaster mrna sequences\n",
      "messenger rna (mrna) is an intermediate molecule created as part of the cellular process of converting genomic information into proteins. some mrna are never translated into proteins and have functional roles in the cell on their own. collectively, organism mrna information is known as a transcriptome. mrna files included in this dataset give insight into the activity of genes in the organism.\n",
      "https://en.wikipedia.org/wiki/messenger_rna\n",
      "mrna-genbank.fa\n",
      "this file includes all mrna sequences from genbank associated with drosophila melanogaster.\n",
      "http://www.ncbi.nlm.nih.gov/genbank/\n",
      "mrna-refseq.fa\n",
      "this file includes all mrna sequences from refseq associated with drosophila melanogaster.\n",
      "http://www.ncbi.nlm.nih.gov/refseq/\n",
      "gene predictions\n",
      "a gene is a segment of dna on the genome which, through mrna, is used to create proteins in the organism. knowing which parts of dna are coding (genes) or non-coding is difficult, and a number of different systems for prediction exist. this dataset includes a number of different gene prediction systems applied to the drosophila melanogaster genome.\n",
      "https://en.wikipedia.org/wiki/gene_prediction\n",
      "genes-augustus.csv\n",
      "augustus is a piece of software that predicts genes ab initio using hidden markov models. http://www.ncbi.nlm.nih.gov/pmc/articles/pmc441517/\n",
      "genes-genscan.csv\n",
      "genscan is an older ab initio software for predicting genes. http://genes.mit.edu/genscaninfo.html\n",
      "genes-ensembl.csv\n",
      "ensembl-gtp.csv\n",
      "ensembl-pep.csv\n",
      "ensembl-source.csv\n",
      "ensembl-to-gene-name.csv\n",
      "ensembl provides gene annotation generated by their software genebuild. this process combines automatic annotation alongside manual curation. http://uswest.ensembl.org/info/genome/genebuild/genome_annotation.html\n",
      "we have also included some supplementary files for these, including predicted protein peptide sequences for each predicted gene.\n",
      "genes-refseq.csv\n",
      "genes-xeno-refseq.csv\n",
      "refseq-link.csv\n",
      "refseq-summary.csv\n",
      "we have included two refseq gene predictions in this dataset. the first is based solely on information from the drosophila melanogaster genome. the second (genes-xeno-refseq.csv) uses genes from other organisms as a basis for predicting genes in drosophila melanogaster.\n",
      "refseq rnas were aligned against the d. melanogaster genome using blat; those with an alignment of less than 15% were discarded. when a single rna aligned in multiple places, the alignment having the highest base identity was identified. only alignments having a base identity level within 0.1% of the best and at least 96% base identity with the genomic sequence were kept.\n",
      "we have also included supplementary files for these which include information about the genes that have been identified.\n",
      "http://www.ncbi.nlm.nih.gov/refseq/\n",
      "what can you do with this data?\n",
      "genomic data is the foundation of bioinformatics, and there is an incredible array of things you can do with this data. a good place to start is to look at some of the meta supplementary files alongside the genomic sequence itself.\n",
      "we have a number of different gene prediction systems in the dataset, how do they compare to each other? how do they compare to the mrna data?\n",
      "working back from the refseq-summary.csv file, you can look at genes that code for particular proteins - can you find these genes in the genome?\n",
      "how much of the genome codes for the mrna's found in our mrna data? of the mrna's we have, how many map to the predicted genes and the predicted peptided sequence data? how much of the mrna seems to be protein-coding vs how much looks like it is mirna? can you find pre-mrna or splice variants within the mrna data? does meta information like cytogenic bands or cpg sites correspond with splice variants or a lack of mrna altogether?\n",
      "those are just some of many ideas that could get you started.\n",
      "looking for feedback\n",
      "this is the first genomic dataset on kaggle and we are looking for feedback from our community about how interesting this dataset is to them, or if there are ways we could improve it to better suit analysis. please post suggestions for supplementary data, future genomes we could host, bioinformatics packages we should include on scripts, and any other feedback on the dataset forum.\n",
      "context\n",
      "i wanted to see how articles clustered together if the articles were rendered into document-term matrices---would there be greater affinity among political affiliations, or medium, subject matter, etc. the data was scraped using beautifulsoup and stored in sqlite, but i've chopped it up into three separate csvs here, because the entire sqlite database came out to about 1.2 gb, beyond kaggle's max.\n",
      "content\n",
      "each row contains:\n",
      "an id for the sqlite database\n",
      "author name\n",
      "full date\n",
      "month\n",
      "year\n",
      "title\n",
      "publication name\n",
      "article url (not available for all articles)\n",
      "full article content\n",
      "the publications include the new york times, breitbart, cnn, business insider, the atlantic, fox news, talking points memo, buzzfeed news, national review, new york post, the guardian, npr, reuters, vox, and the washington post. sampling wasn't quite scientific; i chose publications based on my familiarity of the domain and tried to get a range of political alignments, as well as a mix of print and digital publications. by count, the publications break down accordingly:\n",
      "it's not entirely even---this was something of a collect-it-all approach, and some sites are more prolific than others, and some have data that maintains integrity after scraping more easily than others.\n",
      "for each publication, i used archive.org to grab the past year-and-a-half of either home-page headlines or rss feeds and ran those links through the scraper. that is, the articles are not the product of scraping an entire site, but rather their more prominently placed articles. for example, cnn's articles from 5/6/16 were what appeared on the homepage of cnn.com proper, not everything within the cnn.com domain. vox's articles from 5/6/16 were everything that appeared in the vox rss reader. on 5/6/16, and so on. rss readers are a breeze to scrape, and so i used them when possible, but not every publication uses them or makes them easy to find.\n",
      "the data primarily falls between the years of 2016 and july 2017, although there is a not-insignificant number of articles from 2015, and a possibly insignificant number from before then.\n",
      "a note: there are some stray spaces between non-word characters at times as well as some other minor blemishes and imperfections here and there, the result of cleaning a very messy dataset.\n",
      "acknowledgements\n",
      "thanks mostly go to the maesters of stack overflow.\n",
      "this dataset consists of 101 animals from a zoo. there are 16 variables with various traits to describe the animals. the 7 class types are: mammal, bird, reptile, fish, amphibian, bug and invertebrate\n",
      "the purpose for this dataset is to be able to predict the classification of the animals, based upon the variables. it is the perfect dataset for those who are new to learning machine learning.\n",
      "zoo.csv\n",
      "attribute information: (name of attribute and type of value domain)\n",
      "animal_name: unique for each instance\n",
      "hair boolean\n",
      "feathers boolean\n",
      "eggs boolean\n",
      "milk boolean\n",
      "airborne boolean\n",
      "aquatic boolean\n",
      "predator boolean\n",
      "toothed boolean\n",
      "backbone boolean\n",
      "breathes boolean\n",
      "venomous boolean\n",
      "fins boolean\n",
      "legs numeric (set of values: {0,2,4,5,6,8})\n",
      "tail boolean\n",
      "domestic boolean\n",
      "catsize boolean\n",
      "class_type numeric (integer values in range [1,7])\n",
      "class.csv\n",
      "this csv describes the dataset\n",
      "class_number numeric (integer values in range [1,7])\n",
      "number_of_animal_species_in_class numeric\n",
      "class_type character -- the actual word description of the class\n",
      "animal_names character -- the list of the animals that fall in the category of the class\n",
      "acknowledgements\n",
      "uci machine learning: https://archive.ics.uci.edu/ml/datasets/zoo\n",
      "source information -- creator: richard forsyth -- donor: richard s. forsyth 8 grosvenor avenue mapperley park nottingham ng3 5dx 0602-621676 -- date: 5/15/1990\n",
      "inspiration\n",
      "what are the best machine learning ensembles/methods for classifying these animals based upon the variables given?\n",
      "acknowledgements\n",
      "the data was scraped from booking.com. all data in the file is publicly available to everyone already. data is originally owned by booking.com. please contact me through my profile if you want to use this dataset somewhere else.\n",
      "data context\n",
      "this dataset contains 515,000 customer reviews and scoring of 1493 luxury hotels across europe. meanwhile, the geographical location of hotels are also provided for further analysis.\n",
      "data content\n",
      "the csv file contains 17 fields. the description of each field is as below:\n",
      "hotel_address: address of hotel.\n",
      "review_date: date when reviewer posted the corresponding review.\n",
      "average_score: average score of the hotel, calculated based on the latest comment in the last year.\n",
      "hotel_name: name of hotel\n",
      "reviewer_nationality: nationality of reviewer\n",
      "negative_review: negative review the reviewer gave to the hotel. if the reviewer does not give the negative review, then it should be: 'no negative'\n",
      "review_total_negative_word_counts: total number of words in the negative review.\n",
      "positive_review: positive review the reviewer gave to the hotel. if the reviewer does not give the negative review, then it should be: 'no positive'\n",
      "review_total_positive_word_counts: total number of words in the positive review.\n",
      "reviewer_score: score the reviewer has given to the hotel, based on his/her experience\n",
      "total_number_of_reviews_reviewer_has_given: number of reviews the reviewers has given in the past.\n",
      "total_number_of_reviews: total number of valid reviews the hotel has.\n",
      "tags: tags reviewer gave the hotel.\n",
      "days_since_review: duration between the review date and scrape date.\n",
      "additional_number_of_scoring: there are also some guests who just made a scoring on the service rather than a review. this number indicates how many valid scores without review in there.\n",
      "lat: latitude of the hotel\n",
      "lng: longtitude of the hotel\n",
      "in order to keep the text data clean, i removed unicode and punctuation in the text data and transform text into lower case. no other preprocessing was performed.\n",
      "inspiration\n",
      "the dataset is large and informative, i believe you can have a lot of fun with it! let me put some ideas below to futher inspire kagglers!\n",
      "fit a regression model on reviews and score to see which words are more indicative to a higher/lower score\n",
      "perform a sentiment analysis on the reviews\n",
      "find correlation between reviewer's nationality and scores.\n",
      "beautiful and informative visualization on the dataset.\n",
      "clustering hotels based on reviews\n",
      "simple recommendation engine to the guest who is fond of a special characteristic of hotel.\n",
      "the idea is unlimited! please, have a look into data, generate some ideas and leave a master kernel here! i am ready to upvote your ideas and kernels! cheers!\n",
      "this dataset contains historical prices as tracked by www.coinmarketcap.com for the top 100 cryptocurrencies by market capitalization as of september 22, 2017, and is current to that date.\n",
      "each csv file is named by its cryptocurrency as named on www.coinmarketcap.com, with the sole exception of \"i-o coin\" in place of i/o coin for ease of importing.\n",
      "also accompanying the zip of the top 100 csvs is a csv named \"top 100.csv\", which is a list of the top 100, ordered 1 to 100 with bitcoin at the beginning and gridcoin at the end. the second row of this csv is the market cap as of september 22, 2017.\n",
      "row descriptions - date, string, e.g. \"sep 22, 2017\" - open, float (2 decimal places), e.g. 1234.00 - high, float (2 decimal places), e.g. 1234.00 - low, float (2 decimal places), e.g. 1234.00 - close, float (2 decimal places), e.g. 1234.00 - volume [traded in 24 hours], string, e.g. \"1,234,567,890\" - market cap [market capitalization], string, e.g. \"1,234,567,890\"\n",
      "this is my first dataset and i would greatly appreciate your feedback. thanks and enjoy!\n",
      "context\n",
      "ihme united states mortality rates by county 1980-2014: national - all. (deaths per 100,000 population)\n",
      "to quickly get started creating maps, like the one below, see the quick start r kernel.\n",
      "how the dataset was created\n",
      "this dataset was created from the excel spreadsheet, which can be found in the download. or, you can view the source here. if you take a look at the row for united states, for the column mortality rate, 1980*, you'll see the set of numbers 1.52 (1.44, 1.61). numbers in parentheses are 95% uncertainty. the 1.52 is an age-standardized mortality rate for both sexes combined (deaths per 100,000 population).\n",
      "in this dataset 1.44 will be placed in the named column mortality rage, 1989 (min)* and 1.61 is in column named mortality rate, 1980 (max)* . for information on how these age-standardized mortality rates were calculated, see the december jama 2016 article, which you can download for free.\n",
      "reference\n",
      "jama full article\n",
      "video describing this study (short and this is worth viewing)\n",
      "data resources\n",
      "how americans die may depend on where they live, by anna maria barry-jester (fivethirtyeight)\n",
      "interactive map from healthdata.org\n",
      "ihme data\n",
      "acknowledgements\n",
      "this dataset was provided by ihme\n",
      "institute for health metrics and evaluation 2301 fifth ave., suite 600, seattle, wa 98121, usa tel: +1.206.897.2800 fax: +1.206.897.2899 © 2016 university of washington\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "india - annual health survey(ahs) 2012-13:\n",
      "the survey was conducted in empowered action group (eag) states uttarakhand, rajasthan, uttar pradesh, bihar, jharkhand, odisha, chhattisgarh & madhya pradesh and assam. these nine states, which account for about 48 percent of the total population, 59 percent of births, 70 percent of infant deaths, 75 percent of under 5 deaths and 62 percent of maternal deaths in the country, are the high focus states in view of their relatively higher fertility and mortality.\n",
      "a representative sample of about 21 million population and 4.32 million households were covered 20k+ sample units which is spread across rural and urban area of these 9 states.\n",
      "the objective of the ahs is to yield a comprehensive, representative and reliable dataset on core vital indicators including composite ones like infant mortality rate, maternal mortality ratio and total fertility rate along with their co-variates (process and outcome indicators) at the district level and map the changes therein on an annual basis. these benchmarks would help in better and holistic understanding and timely monitoring of various determinants on well-being and health of population particularly reproductive and child health. source\n",
      "content\n",
      "this dataset contains the data about the below 26 key indicators.\n",
      "aa. sample particulars\n",
      "sample units\n",
      "households\n",
      "population\n",
      "ever married women (aged 15-49 years)\n",
      "currently married women (aged 15-49 years)\n",
      "children 12-23 months\n",
      "bb. household characteristics\n",
      "average household size\n",
      "sc\n",
      "st\n",
      "all\n",
      "population below age 15 years (%)\n",
      "dependency ratio\n",
      "currently married illiterate women aged 15-49 years (%)\n",
      "cc. sex ratio\n",
      "sex ratio at birth\n",
      "sex ratio (0- 4 years)\n",
      "sex ratio (all ages)\n",
      "dd. effective literacy rate\n",
      "ee. marriage\n",
      "marriages among females below legal age (18 years) (%)\n",
      "marriages among males below legal age (21 years) (%)\n",
      "currently married women aged 20-24 years married before legal age (18 years) (%)\n",
      "currently married men aged 25-29 years married before legal age (21 years) (%)\n",
      "mean age at marriage# - male\n",
      "mean age at marriage# - female\n",
      "ff. schooling status\n",
      "children currently attending school (age 6-17 years) (%)\n",
      "children attended before / drop out (age 6-17 years) (%)\n",
      "gg. work status\n",
      "children aged 5-14 years engaged in work (%)\n",
      "work participation rate (15 years and above)\n",
      "hh. disability\n",
      "prevalence of any type of disability (per 100,000 population)\n",
      "ii. injury\n",
      "number of injured persons by type of treatment received (per 100,000 population)\n",
      "severe\n",
      "major\n",
      "minor\n",
      "jj. acute illness\n",
      "persons suffering from acute illness (per 100,000 population)\n",
      "diarrhoea/dysentery\n",
      "acute respiratory infection (ari)\n",
      "fever (all types)\n",
      "any type of acute illness\n",
      "persons suffering from acute illness and taking treatment from any source (%)\n",
      "persons suffering from acute illness and taking treatment from government source (%)\n",
      "kk. chronic illness\n",
      "having any kind of symptoms of chronic illness (per 100,000 population)\n",
      "having any kind of symptoms of chronic illness and sought medical care (%)\n",
      "having diagnosed for chronic illness (per 100,000 population)\n",
      "diabetes\n",
      "hypertension\n",
      "tuberculosis (tb)\n",
      "asthma / chronic respiratory disease\n",
      "arthritis\n",
      "any kind of chronic illness\n",
      "having diagnosed for any kind of chronic illness and getting regular treatment (%)\n",
      "having diagnosed for any kind of chronic illness and getting regular treatment from government source (%)\n",
      "ll. fertility\n",
      "crude birth rate (cbr)\n",
      "natural growth rate\n",
      "total fertility rate\n",
      "women aged 20-24 reporting birth of order 2 & above (%)\n",
      "women reporting birth of order 3 & above (%)\n",
      "women with two children wanting no more children (%)\n",
      "women aged 15-19 years who were already mothers or pregnant at the time of survey (%)\n",
      "median age at first live birth of women aged 15-49 years\n",
      "median age at first live birth of women aged 25-49 years\n",
      "live births taking place after an interval of 36 months (%)\n",
      "mean number of children ever born to women aged 15-49 years\n",
      "mean number of children surviving to women aged 15-49 years\n",
      "mean number of children ever born to women aged 45-49 years\n",
      "mm. abortion\n",
      "pregnancy to women aged 15-49 years resulting in abortion (%)\n",
      "women who received any anc before abortion (%)\n",
      "women who went for ultrasound before abortion (%)\n",
      "average month of pregnancy at the time of abortion\n",
      "abortion performed by skilled health personnel (%)\n",
      "abortion taking place in institution (%)\n",
      "nn. family planning practices (cmw aged 15-49 years)\n",
      "current usage\n",
      "any method (%)\n",
      "any modern method (%)\n",
      "female sterilization (%)\n",
      "male sterilization (%)\n",
      "copper-t/iud (%)\n",
      "pills (%)\n",
      "condom/nirodh (%)\n",
      "emergency contraceptive pills (%)\n",
      "any traditional method (%)\n",
      "periodic abstinence (%)\n",
      "withdrawal (%)\n",
      "lam (%)\n",
      "oo. unmet need for family planning\n",
      "unmet need for spacing (%)\n",
      "unmet need for limiting (%)\n",
      "total unmet need (%)\n",
      "pp. ante natal care\n",
      "currently married pregnant women aged 15-49 years registered for anc (%)\n",
      "mothers who received any antenatal check-up (%)\n",
      "mothers who had antenatal check-up in first trimester (%)\n",
      "mothers who received 3 or more antenatal care (%)\n",
      "mothers who received at least one tetanus toxoid (tt) injection (%)\n",
      "mothers who consumed ifa for 100 days or more (%)\n",
      "mothers who had full antenatal check-up (%)\n",
      "mothers who received anc from govt. source (%)\n",
      "mothers whose blood pressure (bp) taken (%)\n",
      "mothers whose blood taken for hb (%)\n",
      "mothers who underwent ultrasound (%)\n",
      "qq. delivery care\n",
      "institutional delivery (%)\n",
      "delivery at government institution (%)\n",
      "delivery at private institution (%)\n",
      "delivery at home (%)\n",
      "delivery at home conducted by skilled health personnel (%)\n",
      "safe delivery (%)\n",
      "caesarean out of total delivery taken place in government institutions (%)\n",
      "caesarean out of total delivery taken place in private institutions (%)\n",
      "rr. post natal care\n",
      "less than 24 hrs. stay in institution after delivery (%)\n",
      "mothers who received post-natal check-up within 48 hrs. of delivery (%)\n",
      "mothers who received post-natal check-up within 1 week of delivery (%)\n",
      "mothers who did not receive any post-natal check-up (%)\n",
      "new borns who were checked up within 24 hrs. of birth (%)\n",
      "ss. janani suraksha yojana (jsy)\n",
      "mothers who availed financial assistance for delivery under jsy (%)\n",
      "mothers who availed financial assistance for institutional delivery under jsy (%)\n",
      "mothers who availed financial assistance for government institutional delivery under jsy (%)\n",
      "tt. immunization, vitamin a & iron supplement and birth weight\n",
      "children aged 12-23 months having immunization card (%)\n",
      "children aged 12-23 months who have received bcg (%)\n",
      "children aged 12-23 months who have received 3 doses of polio vaccine (%)\n",
      "children aged 12-23 months who have received 3 doses of dpt vaccine (%)\n",
      "children aged 12-23 months who have received measles vaccine (%)\n",
      "children aged 12-23 months fully immunized (%)\n",
      "children who have received polio dose at birth (%)\n",
      "children who did not receive any vaccination (%)\n",
      "children (aged 6-35 months) who received at least one vitamin a dose during last six months (%)\n",
      "children (aged 6-35 months) who received ifa tablets/syrup during last 3 months (%)\n",
      "children whose birth weight was taken (%)\n",
      "children with birth weight less than 2.5 kg. (%)\n",
      "uu. childhood diseases\n",
      "children suffering from diarrhoea (%)\n",
      "children suffering from diarrhoea who received haf/ors/ort (%)\n",
      "children suffering from acute respiratory infection (%)\n",
      "children suffering from acute respiratory infection who sought treatment (%)\n",
      "children suffering from fever (%)\n",
      "children suffering from fever who sought treatment (%)\n",
      "vv. breastfeeding and supplementation\n",
      "children breastfed within one hour of birth (%)\n",
      "children (aged 6-35 months) exclusively breastfed for at least six months (%)\n",
      "children who received foods other than breast milk during first 6 months\n",
      "water (%)\n",
      "animal/formula milk (%)\n",
      "semi-solid mashed food (%)\n",
      "solid (adult) food (%)\n",
      "vegetables/fruits (%)\n",
      "average month by which children received foods other than breast milk\n",
      "water (%)\n",
      "animal/formula milk (%)\n",
      "semi-solid mashed food (%)\n",
      "solid (adult) food (%)\n",
      "vegetables/fruits (%)\n",
      "ww. birth registration\n",
      "birth registered (%)\n",
      "children whose birth was registered and received birth certificate (%)\n",
      "xx. awareness on hiv/aids, rti/sti, haf/ors/ort/zinc and ari/pneumonia\n",
      "women who are aware of:\n",
      "hiv/aids\n",
      "rti/sti\n",
      "haf/ors/ort/zinc\n",
      "danger signs of ari/pneumonia (%)\n",
      "yy. mortality (unit level data of mortality is available here)\n",
      "crude death rate (cdr)\n",
      "infant mortality rate (imr)\n",
      "neo-natal mortality rate\n",
      "under five mortality rate (u5mr)\n",
      "zz. confidence interval (95%) for some important indicators\n",
      "crude birth rate - (lower and upper limit)\n",
      "crude death rate - (lower and upper limit)\n",
      "infant mortality rate - (lower and upper limit)\n",
      "under five mortality rate (u5mr) - (lower and upper limit)\n",
      "sex ratio at birth - (lower and upper limit)\n",
      "acknowledgements\n",
      "department of health and family welfare, govt. of india has published this data in open govt data platform india portal under govt. open data license - india.\n",
      "context\n",
      "the paradise papers is a cache of some 13gb of data that contains 13.4 million confidential records of offshore investment by 120,000 people and companies in 19 tax jurisdictions (tax heavens - an awesome video to understand this); that was published by the international consortium of investigative journalists (icij) on november 5, 2017. here is a brief video about the leak. the people include queen elizabeth ii, the president of columbia (juan manuel santos), former prime minister of pakistan (shaukat aziz), u.s secretary of commerce (wilbur ross) and many more. according to an estimate by the boston consulting group, the amount of money involved is around $10 trillion. the leak contains many famous companies, including facebook, apple, uber, nike, walmart, allianz, siemens, mcdonald’s and yahoo.\n",
      "it also contains a lot of u. s president donald trump allies including rax tillerson, wilbur ross, koch brothers, paul singer, sheldon adelson, stephen schwarzman, thomas barrack and steve wynn etc. the complete list of politicians involve is avaiable here.\n",
      "the panama papers in the cache of 38gb of data from the national corporate registry of bahamas. it contains world’s top politicians and influential persons as head and director of offshore companies registered in bahamas.\n",
      "offshore leaks details 13,000 offshore accounts in a report.\n",
      "i am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye.\n",
      "content\n",
      "the data is the effort of more than 100 journalists from 60+ countries\n",
      "the original data is available under creative common license and can be downloaded from this link.\n",
      "i will keep updating the datasets with more leaks and data as it’s available\n",
      "acknowledgements\n",
      "international consortium of investigative journalists (icij)\n",
      "paradise papers update\n",
      "paradise papers data has been uploaded as released by icij on nov 21, 2017. you can find paradise papers zip file and six extracted files in csv format, all starting with a prefix of paradise. happy coding!\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "how many companies and individuals are there in all of the leaks data\n",
      "how many countries involved\n",
      "total money involved\n",
      "what is the biggest best tax heaven\n",
      "can we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country\n",
      "who are the biggest cheaters and where they live\n",
      "what role fortune 500 companies play in this game\n",
      "i need your help to make this world corruption free in the age of nlp and big data\n",
      "many people say the gender gap in income levels is overstated in the united states, where some say that inequality in the labor force is a thing of the past. is there a gender gap at all? is it stronger in some industries than in others?\n",
      "this dataset, retrieved from the bureau of labor statistics, shows the median weekly incomes for 535 different occupations. the data encompasses information for all working american citizens as of january 2015. the incomes are broken into male and female statistics, preceded by the total median income when including both genders. the data has been re-formatted from the original pdf-friendly arrangement to make it easier to clean and analyze.\n",
      "analysis thus far has found that there is indeed a sizable gender gap between male and female incomes. use of this dataset should cite the bureau of labor statistics as per their copyright information:\n",
      "the bureau of labor statistics (bls) is a federal government agency and everything that we publish, both in hard copy and electronically, is in the public domain, except for previously copyrighted photographs and illustrations. you are free to use our public domain material without specific permission, although we do ask that you cite the bureau of labor statistics as the source.\n",
      "osmi mental health in tech survey 2016\n",
      "currently over 1400 responses, the ongoing 2016 survey aims to measure attitudes towards mental health in the tech workplace, and examine the frequency of mental health disorders among tech workers.\n",
      "how will this data be used?\n",
      "we are interested in gauging how mental health is viewed within the tech/it workplace, and the prevalence of certain mental health disorders within the tech industry. the open sourcing mental illness team of volunteers will use this data to drive our work in raising awareness and improving conditions for those with mental health disorders in the it workplace.\n",
      "context\n",
      "the national health and nutrition examination survey (nhanes) is a program of studies designed to assess the health and nutritional status of adults and children in the united states. the survey is unique in that it combines interviews and physical examinations. nhanes is a major program of the national center for health statistics (nchs). nchs is part of the centers for disease control and prevention (cdc) and has the responsibility for producing vital and health statistics for the nation.\n",
      "the nhanes program began in the early 1960s and has been conducted as a series of surveys focusing on different population groups or health topics. in 1999, the survey became a continuous program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs. the survey examines a nationally representative sample of about 5,000 persons each year. these persons are located in counties across the country, 15 of which are visited each year.\n",
      "the nhanes interview includes demographic, socioeconomic, dietary, and health-related questions. the examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel.\n",
      "to date, thousands of research findings have been published using the nhanes data.\n",
      "content\n",
      "the 2013-2014 nhanes datasets include the following components:\n",
      "demographics dataset:\n",
      "a complete variable dictionary can be found here\n",
      "examinations dataset, which contains:\n",
      "blood pressure\n",
      "body measures\n",
      "muscle strength - grip test\n",
      "oral health - dentition\n",
      "taste & smell\n",
      "a complete variable dictionary can be found here\n",
      "dietary data - total nutrient intake, first day:\n",
      "a complete variable dictionary can be found here\n",
      "laboratory dataset, which includes:\n",
      "albumin & creatinine - urine\n",
      "apolipoprotein b\n",
      "blood lead, cadmium, total mercury, selenium, and manganese\n",
      "blood mercury: inorganic, ethyl and methyl\n",
      "cholesterol - hdl\n",
      "cholesterol - ldl & triglycerides\n",
      "cholesterol - total\n",
      "complete blood count with 5-part differential - whole blood\n",
      "copper, selenium & zinc - serum\n",
      "fasting questionnaire\n",
      "fluoride - plasma\n",
      "fluoride - water\n",
      "glycohemoglobin\n",
      "hepatitis a\n",
      "hepatitis b surface antibody\n",
      "hepatitis b: core antibody, surface antigen, and hepatitis d antibody\n",
      "hepatitis c rna (hcv-rna) and hepatitis c genotype\n",
      "hepatitis e: igg & igm antibodies\n",
      "herpes simplex virus type-1 & type-2\n",
      "hiv antibody test\n",
      "human papillomavirus (hpv) - oral rinse\n",
      "human papillomavirus (hpv) dna - vaginal swab: roche cobas & roche linear array\n",
      "human papillomavirus (hpv) dna results from penile swab samples: roche linear array\n",
      "insulin\n",
      "iodine - urine\n",
      "perchlorate, nitrate & thiocyanate - urine\n",
      "perfluoroalkyl and polyfluoroalkyl substances (formerly polyfluoroalkyl chemicals - pfc)\n",
      "personal care and consumer product chemicals and metabolites\n",
      "phthalates and plasticizers metabolites - urine\n",
      "plasma fasting glucose\n",
      "polycyclic aromatic hydrocarbons (pah) - urine\n",
      "standard biochemistry profile\n",
      "tissue transglutaminase assay (iga-ttg) & iga endomyseal antibody assay (iga ema)\n",
      "trichomonas - urine\n",
      "two-hour oral glucose tolerance test\n",
      "urinary chlamydia\n",
      "urinary mercury\n",
      "urinary speciated arsenics\n",
      "urinary total arsenic\n",
      "urine flow rate\n",
      "urine metals\n",
      "urine pregnancy test\n",
      "vitamin b12\n",
      "a complete data dictionary can be found here\n",
      "questionnaire dataset, which includes information on:\n",
      "acculturation\n",
      "alcohol use\n",
      "blood pressure & cholesterol\n",
      "cardiovascular health\n",
      "consumer behavior\n",
      "current health status\n",
      "dermatology\n",
      "diabetes\n",
      "diet behavior & nutrition\n",
      "disability\n",
      "drug use\n",
      "early childhood\n",
      "food security\n",
      "health insurance\n",
      "hepatitis\n",
      "hospital utilization & access to care\n",
      "housing characteristics\n",
      "immunization\n",
      "income\n",
      "medical conditions\n",
      "mental health - depression screener\n",
      "occupation\n",
      "oral health\n",
      "osteoporosis\n",
      "pesticide use\n",
      "physical activity\n",
      "physical functioning\n",
      "preventive aspirin use\n",
      "reproductive health\n",
      "sexual behavior\n",
      "sleep disorders\n",
      "smoking - cigarette use\n",
      "smoking - household smokers\n",
      "smoking - recent tobacco use\n",
      "smoking - secondhand smoke exposure\n",
      "taste & smell\n",
      "weight history\n",
      "weight history - youth\n",
      "a complete variable dictionary can be found here\n",
      "medication dataset, which includes prescription medications:\n",
      "a complete variable dictionary can be found here\n",
      "acknowledgements\n",
      "original data and additional documents related to the datasets or nhanes can be found here.\n",
      "twitter has played an increasingly prominent role in the 2016 us presidential election. debates have raged and candidates have risen and fallen based on tweets.\n",
      "this dataset provides ~3000 recent tweets from hillary clinton and donald trump, the two major-party presidential nominees.\n",
      "kaggle’s march machine learning mania competition challenged data scientists to predict winners and losers of the men's 2016 ncaa basketball tournament. this dataset contains the 1070 selected predictions of all kaggle participants. these predictions were collected and locked in prior to the start of the tournament.\n",
      "how can this data be used? you can pivot it to look at both kaggle and ncaa teams alike. you can look at who will win games, which games will be close, which games are hardest to forecast, or which kaggle teams are gambling vs. sticking to the data.\n",
      "the ncaa tournament is a single-elimination tournament that begins with 68 teams. there are four games, usually called the “play-in round,” before the traditional bracket action starts. due to competition timing, these games are included in the prediction files but should not be used in analysis, as it’s possible that the prediction was submitted after the play-in round games were over.\n",
      "data description\n",
      "each kaggle team could submit up to two prediction files. the prediction files in the dataset are in the 'predictions' folder and named according to:\n",
      "teamname_teamid_submissionid.csv\n",
      "the file format contains a probability prediction for every possible game between the 68 teams. this is necessary to cover every possible tournament outcome. each team has a unique numerical id (given in teams.csv). each game has a unique id column created by concatenating the year and the two team ids. the format is the following:\n",
      "id,pred\n",
      "2016_1112_1114,0.6\n",
      "2016_1112_1122,0\n",
      "...\n",
      "the team with the lower numerical id is always listed first. “pred” represents the probability that the team with the lower id beats the team with the higher id. for example, \"2016_1112_1114,0.6\" indicates team 1112 has a 0.6 probability of beating team 1114.\n",
      "for convenience, we have included the data files from the 2016 march mania competition dataset in the scripts environment (you may find tourneyslots.csv and tourneyseeds.csv useful for determining matchups, see the documentation). however, the focus of this dataset is on kagglers' predictions.\n",
      "context: annotated corpus for named entity recognition using gmb(groningen meaning bank) corpus for entity classification with enhanced and popular features by natural language processing applied to the data set.\n",
      "tip: use pandas dataframe to load dataset if using python for convenience.\n",
      "content: this is the extract from gmb corpus which is tagged, annotated and built specifically to train the classifier to predict named entities such as name, location, etc.\n",
      "number of tagged entities:\n",
      "'o': 1146068', geo-nam': 58388, 'org-nam': 48034, 'per-nam': 23790, 'gpe-nam': 20680, 'tim-dat': 12786, 'tim-dow': 11404, 'per-tit': 9800, 'per-fam': 8152, 'tim-yoc': 5290, 'tim-moy': 4262, 'per-giv': 2413, 'tim-clo': 891, 'art-nam': 866, 'eve-nam': 602, 'nat-nam': 300, 'tim-nam': 146, 'eve-ord': 107, 'per-ini': 60, 'org-leg': 60, 'per-ord': 38, 'tim-dom': 10, 'per-mid': 1, 'art-add': 1\n",
      "essential info about entities:\n",
      "geo = geographical entity\n",
      "org = organization\n",
      "per = person\n",
      "gpe = geopolitical entity\n",
      "tim = time indicator\n",
      "art = artifact\n",
      "eve = event\n",
      "nat = natural phenomenon\n",
      "total words count = 1354149 target data column: \"tag\"\n",
      "inspiration: this dataset is getting more interested because of more features added to the recent version of this dataset. also, it helps to create a broad view of feature engineering with respect to this dataset.\n",
      "why this dataset is helpful or playful?\n",
      "it might not sound so interested for earlier versions, but when you are able to pick intent and custom named entities from your own sentence with more features then, it is getting interested and helps you solve real business problems(like picking entities from electronic medical records, etc)\n",
      "please, feel free to ask questions, do variations and let's play together!\n",
      "contents\n",
      "this dataset contains detailed specifications, release dates, and release prices of computer parts.\n",
      "the dataset contains two csv files: gpus.csv for graphics processing units (gpus), and cpus.csv for central processing units (cpus). each table has its own list of unique entries, but the list of features includes: clock speeds, maximum temperatures, display resolutions, power draws, number of threads, release dates, release prices, die size, virtualization support, and many other similar fields. for more specific column-level metadata refer to the column metadata.\n",
      "looking for inspiration? try starting by reading \"using regression to predict the gpus of the future\".\n",
      "inspiration\n",
      "how did performance over price ratio evolve over time?\n",
      "how about general computing power?\n",
      "are there any manufacturers that are known for some specific range of performance & price?\n",
      "acknowledgements\n",
      "the data given here belongs mainly to intel, game-debate, and the companies involved in producing the part. i do not own the data i uploaded it solely for informative purposes, under their original license.\n",
      "context\n",
      "eeg devices are becoming cheaper and more inconspicuous, but few applications leverage eeg data effectively, in part because there are few large repositories of eeg data. the mids class at the uc berkeley school of information is sharing a dataset collected using consumer-grade brainwave-sensing headsets, along with the software code and visual stimulus used to collect the data. the dataset includes all subjects' readings during the stimulus presentation, as well as readings from before the start and after the end of the stimulus.\n",
      "content\n",
      "we presented two slightly different stimuli to two different groups. stimuli 1 is available here, and stimuli 2 is available here.\n",
      "for both stimuli, a group of about 15 people saw the stimuli at the same time, while eeg data was being collected. the stimuli each person saw is available in the session field of subject-metadata.csv. (subjects who saw stimulus 2 left the room during stimulus 1, and vice versa).\n",
      "find the synchronized times for both stimuli in stimulus-timing.csv.\n",
      "for each participant, we also anonymously collected some other metadata: (1) whether or not they had previously seen the video displayed during the stimulus (a superbowl ad), (2) gender, (3) whether or not they saw hidden icons displayed during the color counting exercise, and (4) their chosen color during the color counting exercise. all of these can be found in subject-metadata.csv.\n",
      "we also collected the timing (in indra_time) of all stimulus events for both session 1 and session 2. these times are included in stimulus-times.csv.\n",
      "the server receives one data packet every second from each mindwave mobile device, and stores the data in one row entry.\n",
      "acknowledgements\n",
      "please use the following citation if you publish your research results using this dataset or software code or stimulus file:\n",
      "john chuang, nick merrill, thomas maillart, and students of the uc berkeley spring 2015 mids immersion class. \"synchronized brainwave recordings from a group presented with a common audio-visual stimulus (may 9, 2015).\" may 2015.\n",
      "summary\n",
      "the data is a preprocessed subset of the tcia study named soft tissue sarcoma. the data have been converted from dicom folders of varying resolution and data types to 3d hdf5 arrays with isotropic voxel size. this should make it easier to get started and test out various approaches (nn, rf, crf, etc) to improve segmentations.\n",
      "tcia summary\n",
      "this collection contains fdg-pet/ct and anatomical mr (t1-weighted, t2-weighted with fat-suppression) imaging data from 51 patients with histologically proven soft-tissue sarcomas (stss) of the extremities. all patients had pre-treatment fdg-pet/ct and mri scans between november 2004 and november 2011. (note: date in the tcia images have been changed in the interest of de-identification; the same change was applied across all images, preserving the time intervals between serial scans). during the follow-up period, 19 patients developed lung metastases. imaging data and lung metastases development status were used in the following study:\n",
      "vallières, m. et al. (2015). a radiomics model from joint fdg-pet and mri texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. physics in medicine and biology, 60(14), 5471-5496. doi:10.1088/0031-9155/60/14/5471.\n",
      "imaging data, tumor contours (rtstruct dicom objects), clinical data and source code is available for this study. see the doi below for more details and links to access the whole dataset. please contact martin vallières (mart.vallieres@gmail.com) of the medical physics unit of mcgill university for any scientific inquiries about this dataset.\n",
      "acknowledgements\n",
      "we would like to acknowledge the individuals and institutions that have provided data for this collection: mcgill university, montreal, canada - special thanks to martin vallières of the medical physics unit\n",
      "license\n",
      "this collection is freely available to browse, download, and use for commercial, scientific and educational purposes as outlined in the creative commons attribution 3.0 unported license. see tcia's data usage policies and restrictions for additional details. questions may be directed to help@cancerimagingarchive.net.\n",
      "citation\n",
      "data citation\n",
      "vallières, martin, freeman, carolyn r., skamene, sonia r., & el naqa, issam. (2015). a radiomics model from joint fdg-pet and mri texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. the cancer imaging archive. http://doi.org/10.7937/k9/tcia.2015.7go2gsks\n",
      "publication citation\n",
      "vallières, m., freeman, c. r., skamene, s. r., & naqa, i. el. (2015, june 29). a radiomics model from joint fdg-pet and mri texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. physics in medicine and biology. iop publishing. http://doi.org/10.1088/0031-9155/60/14/5471\n",
      "tcia citation\n",
      "clark k, vendt b, smith k, freymann j, kirby j, koppel p, moore s, phillips s, maffitt d, pringle m, tarbox l, prior f. the cancer imaging archive (tcia): maintaining and operating a public information repository, journal of digital imaging, volume 26, number 6, december, 2013, pp 1045-1057. (paper)\n",
      "context\n",
      "zillow operates an industry-leading economics and analytics bureau led by zillow’s chief economist, dr. stan humphries. at zillow, dr. humphries and his team of economists and data analysts produce extensive housing data and analysis covering more than 500 markets nationwide. zillow research produces various real estate, rental and mortgage-related metrics and publishes unique analyses on current topics and trends affecting the housing market.\n",
      "at zillow’s core is our living database of more than 100 million u.s. homes, featuring both public and user-generated information including number of bedrooms and bathrooms, tax assessments, home sales and listing data of homes for sale and for rent. this data allows us to calculate, among other indicators, the zestimate, a highly accurate, automated, estimated value of almost every home in the country as well as the zillow home value index and zillow rent index, leading measures of median home values and rents.\n",
      "content\n",
      "the zillow rent index is the median estimated monthly rental price for a given area, and covers multifamily, single family, condominium, and cooperative homes in zillow’s database, regardless of whether they are currently listed for rent. it is expressed in dollars and is seasonally adjusted. the zillow rent index is published at the national, state, metro, county, city, neighborhood, and zip code levels.\n",
      "zillow produces rent estimates (rent zestimates) based on proprietary statistical and machine learning models. within each county or state, the models observe recent rental listings and learn the relative contribution of various home attributes in predicting prevailing rents. these home attributes include physical facts about the home, prior sale transactions, tax assessment information and geographic location as well as the estimated market value of the home (zestimate). based on the patterns learned, these models estimate rental prices on all homes, including those not presently for rent. because of the availability of zillow rental listing data used to train the models, rent zestimates are only available back to november 2010; therefore, each zri time series starts on the same date.\n",
      "acknowledgements\n",
      "the rent index data was calculated from zillow's proprietary rent zestimates and published on its website.\n",
      "inspiration\n",
      "what city has the highest and lowest rental prices in the country? which metropolitan area is the most expensive to live in? where have rental prices increased in the past five years and where have they remained the same? what city or state has the lowest cost per square foot?\n",
      "openaq is an open-source project to surface live, real-time air quality data from around the world. their “mission is to enable previously impossible science, impact policy and empower the public to fight air pollution.” the data includes air quality measurements from 5490 locations in 47 countries.\n",
      "scientists, researchers, developers, and citizens can use this data to understand the quality of air near them currently. the dataset only includes the most current measurement available for the location (no historical data).\n",
      "update frequency: weekly\n",
      "querying bigquery tables\n",
      "you can use the bigquery python client library to query tables in this dataset in kernels. note that methods available in kernels are limited to querying data. tables are at bigquery-public-data.openaq.[tablename]. fork this kernel to get started.\n",
      "acknowledgements\n",
      "dataset source: openaq.org\n",
      "use: this dataset is publicly available for anyone to use under the following terms provided by the dataset source and is provided \"as is\" without any warranty, express or implied.\n",
      "this dataset contains headlines, urls, and categories for 422,937 news stories collected by a web aggregator between march 10th, 2014 and august 10th, 2014.\n",
      "news categories included in this dataset include business; science and technology; entertainment; and health. different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.\n",
      "content\n",
      "the columns included in this dataset are:\n",
      "id : the numeric id of the article\n",
      "title : the headline of the article\n",
      "url : the url of the article\n",
      "publisher : the publisher of the article\n",
      "category : the category of the news item; one of: -- b : business -- t : science and technology -- e : entertainment -- m : health\n",
      "story : alphanumeric id of the news story that the article discusses\n",
      "hostname : hostname where the article was posted\n",
      "timestamp : approximate timestamp of the article's publication, given in unix time (seconds since midnight on jan 1, 1970)\n",
      "acknowledgments\n",
      "this dataset comes from the uci machine learning repository. any publications that use this data should cite the repository as follows:\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "this specific dataset can be found in the uci ml repository at this url\n",
      "inspiration\n",
      "what kinds of questions can we explore using this dataset? here are a few possibilities:\n",
      "can we predict the category (business, entertainment, etc.) of a news article given only its headline?\n",
      "can we predict the specific story that a news article refers to, given only its headline?\n",
      "context\n",
      "a recent guardian blog post asks: \"how many endangered languages are there in the world and what are the chances they will die out completely?\" the united nations education, scientific and cultural organisation (unesco) regularly publishes a list of endangered languages, using a classification system that describes its danger (or completion) of extinction.\n",
      "content\n",
      "the full detailed dataset includes names of languages, number of speakers, the names of countries where the language is still spoken, and the degree of endangerment. the unesco endangerment classification is as follows:\n",
      "vulnerable: most children speak the language, but it may be restricted to certain domains (e.g., home)\n",
      "definitely endangered: children no longer learn the language as a 'mother tongue' in the home\n",
      "severely endangered: language is spoken by grandparents and older generations; while the parent generation may understand it, they do not speak it to children or among themselves\n",
      "critically endangered: the youngest speakers are grandparents and older, and they speak the language partially and infrequently\n",
      "extinct: there are no speakers left\n",
      "acknowledgements\n",
      "data was originally organized and published by the guardian, and can be accessed via this datablog post.\n",
      "inspiration\n",
      "how can you best visualize this data?\n",
      "which rare languages are more isolated (sicilian, for example) versus more spread out? can you come up with a hypothesis for why that is the case?\n",
      "can you compare the number of rare speakers with more relatable figures? for example, are there more romani speakers in the world than there are residents in a small city in the united states?\n",
      "curious about the growth of wind energy? the extent to which the decline of coal is an american or international trend? interested in using energy consumption as an alternate method of comparing national economies? this dataset has you covered.\n",
      "the energy statistics database contains comprehensive energy statistics on the production, trade, conversion and final consumption of primary and secondary; conventional and non-conventional; and new and renewable sources of energy.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nations statistics division on the undata site. you can find the original dataset here.\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "introduction\n",
      "the mobile phone activity dataset is composed by one week of call details records (cdrs) from the city of milan and the province of trentino (italy).\n",
      "description of the dataset\n",
      "every time a user engages a telecommunication interaction, a radio base station (rbs) is assigned by the operator and delivers the communication through the network. then, a new cdr is created recording the time of the interaction and the rbs which handled it. the following activities are present in the dataset:\n",
      "received sms\n",
      "sent sms\n",
      "incoming calls\n",
      "outgoing calls\n",
      "internet activity\n",
      "in particular, internet activity is generated each time a user starts an internet connection or ends an internet connection. moreover, during the same connection a cdr is generated if the connection lasts for more than 15 min or the user transferred more than 5 mb.\n",
      "the datasets is spatially aggregated in a square cells grid. the area of milan is composed of a grid overlay of 1,000 (squares with size of about 235×235 meters. this grid is projected with the wgs84 (epsg:4326) standard. for more details we link the original paper http://go.nature.com/2fcox5e\n",
      "the data provides cellid, countrycode and all the aforementioned telecommunication activities aggregated every 60 minutes.\n",
      "original datasource\n",
      "the mobile phone activity dataset is a part of the telecom italia big data challenge 2014, which is a rich and open multi-source aggregation of telecommunications, weather, news, social networks and electricity data from the city of milan and the province of trentino (italy). the original dataset has been created by telecom italia in association with eit ict labs, spaziodati, mit media lab, northeastern university, polytechnic university of milan, fondazione bruno kessler, university of trento and trento rise. in order to make it easy-to-use, here we provide a subset of telecommunications data that allows researchers to design algorithms able to exploit an enormous number of behavioral and social indicators. the complete version of the dataset is available at the following link: http://go.nature.com/2fz4afr\n",
      "relevant, external, data sources\n",
      "the presented datasets can be enriched by using census data provided by the italian national institute of statistics (istat) (http://www.istat.it/en/), a public research organization and the main provider of official statistics in italy. the census data have been released for 1999, 2001 and 2011. the dataset (http://www.istat.it/it/archivio/104317), released in italian, is composed of four parts: territorial bases (basi territoriali), administrative boundaries (confini amministrativi), census variables (variabili censuarie) and data about toponymy (dati toponomastici).\n",
      "motivational video: https://www.youtube.com/watch?v=_d2_rwmsukc\n",
      "relevant papers\n",
      "blondel, vincent d., adeline decuyper, and gautier krings. \"a survey of results on mobile phone datasets analysis.\" epj data science 4, no. 1 (2015): 1.\n",
      "francesco calabrese, laura ferrari, and vincent d. blondel. 2014. urban sensing using mobile phone network data: a survey of research. acm comput. surv. 47, 2, article 25 (november 2014), 20 pages.\n",
      "eagle, nathan, michael macy, and rob claxton. \"network diversity and economic development.\" science 328, no. 5981 (2010): 1029-1031.\n",
      "lenormand, maxime, miguel picornell, oliva g. cantú-ros, thomas louail, ricardo herranz, marc barthelemy, enrique frías-martínez, maxi san miguel, and josé j. ramasco. \"comparing and modelling land use organization in cities.\" royal society open science 2, no. 12 (2015): 150449.\n",
      "louail, thomas, maxime lenormand, oliva g. cantu ros, miguel picornell, ricardo herranz, enrique frias-martinez, josé j. ramasco, and marc barthelemy. \"from mobile phone data to the spatial structure of cities.\" scientific reports 4 (2014).\n",
      "de nadai, marco, jacopo staiano, roberto larcher, nicu sebe, daniele quercia, and bruno lepri. \"the death and life of great italian cities: a mobile phone data perspective.\" www, 2016.\n",
      "citation\n",
      "we kindly ask people who use this dataset to cite the following paper, where this aggregation comes from:\n",
      "barlacchi, gianni, marco de nadai, roberto larcher, antonio casella, cristiana chitic, giovanni torrisi, fabrizio antonelli, alessandro vespignani, alex pentland, and bruno lepri. \"a multi-source dataset of urban life in the city of milan and the province of trentino.\" scientific data 2 (2015).\n",
      "magic the gathering (mtg, or just magic) is a trading card game first published in 1993 by wizards of the coast. this game has seen immense popularity and new cards are still released every few months. the strength of different cards in the game can vary wildly and as a result some cards now sell on secondary markets for as high as thousands of dollars.\n",
      "mtg json has an excellent collection of every single magic card - stored in json data. version 3.6 (collected september 21, 2016) of their database is provided here.\n",
      "full documentation for the data is provided here: http://mtgjson.com/documentation.html\n",
      "also, if you want to include images of the cards in your writeups, you can grab them from the official wizards of the coast website using the following url:\n",
      "http://gatherer.wizards.com/handlers/image.ashx?multiverseid=180607&type=card\n",
      "just replace the multiverse id with the one provided in the mtgjson file.\n",
      "context\n",
      "i created this dataset to explore different factors affecting people's enjoyment of food and/or cooking!\n",
      "content\n",
      "over 20k recipes listed by recipe rating, nutritional information and assigned category (sparse). i may later upload a version binned by recipe creation date and also including recipe ingredients.\n",
      "use the 'full_format_recipes.json' file to interact with all recipe data, 'epi_r.csv' drops ingredients and directions in favour of sparse category dummies.\n",
      "acknowledgements\n",
      "recipe information lifted from: http://www.epicurious.com/recipes-menus\n",
      "problem statement\n",
      "kickstarter is a community of more than 10 million people comprising of creative, tech enthusiasts who help in bringing creative project to life. till now, more than $3 billion dollars have been contributed by the members in fueling creative projects. the projects can be literally anything – a device, a game, an app, a film etc.\n",
      "kickstarter works on all or nothing basis i.e if a project doesn’t meet it goal, the project owner gets nothing. for example: if a projects’s goal is $500. even if it gets funded till $499, the project won’t be a success.\n",
      "recently, kickstarter released its public data repository to allow researchers and enthusiasts like us to help them solve a problem. will a project get fully funded ?\n",
      "in this challenge, you have to predict if a project will get successfully funded or not.\n",
      "data description\n",
      "there are three files given to download: train.csv, test.csv and sample_submission.csv the train data consists of sample projects from the may 2009 to may 2015. the test data consists of projects from june 2015 to march 2017.\n",
      "dataset information\n",
      "this dataset is a collection of state and national polls conducted from november 2015-november 2016 on the 2016 presidential election. data on the raw and weighted poll results by state, date, pollster, and pollster ratings are included.\n",
      "content\n",
      "there are 27 variables:\n",
      "cycle\n",
      "branch\n",
      "type\n",
      "matchup\n",
      "forecastdate\n",
      "state:\n",
      "startdate\n",
      "enddate\n",
      "pollster\n",
      "grade\n",
      "samplesize\n",
      "populaion\n",
      "poll_wt\n",
      "rawpoll_clinton\n",
      "rawpoll_trump\n",
      "rawpoll_johnson\n",
      "rawpoll_mcmullin\n",
      "adjpoll_clinton\n",
      "adjpoll_trump\n",
      "adjpoll_johnson\n",
      "adjpoll_mcmullin\n",
      "multiversions\n",
      "url\n",
      "poll_id\n",
      "question_id\n",
      "createddate\n",
      "timestamp\n",
      "inspiration\n",
      "some questions for exploring this dataset are:\n",
      "what are the trends of the polls over time (by day/week/month)?\n",
      "how do the trends vary by state?\n",
      "what is the probability that trump/clinton will win the 2016 election?\n",
      "acknowledgements\n",
      "the original dataset is from the fivethirtyeight 2016 election forecast and can be downloaded from here. poll results were aggregated from huffpost pollster, realclearpolitics, polling firms and news reports.\n",
      "context\n",
      "the datasets describe ratings and free-text tagging activities from movielens, a movie recommendation service. it contains 20000263 ratings and 465564 tag applications across 27278 movies. these data were created by 138493 users between january 09, 1995 and march 31, 2015. this dataset was generated on october 17, 2016.\n",
      "users were selected at random for inclusion. all selected users had rated at least 20 movies.\n",
      "content\n",
      "no demographic information is included. each user is represented by an id, and no other information is provided.\n",
      "the data are contained in six files.\n",
      "tag.csv that contains tags applied to movies by users:\n",
      "userid\n",
      "movieid\n",
      "tag\n",
      "timestamp\n",
      "rating.csv that contains ratings of movies by users:\n",
      "userid\n",
      "movieid\n",
      "rating\n",
      "timestamp\n",
      "movie.csv that contains movie information:\n",
      "movieid\n",
      "title\n",
      "genres\n",
      "link.csv that contains identifiers that can be used to link to other sources:\n",
      "movieid\n",
      "imdbid\n",
      "tmbdid\n",
      "genome_scores.csv that contains movie-tag relevance data:\n",
      "movieid\n",
      "tagid\n",
      "relevance\n",
      "genome_tags.csv that contains tag descriptions:\n",
      "tagid\n",
      "tag\n",
      "acknowledgements\n",
      "the original datasets can be found here. to acknowledge use of the dataset in publications, please cite the following paper:\n",
      "f. maxwell harper and joseph a. konstan. 2015. the movielens datasets: history and context. acm transactions on interactive intelligent systems (tiis) 5, 4, article 19 (december 2015), 19 pages. doi=http://dx.doi.org/10.1145/2827872\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "which genres receive the highest ratings? how does this change over time?\n",
      "determine the temporal trends in the genres/tagging activity of the movies released\n",
      "context\n",
      "the stanford question answering dataset (squad) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of wikipedia articles. the answer to every question is a segment of text, or span, from the corresponding reading passage. there are 100,000+ question-answer pairs on 500+ articles.\n",
      "content\n",
      "there are two files to help you get started with the dataset and evaluate your models:\n",
      "*train-v1.1.json*fasdf\n",
      "dev-v1.1.json\n",
      "acknowledgements\n",
      "the original datasets can be found here.\n",
      "inspiration\n",
      "can you build a prediction model that can accurately predict answers to different types of questions?\n",
      "you can also explore squad here\n",
      "context\n",
      "promptcloud extracted 400 thousand reviews of unlocked mobile phones sold on amazon.com to find out insights with respect to reviews, ratings, price and their relationships.\n",
      "content\n",
      "given below are the fields:\n",
      "product title\n",
      "brand\n",
      "price\n",
      "rating\n",
      "review text\n",
      "number of people who found the review helpful\n",
      "data was acquired in december, 2016 by the crawlers build to deliver our data extraction services.\n",
      "initial analysis\n",
      "it can be accessed here: http://www.kdnuggets.com/2017/01/data-mining-amazon-mobile-phone-reviews-interesting-insights.html\n",
      "background:\n",
      "the underlying concept behind hedging strategies is simple, create a model, and make money doing it. the hardest part is finding the features that matter. for a more in-depth look at hedging strategies, i have attached one of my graduate papers to get you started.\n",
      "mortgage-backed securities\n",
      "geographic business investment\n",
      "real estate analysis\n",
      "for any questions, you may reach us at research_development@goldenoakresearch.com. for immediate assistance, you may reach me on at 585-626-2965. please note: the number is my personal number and email is preferred\n",
      "statistical fields:\n",
      "note: all interpolated statistical data include mean, median, and standard deviation statistics. for more information view the variable definitions document.\n",
      "monthly mortgage & owner costs: sum of mortgage payments, home equity loans, utilities, property taxes\n",
      "monthly owner costs: sum of utilities, property taxes\n",
      "gross rent: contract rent plus the estimated average monthly cost of utilities\n",
      "household income: sum of the householder and all other individuals +15 years who reside in the household\n",
      "family income: sum of incomes of all members +15 years of age related to the householder.\n",
      "location fields:\n",
      "note: the location fields were derived from a variety of sources. the zip code, area code, and city were derived using a heuristic. all other locations used the census geo id to cross reference data between multiple datasets.\n",
      "state name, abbreviation, number: reported by the u.s. census bureau\n",
      "county name: reported by the u.s. census bureau\n",
      "location type: specifies classification of location {city, village, town, cpd, ..., etc.}\n",
      "area code: defined via heuristic.\n",
      "zip code: defined via heuristic.\n",
      "city: defined via heuristic.\n",
      "access all 325,258 location of our most complete database ever:\n",
      "monetize risk and optimize your portfolio instantly at an unbeatable price. don't settle. go big and win big. access all gross rent records and more on a scale roughly equivalent to a neighborhood, see link below:\n",
      "full dataset: view full dataset\n",
      "real estate research: view research\n",
      "dataset for people who double on music and data science\n",
      "how it began\n",
      "one fine day, when someone with the idea of self sufficient ai capable of writing it's own poems wanted data, he stumbled upon the idea of using songs as a source. the journey wasn't easy, since each song has it's own page with the lyrics and scraping pages one at a time (when there are over a million of them) is a slow task. i worked up some optimisations here and there. for people interested in going through the process:\n",
      "github project.\n",
      "content\n",
      "there is a lot to play with here, though all of it vertically. there isn't a lot of variation in the types of fields until you look deep enough.\n",
      "the main dataset is split up into 2 files, each containing ~250,000 songs with their artists and lyrics. the files are titled lyrics1 and lyrics2. the fields include band name, song name and lyrics.\n",
      "go through the exploration scripts for how to use the lyrics data set and other interesting observations.\n",
      "another file containing urls of description pages of different artists is also provided, titled artisturls.\n",
      "acknowledgements\n",
      "after seeing the response for the fifa player dataset it was exciting as ever to get this one off.\n",
      "soon it was realised, not a lot of places exist where extracting data is straightforward.\n",
      "then one stumbles upon the perfectly indexed page : https://www.lyrics.com/. they deserve major credit for the existence of this dataset.\n",
      "inspiration\n",
      "this started off as a source for some kind of intelligent poet which writes poems on it's own. it would be great to see what the artificially intelligent world has to express once it knows enough, and beautifully if at all?\n",
      "pokemon sun and moon (released november 18th, 2016) are the latest games in the widely popular pokemon video game franchise. pokemon games are usually released in pairs (red and blue, gold and silver, x and y, etc.) and collectively each pair that introduces new pokemon to the game is known as a generation. pokemon sun and moon are the 7th generation, adding new pokemon to the franchise as well as adjusting the stats and movesets of some of the older pokemon. (please note that the recently popular pokemongo games are unrelated to the pokemon video games).\n",
      "this dataset contains a full set of in-game statistics for all 802 pokemon in the sun and moon. it also includes full information on which pokemon can learn which moves (movesets.csv), what moves can do (moves.csv), and how damage is modified by pokemon type (type-chart.csv).\n",
      "pokemon battle simulation\n",
      "with the level of detail in the data provided here it is possible to almost fully simulate pokemon battles using the information provided (status effects and some other nuances are still missing). if you are interested in simulating how pokemon battles would pan out between different opponents make sure to read up on the math behind how pokemon battles work.\n",
      "context\n",
      "insee is the official french institute gathering data of many types around france. it can be demographic (births, deaths, population density...), economic (salary, firms by activity / size...) and more.\n",
      "it can be a great help to observe and measure inequality in the french population.\n",
      "content\n",
      "four files are in the dataset :\n",
      "base_etablissement_par_tranche_effectif : give information on the number of firms in every french town, categorized by size , come from insee.\n",
      "codgeo : geographique code for the town (can be joined with *code_insee* column from \"name_geographic_information.csv')\n",
      "libgeo : name of the town (in french)\n",
      "reg : region number\n",
      "dep : depatment number\n",
      "e14tst : total number of firms in the town\n",
      "e14ts0nd : number of unknown or null size firms in the town\n",
      "e14ts1 : number of firms with 1 to 5 employees in the town\n",
      "e14ts6 : number of firms with 6 to 9 employees in the town\n",
      "e14ts10 : number of firms with 10 to 19 employees in the town\n",
      "e14ts20 : number of firms with 20 to 49 employees in the town\n",
      "e14ts50 : number of firms with 50 to 99 employees in the town\n",
      "e14ts100 : number of firms with 100 to 199 employees in the town\n",
      "e14ts200 : number of firms with 200 to 499 employees in the town\n",
      "e14ts500 : number of firms with more than 500 employees in the town\n",
      "name_geographic_information : give geographic data on french town (mainly latitude and longitude, but also region / department codes and names )\n",
      "eu_circo : name of the european union circonscription\n",
      "code_région : code of the region attached to the town\n",
      "nom_région : name of the region attached to the town\n",
      "chef.lieu_région : name the administrative center around the town\n",
      "numéro_département : code of the department attached to the town\n",
      "nom_département : name of the department attached to the town\n",
      "préfecture : name of the local administrative division around the town\n",
      "numéro_circonscription : number of the circumpscription\n",
      "nom_commune : name of the town\n",
      "codes_postaux : post-codes relative to the town\n",
      "code_insee : unique code for the town\n",
      "latitude : gps latitude\n",
      "longitude : gps longitude\n",
      "éloignement : i couldn't manage to figure out what was the meaning of this number\n",
      "net_salary_per_town_per_category : salaries around french town per job categories, age and sex\n",
      "codgeo : unique code of the town\n",
      "libgeo : name of the town\n",
      "snhm14 : mean net salary\n",
      "snhmc14 : mean net salary per hour for executive\n",
      "snhmp14 : mean net salary per hour for middle manager\n",
      "snhme14 : mean net salary per hour for employee\n",
      "snhmo14 : mean net salary per hour for worker\n",
      "snhmf14 : mean net salary for women\n",
      "snhmfc14 : mean net salary per hour for feminin executive\n",
      "snhmfp14 : mean net salary per hour for feminin middle manager\n",
      "snhmfe14 : mean net salary per hour for feminin employee\n",
      "snhmfo14 : mean net salary per hour for feminin worker\n",
      "snhmh14 : mean net salary for man\n",
      "snhmhc14 : mean net salary per hour for masculin executive\n",
      "snhmhp14 : mean net salary per hour for masculin middle manager\n",
      "snhmhe14 : mean net salary per hour for masculin employee\n",
      "snhmho14 : mean net salary per hour for masculin worker\n",
      "snhm1814 : mean net salary per hour for 18-25 years old\n",
      "snhm2614 : mean net salary per hour for 26-50 years old\n",
      "snhm5014 : mean net salary per hour for >50 years old\n",
      "snhmf1814 : mean net salary per hour for women between 18-25 years old\n",
      "snhmf2614 : mean net salary per hour for women between 26-50 years old\n",
      "snhmf5014 : mean net salary per hour for women >50 years old\n",
      "snhmh1814 : mean net salary per hour for men between 18-25 years old\n",
      "snhmh2614 : mean net salary per hour for men between 26-50 years old\n",
      "snhmh5014 : mean net salary per hour for men >50 years old\n",
      "population : demographic information in france per town, age, sex and living mode\n",
      "nivgeo : geographic level (arrondissement, communes...)\n",
      "codgeo : unique code for the town\n",
      "libgeo : name of the town (might contain some utf-8 errors, this information has better quality name_geographic_information)\n",
      "moco : cohabitation mode : [list and meaning available in data description]\n",
      "age80_17 : age category (slice of 5 years) | ex : 0 -> people between 0 and 4 years old\n",
      "sexe : sex, 1 for men | 2 for women\n",
      "nb : number of people in the category\n",
      "departments.geojson : contains the borders of french departments. from gregoire david (github)\n",
      "these datasets can be merged by : codgeo = code_insee\n",
      "acknowledgements\n",
      "the entire dataset has been created (and actualized) by insee, i just uploaded it on kaggle after doing some jobs and checks on it. i haven't seen insee on kaggle yet but i think it would be great to bring the organization in as a kaggle actor.\n",
      "inspiration\n",
      "first aim i had creating that dataset was to provide a map of french towns with the number of firm that are settled in by size.\n",
      "now my goal is to explore inequality between men and women, youngsters and elders, working / social classes.\n",
      "population can also be a great filter to explain some phenomenons on the maps.\n",
      "at the end of 2015, the jupyter project conducted a ux survey for jupyter notebook users. this dataset, survey.csv, contains the raw responses.\n",
      "see the google group thread for more context around this dataset.\n",
      "context\n",
      "zeeshan-ul-hassan usmani’s genome phenotype snps raw data\n",
      "genomics is a branch of molecular biology that involves structure, function, variation, evolution and mapping of genomes. there are several companies offering next generation sequencing of human genomes from complete 3 billion base-pairs to a few thousand phenotype snps. i’ve used 23andme (using illumina humanomniexpress-24) for my dna’s phenotype snps. i am sharing the entire raw dataset here for the international research community for following reasons:\n",
      "i am a firm believer in open dataset, transparency, and the right to learn, research, explores, and educate. i do not want to restrict the knowledge flow for mere privacy concerns. hence, i am offering my entire dna raw data for the world to use for research without worrying about privacy. i call it copyleft dataset.\n",
      "most of available test datasets for research come from western world and we don’t see much from under-developing countries. i thought to share my data to bridge the gap and i expect others to follow the trend.\n",
      "i would be the happiest man on earth, if a life can be saved, knowledge can be learned, an idea can be explore, or a fact can be found using my dna data. please use it the way you will\n",
      "content\n",
      "name: zeeshan-ul-hassan usmani\n",
      "age: 38 years\n",
      "country of birth: pakistan\n",
      "country of ancestors: india (utter pradesh - up)\n",
      "file: genomezeeshanusmani.csv\n",
      "size: 15 mb\n",
      "sources: 23andme personalized genome report\n",
      "the research community is still progressively working in this domain and it is agreed upon by professionals that genomics is still in its infancy. you now have the chance to explore this novel domain via the dataset and become one of the few genomics early adopters.\n",
      "the data-set is a complete genome extracted from www.23andme.com and is represented as a sequence of snps represented by the following symbols: a (adenine), c (cytosine), g (guanine), t (thymine), d (base deletions), i (base insertions), and '_' or '-' if the snp for particular location is not accessible. it contains chromosomes 1-22, x, y, and mitochondrial dna.\n",
      "a complete list of the exact snps (base pairs) available and their data-set index can be found at https://api.23andme.com/res/txt/snps.b4e00fe1db50.data\n",
      "for more information about how the data-set was extracted follow https://api.23andme.com/docs/reference/#genomes\n",
      "moreover, for a more detailed understanding of the data-set content please acquaint yourself with the description of https://api.23andme.com/docs/reference/#genotypes\n",
      "acknowledgements\n",
      "users are allowed to use, copy, distribute and cite the dataset as follows: “zeeshan-ul-hassan usmani, genome phenotype snps raw data file by 23andme, kaggle dataset repository, jan 25, 2017.”\n",
      "useful links\n",
      "you may use the following human genome database sites for help:\n",
      "genbank - https://www.ncbi.nlm.nih.gov/genbank/\n",
      "the human genome project - https://www.genome.gov/hgp/\n",
      "genomes online database (gold) - https://gold.jgi.doe.gov\n",
      "complete genomics - http://www.completegenomics.com/public-data/\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "is the individual in question more susceptible to cancer?\n",
      "does he tend to gain weight?\n",
      "where is his place of origin?\n",
      "which gene determines certain biological feature (cancer susceptibility, fat generation rate, hair color etc.\n",
      "how does this phenotype snps compare with other similar datasets from the western-world?\n",
      "what would be the likely cause of death for this person?\n",
      "what are the most likely diseases/illnesses this person is going to face in lifetime?\n",
      "what is unique about this dataset?\n",
      "what else you can extract from this dataset when it comes to personal trait, intelligence level, ancestry and body makeup?\n",
      "sample reports\n",
      "please check out following reports to understand what can be done with this data\n",
      "ancestry – https://www.23andme.com/published-report/eeb4f9bbd6b5474f/?share_id=f6c5562848e84586\n",
      "weight report - https://you.23andme.com/published/reports/65c9af9f8223456d/?share_id=0126f129e4f3458b\n",
      "the purpose of this data set is to allow exploration between various types of data that is commonly collected by the us government across the states and the usa as a whole. the data set consists of three different types of data:\n",
      "census and geographic data;\n",
      "energy data; and\n",
      "economic data.\n",
      "when creating the data set, i combined data from many different types of sources, all of which are cited below. i have also provided the fields included in the data set and what they represent below. i have not performed any research on the data yet, but am going to dive in soon. i am particularly interested in the relationships between various types of data (i.e. gdp or birth rate) in prediction algorithms. given that i have compiled 5 years’ worth of data, this data set was primarily constructed with predictive algorithms in mind.\n",
      "an additional note before you delve into the fields: * there could have been many more variables added across many different fields of metrics. i have stopped here, but it could potentially be beneficial to observe the interaction of these variables with others (i.e. the gdp of certain industries, the average age in a state, the male/female gender ratio, etc.) to attempt to find additional trends.\n",
      "census and geographic data\n",
      "statecodes: the state 2-letter abbreviations. note that i added \"us\" for the united states.\n",
      "region: the number corresponding to the region the state lies within, according to the 2010 census. (1 = northeast, 2 = midwest, 3 = south, 4 = west)\n",
      "division: the number corresponding to the division the state lies within, according to the 2010 census. (1 = new england, 2 = middle atlantic, 3 = east north central, 4 = west north central, 5 = south atlantic, 6 = east south central, 7 = west south central, 8 = mountain, 9 = pacific)\n",
      "coast: whether the state shares a border with an ocean. (1 = yes, 0 = no)\n",
      "great lakes: whether the state shares a border with a great lake. (1 = yes, 0 = no\n",
      "census2010pop: 4/1/2010 resident total census 2010 population\n",
      "popestimate{year}: 7/1/{year} resident total population estimate\n",
      "rbirth{year}: birth rate in period 7/1/{year - 1} to 6/30/{year}\n",
      "rdeath{year}: death rate in period 7/1/{year - 1} to 6/30/{year}\n",
      "rnaturalinc{year}: natural increase rate in period 7/1/{year - 1} to 6/30/{year}\n",
      "rinternationalmig{year}: net international migration rate in period 7/1/{year - 1} to 6/30/{year}\n",
      "rdomesticmig{year}: net domestic migration rate in period 7/1/{year - 1} to 6/30/{year}\n",
      "rnetmig{year}: net migration rate in period 7/1/{year - 1} to 6/30/{year}\n",
      "as noted from the census:\n",
      "net international migration for the united states includes the international migration of both native and foreign-born populations. specifically, it includes: (a) the net international migration of the foreign born, (b) the net migration between the united states and puerto rico, (c) the net migration of natives to and from the united states, and (d) the net movement of the armed forces population between the united states and overseas. net international migration for puerto rico includes the migration of native and foreign-born populations between the united states and puerto rico.\n",
      "codes for most of the data, information about the geographic terms and coditions, and more information about the methodology behind the population estimates can be found on the us census website.\n",
      "energy data\n",
      "totalc{year}: total energy consumption in billion btu in given year.\n",
      "totalp{year}: total energy production in billion btu in given year.\n",
      "totale{year}: total energy expenditures in million usd in given year.\n",
      "totalprice{year}: total energy average price in usd/million btu in given year.\n",
      "totalc{first year}–{second year}: the first year’s total energy consumption divided by the second year’s total energy consumption, times 100. (the percent change between years in total energy consumption.)\n",
      "totalp{first year}–{second year}: the first year’s total energy production divided by the second year’s total energy production, times 100. (the percent change between years in total energy production.)\n",
      "totale{first year}–{second year}: the first year’s total energy expenditure divided by the second year’s total energy expenditure, times 100. (the percent change between years in total energy expenditure.)\n",
      "totalprice{first year}–{second year}: the first year’s total energy average price divided by the second year’s total energy average price, times 100. (the percent change between years in total energy average price.)\n",
      "biomassc{year}: biomass total consumption in billion btu in given year.\n",
      "coalc{year}: coal total consumption in billion btu in given year.\n",
      "coalp{year}: coal total production in billion btu in given year.\n",
      "coale{year}: coal total expenditures in million usd in given year.\n",
      "coalprice{year}: coal average price in usd per million btu in given year.\n",
      "elecc{year}: electricity total consumption in billion btu in given year.\n",
      "elece{year}: electricity total expenditures in million usd in given year.\n",
      "elecprice{year}: electricity average price in usd per million btu in given year.\n",
      "fossfuelc{year}: fossil fuels total consumption in billion btu in given year.\n",
      "geoc{year}: geothermal energy total consumption in billion btu in given year.\n",
      "geop{year}: geothermal energy net generation in the electric power sector in million kilowatt hours in given year.\n",
      "hydroc{year}: hydropower total consumption in billion btu in given year.\n",
      "hydrop{year}: hydropower total net generation in million kilowatt hours in given year.\n",
      "natgasc{year}: natural gas total consumption (including supplemental gaseous fuels) in billion btu in given year.\n",
      "natgase{year}: natural gas total expenditures in million usd in given year.\n",
      "natgasprice{year}: natural gas average price in usd per million btu in given year.\n",
      "lpgc{year}: lpg total consumption in billion btu in given year.\n",
      "lpge{year}: lpg total expenditures in million usd in given year.\n",
      "lpgprice{year}: lpg average price in usd per million btu in given year.\n",
      "notes:\n",
      "btu stands for british thermal unit and is a unit of measurement for energy. one btu is equal to the amount of energy used to raise the temperature of one pound of water on degree fahrenheit.\n",
      "many other types of energy and their associated consumption, production, expenditure, and price totals can be found from the eia; this is where i received the data i used in compiling this dataset.\n",
      "economic data\n",
      "gdp{year}{quarter}: the gdp in the provided quarter of the given year (in million usd).\n",
      "gdp{year}: the average gdp throughout the given year (in million usd).\n",
      "notes:\n",
      "the gdp is reported by the bureau of economic analysis from the u.s. department of commerce and measures the value of the goods and services produced by the economy in a given period.\n",
      "the quarterly gdp data can be downloaded from the bea.\n",
      "the yearly gdp data can be downloaded from the bea.\n",
      "image credit: http://www.freelargeimages.com/wp-content/uploads/2014/11/map_of_united_states-3.jpg\n",
      "background\n",
      "when is my university campus gym least crowded, so i know when to work out? we measured how many people were in this gym once every 10 minutes over the last year. we want to be able to predict how crowded the gym will be in the future.\n",
      "goals\n",
      "given a time of day (and maybe some other features, including weather), predict how crowded the gym will be.\n",
      "figure out which features are actually important, which are redundant, and what features could be added to make the predictions more accurate.\n",
      "data\n",
      "the dataset consists of 26,000 people counts (about every 10 minutes) over the last year. in addition, i gathered extra info including weather and semester-specific information that might affect how crowded it is. the label is the number of people, which i'd like to predict given some subset of the features.\n",
      "label:\n",
      "number of people\n",
      "features:\n",
      "date (string; datetime of data)\n",
      "timestamp (int; number of seconds since beginning of day)\n",
      "day_of_week (int; 0 [monday] - 6 [sunday])\n",
      "is_weekend (int; 0 or 1) [boolean, if 1, it's either saturday or sunday, otherwise 0]\n",
      "is_holiday (int; 0 or 1) [boolean, if 1 it's a federal holiday, 0 otherwise]\n",
      "temperature (float; degrees fahrenheit)\n",
      "is_start_of_semester (int; 0 or 1) [boolean, if 1 it's the beginning of a school semester, 0 otherwise]\n",
      "month (int; 1 [jan] - 12 [dec])\n",
      "hour (int; 0 - 23)\n",
      "acknowledgements\n",
      "this data was collected with the consent of the university and the gym in question.\n",
      "results for the men's atp tour date back to january 2000, including grand slams, masters series, masters cup and international series competitions. historical head-to-head betting odds go back to 2001.\n",
      "see here for more details about the metadata : http://www.tennis-data.co.uk/notes.txt\n",
      "source - http://www.tennis-data.co.uk/data.php\n",
      "there is a lot you can do with this data set. the ultimate goal is obviously to predict the outcome of the game or to build an efficient betting strategy based on your model(s).\n",
      "you can also\n",
      "compare the betting agencies (bet365, bet&win, ladbrokes, pinnacles...) in terms of predictions quality or measure the progress of these betting agencies over the years\n",
      "discover if it's possible to predict specific events (3 sets match, retirement, walk over...) or the evolution of players.\n",
      "context\n",
      "the aim of this dataset is to provide a simple way to get started with 3d computer vision problems such as 3d shape recognition.\n",
      "accurate 3d point clouds can (easily and cheaply) be adquired nowdays from different sources:\n",
      "rgb-d devices: google tango, microsoft kinect, etc.\n",
      "lidar.\n",
      "3d reconstruction from multiple images.\n",
      "however there is a lack of large 3d datasets (you can find a good one here based on triangular meshes); it's especially hard to find datasets based on point clouds (wich is the raw output from every 3d sensing device).\n",
      "this dataset contains 3d point clouds generated from the original images of the mnist dataset to bring a familiar introduction to 3d to people used to work with 2d datasets (images).\n",
      "in the 3d_from_2d notebook you can find the code used to generate the dataset.\n",
      "you can use the code in the notebook to generate a bigger 3d dataset from the original.\n",
      "content\n",
      "full_dataset_vectors.h5\n",
      "the entire dataset stored as 4096-d vectors obtained from the voxelization (x:16, y:16, z:16) of all the 3d point clouds.\n",
      "in adition to the original point clouds, it contains randomly rotated copies with noise.\n",
      "the full dataset is splitted into arrays:\n",
      "x_train (10000, 4096)\n",
      "y_train (10000)\n",
      "x_test(2000, 4096)\n",
      "y_test (2000)\n",
      "example python code reading the full dataset:\n",
      " with h5py.file(\"../input/train_point_clouds.h5\", \"r\") as hf:    \n",
      "     x_train = hf[\"x_train\"][:]\n",
      "     y_train = hf[\"y_train\"][:]    \n",
      "     x_test = hf[\"x_test\"][:]  \n",
      "     y_test = hf[\"y_test\"][:]  \n",
      "train_point_clouds.h5 & test_point_clouds.h5\n",
      "5000 (train), and 1000 (test) 3d point clouds stored in hdf5 file format. the point clouds have zero mean and a maximum dimension range of 1.\n",
      "each file is divided into hdf5 groups\n",
      "each group is named as its corresponding array index in the original mnist dataset and it contains:\n",
      "\"points\" dataset: x, y, z coordinates of each 3d point in the point cloud.\n",
      "\"normals\" dataset: nx, ny, nz components of the unit normal associate to each point.\n",
      "\"img\" dataset: the original mnist image.\n",
      "\"label\" attribute: the original mnist label.\n",
      "example python code reading 2 digits and storing some of the group content in tuples:\n",
      "with h5py.file(\"../input/train_point_clouds.h5\", \"r\") as hf:    \n",
      "    a = hf[\"0\"]\n",
      "    b = hf[\"1\"]    \n",
      "    digit_a = (a[\"img\"][:], a[\"points\"][:], a.attrs[\"label\"]) \n",
      "    digit_b = (b[\"img\"][:], b[\"points\"][:], b.attrs[\"label\"]) \n",
      "voxelgrid.py\n",
      "simple python class that generates a grid of voxels from the 3d point cloud. check kernel for use.\n",
      "plot3d.py\n",
      "module with functions to plot point clouds and voxelgrid inside jupyter notebook. you have to run this locally due to kaggle's notebook lack of support to rendering iframes. see github issue here\n",
      "functions included:\n",
      "array_to_color converts 1d array to rgb values use as kwarg color in plot_points()\n",
      "plot_points(xyz, colors=none, size=0.1, axis=false)\n",
      "plot_voxelgrid(v_grid, cmap=\"oranges\", axis=false)\n",
      "acknowledgements\n",
      "website of the original mnist dataset\n",
      "website of the 3d mnist dataset\n",
      "have fun!\n",
      "new upload:\n",
      "added +32,000 more locations. for information on data calculations please refer to the methodology pdf document. information on how to calculate the data your self is also provided as well as how to buy data for $1.29 dollars.\n",
      "what you get:\n",
      "the database contains 32,000 records on us household income statistics & geo locations. the field description of the database is documented in the attached pdf file. to access, all 348,893 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to up vote. up vote right now, please. enjoy!\n",
      "household & geographic statistics:\n",
      "mean household income (double)\n",
      "median household income (double)\n",
      "standard deviation of household income (double)\n",
      "number of households (double)\n",
      "square area of land at location (double)\n",
      "square area of water at location (double)\n",
      "geographic location:\n",
      "longitude (double)\n",
      "latitude (double)\n",
      "state name (character)\n",
      "state abbreviated (character)\n",
      "state_code (character)\n",
      "county name (character)\n",
      "city name (character)\n",
      "name of city, town, village or cpd (character)\n",
      "primary, defines if the location is a track and block group.\n",
      "zip code (character)\n",
      "area code (character)\n",
      "abstract\n",
      "the dataset originally developed for real estate and business investment research. income is a vital element when determining both quality and socioeconomic features of a given geographic location. the following data was derived from over +36,000 files and covers 348,893 location records.\n",
      "license\n",
      "only proper citing is required please see the documentation for details. have fun!!!\n",
      "golden oak research group, llc. “u.s. income database kaggle”. publication: 5, august 2017. accessed, day, month year.\n",
      "sources, don't have 2 dollars? get the full information yourself!\n",
      "2011-2015 acs 5-year documentation was provided by the u.s. census reports. retrieved august 2, 2017, from https://www2.census.gov/programs-surveys/acs/summary_file/2015/data/5_year_by_state/\n",
      "found errors?\n",
      "please tell us so we may provide you the most accurate data possible. you may reach us at: research_development@goldenoakresearch.com\n",
      "for any questions you can reach me on at 585-626-2965\n",
      "please note: it is my personal number and email is preferred\n",
      "check our data's accuracy: census fact checker\n",
      "access all 348,893 location records and more:\n",
      "don't settle. go big and win big. optimize your potential. overcome limitation and outperform expectation. access all household income records on a scale roughly equivalent to a neighborhood, see link below:\n",
      "website: golden oak research kaggle deals all databases $1.29 limited time only\n",
      "a small startup with big dreams, giving the every day, up and coming data scientist professional grade data at affordable prices it's what we do.\n",
      "context\n",
      "crime statistics: integrity\n",
      "the south african police service (saps) has accepted a new and challeging objective of ensuring that its crime statistics are in line with international best practice. this will be achieved through a memorandum of understanding with statistics south africa (stats sa), aimed at further enhancing the quality and integrity of the south african crime statistics.\n",
      "the crime statistics generated by saps are an important link in the value chain of the statistics system informs policy development and planning in the criminal justice system. the collaboration with statssa will go a long way in enhancing the integrity of the saps crime statistics and ensuring that policy-makers have quality data to assist them with making policy decisions.\n",
      "content\n",
      "the dataset contains south african crime statistics, broken down per province, station and crime type.\n",
      "acknowledgements\n",
      "data as published from:\n",
      "http://www.saps.gov.za/resource_centre/publications/statistics/crimestats/2015/crime_stats.php\n",
      "further sources:\n",
      "http://www.saps.gov.za/services/crimestats.php\n",
      "an overview presentation:\n",
      "http://www.saps.gov.za/services/final-crime-stats-release-02september2016.pdf\n",
      "introduction\n",
      "video games are a rich area for data extraction due to their digital nature. notable examples such as the complex eve online economy, world of warcraft corrupted blood incident and even grand theft auto self-driving cars tells us that fiction is closer to reality than we really think. data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.\n",
      "in this kaggle dataset, i provide just over 1400 competitive matchmaking matches from valve's game counter-strike: global offensive (cs:go). the data was extracted from competitive matchmaking replays submitted to csgo-stats. i intend for this data-set to be purely exploratory, however users are free to create their own predictive models they see fit.\n",
      "about counter-strike: global offensive\n",
      "counter-strike: global offensive is a first-person shooter game pitting two teams of 5 players against each other. within a maximum of 30 rounds, the two teams find themselves on either side as a counter terrorist or terrorist. both sides are tasked with eliminating the opposition or, as the terrorist team, planting the c4 bomb at a bomb site and allowing it to explode. rounds are played out until either of those two objectives or if the maximum time is reached (in which the counter terrorists then win by default). at the end of the 15th round, the two teams switch sides and continue until one team reaches 16 round wins first. cs:go is widely known for its competitive aspect of technical skill, teamwork and in-game strategies. players are constantly rewarded with the efforts they put it in training and learning through advancing in rank.\n",
      "click here to read more about the competitive mechanics of cs:go.\n",
      "content\n",
      "this dataset within the 1400 matches provides every successful entry of duels (or battle) that took place for a player. that is, each row documents an event when a player is hurt by another player (or world e.g fall damage). there are over 900,000 entries within more than 31500 rounds.\n",
      "mm_master_demos.csv contains information on rounds fired, while mm_grenades_demos.csv contains information on grenades thrown. the fields in the two datasets are similar: highlights include shooters and victims, event coordinates, and timestamps. the datasets also includes static information on the match winner, player ranks before and after the match, and other miscellaneous match-level metadata.\n",
      "for further information on individual fields in the dataset, refer to the column metadata.\n",
      "interpreting positional data\n",
      "this dataset also includes a selection of the game's official radar maps, as well as a table, map_data.csv, to aid in mapping data over them. the x,y coordinates included in the dataset are all in in-game coordinates and need to be linearly scaled to be plotted on any official radar maps. see converting to map coordinates for more information.\n",
      "acknowledgements\n",
      "definitely the guys from csgo-stats, without them, this wouldn't have been possible! :)\n",
      "/r/globaloffensive for many years of lulz\n",
      "akiver of csgo demo manager for spending so much time perfecting his demo parser.\n",
      "context\n",
      "this dataset expands on my earlier new york city census data dataset. it includes data from the entire country instead of just new york city. the expanded data will allow for much more interesting analyses and will also be much more useful at supporting other data sets.\n",
      "content\n",
      "the data here are taken from the dp03 and dp05 tables of the 2015 american community survey 5-year estimates. the full datasets and much more can be found at the american factfinder website. currently, i include two data files:\n",
      "acs2015_census_tract_data.csv: data for each census tract in the us, including dc and puerto rico.\n",
      "acs2015_county_data.csv: data for each county or county equivalent in the us, including dc and puerto rico.\n",
      "the two files have the same structure, with just a small difference in the name of the id column. counties are political subdivisions, and the boundaries of some have been set for centuries. census tracts, however, are defined by the census bureau and will have a much more consistent size. a typical census tract has around 5000 or so residents.\n",
      "the census bureau updates the estimates approximately every year. at least some of the 2016 data is already available, so i will likely update this in the near future.\n",
      "acknowledgements\n",
      "the data here were collected by the us census bureau. as a product of the us federal government, this is not subject to copyright within the us.\n",
      "inspiration\n",
      "there are many questions that we could try to answer with the data here. can we predict things such as the state (classification) or household income (regression)? what kinds of clusters can we find in the data? what other datasets can be improved by the addition of census data?\n",
      "content\n",
      "the ntsb aviation accident database contains information from 1962 and later about civil aviation accidents and selected incidents within the united states, its territories and possessions, and in international waters.\n",
      "acknowledgements\n",
      "generally, a preliminary report is available online within a few days of an accident. factual information is added when available, and when the investigation is completed, the preliminary report is replaced with a final description of the accident and its probable cause. full narrative descriptions may not be available for dates before 1993, cases under revision, or where ntsb did not have primary investigative responsibility.\n",
      "inspiration\n",
      "hope it will teach us how to improve the quality and safety of traveling by airplane.\n",
      "context\n",
      "this dataset contains tree observations from four areas of the roosevelt national forest in colorado. all observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest. there are over half a million measurements total!\n",
      "content\n",
      "this dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography.\n",
      "acknowledgement\n",
      "this dataset is part of the uci machine learning repository, and the original source can be found here. the original database owners are jock a. blackard, dr. denis j. dean, and dr. charles w. anderson of the remote sensing and gis program at colorado state university.\n",
      "inspiration\n",
      "can you build a model that predicts what types of trees grow in an area based on the surrounding characteristics? a past kaggle competition project on this topic can be found here.\n",
      "what kinds of trees are most common in the roosevelt national forest?\n",
      "which tree types can grow in more diverse environments? are there certain tree types that are sensitive to an environmental factor, such as elevation or soil type?\n",
      "introduction\n",
      "this file contains the values of the price for more than 1000 different cryptocurrencies (including scams) recorded on daily base, i decide to include all coins in order to analyze exotic coins and compare with well knows cryptocurrencies. all this dataset come from coinmarketcap historical pages, grabbed using just an r script. thanks coinmarketcap to making this data available for free (and for every kind of usage).\n",
      "the dataset\n",
      "available columns in the dataset:\n",
      "date - the day of recorded values\n",
      "open - the opening price (in usd)\n",
      "high - the highest price (in usd)\n",
      "low - the lowest price (in usd)\n",
      "close - the closing price (in usd)\n",
      "volume - total exchanged volume (in usd)\n",
      "market.cap - the total market capitalization for the coin (in usd)\n",
      "coin - the name of the coin\n",
      "delta - calculated as (close - open) / open\n",
      "these data files contain election results for both the 2012 and 2016 us presidential elections, include proportions of votes cast for romney, obama (2012) and trump, clinton (2016).\n",
      "the election results were obtained from this git repository: https://github.com/tonmcg/county_level_election_results_12-16\n",
      "the county facts data was obtained from another kaggle election data set: https://www.kaggle.com/benhamner/2016-us-election\n",
      "are people that use emoji happier?\n",
      "paper --> https://arxiv.org/abs/1710.00888\n",
      "at asonam2017, pydatadubai vol 1.0 @ awok, pydatabcn2017 @ easde we have presented the paper happiness inside a job?... many people in the various audiences asked why we avoid using emojis to predict and profile employees. the answer is that we prefer to use links of likes because they are more authentic than words or emojis. in the same way that google page rank is more effective when it looks at links between pages rather than content inside the pages. ... still people keep asking about it. but there is one thing emoji are good at estimating: author sentiment and that is just possible thanks to the unique characteristics of the dataset at hand.\n",
      "previous research has traditionally analyzed emoji sentiment from the point of view of the reader of the content not the author. here, we analyze emoji sentiment from the author point of view and present a benchmark that was built from an employee happiness dataset where emoji happen to be annotated with daily happiness of the author of the comment. we also found out that people that use emoji are happier!?, muuch happier... but the question is, what did we miss?\n",
      "content\n",
      "the main table contains columns named after emoji hex codes, a 1 means the emoji appears one time in the comment (row). this dataset is an expanded version of this one, but has different formats, columns and one different table, that is why we decided to release it as separate dataset. as he scripts are not compatible.\n",
      "other stuff\n",
      "the r script written on mac os does not work in the kaggle platform (because numbers become factors and other little changes in how the code is interpreted...), the full working script (tested on r studio mac os) can be found at https://github.com/orioli/emoji-writer-sentiment\n",
      "thank you to lewis michel\n",
      "context\n",
      "hr data can be hard to come by, and hr professionals generally lag behind with respect to analytics and data visualization competency. thus, dr. carla patalano and i set out to create our own hr-related dataset, which is used in one of our graduate mshrm courses called hr metrics and analytics, at new england college of business. we created this data set ourselves.\n",
      "content\n",
      "there are multiple worksheets within the excel workbook. these include\n",
      "core data set\n",
      "production staff\n",
      "sales analysis\n",
      "salaries\n",
      "recruiting sources\n",
      "the excel workbook revolves around a fictitious company, called dental magic, and the core data set contains names, dobs, age, gender, marital status, date of hire, reasons for termination, department, whether they are active or terminated, position title, pay rate, manager name, and performance score.\n",
      "acknowledgements\n",
      "dr. carla patalano provided many suggestions for creating this synthetic data set, which has been used now by over 30 human resource management students at the college. students in the course learn data visualization techniques with tableau desktop and use this data set to complete a series of assignments.\n",
      "inspiration\n",
      "is there any relationship between who a person works for and their performance score?\n",
      "what is the overall diversity profile of the organization?\n",
      "what are our best recruiting sources if we want to ensure a diverse organization?\n",
      "there are so many other interesting questions that could be addressed through this interesting data set. dr. patalano and i look forward to seeing what we can come up with.\n",
      "context:\n",
      "natural language processing(nlp), text similarity(lexical and semantic)\n",
      "content:\n",
      "in each row of the included datasets(train.csv and test.csv), products x(description_x) and y(description_y) are considered to refer to the same security(same_security) if they have the same ticker(ticker_x,ticker_y), even if the descriptions don't exactly match. you can make use of these descriptions to predict whether each pair in the test set also refers to the same security.\n",
      "dataset info:\n",
      "train - description_x, description_y, ticker_x, ticker_y, same_security. test - description_x, description_y, same_security(to be predicted)\n",
      "past research:\n",
      "this dataset is pretty similar to the quora question pairs . you can also check out my kernel for dataset exploration and n-gram analysis n-gram analysis on stock data.\n",
      "how to approach:\n",
      "there are several good ways to approach this, check out this algorithm, and see how far you can go with it: https://en.wikipedia.org/wiki/tf–idf http://scikit learn.org/stable/modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html. you can also try doing n-gram analysis(check out my kernel). i would suggest using log-loss as your evaluation metric since it gives you a number between 0 and 1 instead of binary classification, which is not so effective in this case.\n",
      "acknowledgements:\n",
      "quovo stock data.\n",
      "we’re releasing 30,000+ ocr’d political memes and their captions . with the election just days away, we hope to contribute to the pre and post-election analysis of the most meme-orable election in modern history.\n",
      "v2 of the dataset includes 8 csv’s:\n",
      "bern.csv: 146 rows\n",
      "bernie.csv: 2100 rows\n",
      "clinton.csv: 4362 rows\n",
      "donald.csv: 6499 rows\n",
      "gary_johnston.csv: 140 rows\n",
      "hillary.csv: 7398 rows\n",
      "jill stein.csv: 96 rows\n",
      "trump.csv: 12139 rows\n",
      "with the following columns:\n",
      "timestamp (date published)\n",
      "id (our unique identifier)\n",
      "link (post url)\n",
      "caption (meme caption via ocr)\n",
      "author\n",
      "network\n",
      "likes/upvotes\n",
      "let us know any revisions you'd like to see. and of course, if you find errors in the data, do let us know.\n",
      "context\n",
      "the pronto cycle share system consists of 500 bikes and 54 stations located in seattle. pronto provides open data on individual trips, stations, and daily weather.\n",
      "content\n",
      "there are 3 datasets that provide data on the stations, trips, and weather from 2014-2016.\n",
      "station dataset\n",
      "station_id: station id number\n",
      "name: name of station\n",
      "lat: station latitude\n",
      "long: station longitude\n",
      "install_date: date that station was placed in service\n",
      "install_dockcount: number of docks at each station on the installation date\n",
      "modification_date: date that station was modified, resulting in a change in location or dock count\n",
      "current_dockcount: number of docks at each station on 8/31/2016\n",
      "decommission_date: date that station was placed out of service\n",
      "trip dataset\n",
      "trip_id: numeric id of bike trip taken\n",
      "starttime: day and time trip started, in pst\n",
      "stoptime: day and time trip ended, in pst\n",
      "bikeid: id attached to each bike\n",
      "tripduration: time of trip in seconds\n",
      "from_station_name: name of station where trip originated\n",
      "to_station_name: name of station where trip terminated\n",
      "from_station_id: id of station where trip originated\n",
      "to_station_id: id of station where trip terminated\n",
      "usertype: \"short-term pass holder\" is a rider who purchased a 24-hour or 3-day pass; \"member\" is a rider who purchased a monthly or an annual membership\n",
      "gender: gender of rider\n",
      "birthyear: birth year of rider\n",
      "weather dataset contains daily weather information in the service area\n",
      "acknowledgements\n",
      "the original datasets can be downloaded here.\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "what is the most popular bike route?\n",
      "how are bike uses or routes affected by user characteristics, station features, and weather?\n",
      "context\n",
      "the national snow and ice data center (nsidc) supports research into our world’s frozen realms: the snow, ice, glaciers, frozen ground, and climate interactions that make up earth’s cryosphere. nsidc manages and distributes scientific data, creates tools for data access, supports data users, performs scientific research, and educates the public about the cryosphere.\n",
      "content\n",
      "the dataset provides the total extent for each day for the entire time period (1978-2015). there are 7 variables:\n",
      "year\n",
      "month\n",
      "day\n",
      "extent: unit is 10^6 sq km\n",
      "missing: unit is 10^6 sq km\n",
      "source: source data product web site: http://nsidc.org/data/nsidc-0051.html\n",
      "hemisphere\n",
      "acknowledgements\n",
      "the original datasets can be found here and here.\n",
      "inspiration\n",
      "can you visualize the change in sea ice over time?\n",
      "do changes in sea ice differ between the two hemispheres?\n",
      "context a data driven look into answering the common question while travelling overseas: \"how easy is it to get a job in your country?\"\n",
      "content this dataset contains youth unemployment rates (% of total labor force ages 15-24) (modeled ilo estimate) latest data available from 2010 to 2014.\n",
      "acknowledgements international labour organization.\n",
      "http://data.worldbank.org/indicator/sl.uem.totl.zs\n",
      "released under open license.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "this dataset contains complete information about various aspects of crimes happened in india from 2001. there are many factors that can be analysed from this dataset. over all, i hope this dataset helps us to understand better about india.\n",
      "content\n",
      "i : cases reported and their disposal by police and court\n",
      "indian penal code\n",
      "special & local laws\n",
      "ia : sc/st cases reported and their disposal by police and court\n",
      "crime against scs\n",
      "crime against sts\n",
      "ib : children cases reported and their disposal by police and court\n",
      "abetment of suicide (section 305 ipc)\n",
      "buying of girls for prostitution (section 373 ipc)\n",
      "child marriage restraint act, 1929\n",
      "exposure and abandonment (section 317 ipc)\n",
      "foeticide (section 315 and 316 ipc)\n",
      "infanticide (section 315 ipc)\n",
      "kidnapping & abduction (section 360,361,363,363-a, 363 read with section 384, 366, 367 & 369 ipc)\n",
      "murder (section 302, 315 ipc)\n",
      "other crimes against children\n",
      "other murder of children (section 302 ipc)\n",
      "procuration of minor girls (section 366-a ipc)\n",
      "rape (section 376 ipc)\n",
      "selling of girls for prostitution (section 372 ipc)\n",
      "total crimes against children\n",
      "ii : persons arrested and their disposal by police and court\n",
      "indian penal code\n",
      "special and local laws\n",
      "iia : sc/st persons arrested and their disposal by police and court\n",
      "crime against scs\n",
      "crime against sts\n",
      "iib : children persons arrested and their disposal by police and court\n",
      "abetment of suicide (section 305 ipc)\n",
      "buying of girls for prostitution (section 373 ipc)\n",
      "child marriage restraint act, 1929\n",
      "exposure and abandonment (section 317 ipc)\n",
      "foeticide (section 315 and 316 ipc)\n",
      "kidnapping & abduction (section 360,361,363,363-a, 366, 367 & 369 ipc)\n",
      "murder - infanticide (section 315 ipc)\n",
      "murder - other murder of children\n",
      "murder (section 302, 315 ipc)\n",
      "other crimes against children\n",
      "procuration of minor girls (section 366-a ipc)\n",
      "rape (section 376 ipc)\n",
      "selling of girls for prostitution (section 372 ipc)\n",
      "total crimes against children\n",
      "iv : persons arrested by sex and age group\n",
      "indian penal code\n",
      "special & local laws\n",
      "v : juveniles apprehended\n",
      "indian penal code\n",
      "special & local laws\n",
      "vi : juveniles arrested and their disposal\n",
      "vii : property stolen & recovered (crime head)\n",
      "dacoity\n",
      "robbery\n",
      "burglary\n",
      "theft\n",
      "criminal breach of trust\n",
      "other property\n",
      "total property stolen & recovered\n",
      "viii : property stolen & recovered (nature of property)\n",
      "communation and electricity wire\n",
      "cattle\n",
      "cycle\n",
      "motor vehicles\n",
      "motor vehicles - motor cycle/scooters\n",
      "motor vehicles - motor car/taxi/jeep\n",
      "motor vehicles - other motor vehicles\n",
      "fire arms\n",
      "explosives/explosive substances\n",
      "electronic components\n",
      "cultural property including antiques\n",
      "other kinds of property\n",
      "total property stolen & recovered\n",
      "ix : police strength (actual & sanctioned)\n",
      "a) actual civil police (incl. district armed police and women police)\n",
      "a) acual armed police (incl. women police)\n",
      "a) actual police strength (incl. women)\n",
      "b) acual women civil police (incl. district armed force)\n",
      "b) actual women armed police\n",
      "b) actual women police strength\n",
      "c) sanctioned civil police (incl. district armed police)\n",
      "c) santioned armed police (incl. women police)\n",
      "c) santioned police strength (incl. women)\n",
      "d) sanctioned women civil police (incl. district armed police)\n",
      "d) sanctioned women armed police\n",
      "d) sanctioned women police strength\n",
      "x : police personnel killed or injured on duty\n",
      "constables\n",
      "head constables\n",
      "assistant sub-inspectos\n",
      "sub-inspectors\n",
      "inspectors\n",
      "gazetted officers\n",
      "total police killed or injured\n",
      "x-b : age profile of police personnel killed on duty\n",
      "x-c : natural deaths and suicides of police personnel\n",
      "natural deaths of police personnel (while in service)\n",
      "police personnel committed suicide\n",
      "xi : casualties under police firing and lathicharge\n",
      "riot control\n",
      "anti dacoity operations\n",
      "against extremists & terrorists\n",
      "against others\n",
      "total casualties\n",
      "xii : cases reported value of property stolen under dacoity, robbery, burglary and theft by place of occurance\n",
      "residential premises\n",
      "highways\n",
      "river and sea\n",
      "railways 4.1 in running trains 4.2 others\n",
      "banks\n",
      "commercial establishments (shops etc.)\n",
      "other places\n",
      "total\n",
      "xiii : particulars of juveniles arrested\n",
      "education\n",
      "economic setup\n",
      "family background\n",
      "recidivism\n",
      "xiv : motive/cause of murder and culpable homicide not amounting to murder\n",
      "xv : victims of rape(age group-wise)\n",
      "incest rape cases\n",
      "other rape cases (otherthan incest)\n",
      "total rape cases\n",
      "xv-a : rape offenders relation, nearness to rape victims\n",
      "xvi : persons arrested under recidivism\n",
      "xvii : anti corruption - cases\n",
      "xviii : anti corruption - arrests\n",
      "xix : complaints/cases against police personnel\n",
      "complaints received/cases registered\n",
      "police personnel involved/action taken\n",
      "departmental action/punishments\n",
      "*xx : police budget and infrastructure\n",
      "equipments and transport support\n",
      "distribution of police stations by crime incidences\n",
      "distribution of police stations by police strength\n",
      "organisational set up\n",
      "scs/sts and muslims in police force (actual)\n",
      "xxi : 1. nature of complaints received by police\n",
      "xxi : 2. trial of violent crimes by courts\n",
      "murder\n",
      "attempt to murder\n",
      "c h not amounting to murder\n",
      "rape\n",
      "kidnapping & abduction 5.1 kidnapping & abduction of women & girls 5.2 kidnapping & abduction of others\n",
      "dacoity\n",
      "preparation & assembly for dacoity\n",
      "robbery\n",
      "riots\n",
      "arson\n",
      "dowry deaths\n",
      "total trials (sum of 1-11 above)\n",
      "xxi : 3. period of trials by courts\n",
      "district/session judge\n",
      "additional session judge\n",
      "chief judicial magistrate\n",
      "judicial magistrate (i)\n",
      "judicial magistrate (ii)\n",
      "special judicial magistrate\n",
      "other courts\n",
      "total trials (sum of 1-7 above)\n",
      "xxi : 4.1 autho theft (stolen & recovered)\n",
      "motor cycles/ scooters\n",
      "motor car/taxi/jeep\n",
      "buses\n",
      "goods carrying vehicles (trucks/tempo etc)\n",
      "other motor vehicles\n",
      "total (sum of 1-5 above)\n",
      "xxi : 4.2 serious fraud\n",
      "criminal breach of trust\n",
      "cheating\n",
      "xxi : 5.1 victims of murder (age & sex-wise)\n",
      "male victims\n",
      "female victims\n",
      "total\n",
      "xxi : 5.2 victims of ch not amounting to murder (age & sex-wise)\n",
      "male victims\n",
      "female victims\n",
      "total\n",
      "xxi : 5.3 use of firearms in murder cases\n",
      "xxi : 6. human rights violation by police\n",
      "disappearance of persons\n",
      "illegal detention/arrests\n",
      "fake encounter killings\n",
      "violation against terrorists/extremists\n",
      "extortion\n",
      "torture\n",
      "false implication\n",
      "failure in taking action\n",
      "indignity to women\n",
      "atrocities on sc/st\n",
      "others\n",
      "total (sum of 1-11 above)\n",
      "xxi : 7. police housing\n",
      "for officers (dy.sp & above)\n",
      "upper subordinates (asi to inspectos)\n",
      "lower subordinates (constables, head constables & class-iv subordinate staff)\n",
      "xxi : 8. home guards and auxilliary force\n",
      "xxi : 9. unidentified deadbodies recovered & inquest conducted\n",
      "xxi : 10. victims of kidnapping & abduction for specific purpose\n",
      "for adoption\n",
      "for begging\n",
      "for camel racing\n",
      "for illicit intercourse\n",
      "for marriage\n",
      "for prostitution\n",
      "for ransom\n",
      "for revenge\n",
      "for sale\n",
      "for selling bodyparts\n",
      "for slavery\n",
      "for unlawful activity\n",
      "other purposes\n",
      "total (sum of 1-13 above)\n",
      "xxi : 11. custodial deaths\n",
      "deaths in custody/lockup of persons remanded to police custody by court\n",
      "deaths in custody/lockup of persons not remanded to police custody by court\n",
      "deaths in custody during production/process in courts/journey connected with investigation\n",
      "deaths during hospitalisation/treatment\n",
      "deaths due to other reasons\n",
      "xxi : 12. escapes from police custody\n",
      "cases under crime against women\n",
      "rape\n",
      "kidnapping & abduction of women & girls\n",
      "dowry deaths\n",
      "molestation\n",
      "sexual harassment\n",
      "cruelty by husband and relatives\n",
      "importation of girls\n",
      "immoral traffic prevention act, 1956\n",
      "dowry prohibition act, 1961\n",
      "indecent representation of women(prohibition) act, 1986\n",
      "sati prevention act, 1987\n",
      "total crimes against women\n",
      "arrests under crime against women\n",
      "rape\n",
      "kidnapping & abduction of women & girls\n",
      "dowry deaths\n",
      "molestation\n",
      "sexual harassment\n",
      "cruelty by husband and relatives\n",
      "importation of girls\n",
      "immoral traffic prevention act, 1956\n",
      "dowry prohibition, 1961\n",
      "indecent representation of women(prohibition) act, 1986\n",
      "sati prevention act, 1987\n",
      "total crimes against women\n",
      "some of the data contains district level data. the districts are police districts and also include special police unit. therefore these may be different from revenue districts. most of the data is from 2001 to 2010. but there are few files which has data only from 2011 and few are having 2001-14.\n",
      "inspiration\n",
      "there could be many things one can understand by analyzing this dataset. few inspirations for you to start with.\n",
      "what is the major reason people being kidnapped in each and every state?\n",
      "offenders relation to the rape victim\n",
      "juveniles family background, education and economic setup.\n",
      "which state has more crime against children and women?\n",
      "age group wise murder victim\n",
      "crime by place of occurrence.\n",
      "anti corruption cases vs arrests.\n",
      "which state has more number of complaints against police?\n",
      "which state is the safest for foreigners?\n",
      "acknowledgements\n",
      "national crime records bureau (ncrb), govt of india has published this dataset on their website and also has shared on open govt data platform india portal under govt. open data license - india.\n",
      "context\n",
      "if you like to eat cereal, do yourself a favor and avoid this dataset at all costs. after seeing these data it will never be the same for me to eat fruity pebbles again.\n",
      "content\n",
      "fields in the dataset:\n",
      "name: name of cereal\n",
      "mfr: manufacturer of cereal\n",
      "a = american home food products;\n",
      "g = general mills\n",
      "k = kelloggs\n",
      "n = nabisco\n",
      "p = post\n",
      "q = quaker oats\n",
      "r = ralston purina\n",
      "type:\n",
      "cold\n",
      "hot\n",
      "calories: calories per serving\n",
      "protein: grams of protein\n",
      "fat: grams of fat\n",
      "sodium: milligrams of sodium\n",
      "fiber: grams of dietary fiber\n",
      "carbo: grams of complex carbohydrates\n",
      "sugars: grams of sugars\n",
      "potass: milligrams of potassium\n",
      "vitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of fda recommended\n",
      "shelf: display shelf (1, 2, or 3, counting from the floor)\n",
      "weight: weight in ounces of one serving\n",
      "cups: number of cups in one serving\n",
      "rating: a rating of the cereals (possibly from consumer reports?)\n",
      "acknowledgements\n",
      "these datasets have been gathered and cleaned up by petra isenberg, pierre dragicevic and yvonne jansen. the original source can be found here\n",
      "this dataset has been converted to csv\n",
      "inspiration\n",
      "eat too much sugary cereal? ruin your appetite with this dataset!\n",
      "context\n",
      "netflix in the past 5-10 years has captured a large populate of viewers. with more viewers, there most likely an increase of show variety. however, do people understand the distribution of ratings on netflix shows?\n",
      "content\n",
      "because of the vast amount of time it would take to gather 1,000 shows one by one, the gathering method took advantage of the netflix’s suggestion engine. the suggestion engine recommends shows similar to the selected show. as part of this data set, i took 4 videos from 4 ratings (totaling 16 unique shows), then pulled 53 suggested shows per video. the ratings include: g, pg, tv-14, tv-ma. i chose not to pull from every rating (e.g. tv-g, tv-y, etc.).\n",
      "acknowledgements\n",
      "the data set and the research article can be found at the concept center\n",
      "inspiration\n",
      "i was watching netflix with my wife and we asked ourselves, why are there so many r and tv-ma rating shows?\n",
      "context\n",
      "our world population is expected to grow from 7.3 billion today to 9.7 billion in the year 2050. finding solutions for feeding the growing world population has become a hot topic for food and agriculture organizations, entrepreneurs and philanthropists. these solutions range from changing the way we grow our food to changing the way we eat. to make things harder, the world's climate is changing and it is both affecting and affected by the way we grow our food – agriculture. this dataset provides an insight on our worldwide food production - focusing on a comparison between food produced for human consumption and feed produced for animals.\n",
      "content\n",
      "the food and agriculture organization of the united nations provides free access to food and agriculture data for over 245 countries and territories, from the year 1961 to the most recent update (depends on the dataset). one dataset from the fao's database is the food balance sheets. it presents a comprehensive picture of the pattern of a country's food supply during a specified reference period, the last time an update was loaded to the fao database was in 2013. the food balance sheet shows for each food item the sources of supply and its utilization. this chunk of the dataset is focused on two utilizations of each food item available:\n",
      "food - refers to the total amount of the food item available as human food during the reference period.\n",
      "feed - refers to the quantity of the food item available for feeding to the livestock and poultry during the reference period.\n",
      "dataset's attributes:\n",
      "area code - country name abbreviation\n",
      "area - county name\n",
      "item - food item\n",
      "element - food or feed\n",
      "latitude - geographic coordinate that specifies the north–south position of a point on the earth's surface\n",
      "longitude - geographic coordinate that specifies the east-west position of a point on the earth's surface\n",
      "production per year - amount of food item produced in 1000 tonnes\n",
      "acknowledgements\n",
      "this dataset was meticulously gathered, organized and published by the food and agriculture organization of the united nations.\n",
      "inspiration\n",
      "animal agriculture and factory farming is a a growing interest of the public and of world leaders.\n",
      "can you find interesting outliers in the data?\n",
      "what are the fastest growing countries in terms of food production\\consumption?\n",
      "compare between food and feed consumption.\n",
      "context:\n",
      "besides coffee, grunge and technology companies, one of the things that seattle is most famous for is how often it rains. this dataset contains complete records of daily rainfall patterns from january 1st, 1948 to december 12, 2017.\n",
      "content\n",
      "this data was collected at the seattle-tacoma international airport. the dataset contains five columns:\n",
      "date = the date of the observation\n",
      "prcp = the amount of precipitation, in inches\n",
      "tmax = the maximum temperature for that day, in degrees fahrenheit\n",
      "tmin = the minimum temperature for that day, in degrees fahrenheit\n",
      "rain = true if rain was observed on that day, false if it was not\n",
      "acknowledgements:\n",
      "this dataset was compiled by noaa and is in the public domain.\n",
      "inspiration:\n",
      "can you use this dataset to build a model of whether it will rain on a specific day given information on the previous days?\n",
      "is there a correlation between the minimum and maximum temperature? can you predict one given the other?\n",
      "can you model changes in the amount of precipitation over time? is there seasonality?\n",
      "context\n",
      "since 2008, guests and hosts have used airbnb to travel in a more unique, personalized way. as part of the airbnb inside initiative, this dataset describes the listing activity of homestays in boston, ma.\n",
      "content\n",
      "the following airbnb activity is included in this boston dataset: * listings, including full descriptions and average review score * reviews, including unique id for each reviewer and detailed comments * calendar, including listing id and the price and availability for that day\n",
      "inspiration\n",
      "can you describe the vibe of each boston neighborhood using listing descriptions?\n",
      "what are the busiest times of the year to visit boston? by how much do prices spike?\n",
      "is there a general upward trend of both new airbnb listings and total airbnb visitors to boston?\n",
      "for more ideas, visualizations of all boston datasets can be found here.\n",
      "acknowledgement\n",
      "this dataset is part of airbnb inside, and the original source can be found here.\n",
      "nih chest x-ray dataset sample\n",
      "national institutes of health chest x-ray dataset\n",
      "chest x-ray exams are one of the most frequent and cost-effective medical imaging examinations available. however, clinical diagnosis of a chest x-ray can be challenging and sometimes more difficult than diagnosis via chest ct imaging. the lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (cad) in real world medical sites with chest x-rays. one major hurdle in creating large x-ray image datasets is the lack resources for labeling so many images. prior to the release of this dataset, openi was the largest publicly available source of chest x-ray images with 4,143 images available.\n",
      "this nih chest x-ray dataset is comprised of 112,120 x-ray images with disease labels from 30,805 unique patients. to create these labels, the authors used natural language processing to text-mine disease classifications from the associated radiological reports. the labels are expected to be >90% accurate and suitable for weakly-supervised learning. the original radiology reports are not publicly available but you can find more details on the labeling process in this open access paper: \"chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.\" (wang et al.)\n",
      "link to paper\n",
      "\n",
      "file contents - this is a random sample (5%) of the full dataset:\n",
      "sample.zip: contains 5,606 images with size 1024 x 1024\n",
      "sample_labels.csv: class labels and patient data for the entire dataset\n",
      "image index: file name\n",
      "finding labels: disease type (class label)\n",
      "follow-up #\n",
      "patient id\n",
      "patient age\n",
      "patient gender\n",
      "view position: x-ray orientation\n",
      "originalimagewidth\n",
      "originalimageheight\n",
      "originalimagepixelspacing_x\n",
      "originalimagepixelspacing_y\n",
      "\n",
      "class descriptions\n",
      "there are 15 classes (14 diseases, and one for \"no findings\") in the full dataset, but since this is drastically reduced version of the full dataset, some of the classes are sparse with the labeled as \"no findings\"\n",
      "hernia - 13 images\n",
      "pneumonia - 62 images\n",
      "fibrosis - 84 images\n",
      "edema - 118 images\n",
      "emphysema - 127 images\n",
      "cardiomegaly - 141 images\n",
      "pleural_thickening - 176 images\n",
      "consolidation - 226 images\n",
      "pneumothorax - 271 images\n",
      "mass - 284 images\n",
      "nodule - 313 images\n",
      "atelectasis - 508 images\n",
      "effusion - 644 images\n",
      "infiltration - 967 images\n",
      "no finding - 3044 images\n",
      "\n",
      "full dataset content\n",
      "the full dataset can be found here. there are 12 zip files in total and range from ~2 gb to 4 gb in size.\n",
      "\n",
      "data limitations:\n",
      "the image labels are nlp extracted so there could be some erroneous labels but the nlp labeling accuracy is estimated to be >90%.\n",
      "very limited numbers of disease region bounding boxes (see bbox_list_2017.csv)\n",
      "chest x-ray radiology reports are not anticipated to be publicly shared. parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation\n",
      "\n",
      "modifications to original data\n",
      "original tar archives were converted to zip archives to be compatible with the kaggle platform\n",
      "csv headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory\n",
      "\n",
      "citations\n",
      "wang x, peng y, lu l, lu z, bagheri m, summers rm. chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. ieee cvpr 2017, chestx-ray8_hospital-scale_chest_cvpr_2017_paper.pdf\n",
      "nih news release: nih clinical center provides one of the largest publicly available chest x-ray datasets to scientific community\n",
      "original source files and documents: https://nihcc.app.box.com/v/chestxray-nihcc/folder/36938765345\n",
      "\n",
      "acknowledgements\n",
      "this work was supported by the intramural research program of the nclinical center (clinicalcenter.nih.gov) and national library of medicine (www.nlm.nih.gov).\n",
      "context\n",
      "hearthstone is a very popular collectible card game published by blizzard entertainment in 2014. the goal of the game consists in building a 30 cards deck in order to beat your opponent. a few weeks ago, i decided to download all the decks posted by players at hearthpwn. the code to download the data is available here.\n",
      "content\n",
      "this upload is composed of two files :\n",
      "data.json / data.csv\n",
      "contains the actual hearthstone deck records. each record features :\n",
      "date (str) : the date of publication (or last update) of the deck.\n",
      "user (str) : the user who uploaded the deck.\n",
      "deck_class (str) : one of the nine character class in hearthstone (druid, priest, ...).\n",
      "deck_archetype (str) : the theme of deck labelled by the user (aggro druid, dragon priest, ...).\n",
      "deck_format (str) : the game format of the deck on the day data was recorded (w for \"wild\" or s for \"standard\").\n",
      "deck_set (str) : the latest expansion published prior the deck publication (naxxramas, tgt launch, ...).\n",
      "deck_id (int) : the id of the deck.\n",
      "deck_type (str) : the type of the deck labelled by the user :\n",
      "ranked deck : a deck played on ladder.\n",
      "theorycraft : a deck built with unreleased cards to get a gist of the future metagame.\n",
      "pve adventure : a deck built to beat the bosses in adventure mode.\n",
      "arena : a deck built in arena mode.\n",
      "tavern brawl : a deck built for the weekly tavern brawl mode.\n",
      "tournament : a deck brought at tournament by a pro-player.\n",
      "none : the game type was not mentioned.\n",
      "rating (int) : the number of upvotes received by that deck.\n",
      "title (str) : the name of the deck.\n",
      "craft_cost (int) : the amount of dust (in-game craft material) required to craft the deck.\n",
      "cards (list) : a list of 30 card ids. each id can be mapped to the card description using the reference file.\n",
      "refs.json\n",
      "contains the reference to the cards played in hearthstone. this file was originally proposed on hearthstonejson. each record features a lot of informations about the cards, i'll list the most important :\n",
      "dbfid (int) : the id of the card (the one used in data.json).\n",
      "rarity (str) : the rarity of the card (epic, rare, ...).\n",
      "cardclass (str) : the character class (warlock, priest, ...).\n",
      "artist (str) : the artist behind the card's art.\n",
      "collectible (bool) : whether or not the card can be collected.\n",
      "cost (int) : the card play cost.\n",
      "health (int) : the card health (if it's a minion).\n",
      "attack (int) : the card attack (if it's a minion).\n",
      "name (str) : the card name.\n",
      "flavor (str) : the card's flavor text.\n",
      "set (str) : the set / expansion which featured this card.\n",
      "text (int) : the card's text.\n",
      "type (str) : the card's type (minion, spell, ...).\n",
      "race (str) : the card's race (if it's a minion).\n",
      "set (str) : the set / expansion which featured this card.\n",
      "...\n",
      "if you need help cleaning the data take a look at my start over kernel!\n",
      "what you could do :\n",
      "try to predict the deck archetype based on the cards features in the deck.\n",
      "seek relationships between the cost of the deck and it's popularity.\n",
      "describe the evolution of the meta-game over-time.\n",
      "find out unbalanced (overplayed) cards\n",
      "context\n",
      "pakistan suicide bombing attacks (1995-2016)\n",
      "suicide bombing is an operational method in which the very act of the attack is dependent upon the death of the perpetrator. though only 3% of all terrorist attacks around the world can be classified as suicide bombing attacks these account for 48% of the casualties. explosions and suicide bombings have become the modus operandi of terrorist organizations throughout the world. the world is full of unwanted explosives, brutal bombings, accidents, and violent conflicts, and there is a need to understand the impact of these explosions on one’s surroundings, the environment, and most importantly on human bodies. from 1980 to 2001 (excluding 9/11/01) the average number of deaths per incident for suicide bombing attacks was 13. this number is far above the average of less than one death per incident across all types of terrorist attacks over the same time period. suicide bombers, unlike any other device or means of destruction, can think and therefore detonate the charge at an optimal location with perfect timing to cause maximum carnage and destruction. suicide bombers are adaptive and can quickly change targets if forced by security risk or the availability of better targets. suicide attacks are relatively inexpensive to fund and technologically primitive, as ieds can be readily constructed.\n",
      "world has seen more than 3,600 suicide bombing attacks in over 40 countries since 1982. suicide bombing has wreaked havoc in pakistan in the last decade or so. from only a couple of attacks before 2000, it kept escalating after the us operation enduring freedom in afghanistan, promiscuously killing hundreds of people each year, towering as one of the most prominent security threats that every single pakistani faces today. the conundrum of suicide bombing in pakistan has obliterated 6,982 clean-handed civilians and injured another 17,624 in a total of 475 attacks since 1995. more than 94% of these attacks have taken place after year 2006. from 2007 to 2013 the country witnessed a suicide bombing attack on every 6th day that increased to every 4th day in 2013. counting the dead and the injured, each attack victimizes 48 people in pakistan.\n",
      "pakistan body count (www.pakistanbodycount.org) is the oldest and most accurate running tally of suicide bombings in pakistan. the given database (pakistansuicideattacks.csv) has been populated by using majority of the data from pakistan body count, and building up on it by canvassing open source newspapers, media reports, think tank analyses, and personal contacts in media and law enforcement agencies. we provide a count of the people killed and injured in suicide attacks, including the ones who died later in hospitals or homes due to injuries caused or aggravated by these attacks (second and tertiary blast injuries), making it the most authentic source for suicide attacks related data in this region.\n",
      "we will keep releasing the updates every quarter at this page.\n",
      "content\n",
      "geography: pakistan\n",
      "time period: 1995-2016\n",
      "unit of analysis: attack\n",
      "dataset: the dataset contains detailed information of 475 suicide bombing attacks in pakistan that killed an estimated 6,982 and injured 17,624 people.\n",
      "variables: the dataset contains serial no, incident date, islamic date (based on islamic lunar calendar), approximate time, long-lat, city, province, location, location sensitivity & type, target type and sect, open/close space (as it will change the impact of blast waves due to reflection), min and max number of people killed and injured, number of suicide bombers, amount of explosive being used and the name of hospitals where victims went for treatment.\n",
      "sources: unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases.\n",
      "acknowledgements & references\n",
      "pakistan body count has been leveraged extensively in scholarly publications, reports, media articles and books. the website and the dataset has been collected and curated by the founder zeeshan-ul-hassan usmani.\n",
      "users are allowed to use, copy, distribute and cite the dataset as follows: “zeeshan-ul-hassan usmani, pakistan body count, pakistan suicide bombing attacks dataset, kaggle dataset repository, jan 25, 2017.”\n",
      "past work\n",
      "zeeshan-ul-hassan usmani and daniel kirk, “simulation of suicide bombing – using computers to save lives”, i-universe, new york, ny, april 2011\n",
      "zeeshan-ul-hassan usmani and daniel kirk, “modeling and simulation of explosion effectiveness as a function of blast and crowd characteristics”, the journal of defense modeling and simulation: applications, methodology, technology, sage publications with society of simulation, vol. 6, no. 2, pp. 79-95, vista, ca, usa, october 2009\n",
      "muhammad irfan and zeeshan-ul-hassan usmani, “suicide terrorism and its new target –pakistan”, in wars, insurgencies and terrorist attacks: a psychosocial perspective from the muslim world, by unaiza niaz, oxford university press, canada, july 2010\n",
      "sana rasheed, data science for suicide bombings, i-universe, new york, ny, december 2016\n",
      "zeeshan-ul-hassan usmani and sana rasheed, “terrorism: what data sciences can do?”, 20th acm sigkdd conference on knowledge discovery and data mining (kdd 2014, data framework track) at bloomberg, new york, ny, usa (august 24-27, 2014)\n",
      "zeeshan-ul-hassan usmani, “suicide bombing forecaster – novel techniques to predict patterns of suicide bombing in pakistan”, 2012 conference on homeland security, part of 2012 autumn simulation multi-conference, san diego, ca, usa, october 28 – 31, 2012\n",
      "zeeshan-ul-hassan usmani, “blastsim – simulation to save lives”, ieee/sic winter simulation conference, phd colloquium, austin, texas, december 13-16, 2009\n",
      "zeeshan-ul-hassan usmani, fawzi alghamdi, and daniel kirk, “blastsim – multi-agent simulation of suicide bombing“, ieee symposium: computational intelligence for security and defense applications (cisda), ottawa, canada, july 8-10, 2009\n",
      "zeeshan-ul-hassan usmani, eyosias imana and daniel kirk, “virtual iraq – simulation of insurgent attacks”, ieee workshop on computational intelligence in virtual environments (cive), march 30-april 2, 2009\n",
      "zeeshan-ul-hassan usmani, eyosias imana, and daniel kirk, “random walk in extreme conditions – an agent based simulation of suicide bombing”, ieee symposium on intelligent agents, march, 2009\n",
      "zeeshan-ul-hassan usmani, eyosias imana and daniel kirk, “escaping death – geometrical recommendations for high value targets”, ieee international joint conferences on computer, information and systems sciences and engineering (cis2e 08), bridgeport, ct, december 5–13, 2008\n",
      "zeeshan-ul-hassan usmani and daniel kirk, “extreme conditions for intelligent agents”, ieee 2008 wi-iat doctoral workshop, sydney, australia, december 9-12, 2008\n",
      "zeeshan-ul-hassan usmani, andrew english & richard griffith, “the effects of a suicide bombing: crowd formations”, inter-service/industry training, simulation, and education conference (i/itsec), orlando, fl, nov 26-29 2007\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "how many people got killed and injured per year?\n",
      "visualize suicide attacks on timeline\n",
      "find out any correlation with number of suicide bombing attacks with drone attacks\n",
      "find out any correlation with suicide bombing attacks with influencing events given in the dataset\n",
      "can we predict the next suicide bombing attack?\n",
      "find the correlation between blast/explosive weight and number of people killed and injured\n",
      "find the impact of holiday type on number of blast victims\n",
      "find the correlation between islamic date and blast day/time/size/number of victims\n",
      "find the top 10 locations of blasts\n",
      "find the names of hospitals sorted by number of victims\n",
      "questions?\n",
      "for detailed visit www.pakistanbodycount.org\n",
      "or contact pakistan body count staff at info@pakistanbodycount.org\n",
      "full text of questions and answers from stack overflow that are tagged with the r tag, useful for natural language processing and community analysis.\n",
      "this is organized as three tables:\n",
      "questions contains the title, body, creation date, score, and owner id for each r question.\n",
      "answers contains the body, creation date, score, and owner id for each of the answers to these questions. the parentid column links back to the questions table.\n",
      "tags contains the tags on each question besides the r tag.\n",
      "for space reasons only non-deleted and non-closed content are included in the dataset. the dataset contains questions up to 24 september 2017 (utc).\n",
      "license\n",
      "all stack overflow user contributions are licensed under cc-by-sa 3.0 with attribution required.\n",
      "context\n",
      "the atus eating & health (eh) module was fielded from 2006 to 2008 and again in 2014 to 2016. the eh module data files contain additional information related to eating, meal preparation and health.\n",
      "data for 2015 currently are being processed and have not yet been released. data collection is planned to run through december 2016.\n",
      "content\n",
      "there are 3 datasets from 2014:\n",
      "the eh respondent file, which contains information about eh respondents, including general health and body mass index. there are 37 variables.\n",
      "the eh activity file, which contains information such as the activity number, whether secondary eating occurred during the activity, and the duration of secondary eating. there are 5 variables.\n",
      "the eh replicate weights file, which contains miscellaneous eh weights. there are 161 variables.\n",
      "the data dictionary can be found here.\n",
      "acknowledgements\n",
      "the original datasets can be found here.\n",
      "inspiration\n",
      "some ideas for exploring the datasets are:\n",
      "what is the relationship between weight or bmi and meal preparation patterns, consumption of fresh/fast food, or snacking patterns?\n",
      "do grocery shopping patterns differ by income?\n",
      "context\n",
      "each january, the entertainment community and film fans around the world turn their attention to the academy awards. interest and anticipation builds to a fevered pitch leading up to the oscar telecast in february, when hundreds of millions of movie lovers tune in to watch the glamorous ceremony and learn who will receive the highest honors in filmmaking.\n",
      "achievements in up to 25 regular categories will be honored on february 26, 2017, at the 89th academy awards presentation at the dolby theatre at hollywood & highland center.\n",
      "content\n",
      "the academy awards database contains the official record of past academy award winners and nominees. the data is complete through the 2015 (88th) academy awards, presented on february 28, 2016.\n",
      "acknowledgements\n",
      "the awards data was scraped from the official academy awards database; nominees were listed with their name first and film following in some categories, such as best actor/actress, and in the reverse for others.\n",
      "inspiration\n",
      "do the academy awards reflect the diversity of american films or are the #oscarssowhite? which actor/actress has received the most awards overall or in a single year? which film has received the most awards in a ceremony? can you predict who will receive the 2016 awards?\n",
      "context\n",
      "uk police forces collect data on every vehicle collision in the uk on a form called stats19. data from this form ends up at the dft and is published at https://data.gov.uk/dataset/road-accidents-safety-data\n",
      "content\n",
      "there are 3 csvs in this set. accidents is the primary one and has references by accident_index to the casualties and vehicles tables. this might be better done as a database.\n",
      "inspiration\n",
      "questions to ask of this data -\n",
      "combined with population data, how do different areas compare?\n",
      "what trends are there for accidents involving different road users eg motorcycles, peds, cyclists\n",
      "are road safety campaigns effective?\n",
      "likelihood of accidents for different groups / vehicles\n",
      "many more..\n",
      "manifest\n",
      "dft05-15.tgz - tar of accidents0515.csv, casualties0515.csv and vehicles0515.csv tidydata.sh - script to get and tidy data.\n",
      "context\n",
      "being a fan of board games, i wanted to see if there was any correlation with a games rating and any particular quality, the first step was to collect of this data.\n",
      "content\n",
      "the data was collected in march of 2017 from the website https://boardgamegeek.com/, this site has an api to retrieve game information (though sadly xml not json).\n",
      "acknowledgements\n",
      "mainly i want to thank the people who run the board game geek website for maintaining such a great resource for those of us in the hobby.\n",
      "inspiration\n",
      "i wish i had some better questions to ask of the data, perhaps somebody else can think of some good ways to get some insight of this dataset.\n",
      "context\n",
      "san francisco's integrated library system (ils) is composed of bibliographic records including inventoried items, patron records, and circulation data. the data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. this dataset represents the usage of inventoried items by patrons (~420k records).\n",
      "content\n",
      "the dataset includes approximately 420,000 records, with each record representing an anonymized library patron. individual columns include statistics on the type code and age of the patron, the year the patron registered (only since 2003), and how heavily the patron has been utilizing the library system (in terms of number of checkouts) since first registering.\n",
      "for more information on specific columns refer to the official data dictionary and the information in the column metadata on the /data tab.\n",
      "acknowledgements\n",
      "the data is provided by sf public library via the san francisco open data portal, under the pddl 1.0 odc public domain dedication and licence (pddl).\n",
      "photo via flickr kolya miller (cc by-nc-sa 2.0).\n",
      "inspiration\n",
      "what attributes are most associated with library activity (# of checkouts, # of renewals)?\n",
      "can you group the data into type of patrons? what classifiers would you use to predict patron type?\n",
      "context\n",
      "eclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. it is then possible for the moon's penumbral, umbral, or antumbral shadows to sweep across earth's surface thereby producing an eclipse. there are four types of solar eclipses: a partial eclipse, during which the moon's penumbral shadow traverses earth and umbral and antumbral shadows completely miss earth; an annular eclipse, during which the moon's antumbral shadow traverses earth but does not completely cover the sun; a total eclipse, during which the moon's umbral shadow traverses earth and completely covers the sun; and a hybrid eclipse, during which the moon's umbral and antumbral shadows traverse earth and annular and total eclipses are visible in different locations. earth will experience 11898 solar eclipses during the five millennium period -1999 to +3000 (2000 bce to 3000 ce).\n",
      "eclipses of the moon can occur when the moon is near one of its two orbital nodes during the full moon phase. it is then possible for the moon to pass through earth's penumbral or umbral shadows thereby producing an eclipse. there are three types of lunar eclipses: a penumbral eclipse, during which the moon traverses earth's penumbral shadow but misses its umbral shadow; a partial eclipse, during which the moon traverses earth's penumbral and umbral shadows; and a total eclipse, during which the moon traverses earth's penumbral and umbral shadows and passes completely into earth's umbra. earth will experience 12064 lunar eclipses during the five millennium period -1999 to +3000 (2000 bce to 3000 ce).\n",
      "acknowledgements\n",
      "lunar eclipse predictions were produced by fred espenak from nasa's goddard space flight center.\n",
      "the washington post is compiling a database of every fatal shooting in the united states by a police officer in the line of duty since january 1, 2015.\n",
      "in 2015, the post began tracking more than a dozen details about each killing — including the race of the deceased, the circumstances of the shooting, whether the person was armed and whether the victim was experiencing a mental-health crisis — by culling local news reports, law enforcement websites and social media and by monitoring independent databases such as killed by police and fatal encounters.\n",
      "the post is documenting only those shootings in which a police officer, in the line of duty, shot and killed a civilian — the circumstances that most closely parallel the 2014 killing of michael brown in ferguson, missouri, which began the protest movement culminating in black lives matter and an increased focus on police accountability nationwide. the post is not tracking deaths of people in police custody, fatal shootings by off-duty officers or non-shooting deaths.\n",
      "the fbi and the centers for disease control and prevention log fatal shootings by police, but officials acknowledge that their data is incomplete. in 2015, the post documented more than two times more fatal shootings by police than had been recorded by the fbi.\n",
      "the post’s database is updated regularly as fatal shootings are reported and as facts emerge about individual cases. the post is seeking assistance in making the database as comprehensive as possible. to provide information about fatal police shootings, send us an email at policeshootingsfeedback@washpost.com.\n",
      "credits\n",
      "research and reporting: julie tate, jennifer jenkins and steven rich\n",
      "production and presentation: john muyskens, kennedy elliott and ted mellnik\n",
      "content\n",
      "which olympic athletes have the most gold medals? which countries are they from and how has it changed over time?\n",
      "more than 35,000 medals have been awarded at the olympics since 1896. the first two olympiads awarded silver medals and an olive wreath for the winner, and the ioc retrospectively awarded gold, silver, and bronze to athletes based on their rankings. this dataset includes a row for every olympic athlete that has won a medal since the first games.\n",
      "acknowledgements\n",
      "data was provided by the ioc research and reference service and published by the guardian's datablog.\n",
      "challenge description\n",
      "this dataset and accompanying paper present a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an rnn to learn the correct normalization function. that is, a date written \"31 may 2014\" is spoken as \"the thirty first of may twenty fourteen.\" we present a dataset of general text where the normalizations were generated using an existing text normalization component of a text-to-speech (tts) system. this dataset was originally released open-source here and is reproduced on kaggle for the community.\n",
      "the data\n",
      "the data in this directory are the english language training, development and test data used in sproat and jaitly (2016).\n",
      "the following divisions of data were used:\n",
      "training: output_1 through output_21 (corresponding to output-000[0-8]?-of-00100 in the original dataset)\n",
      "runtime eval: output_91 (corresponding to output-0009[0-4]-of-00100 in the original dataset)\n",
      "test data: output_96 (corresponding to output-0009[5-9]-of-00100 in the original dataset)\n",
      "in practice for the results reported in the paper only the first 100,002 lines of output-00099-of-00100 were used (for english).\n",
      "lines with \"\" in two columns are the end of sentence marker, otherwise there are three columns, the first of which is the \"semiotic class\" (taylor, 2009), the second is the input token and the third is the output, following the paper cited above.\n",
      "all text is from wikipedia. all data were extracted on 2016/04/08, and run through the google kestrel tts text normalization system (ebden and sproat, 2015), so that the notion of \"token\", \"semiotic class\" and reference output are all kestrel's notion.\n",
      "our research\n",
      "in this paper, we present our own experiments with this data set with a variety of different rnn architectures. while some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. on the other hand, we show that a simple fst-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the rnn alone.\n",
      "though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure rnn approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general rnn model. and with open-source data, we provide a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions.\n",
      "disclaimer\n",
      "this is not an official google product.\n",
      "references\n",
      "ebden, peter and sproat, richard. 2015. the kestrel tts text normalization system. natural language engineering. 21(3).\n",
      "richard sproat and navdeep jaitly. 2016. rnn approaches to text normalization: a challenge. released on arxiv.org: https://arxiv.org/abs/1611.00068\n",
      "taylor, paul. 2009. text-to-speech synthesis. cambridge university press, cambridge.\n",
      "context\n",
      "consolidated draft data from http://www.pro-football-reference.com/ for all drafts from 1985 to 2015.\n",
      "content\n",
      "pro-football-reference av: approximate value is pfr's attempt to attach a single number to every player-season since 1960. methodology can be found here: http://www.pro-football-reference.com/blog/indexd961.html?page_id=8061\n",
      "player_id pro football reference player id\n",
      "year draft year\n",
      "rnd draft round\n",
      "pick draft pick\n",
      "tm team\n",
      "player player first and last name\n",
      "pos position unfiltered\n",
      "position standard position standardized to one of the following qb, lb, wr, t, de, rb, db, dt, c, c, g, te, fb, p, ls, k\n",
      "first4av av accumulated for this player's first four seasons\n",
      "age age at time of draft\n",
      "to year of last season played\n",
      "ap1 # of first team all-pro selections\n",
      "pb # of pro-bowl selections\n",
      "st # of years as a primary starter in their primary position\n",
      "carav weighted career av - 100% of best season, 95% of second best season, 90% of third best season, and so on\n",
      "drav av accumulated for team that drafted this player\n",
      "g games played\n",
      "cmp pass completions\n",
      "pass_att pass attempts\n",
      "pass_yds yards gained by passing\n",
      "pass_td passing touchdowns\n",
      "pass_int interceptions thrown\n",
      "rush_att rushing attempts\n",
      "rush_yds rushing yards gained\n",
      "rush_tds rushing touchdowns\n",
      "rec receptions\n",
      "rec_yds receiving yards gained\n",
      "rec_tds receiving touchdowns\n",
      "tkl tackles\n",
      "def_int defensive interceptions\n",
      "sk sacks\n",
      "college/univ college/university attended by player\n",
      "acknowledgements\n",
      "http://www.pro-football-reference.com/\n",
      "horse racing - a different and profitable approach\n",
      "the traditional approach in attempting to make a profit from horse-racing, using machine learning techniques, is to use systems involving dozens and dozens of variables. these systems include the following types of variables:\n",
      "horse - name, sex, age, pedigree, weight, speed over various distances, race data with finishing times and positions - etc. trainer info. jockey info. track info - track, track conditions - etc.\n",
      "and a whole lot more.\n",
      "finding, compiling, maintaining and updating this data is a massive task for the individual. unless you have access to a database of such data - where would you even start?\n",
      "we have a different approach.\n",
      "we collect, maintain and use data from various 'tipsters'. the tipsters use their skill to study the horses and make a prediction - that they think a particular horse will win a particular race. we take those tipsters predictions and put them through a machine learning algorithm (microsoft azure) asking it to predict a 'win' or 'lose' based upon the tipsters performance history.\n",
      "we have a database of approx. 39,000 bets using 31 odd tipsters. fifteen tipsters are active and sixteen tipsters are inactive the betting history for the inactive tipsters is used in the dataset as it appears to add 'weight' to the system when considering active tips.\n",
      "we have been using this system live for three months now and although it has it's ups and downs - it makes money! one bookmaker has already closed our account.\n",
      "we are looking to further optimize the system to reach it's maximum efficiency coupled with a betting strategy to increase profitability. we ask for your help. if you can produce an 'azure' system more efficient than ours - then further information will be shared with you.\n",
      "questions\n",
      "are bets from inactive tipsters critical to performance?\n",
      "is it better to have all the tipsters 'stacked on top of each other' in one large dataset or is the system better served by separating them out?\n",
      "predicting bets\n",
      "when we ask the system to predict if a bet will win or lose for say tipster a - we take the last id number for that tipster and add one to it - making it a new id - outside the systems experience. that id is used for all that tipsters bets until the system is updated. the system is updated once a week.\n",
      "good hunting.\n",
      "gunner38\n",
      "nfl-statistics-scrape\n",
      "here are the basic statistics, career statistics and game logs provided by the nfl on their website (http://www.nfl.com) for all players past and present.\n",
      "summary\n",
      "the data was scraped using a python code. the code can be located at github: https://github.com/kendallgillies/nfl-statistics-scrape\n",
      "explanation of data\n",
      "the first main group of statistics is the basic statistics provided for each player. this data is stored in the csv file titled basic_stats.csv along with the player’s name and url identifier. if available the data pulled for each player is as follows:\n",
      "number\n",
      "position\n",
      "current team\n",
      "height\n",
      "weight\n",
      "age\n",
      "birthday\n",
      "birth place\n",
      "college attended\n",
      "high school attended\n",
      "high school location\n",
      "experience\n",
      "the second main group of statistics gathered for each player are their career statistics. while each player has a main position they play, they will have statistics in other areas; therefore, the career statistics are divided into statistics types. the statistics are then stored in csv files based on statistic type along with the player name, url identifier and position (if available). the following are the career statistics types and accompanying csv file names:\n",
      "defensive statistics – career_stats_defensive.csv\n",
      "field goal kickers - career_stats_field_goal_kickers.csv\n",
      "fumbles - career_stats_fumbles.csv\n",
      "kick return - career_stats_kick_return.csv\n",
      "kickoff - career_stats_kickoff.csv\n",
      "offensive line - career_stats_offensive_line.csv\n",
      "passing - career_stats_passing.csv\n",
      "punt return - career_stats_punt_return.csv\n",
      "punting - career_stats_punting.csv\n",
      "receiving - career_stats_receiving.csv\n",
      "rushing - career_stats_rushing.csv\n",
      "the final group of statistics is the game logs for each player. the game logs are stored by position and have the player name, url identifier and position (if available). the following are the game log types and accompanying csv file names:\n",
      "quarterback – game_logs_quarterback.csv\n",
      "running back – game_logs_runningback.csv\n",
      "wide receiver and tight end – game_logs_wide_receiver_and_tight_end.csv\n",
      "offensive line – game_logs_offensive_line.csv\n",
      "defensive lineman – game_logs_defensive_lineman.csv\n",
      "kickers – game_logs_kickers.csv\n",
      "punters – game_logs_punters.csv\n",
      "glossary\n",
      "while most of the abbreviations used by the nfl have been translated in the table headers in the data files, there are still a couple of abbreviations used.\n",
      "fg: field goal\n",
      "td: touchdown\n",
      "int: interception\n",
      "context\n",
      "new york city’s trees shade us in the summer, beautify our neighborhoods, help reduce noise, and support urban wildlife. beyond these priceless benefits, our urban forest provides us a concrete return on the financial investment we put into it. this return includes stormwater interception, energy conservation, air pollutant removal, and carbon dioxide storage. our publicly owned trees are as much of an asset to us as our streets, sewers, bridges, and public buildings.\n",
      "content\n",
      "this dataset includes a record for every tree in new york city and includes the tree's location by borough and latitude/longitude, species by latin name and common names, size, health, and issues with the tree's roots, trunk, and branches.\n",
      "acknowledgements\n",
      "the 2015, 2005, and 1995 tree censuses were conducted by nyc parks and recreation staff, treescount! program staff, and hundreds of volunteers.\n",
      "can happiness predict employee turnover, or is it the other way around?\n",
      "it is the summer of 2016. i am in barcelona and it is hot and humid. by chance i go to a talk where alex rios - the ceo of myhappyforce.com explains his product. he has built an app where employees report daily happiness levels at work. this app is used by companies to track happiness of the workforce. after the talk i ask him if he would opensource the (anonymized data) so we can better understad the phenomenon of employee turnover. here is what we did, we developed a model that predicts which employees will churn. then we looked at the features (used by the model) that are common to employees that churn. the top feautures of employees that churn are:\n",
      "low ratio of likes received (likeability)\n",
      "low posting frequency (engagement),\n",
      "low relative happiness (employee happiness normalized by company mean).\n",
      "surprisingly, a priori expected explanatory features such as mean happiness level and the ratio of likes (positivity), were not significant. precision@50 = 80% out of a test set with 116 churns, sample size n=2k. another surprise was that raw happiness is a bad predictor of churn. but, the question is, what did we miss? can you find more insights?\n",
      "starter script\n",
      "r starter script https://www.kaggle.com/harriken/how-many-unlikes-it-takes-to-get-fired\n",
      "content\n",
      "the data consists of four tables: votes, comments, interactions and churn. a vote was obtained when an employee opened the app and answered the question: how happy are you at work today? to vote the employee indicates their feeling by touching one of four icons that appeared on the screen. after the employee indicates their happiness level, a second screen appears where they can input a text explanation (usually a complaint, suggestion or comment), this is the comments table. out of 4,356 employees, 2,638 employees commented at least once. finally, in a third screen the employee can see their peers’ comments and like or dislike them, this data is stored in the interactions table. 3,516 employees liked or disliked at least one of their peers’ comments. the churn table contains when an employee churned (quit or was fired).\n",
      "acknowledgements\n",
      "python script version with social graph features: http://bit.ly/2v2sezg\n",
      "more detailed r scripts: https://github.com/orioli/e3\n",
      "the paper which was presented at asonam 2017 sydney\n",
      "slides https://www.slideshare.net/harriken/ieee-happiness-an-inside-job-asoman-2017\n",
      "inspiration\n",
      "the cost of employee turnover has been pointed out extensively in the literature. a high turnover rate not only increases human resource costs, which can reach up to 150% of the annual salary per replaced employee, but it also has social costs, as it is correlated with lower wages, lower productivity per employee, and not surprisingly, a less loyal workforce 1. for reference, in 2006, turnover at walmart’s sam’s club was 44% with an average hourly pay of $10.11, while at costco it was a much lower 17% with a higher $17.0 hourly wage 2. in addition, a more recent study correlated companies with low turnover with a series of socially positive characteristics dubbed high-involvement work practices 3. on the other hand, research on employee turnover (churn) is not a prolific topic in the engineering community. in ieee publications, one can find just over 278 publications with titles containing the keyword churn, and the bulk of those focus on customer churn, and specifically churn in the telecommunications industry, while on the topic of employee churn there is just one title indexed 4. the goal is to clarify the characteristics of employees that will churn (or that are at risk of churning), to help companies understand the causes so they can reduce the turnover rate.\n",
      "context\n",
      "at rsna 2017 there was a contest to correctly identify the age of a child from an x-ray of their hand. this is the dataset on kaggle making it easier to experiment with and do educational demos. additionally maybe there are some new ideas for building smarter models for handling x-ray images.\n",
      "content\n",
      "a number of folders full of images (digital and scanned) with a csv containing the age (what is to be predicted) and the gender (useful additional information)\n",
      "acknowledgements\n",
      "the dataset was originally published on cloudapp as an rsna challenge.\n",
      "original dataset acknowledgements\n",
      "the radiological society of north america (rsna) radiology informatics committee (ric) pediatric bone age machine learning challenge organizing committee:\n",
      "kathy andriole, massachusetts general hospital\n",
      "brad erickson, mayo clinic\n",
      "adam flanders, thomas jefferson university\n",
      "safwan halabi, stanford university\n",
      "jayashree kalpathy-cramer, massachusetts general hospital\n",
      "marc kohli, university of california - san francisco\n",
      "luciano prevedello, the ohio state university\n",
      "data sets used in the pediatric bone age challenge have been contributed by stanford university, the university of colorado and the university of california - los angeles.\n",
      "the medici platform (built codalab) used for the challenge is provided by jayashree kalpathy-cramer, supported through nih grants (u24ca180927) and a contract from leidos.\n",
      "inspiration\n",
      "can you predict with better than 4.2 months accuracy?\n",
      "is identifying the joints an important step?\n",
      "what algorithms work best?\n",
      "what do the algorithms focus on?\n",
      "is gender a necessary piece of information or can it be automatically derived from the image?\n",
      "context\n",
      "glove is an unsupervised learning algorithm for obtaining vector representations for words. training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
      "content\n",
      "this dataset contains english word vectors pre-trained on the combined wikipedia 2014 + gigaword 5th edition corpora (6b tokens, 400k vocab). all tokens are in lowercase. this dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. for 300-dimensional word vectors and additional information, please see the project website.\n",
      "acknowledgements\n",
      "this data has been released under the open data commons public domain dedication and license. if you use this dataset in your work, please cite the following paper:\n",
      "jeffrey pennington, richard socher, and christopher d. manning. 2014. glove: global vectors for word representation. url: https://nlp.stanford.edu/pubs/glove.pdf\n",
      "inspiration\n",
      "glove embeddings have been used in more than 2100 papers, and counting! you can use these pre-trained embeddings whenever you need a way to quantify word co-occurrence (which also captures some aspects of word meaning.)\n",
      "context\n",
      "the study of human mobility and activities has opened up to an incredible number of studies in the past, most of which included the use of sensors distributed on the body of the subject. more recently, the use of smart devices has been particularly relevant because they are already everywhere and they come with accurate miniaturized sensors. whether it is smartphones, smartwatches or smartglasses, each device can be used to describe complementary information such as emotions, precise movements, or environmental conditions.\n",
      "content\n",
      "first of all, a smartphone is used to capture mainly contextual data. two applications are used: a simple data collection application based on the swipe open-source sensing system (swipe), and a logbook application for obtaining real data on user activity (timelogger). swipe is a platform for sensing, recording and processing human dynamics using smartwatches and smartphones.\n",
      "then, a smartwatch is used primarily to capture the user's heart rate. motion data is also collected, without being at the heart of the dataset due to its need to be configured with a low sampling frequency, which would drastically increase the dataset and drain the battery as well. an application based on swipe is used.\n",
      "finally, jins meme smartglasses are used. this model has the advantage of being compact and simple to carry. it does not have a camera or a screen; it simply has three types of sensors: an accelerometer (for detecting steps or activities), a gyroscope (for head movements) and an occulographic sensor (eye blinking, eye orientation). the official datalogger application from jins meme is used.\n",
      "for more information on the dataset please refer to the corresponding publication, available at an open dataset for human activity analysis using smart devices.\n",
      "the current dataset on kaggle contains smartglasses data with 20ms interval (due to storage limitations), same data with 10ms interval is also available on demand. contact sasan.jafarnejad [at] uni [dot] lu to receive the 10ms version.\n",
      "acknowledgements\n",
      "this work was performed within the eglasses project, which is partially funded by ncbir, fwf, snsf, anr and fnr under the era-net chist-eraii framework.\n",
      "general info\n",
      "this is a collection of over 50,000 ranked euw games from the game league of legends, as well as json files containing a way to convert between champion and summoner spell ids and their names. for each game, there are fields for:\n",
      "game id\n",
      "creation time (in epoch format)\n",
      "game duration (in seconds)\n",
      "season id\n",
      "winner (1 = team1, 2 = team2)\n",
      "first baron, dragon, tower, blood, inhibitor and rift herald (1 = team1, 2 = team2, 0 = none)\n",
      "champions and summoner spells for each team (stored as riot's champion and summoner spell ids)\n",
      "the number of tower, inhibitor, baron, dragon and rift herald kills each team has\n",
      "the 5 bans of each team (again, champion ids are used)\n",
      "this dataset was collected using the riot games api, which makes it easy to lookup and collect information on a users ranked history and collect their games. however finding a list of usernames is the hard part, in this case i am using a list of usernames scraped from 3rd party lol sites.\n",
      "possible uses\n",
      "there is a vast amount of data in just a single lol game. this dataset takes the most relevant information and makes it available easily for use in things such as attempting to predict the outcome of a lol game, analysing which in-game events are most likely to lead to victory, understanding how big of an effect bans of a specific champion have, and more.\n",
      "summary\n",
      "“meetup is a social networking website that aims to brings people together to do, explore, teach and learn the things that help them come alive.”\n",
      "meetup allows members to find and join groups unified by a common interest. as of 2017, there are 32 million users with 280 thousand groups available across 182 countries.\n",
      "a member needs to be able to identify groups and activities which interest them the most to be able to use this platform to network effectively.\n",
      "the aim of our team was to use this dataset to build a recommender system which will identify and suggest groups and activities to a member based on their interest and additional interests of similar members. furthermore, a social network analysis was done to identify the relationship between groups and people.\n",
      "database eer diagram\n",
      "data collection method\n",
      "data was collected using meetup api.\n",
      "python script was used to ping meetup api and collect responses as json objects.\n",
      "logical chunks of data were exported and saved as csv files.\n",
      "data cleaning\n",
      "data is filtered to include only 3 cities' information (new york, san francisco, chicago).\n",
      "character encoding is normalized to ascii characters across tables.\n",
      "example visualizations\n",
      "are you curious about fertilizer use in developing economies? the growth of chinese steel exports? american chocolate consumption? which parts of the world still use typewriters? you'll find all of that and more here. this dataset covers import and export volumes for 5,000 commodities across most countries on earth over the last 30 years.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nations statistics division on the undata site. you can find the original dataset here.\n",
      "inspiration\n",
      "some of these numbers are more trustworthy than others. i'd expect that british tea imports are fairly accurate, but doubt that afghanistan exported exactly 51 sheep in 2016. can you identify which nations appear to have the most trustworthy data? which industries?\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "job posts dataset\n",
      "the dataset consists of 19,000 job postings that were posted through the armenian human resource portal careercenter. the data was extracted from the yahoo! mailing group https://groups.yahoo.com/neo/groups/careercenter-am. this was the only online human resource portal in the early 2000s. a job posting usually has some structure, although some fields of the posting are not necessarily filled out by the client (poster). the data was cleaned by removing posts that were not job related or had no structure. the data consists of job posts from 2004-2015\n",
      "content\n",
      "jobpost – the original job post\n",
      "date – date it was posted in the group\n",
      "title – job title\n",
      "company - employer\n",
      "announcementcode – announcement code (some internal code, is usually missing)\n",
      "term – full-time, part-time, etc\n",
      "eligibility -- eligibility of the candidates\n",
      "audience --- who can apply?\n",
      "startdate – start date of work\n",
      "duration - duration of the employment\n",
      "location – employment location\n",
      "jobdescription – job description\n",
      "jobrequirment - job requirements\n",
      "requiredqual -required qualification\n",
      "salary - salary\n",
      "applicationp – application procedure\n",
      "openingdate – opening date of the job announcement\n",
      "deadline – deadline for the job announcement\n",
      "notes - additional notes\n",
      "aboutc - about the company\n",
      "attach - attachments\n",
      "year - year of the announcement (derived from the field date)\n",
      "month - month of the announcement (derived from the field date)\n",
      "it – true if the job is an it job. this variable is created by a simple search of it job titles within column “title”\n",
      "acknowledgements\n",
      "the data collection and initial research was funded by the american university of armenia’s research grant (2015).\n",
      "inspiration\n",
      "the online job market is a good indicator of overall demand for labor in the local economy. in addition, online job postings data are easier and quicker to collect, and they can be a richer source of information than more traditional job postings, such as those found in printed newspapers. the data can be used in the following ways: -understand the demand for certain professions, job titles, or industries -help universities with curriculum development -identify skills that are most frequently required by employers, and how the distribution of necessary skills changes over time -make recommendations to job seekers and employers\n",
      "past research\n",
      "we have used association rules mining and simple text mining techniques to analyze the data. some results can be found here (https://www.slideshare.net/habetmadoyan/it-skills-analysis-63686238).\n",
      "introduction\n",
      "video games are a rich area for data extraction due to its digital nature. notable examples such as the complex eve online economy, world of warcraft corrupted blood incident and even grand theft auto self-driving cars tells us that fiction is closer to reality than we really think. data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.\n",
      "in this kaggle dataset, i provide over 720,000 competitive matches from the popular game playerunknown's battlegrounds. the data was extracted from pubg.op.gg, a game tracker website. i intend for this data-set to be purely exploratory, however users are free to create their own predictive models they see fit.\n",
      "playerunknown's battlegrounds\n",
      "pubg is a first/third-person shooter battle royale style game that matches over 90 players on a large island where teams and players fight to the death until one remains. players are airdropped from an airplane onto the island where they are to scavenge towns and buildings for weapons, ammo, armor and first-aid. players will then decide to either fight or hide with the ultimate goal of being the last one standing. a bluezone (see below) will appear a few minutes into the game to corral players closer and closer together by dealing damage to anyone that stands within the bluezone and sparing whoever is within the safe zone.\n",
      "read more about pubg here\n",
      "the dataset\n",
      "this dataset provides two zips: aggregate and deaths.\n",
      "in deaths, the files record every death that occurred within the 720k matches. that is, each row documents an event where a player has died in the match.\n",
      "in aggregate, each match's meta information and player statistics are summarized (as provided by pubg). it includes various aggregate statistics such as player kills, damage, distance walked, etc as well as metadata on the match itself such as queue size, fpp/tpp, date, etc.\n",
      "the uncompressed data is divided into 5 chunks of approximately 2gb each. for details on columns, please see the file descriptions.\n",
      "interpreting positional data\n",
      "the x,y coordinates are all in in-game coordinates and need to be linearly scaled to be plotted on square erangel and miramar maps. the coordinate min,max are 0,800000 respectively.\n",
      "potential bias in the data\n",
      "the scraping methodology first starts with an initial seed player, i chose this to be my own account (a rather low rank individual). i then use the seed player to scrape for all players that it has encountered in its historical matches. i then take a random subset of 5000 players from this and then scrape for their historical games for the final dataset. what this could produce is an unrepresentative sample of all games played as it is more likely that i queued and matched with lower rated players and those players more than likely also got matched against lower rated players as well. thus, the matches and deaths are more representative of lower tier gameplay but given the simplicity of the dataset, this shouldn't be an issue.\n",
      "acknowledgements\n",
      "pubg.op.gg, if this is against the tos, please let me know and i will take it down\n",
      "context\n",
      "with this dataset we hope to do a nice cheeky wink to the \"cats and dogs\" image dataset. in fact, this dataset is aimed to be the audio counterpart of the famous \"cats and dogs\" image classification task, here available on kaggle.\n",
      "content\n",
      "the dataset consists in many \"wav\" files for both the cat and dog classes :\n",
      "cat has 164 wav files to which corresponds 1323 sec of audio\n",
      "dog has 113 wav files to which corresponds 598 sec of audio\n",
      "you can have an visual description of the wav here : visualizing woofs & meows 🐱. in accessing the dataset 2 we propose a train / test split which can be used.\n",
      "all the wav files contains 16khz audio and have variable length.\n",
      "acknowledgements\n",
      "we have not much credit in proposing the dataset here. much of the work have been done by the ae-dataset creator (from which we extracted the two classes) and by the humans behind freesound from which was extracted the ae-dataset.\n",
      "inspiration\n",
      "you might use this dataset to test raw audio classification challenge ;)\n",
      "a more challenging dataset is available here\n",
      "these are 5 texts taken from project gutenberg, uploaded to kaggle to encourage things like text-mining and sentiment analysis. these are fun skills to develop and many existing datasets on kaggle don't lend themselves to these sorts of analyses.\n",
      "the 5 books are,\n",
      "the king james bible\n",
      "the quran\n",
      "the book of mormon\n",
      "the gospel of buddha\n",
      "meditations, by marcus aurelius\n",
      "project gutenberg is an online archive of books that are free to download and distribute. these files are taken without alteration (filenames or contents) from the project gutenberg website\n",
      "context\n",
      "the diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. automated methods to detect and classify blood cell subtypes have important medical applications.\n",
      "content\n",
      "this dataset contains 12,500 augmented images of blood cells (jpeg) with accompanying cell type labels (csv). there are approximately 3,000 images for each of 4 different cell types grouped into 4 different folders (according to cell type). the cell types are eosinophil, lymphocyte, monocyte, and neutrophil. this dataset is accompanied by an additional dataset containing the original 410 images (pre-augmentation) as well as two additional subtype labels (wbc vs wbc) and also bounding boxes for each cell in each of these 410 images (jpeg + xml metadata). more specifically, the folder 'dataset-master' contains 410 images of blood cells with subtype labels and bounding boxes (jpeg + xml), while the folder 'dataset2-master' contains 2,500 augmented images as well as 4 additional subtype labels (jpeg + csv). there are approximately 3,000 augmented images for each class of the 4 classes as compared to 88, 33, 21, and 207 images of each in folder 'dataset-master'.\n",
      "acknowledgements\n",
      "https://github.com/shenggan/bccd_dataset mit license\n",
      "inspiration\n",
      "the diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. automated methods to detect and classify blood cell subtypes have important medical applications.\n",
      "context\n",
      "this dataset was used for a study where the task was to generate a top-n list of restaurants according to the consumer preferences and finding the significant features. two approaches were tested: a collaborative filter technique and a contextual approach: (i) the collaborative filter technique used only one file i.e., rating_final.csv that comprises the user, item and rating attributes. (ii) the contextual approach generated the recommendations using the remaining eight data files.\n",
      "content\n",
      "there are 9 data files and a readme, and are grouped like this:\n",
      "restaurants\n",
      "1 chefmozaccepts.csv\n",
      "2 chefmozcuisine.csv\n",
      "3 chefmozhours4.csv\n",
      "4 chefmozparking.csv\n",
      "5 geoplaces2.csv\n",
      "consumers\n",
      "6 usercuisine.csv\n",
      "7 userpayment.csv\n",
      "8 userprofile.csv\n",
      "user-item-rating\n",
      "9 rating_final.csv\n",
      "more detailed file descriptions can also be found in the readme:\n",
      "1 chefmozaccepts.csv\n",
      "instances: 1314\n",
      "attributes: 2\n",
      "placeid: nominal\n",
      "rpayment: nominal, 12\n",
      "2 chefmozcuisine.csv\n",
      "instances: 916\n",
      "attributes: 2\n",
      "placeid: nominal\n",
      "rcuisine: nominal, 59\n",
      "3 chefmozhours4.csv\n",
      "instances: 2339\n",
      "attributes: 3\n",
      "placeid: nominal\n",
      "hours: nominal, range:00:00-23:30\n",
      "days: nominal, 7\n",
      "4 chefmozparking.csv\n",
      "instances: 702\n",
      "attributes: 2\n",
      "placeid: nominal\n",
      "parking_lot: nominal, 7\n",
      "5 geoplaces2.csv\n",
      "instances: 130\n",
      "attributes: 21\n",
      "placeid: nominal\n",
      "latitude: numeric\n",
      "longitude: numeric\n",
      "the_geom_meter: nominal (geospatial)\n",
      "name: nominal\n",
      "address: nominal,missing: 27\n",
      "city: nominal, missing: 18\n",
      "state: nominal, missing: 18\n",
      "country: nominal, missing: 28\n",
      "fax: numeric, missing: 130\n",
      "zip: nominal,missing: 74\n",
      "alcohol: nominal, values: 3\n",
      "smoking_area: nominal, 5\n",
      "dress_code: nominal, 3\n",
      "accessibility: nominal, 3\n",
      "price: nominal, 3\n",
      "url: nominal, missing: 116\n",
      "rambience: nominal, 2\n",
      "franchise: nominal, 2\n",
      "area: nominal, 2\n",
      "other_services: nominal, 3\n",
      "6 rating_final.csv\n",
      "instances: 1161\n",
      "attributes: 5\n",
      "userid: nominal\n",
      "placeid: nominal\n",
      "rating: numeric, 3\n",
      "food_rating: numeric, 3\n",
      "service_rating: numeric, 3\n",
      "7 usercuisine.csv\n",
      "instances: 330\n",
      "attributes: 2\n",
      "userid: nominal\n",
      "rcuisine: nominal, 103\n",
      "8 userpayment.csv\n",
      "instances: 177\n",
      "attributes: 2\n",
      "userid: nominal\n",
      "upayment: nominal, 5\n",
      "9 userprofile\n",
      "instances: 138\n",
      "attributes: 19\n",
      "userid: nominal\n",
      "latitude: numeric\n",
      "longitude: numeric\n",
      "the_geom_meter: nominal (geospatial)\n",
      "smoker: nominal\n",
      "drink_level: nominal, 3\n",
      "dress_preference:nominal, 4\n",
      "ambience: nominal, 3\n",
      "transport: nominal, 3\n",
      "marital_status: nominal, 3\n",
      "hijos: nominal, 3\n",
      "birth_year: nominal\n",
      "interest: nominal, 5\n",
      "personality: nominal, 4\n",
      "religion: nominal, 5\n",
      "activity: nominal, 4\n",
      "color: nominal, 8\n",
      "weight: numeric\n",
      "budget: nominal, 3\n",
      "height: numeric\n",
      "acknowledgements\n",
      "this dataset was originally downloaded from the uci ml repository: uci ml\n",
      "creators: rafael ponce medellín and juan gabriel gonzález serna rafaponce@cenidet.edu.mx, gabriel@cenidet.edu.mx department of computer science. national center for research and technological development cenidet, méxico\n",
      "donors of database: blanca vargas-govea and juan gabriel gonzález serna blanca.vargas@cenidet.edu.mx/blanca.vg@gmail.com, gabriel@cenidet.edu.mx department of computer science. national center for research and technological development cenidet, méxico\n",
      "inspiration\n",
      "use this data to create a restaurant recommender or determine which restaurants a person is most likely to visit.\n",
      "content\n",
      "the united states census bureau’s international dataset provides estimates of country populations since 1950 and projections through 2050. specifically, the data set includes midyear population figures broken down by age and gender assignment at birth. additionally, they provide time-series data for attributes including fertility rates, birth rates, death rates, and migration rates.\n",
      "the full documentation is available here. for basic field details, please see the data dictionary.\n",
      "note: the u.s. census bureau provides estimates and projections for countries and areas that are recognized by the u.s. department of state that have a population of at least 5,000.\n",
      "acknowledgements\n",
      "this dataset was created by the united states census bureau.\n",
      "inspiration\n",
      "which countries have made the largest improvements in life expectancy? based on current trends, how long will it take each country to catch up to today’s best performers?\n",
      "use this dataset with bigquery\n",
      "you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too: https://cloud.google.com/bigquery/public-data/international-census.\n",
      "context\n",
      "the national hurricane center (nhc) conducts a post-storm analysis of each tropical cyclone in the atlantic basin (i.e., north atlantic ocean, gulf of mexico, and caribbean sea) and and the north pacific ocean to determine the official assessment of the cyclone's history. this analysis makes use of all available observations, including those that may not have been available in real time. in addition, nhc conducts ongoing reviews of any retrospective tropical cyclone analyses brought to its attention and on a regular basis updates the historical record to reflect changes introduced.\n",
      "content\n",
      "the nhc publishes the tropical cyclone historical database in a format known as hurdat, short for hurricane database. these databases (atlantic hurdat2 and ne/nc pacific hurdat2) contain six-hourly information on the location, maximum winds, central pressure, and (starting in 2004) size of all known tropical cyclones and subtropical cyclones.\n",
      "context\n",
      "this data was originally posted on my personal onedrive account.\n",
      "it represent fictitious/fake data on terminations. for each of 10 years it show employees that are active and those that terminated.\n",
      "the intent is to see if individual terminations can be predicted from the data provided.\n",
      "the thing to be predicted is status of active or terminated\n",
      "content\n",
      "the data contains\n",
      "employee id employee record date ( year of data) birth date hire date termination date age length of service city department job title store number gender termination reason termination type status year status business unit\n",
      "these might be typical types of data in hris\n",
      "acknowledgements\n",
      "none- its fake data\n",
      "inspiration\n",
      "a lot of turnover analyses occur at an aggregate level-such as turnover rates. but few analyses concentrate on trying to identify exactly which individuals might leave based on patterns that might be present in existing data.\n",
      "machine learning algorithms often showcase customer churn examples for telcos or product marketing. those algorithms equally apply to employee churn.\n",
      "here's a data set of 1,000 most popular movies on imdb in the last 10 years. the data points included are:\n",
      "title, genre, description, director, actors, year, runtime, rating, votes, revenue, metascrore\n",
      "feel free to tinker with it and derive interesting insights.\n",
      "what you get:\n",
      "upvote! the database contains +40,000 records on us gross rent & geo locations. the field description of the database is documented in the attached pdf file. to access, all 325,272 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to upvote. upvote right now, please. enjoy!\n",
      "get the full free database with coupon code: freedatabase, see directions at the bottom of the description... and make sure to upvote :) coupon ends at 2:00 pm 8-23-2017\n",
      "gross rent & geographic statistics:\n",
      "mean gross rent (double)\n",
      "median gross rent (double)\n",
      "standard deviation of gross rent (double)\n",
      "number of samples (double)\n",
      "square area of land at location (double)\n",
      "square area of water at location (double)\n",
      "geographic location:\n",
      "longitude (double)\n",
      "latitude (double)\n",
      "state name (character)\n",
      "state abbreviated (character)\n",
      "state_code (character)\n",
      "county name (character)\n",
      "city name (character)\n",
      "name of city, town, village or cpd (character)\n",
      "primary, defines if the location is a track and block group.\n",
      "zip code (character)\n",
      "area code (character)\n",
      "abstract\n",
      "the data set originally developed for real estate and business investment research. income is a vital element when determining both quality and socioeconomic features of a given geographic location. the following data was derived from over +36,000 files and covers 348,893 location records.\n",
      "license\n",
      "only proper citing is required please see the documentation for details. have fun!!!\n",
      "golden oak research group, llc. “u.s. income database kaggle”. publication: 5, august 2017. accessed, day, month year.\n",
      "for any questions, you may reach us at research_development@goldenoakresearch.com. for immediate assistance, you may reach me on at 585-626-2965\n",
      "please note: it is my personal number and email is preferred\n",
      "check our data's accuracy: census fact checker\n",
      "access all 325,272 location for free database coupon code:\n",
      "don't settle. go big and win big. optimize your potential**. access all gross rent records and more on a scale roughly equivalent to a neighborhood, see link below:\n",
      "website: golden oak research make sure to upvote\n",
      "a small startup with big dreams, giving the every day, up and coming data scientist professional grade data at affordable prices it's what we do.\n",
      "context\n",
      "the image at the top of the page is a frame from today's (7/26/2016) isis #tweetmovie from twitter, a \"normal\" day when two isis operatives murdered a priest saying mass in a french church. (you can see this in the center left). a selection of data from this site is being made available here to kaggle users.\n",
      "update: an excellent study by audrey alexander titled digital decay? is now available which traces the \"change over time among english-language islamic state sympathizers on twitter.\n",
      "intent\n",
      "this data set is intended to be a counterpoise to the how isis uses twitter data set. that data set contains 17k tweets alleged to originate with \"100+ pro-isis fanboys\". this new set contains 122k tweets collected on two separate days, 7/4/2016 and 7/11/2016, which contained any of the following terms, with no further editing or selection:\n",
      "isis\n",
      "isil\n",
      "daesh\n",
      "islamicstate\n",
      "raqqa\n",
      "mosul\n",
      "\"islamic state\"\n",
      "this is not a perfect counterpoise as it almost surely contains a small number of pro-isis fanboy tweets. however, unless some entity, such as kaggle, is willing to expend significant resources on a service something like an expert level mechanical turk or zooniverse, a high quality counterpoise is out of reach.\n",
      "a counterpoise provides a balance or backdrop against which to measure a primary object, in this case the original pro-isis data. so if anyone wants to discriminate between pro-isis tweets and other tweets concerning isis you will need to model the original pro-isis data or signal against the counterpoise which is signal + noise. further background and some analysis can be found in this forum thread.\n",
      "this data comes from postmodernnews.com/token-tv.aspx which daily collects about 25mb of isis tweets for the purposes of graphical display. please note: this server is not currently active.\n",
      "data details\n",
      "there are several differences between the format of this data set and the pro-isis fanboy dataset. 1. all the twitter t.co tags have been expanded where possible 2. there are no \"description, location, followers, numberstatuses\" data columns.\n",
      "i have also included my version of the original pro-isis fanboy set. this version has all the t.co links expanded where possible.\n",
      "contains datascience programs offered by universities along with program details, world ranking and a lot lot more. happy exploring !!!\n",
      "inspiration\n",
      "i'm a big fan of techcrunch for a while now. kind of because i get to know about new startups that's coming up or maybe just because i find tito hamze videos fun. but techcrunch got plenty of good content. and where we find good content we produce great exploratory analysis.\n",
      "this dataset is a great opportunity for you to boost your skills as an eda expert! it provides several features that make you able to create different analyses such as time series, clustering, predictive, segmenting, classification and tons of others. let's not forget about word2vec for that. it would be awesome to see that in action here!\n",
      "i've made the scraper available on github, if you want to check it out, here is the link: techcrunch scraper repo\n",
      "content\n",
      "this dataset comes with a rich set of features. you will have:\n",
      "authors: authors of the post - can be one or multiple authors\n",
      "category: post category\n",
      "content: post content - each paragraph can be extracted by splitting on the \\n\n",
      "date: post date\n",
      "id: post id - the same id used on techcrunch website\n",
      "img_src: post main image url\n",
      "section: post section - each section is one of the options on the main page dropdown menu\n",
      "tags: post tags - can be zero or multiple tags\n",
      "title: post title\n",
      "topics: post topics\n",
      "url: post url\n",
      "acknowledgements\n",
      "all posts were scraped from the techcrunch website on mid oct-16. each line contains information about one post and each post appear in no more than one line.\n",
      "context\n",
      "generating humor is a complex task in the domain of machine learning, and it requires the models to understand the deep semantic meaning of a joke in order to generate new ones. such problems, however, are difficult to solve due to a number of reasons, one of which is the lack of a database that gives an elaborate list of jokes. thus, a large corpus of over 0.2 million jokes has been collected by scraping several websites containing funny and short jokes.\n",
      "visit my github repository for more information regarding collection of data and the scripts used.\n",
      "content\n",
      "this dataset is in the form of a csv file containing 231,657 jokes. length of jokes ranges from 10 to 200 characters. each line in the file contains a unique id and joke.\n",
      "disclaimer\n",
      "it has been attempted to keep the jokes as clean as possible. since the data has been collected by scraping websites, it is possible that there may be a few jokes that are inappropriate or offensive to some people.\n",
      "context\n",
      "a dataset of 2017 songs with attributes from spotify's api. each song is labeled \"1\" meaning i like it and \"0\" for songs i don't like. i used this to data to see if i could build a classifier that could predict whether or not i would like a song.\n",
      "i wrote an article about the project i used this data for. it includes code on how to grab this data from the spotipy api wrapper and the methods behind my modeling. https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/\n",
      "content\n",
      "each row represents a song.\n",
      "there are 16 columns. 13 of which are song attributes, one column for song name, one for artist, and a column called \"target\" which is the label for the song.\n",
      "here are the 13 track attributes: acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence.\n",
      "information on what those traits mean can be found here: https://developer.spotify.com/web-api/get-audio-features/\n",
      "acknowledgements\n",
      "i would like to thank spotify for providing this readily accessible data.\n",
      "inspiration\n",
      "i'm a music lover who's curious about why i love the music that i love.\n",
      "context\n",
      "grab your pans...\n",
      "player statistics for approximately 85,000 of the top pubg players (as tracked by https://pubgtracker.com/). all statistics were gathered using aggregate region filters (all regions) and feature labels are subdivided by server type: solo, duo, and squad.\n",
      "87,898 players with 150 numerical game-play features per player (+2 for player name and pubg tracker id).\n",
      "content\n",
      "features include kd ratios, wins, losses, damage, wins, top 10's, and movement characteristics (walking/riding distance etc...)\n",
      "acknowledgements\n",
      "special thanks to pubgtracker.com for their support and aid with gathering this data. more information can be found here: https://pubgtracker.com/\n",
      "playerunknown's battlegrounds is a registered trademark, trademark or service mark of bluehole, inc. and its affiliates https://www.playbattlegrounds.com/main.pu\n",
      "inspiration\n",
      "as a gamer addicted to pubg, it was a blast putting this data set together. some great project ideas include:\n",
      "a. visualizations of player skill vs. specific strategies\n",
      "b. unsupervised clustering of players based on strategy (for matchmaking or team building)\n",
      "c. prediction of features based on player skill and/or strategies\n",
      "context\n",
      "dataset is based on box score and standing statistics from the nba.\n",
      "calculations such as number of possessions, floor impact counter, strength of schedule, and simple rating system are performed.\n",
      "finally, extracts are created based on a perspective:\n",
      "teamboxscore.csv communicates game data from each teams perspective\n",
      "officialboxscore.csv communicates game data from each officials perspective\n",
      "playerboxscore.csv communicates game data from each players perspective\n",
      "standing.csv communicates standings data for each team every day during the season\n",
      "content\n",
      "data sources\n",
      "box score and standing statistics were obtained by a java application using restful apis provided by xmlstats.\n",
      "calculation sources\n",
      "another java application performs advanced calculations on the box score and standing data.\n",
      "formulas for these calculations were primarily obtained from these sources:\n",
      "https://basketball.realgm.com/info/glossary\n",
      "https://www.nbastuffer.com/team-evaluation-metrics/\n",
      "https://www.basketball-reference.com/about/glossary.html\n",
      "inspiration\n",
      "favoritism\n",
      "does a referee impact the number of fouls made against a player or the pace of a game?\n",
      "forcasting\n",
      "can the aggregated points scored by and against a team along with their strength of schedule be used to determine their projected winning percentage for the season?\n",
      "predicting the past\n",
      "for a given game, can games played earlier in the season help determine how a team will perform?\n",
      "lots of data elements and possibilities. let your imagination roam!\n",
      "context\n",
      "i love movies. \n",
      "i tend to avoid marvel-transformers-standardized products, and prefer a mix of classic hollywood-golden-age and obscure polish artsy movies. throw in an occasional japanese-zombie-slasher-giallo as an alibi. good movies don't exist without bad movies. \n",
      "on average i watch 200+ movies each year, with peaks at more than 500 movies. nine years ago i started to log my movies to avoid watching the same movie twice, and also assign scores. over the years, it gave me a couple insights on my viewing habits but nothing more than what a tenth-grader would learn at school. \n",
      "i've recently suscribed to netflix and it pains me to see the global inefficiency of recommendation systems for people like me, who mostly swear by \"la politique des auteurs\". it's a term coined by famous new-wave french movie critic andré bazin, meaning that the quality of a movie is essentially linked to the director and it's capacity to execute his vision with his crew. we could debate it depends on movie production pipeline, but let's not for now. practically, what it means, is that i essentially watch movies from directors who made films i've liked.\n",
      "i suspect neflix calibrate their recommandation models taking into account the way the \"average-joe\" chooses a movie. a few months ago i had read a study based on a survey, showing that people chose a movie mostly based on genre (55%), then by leading actors (45%). director or release date were far behind around 10% each. it is not surprising, since most people i know don't care who the director is. lots of us blockbusters don't even mention it on the movie poster. i am aware that collaborative filtering is based on user proximity , which i believe decreases (or even eliminates) the need to characterize a movie. so here i'm more interested in content based filtering which is based on product proximity for several reasons :\n",
      "users tastes are not easily accessible. it is, after all, netflix treasure chest\n",
      "movie offer on netflix is so bad for someone who likes author's movies that it wouldn't help\n",
      "modeling a movie intrinsic qualities is a nice challenge\n",
      "enough.\n",
      "\"the secret of getting ahead is getting started\" (mark twain)\n",
      "content\n",
      "the primary source is www.themoviedb.org. if you watch obscure artsy romanian homemade movies you may find only 95% of your movies referenced...but for anyone else it should be in the 98%+ range.\n",
      "movies details are from www.themoviedb.org api : movies/details\n",
      "movies crew & casting are from www.themoviedb.org api : movies/credits\n",
      "both can be joined by id\n",
      "they contain all 350k movies up, from end of 19th century to august 2017. if you remove short movies from imdb you get similar amounts of movies.\n",
      "i uploaded the program to retrieve incremental movie details on github : https://github.com/stephanerappeneau/scienceofmovies/tree/master/pycharmprojects/getallmovies (need a dev api key from themoviedb.org though)\n",
      "i have tried various supervised (decision tree) / unsupervised (clustering, nlp) approaches described in the discussions, source code is on github : https://github.com/stephanerappeneau/scienceofmovies\n",
      "as a bonus i've uploaded the bio summary from top 500 critically-acclaimed directors from wikipedia, for some interesting nltk analysis\n",
      "here is overview of the available sources that i've tried :\n",
      "• imdb.com free csv dumps (ftp://ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/temporaryaccess/) are badly documented, incomplete, loosely structured and impossible to join/merge. there's an api hosted by amazon web service : 1€ every 100 000 requests. with around 1 million movies, it could become expensive also features are bare. so i've searched for other sources. \n",
      "• www.themoviedb.org is based on crowdsourcing and has an excellent api, limited to 40 requests every 10 seconds. it is quite generous, well documented, and enough to sweep the 450 000 movies in a few days. for my purpose, data quality is not significantly worse than imdb, and as imdb key is also included there's always the possibility to complete my dataset later (i actually did it)\n",
      "• www.boxofficemojo.com has some interesting budget/revenue figures (which are sorely lacking in both imdb & tmdb), but it actually tracks only a few thousand movies, mainly blockbusters. there are other professional sources that are used by film industry to get better predictive / marketing insights but that's beyond my reach for this experiment.   • www.wikipedia.com is an interesting source with no real cap on api calls, however it requires a bit of webscraping and for movies or directors the layout and quality varies a lot, so i suspected it'd get a lot of work to get insights so i put this source in lower priority.\n",
      "• www.google.com will ban you after a few minutes of web scraping because their job is to scrap data from others, than sell it, duh.   • it's worth mentionning that there are a few dumps of netflix anonymized user tastes on kaggle, because they've organised a few competitions to improve their recommendation models. https://www.kaggle.com/netflix-inc/netflix-prize-data\n",
      "• online databases are largely white anglo-saxon centric, meaning bollywood (india is the 2nd bigger producer of movies) offer is mostly absent from datasets. i'm fine with that, as it's not my cup of tea plus i lack domain knowledge. the sheer amount of indian movies would probably skew my results anyway (i don't want to have too many martial-arts-musicals in my recommendations ;-)). i have, however, tremendous respect for indian movie industry so i'd love to collaborate with an indian cinephile !\n",
      "inspiration\n",
      "starting from there, i had multiple problem statements for both supervised / unsupervised machine learning\n",
      "can i program a tailored-recommendation system based on my own criteria ?\n",
      "what are the characteristics of movies/directors i like the most ?\n",
      "what is the probability that i will like my next movie ?\n",
      "can i find the data ?\n",
      "one of the objectives of sharing my work here is to find cinephile data-scientists who might be interested and, hopefully, contribute or share insights :) other interesting leads : use tagline for nlp/clustering/genre guessing, leverage on budget/revenue, link with other data sources using the imdb normalized title, etc.\n",
      "motivation, disclaimer and acknowledgements\n",
      "i've graduated from an french engineering school, majoring in artificial intelligence, but that was 17 years ago right in the middle of a.i-winter. like a lot of white male rocket scientists, i've ended up in one of the leading european investment bank, quickly abandonning it development to specialize in trading/risk project management and internal politics. my recent appointment in the data office made me aware of recent breakthroughts in datascience, and i thought that developing a side project would be an excellent occasion to learn something new. plus it'd give me a well-needed credibility which too often lack decision makers when it comes to datascience.\n",
      "i've worked on some of the features with cédric paternotte, a fellow friend of mine who is a professor of philosophy of sciences in la sorbonne. working with someone with a different background seem a good idea for motivation, creativity and rigor.\n",
      "kudos to www.themoviedb.org or www.wikipedia.com sites, who really have a great attitude towards open data. this is typically not the case of modern-bigdata companies who mostly keep data to themselves to try to monetize it. such a huge contrast with imdb or instagram api, which generously let you grab your last 3 comments at a miserable rate. even if 15 years ago this seemed a mandatory path to get services for free, i predict one day governments will need to break this data monopoly.\n",
      "[disclaimer : i apologize in advance for my engrish (i'm french ^-^), any bad-code i've written (there are probably hundreds of way to do it better and faster), any pseudo-scientific assumption i've made, i'm slowly getting back in statistics and lack senior guidance, one day i regress a non-stationary time series and the day after i'll discover i shouldn't have, and any incorrect use of machine-learning models]\n",
      "this dataset reflects incidents of crime in the city of los angeles dating back to 2010. this data is transcribed from original crime reports that are typed on paper and therefore there may be some inaccuracies within the data. some location fields with missing data are noted as (0°, 0°). address fields are only provided to the nearest hundred block in order to maintain privacy.\n",
      "reporting district shapefile attributes\n",
      "repdist, number, min: 101 max: 2,199 avg: 1,162 count: 1,135\n",
      "prec, number, min: 1 max: 21 avg: 11 count: 1,135\n",
      "aprec, text, pacific (74), devonshire (70), west los angeles (69), northeast (64), hollenbeck (63), mission (62)... (15 more)\n",
      "bureau, text, valley bureau (399), west bureau (288), central bureau (267), south bureau (181)\n",
      "basiccar, text, 8a29 (17), 17a35 (17), 1a1 (15), 17a49 (14), 16a35 (14), 14a73 (14), 19a43 (13), 8a95 (12), 19a7 (12)... (160 more)\n",
      "tooltip, text, bureau: south bureau\\ndistrict: 562\\ndivision: harbor (1)... (1134 more)\n",
      "objectid\n",
      "unique id\n",
      "acknowledgements\n",
      "this dataset was kindly released by the city of los angeles. you can find the original dataset, updated weekly, here.\n",
      "inspiration\n",
      "some of the mo codes seem unlikely or unrelated to crime. can you find out what would lead to the use of code 0107 god or 1021 repair?\n",
      "data scraped from www.ycombinator.com/companies on september 8, 2016.\n",
      "the strategic board game the settlers of catan is a modern classic. introduced in 1995, it has sold over 22 million copies worldwide. learning how to play the game well requires an inherent understanding of probability, economics, game theory, and social interactions.\n",
      "this is my personal dataset of 50 4-player games i played on playcatan.com in 2014. using the ingame statistics page and a spreadsheet, i logged starting position choices, the distribution of dice rolls, and how each player spent the resources they acquired by the end of the game. note, of course, because this dataset only consists of my games, any analysis done is most relevant for games involving me...\n",
      "my personal analysis of this dataset consisted of a best subsets regression, and resulted a 4-variable model that likely overfitted, but managed to ascertain the winner correctly, in 40 of 50 games.\n",
      "questions to possibly consider:\n",
      "how much luck is involved in winning a catan game?\n",
      "does starting position matter? if so, what starting settlements lead to success from each position?\n",
      "how much information on the eventual winner can be gained from starting position/settlements alone?\n",
      "by looking at postgame stats, what leads to a win? can these statistics be a guide for ingame strategy?\n",
      "data details/guide:\n",
      "gamenum - each game i played has 4 corresponding rows, 1 per player.\n",
      "player - the starting position corresponding to each row\n",
      "points - how many points the player ended the game with (the game is won with 10 or more)\n",
      "me - the position i played during the game\n",
      "2, 3, ..., 12 - how many rolls of each value occurred during the game (game is played with 2 dice)\n",
      "settlement1, settlement2 - each starting settlement is logged as 3 pairs of [number, resource]:\n",
      "l = lumber\n",
      "c = clay\n",
      "s = sheep\n",
      "w = wheat\n",
      "o = ore\n",
      "3g = 3:1 general port\n",
      "2(x) = 2:1 port for resource x\n",
      "d = desert\n",
      "ex: in game 1, player 1's first settlement was on a 6-lumber, 3-clay, and 11-clay.\n",
      "production - total cards gained from settlements and cities during game\n",
      "tradegain - total cards gained from peer and bank trades during game\n",
      "robbercardsgain - total cards gained from stealing with the robber, plus cards gained with non-knight development cards. a road building card is +4 resources.\n",
      "totalgain - sum of previous 3 columns.\n",
      "tradeloss - total cards lost from peer and bank trades during game\n",
      "robbercardsloss - total cards lost from robbers, knights, and other players' monopoly cards\n",
      "tribute - total cards lost when player had to discard on a 7 roll (separate from previous column.)\n",
      "totalloss - sum of previous 3 columns.\n",
      "totalavailable - totalgain minus totalloss.\n",
      "i only ask that if you produce a good model, you share it with me! please don't hesitate to ask any clarifying questions.\n",
      "twitter friends and hashtags\n",
      "context\n",
      "this datasets is an extract of a wider database aimed at collecting twitter user's friends (other accound one follows). the global goal is to study user's interest thru who they follow and connection to the hashtag they've used.\n",
      "content\n",
      "it's a list of twitter user's informations. in the json format one twitter user is stored in one object of this more that 40.000 objects list. each object holds :\n",
      "avatar : url to the profile picture\n",
      "followercount : the number of followers of this user\n",
      "friendscount : the number of people following this user.\n",
      "friendname : stores the @name (without the '@') of the user (beware this name can be changed by the user)\n",
      "id : user id, this number can not change (you can retrieve screen name with this service : https://tweeterid.com/)\n",
      "friends : the list of ids the user follows (data stored is ids of users followed by this user)\n",
      "lang : the language declared by the user (in this dataset there is only \"en\" (english))\n",
      "lastseen : the time stamp of the date when this user have post his last tweet.\n",
      "tags : the hashtags (whith or without #) used by the user. it's the \"trending topic\" the user tweeted about.\n",
      "tweetid : id of the last tweet posted by this user.\n",
      "you also have the csv format which uses the same naming convention.\n",
      "these users are selected because they tweeted on twitter trending topics, i've selected users that have at least 100 followers and following at least 100 other account (in order to filter out spam and non-informative/empty accounts).\n",
      "acknowledgements\n",
      "this data set is build by hubert wassner (me) using the twitter public api. more data can be obtained on request (hubert.wassner at gmail.com), at this time i've collected over 5 milions in different languages. some more information can be found here (in french only) : http://wassner.blogspot.fr/2016/06/recuperer-des-profils-twitter-par.html\n",
      "past research\n",
      "no public research have been done (until now) on this dataset. i made a private application which is described here : http://wassner.blogspot.fr/2016/09/twitter-profiling.html (in french) which uses the full dataset (millions of full profiles).\n",
      "inspiration\n",
      "on can analyse a lot of stuff with this datasets :\n",
      "stats about followers & followings\n",
      "manyfold learning or unsupervised learning from friend list\n",
      "hashtag prediction from friend list\n",
      "contact\n",
      "feel free to ask any question (or help request) via twitter : @hwassner\n",
      "enjoy! ;)\n",
      "overview\n",
      "we could take a music theory class to understand what a note is, but why don't we just find out for ourselves? in this data set we have the notes on a guitar on open strings, and on the 1st-8th frets on every string. the notes were recorded on a nice but low-end guitar called the yamaha c-40. the guitar is in standard tuning (from the top string to the bottom on we have e low, a, d, g, b, e high).\n",
      "what to do with this dataset\n",
      "i didn't label any notes (except the open ones - an open a string is an a). if you want a challenge you can cluster the notes and see if your clustering lumps all the same notes together. if you want labels so you can do some inspection of notes that are the same you can look on google for a guitar fretboard diagram. i have done both of these experiments and learned a bit about music which i 'm hoping to verify soon when i find a good music theory book.\n",
      "this data set might be interesting to use to be able to write sheet music from an audio sample of some finger-picked music. identifying chords is a difficult computational task, but finger-style guitar, with clear, individual notes, might be easier. if this is the case, a simple script could be written to write sheet music from an audio sample.\n",
      "one draw back about the data set is that some non-plucked strings were vibrating when i played a note. i tried various techniques to muffle them, but there is still some noise in the background. i don't know if this is because of my technique or something that happens to all players on all guitars. in any event, this noise didn't hurt my analysis.\n",
      "about the data\n",
      "they were recorded by me, melvyn, using a program called audacity.\n",
      "there is a directory with the name of the string. inside the directory you will find .wav files named either open, 1, 2, ....8 for the fingering of the string. there is also a directory called \"scale\" i recorded some notes that make a \"do-re-mi...\" scale. you can use these for a number of things.\n",
      "i use the guitartuner app to tune the guitar - i'm just learning so i don't have an ear for notes yet. after some initial analysis it looks like the guitar might be a bit out of tune, so the resonant frequencies are a bit off from what they should be. another thing that is interesting to think about it is how far a frequency must be from the proper on until it becomes distinguishable as a different note.\n",
      "chronist is a project to quantitatively monitor the emotional and physical changes of an individual over periods of time. my thesis is that if you can accurately show emotional or physical change over time, you can objectively pinpoint how an environmental change such as a career change, moving to a new city, starting or ending a relationship, or starting a new habit like going to the gym affected your physical and emotional health. this can lead to important insights on an individual level and for a population as a whole.\n",
      "if you are interested in hearing more about this project, contributing your data, or collaborating, contact me at chris@cjroth.com.\n",
      "see the github repository to read more about the tools that were used to generate the dataset.\n",
      "context\n",
      "space apps moscow was held on april 29th & 30th. thank you to the 175 people who joined the international space apps challenge at this location!\n",
      "content\n",
      "the dataset contains such columns as: \"wind direction\", \"wind speed\", \"humidity\" and temperature. the response parameter that is to be predicted is: \"solar_radiation\". it contains measurements for the past 4 months and you have to predict the level of solar radiation. just imagine that you've got solar energy batteries and you want to know will it be reasonable to use them in future?\n",
      "acknowledgements\n",
      "thanks nasa for the dataset.\n",
      "inspiration\n",
      "predict the level of solar radiation. here are some intersecting dependences that i have figured out: 1. humidity & solar_radiation. 2.temeperature & solar_radiation.\n",
      "the best result of accuracy i could get using cross-validation was only 55%.\n",
      "context\n",
      "data from the coursera course: neurohacking in r taught by dr. elizabeth sweeney , rice academy postdoctoral fellow, ciprian m. crainiceanu, professor and john muschelli iii , assistant scientist\n",
      "please see https://www.coursera.org/learn/neurohacking for the lecture notes and example code from the instructors.\n",
      "content\n",
      "structural mri images for visualization and image processing\n",
      "from the instructors:\n",
      "about this course: neurohacking describes how to use the r programming language (https://cran.r-project.org/) and its associated packages to perform manipulation, processing, and analysis of neuroimaging data. we focus on publicly-available structural magnetic resonance imaging (mri). we discuss concepts such as inhomogeneity correction, image registration, and image visualization. by the end of this course, you will be able to: read/write images of the brain in the nifti (neuroimaging informatics technology initiative) format visualize and explore these images, perform inhomogeneity correction, brain extraction, and image registration (within a subject and to a template).\n",
      "acknowledgements\n",
      "dataset is public domain and was originally posted for the coursera online course neurohacking in r.\n",
      "notes\n",
      "a. when you download the zip archive, double clicking might try to compress the file instead of extracting it. unzipping on terminal (mac) correctly decompresses the archive.\n",
      "b. the zip file contains a directory structure:\n",
      "       brainix\n",
      "            -----dicom\n",
      "                ----flair\n",
      "                ----roi\n",
      "                ----t1\n",
      "                ----t2\n",
      "            -----nifti\n",
      "      kirby21\n",
      "            -----visit 1\n",
      "                ----113\n",
      "            -----visit 2\n",
      "                ----113\n",
      "      template\n",
      "however, when it is unzipped here on kaggle environment, somehow the directory structure is not maintained, therefore files with the same names are being overwritten. as a workaround, i added the directory names to the files ie. brainix_dicom_t1_im_0001_0011.dcm instead of just im_0001_0011.dcm.\n",
      "check out script https://www.kaggle.com/ilknuricke/d/ilknuricke/neurohackinginrimages/structural-mri-visualization/code for example use.\n",
      "context\n",
      "how good of an arbitrageur would you be?\n",
      "find it out in the world tennis database which gathers more than 139k matches with odds from 15 different bookies (49 mb).\n",
      "if you are looking to predict the outcome of a tennis match, to find arbitrage opportunities, inspecting variations in a particular player odds or simply searching to improve your machine learning or visualisation skills, then this dataset might be looking for you too.\n",
      "content\n",
      "data is packed in csv format, ready to spit out some interesting statistics. it is composed of the following 72 columns:\n",
      "url, string\n",
      "country, string\n",
      "date, (yyyy-mm-dd hh:mm), to ease date-time transformations.\n",
      "day, string\n",
      "tournament name, string\n",
      "doubles, either 0 or 1, when it is not a single player match\n",
      "player 1(2) name, string\n",
      "player 1(2) score, int, number of sets won\n",
      "player 1(2) set 0 score, int, up until set 4 - indexing of sets starts at 0, ask why to python ;)\n",
      "no set info, either 0 or 1, when there is no informations about the final set scores\n",
      "missing bookies, either 0 or 1, when there is no informations about any bookies odds\n",
      "retired player, either 0 or 1, when one player retired\n",
      "cancelled game, either 0 or 1, when the game got cancelled\n",
      "comments, string used to insert any comments during the scraping process\n",
      "walkover, either 0 or 1, when one player chose to walkover\n",
      "awarded player, either 0 or 1, when one player got awarded\n",
      "fifteen bookies were then taken into account, each having three type of infos: player 1 odd, player 2 odd, payout. this result in adding to the preceding 27 columns, 45 others.\n",
      "bookies were sorted alphabetically:\n",
      "10bet\n",
      "18bet\n",
      "5dimes\n",
      "bet at home\n",
      "bet365\n",
      "bethard\n",
      "betolimp\n",
      "betrally\n",
      "bwin\n",
      "jetbull\n",
      "marathonbet\n",
      "pinnacle\n",
      "tempobet\n",
      "tonybet\n",
      "unibet\n",
      "acknowledgements\n",
      "huge kudos to the oddsportal website for their wonderful archiving job.\n",
      "cover photo by jeremy galliani on unsplash.\n",
      "inspiration\n",
      "various interesting infos and predictions can be made out of this dataset.\n",
      "individual players trajectories and their respective odds movements.\n",
      "bookies respective strategies. who sets the pace?\n",
      "detecting patterns in arbitrage situations (arbitrageur perspective).\n",
      "and of course, predicting the winner of a game, as draws are not allowed.\n",
      "of course, i got inspired by the european soccer database.\n",
      "finally, for details about the scraping process, visit https://dmpierre.github.io/.\n",
      "context\n",
      "the original dataset contains 1000 entries with 20 categorial/symbolic attributes prepared by prof. hofmann. in this dataset, each entry represents a person who takes a credit by a bank. each person is classified as good or bad credit risks according to the set of attributes. the link to the original dataset can be found below.\n",
      "content\n",
      "it is almost impossible to understand the original dataset due to its complicated system of categories and symbols. thus, i wrote a small python script to convert it into a readable csv file. several columns are simply ignored, because in my opinion either they are not important or their descriptions are obscure. the selected attributes are:\n",
      "age (numeric)\n",
      "sex (text: male, female)\n",
      "job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
      "housing (text: own, rent, or free)\n",
      "saving accounts (text - little, moderate, quite rich, rich)\n",
      "checking account (numeric, in dm - deutsch mark)\n",
      "credit amount (numeric, in dm)\n",
      "duration (numeric, in month)\n",
      "purpose (text: car, furniture/equipment, radio/tv, domestic appliances, repairs, education, business, vacation/others)\n",
      "acknowledgements\n",
      "source: uci\n",
      "context\n",
      "there are all sorts of reasons why you'd want to know a hospital's quality rating.\n",
      "your mom is having her second hip replacement. her first one went terribly and you're nervous about how she'll do. which hospital would you suggest she have her surgery?\n",
      "you're selecting a health plan on your state's exchange, but your top two choices partner with different hospitals. how will you decide which plan to pick?\n",
      "your brother has cystic fibrosis and has to go to the er frequently. he hates waiting. which hospitals/states provide care in the timeliest manner?\n",
      "your in-laws moved to florida recently to retire, and have been trying to convince you to move there too. you're looking for any way possible to show them that your state is better. does your state have better hospitals?\n",
      "every hospital in the united states of america that accepts publicly insured patients (medicaid or medicare) is required to submit quality data, quarterly, to the centers for medicare & medicaid services (cms). there are very few hospitals that do not accept publicly insured patients, so this is quite a comprehensive list.\n",
      "content\n",
      "this file contains general information about all hospitals that have been registered with medicare, including their addresses, type of hospital, and ownership structure. it also contains information about the quality of each hospital, in the form of an overall rating (1-5, where 5 is the best possible rating & 1 is the worst), and whether the hospital scored above, same as, or below the national average for a variety of measures.\n",
      "this data was updated by cms on july 25, 2017. cms' overall rating includes 60 of the 100 measures for which data is collected & reported on hospital compare website (https://www.medicare.gov/hospitalcompare/search.html). each of the measures have different collection/reporting dates, so it is impossible to specify exactly which time period this dataset covers. for more information about the timeframes for each measure, see: https://www.medicare.gov/hospitalcompare/data/data-updated.html# for more information about the data itself, apis and a variety of formats, see: https://data.medicare.gov/hospital-compare\n",
      "acknowledgements\n",
      "attention: works of the u.s. government are in the public domain and permission is not required to reuse them. an attribution to the agency as the source is appreciated. your materials, however, should not give the false impression of government endorsement of your commercial products or services. see 42 u.s.c. 1320b-10.\n",
      "inspiration\n",
      "which hospital types & hospital ownerships are most common?\n",
      "which hospital types & ownerships are associated with better than average ratings/mortality/readmission/etc?\n",
      "what is the average hospital rating, by state?\n",
      "which hospital types & hospital ownerships are more likely to have not submitted proper data (\"not available\" & \"results are not available for this reporting period\")?\n",
      "which parts of the country have the highest & lowest density of religious hospitals?\n",
      "about this data\n",
      "this is a list of 10,000 women's shoes and their product information provided by datafiniti's product database.\n",
      "the dataset includes shoe name, brand, price, and more. note that each shoe will have an entry for each price found for it and some shoes may have multiple entries.\n",
      "what you can do with this data\n",
      "you can use this data to determine brand markups, pricing strategies, and trends for luxury shoes. e.g.:\n",
      "what is the average price of each distinct brand listed?\n",
      "which brands have the highest prices?\n",
      "which ones have the widest distribution of prices?\n",
      "is there a typical price distribution (e.g., normal) across brands or within specific brands?\n",
      "further processing data would also let you:\n",
      "correlate specific product features with changes in price.\n",
      "you can cross-reference this data with a sample of our men's shoe prices to see if there are any differences between women's brands and men's brands.\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "spacex has gained worldwide attention for a series of historic milestones. it is the only private company ever to return a spacecraft from low-earth orbit, which it first accomplished in december 2010. the company made history again in may 2012 when its dragon spacecraft attached to the international space station, exchanged cargo payloads, and returned safely to earth — a technically challenging feat previously accomplished only by governments. since then dragon has delivered cargo to and from the space station multiple times, providing regular cargo resupply missions for nasa.\n",
      "under a $1.6 billion contract with nasa, spacex is flying numerous cargo resupply missions to the international space station, for a total of at least 20 flights under the commercial resupply services contract. in 2016, nasa awarded spacex a second version of that contract that will cover a minimum of 6 additional flights from 2019 onward. in the near future, spacex will carry crew as part of nasa’s commercial crew program as well. dragon was designed from the outset to carry astronauts and spacex is in the process of upgrading dragon to make it crew-ready. spacex is the world’s fastest-growing provider of launch services and has over 70 future missions on its manifest, representing over $10 billion in contracts. these include commercial satellite launches as well as nasa and other us government missions.\n",
      "content\n",
      "this dataset includes a record for each payload carried during a spacex mission into outer space.\n",
      "acknowledgements\n",
      "the data was scraped from the spacex and nasa website.\n",
      "inspiration\n",
      "has the rate of spacex rocket launches increased in the past decade? how many missions do you predict will be launched in 2018?\n",
      "contact\n",
      "email me at: itsawesome17@gmail.com\n",
      "my blog: http://blog.mathocr.com/\n",
      "content\n",
      "dataset consists of jpg files(45x45) disclaimer: dataset does not contain hebrew alphabet at all. it includes basic greek alphabet symbols like: alpha, beta, gamma, mu, sigma, phi and theta. english alphanumeric symbols are included. all math operators, set operators. basic pre-defined math functions like: log, lim, cos, sin, tan. math symbols like: \\int, \\sum, \\sqrt, \\delta and more.\n",
      "acknowledgements\n",
      "original source, that was parsed, extracted and modified is crohme dataset.\n",
      "visit crohme at http://www.isical.ac.in/~crohme/index.html.\n",
      "inspiration\n",
      "due to the technological advances in recent years, paper scientific documents are used less and less. thus, the trend in the scientific community to use digital documents has increased considerably. among these documents, there are scientific documents and more specifically mathematics documents. so i give a tool, to research recognizing handwritten math language in variety of applications.\n",
      "source code\n",
      "you can find source code responsible for parsing original crohme dataset here:\n",
      "https://github.com/xainano/crohme_extractor\n",
      "this parser allows you not only to extract math symbols into square images of desired size, but also lets you specify categories of classes to be extracted like: digits, greek_letters, lowercase_letters, operators, and more. it also contains visualization tools and histograms showing appearances of each class in the dataset.\n",
      "commercial use\n",
      "rights for commercial usage cannot be granted.\n",
      "overview\n",
      "the dataset contains a number of different subsets of the full food-101 data. the idea is to make a more exciting simple training set for image analysis than cifar10 or mnist. for this reason the data includes massively downscaled versions of the images to enable quick tests. the data has been reformatted as hdf5 and specifically keras hdf5matrix which allows them to be easily read in. the file names indicate the contents of the file. for example\n",
      "food_c101_n1000_r384x384x3.h5 means there are 101 categories represented, with n=1000 images, that have a resolution of 384x384x3 (rgb, uint8)\n",
      "food_test_c101_n1000_r32x32x1.h5 means the data is part of the validation set, has 101 categories represented, with n=1000 images, that have a resolution of 32x32x1 (float32 from -1 to 1)\n",
      "challenge\n",
      "the first goal is to be able to automatically classify an unknown image using the dataset, but beyond this there are a number of possibilities for looking at what regions / image components are important for making classifications, identify new types of food as combinations of existing tags, build object detectors which can find similar objects in a full scene.\n",
      "data acknowledgement\n",
      "the data was repackaged from the original source (gzip) available at https://www.vision.ee.ethz.ch/datasets_extra/food-101/\n",
      "license\n",
      "the food-101 data set consists of images from foodspotting [1]. any use beyond scientific fair use must be negotiated with the respective picture owners according to the foodspotting terms of use [2].\n",
      "[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/\n",
      "historical hourly weather data\n",
      "who amongst us doesn't small talk about the weather every once in a while?\n",
      "the goal of this dataset is to elevate this small talk to medium talk.\n",
      "just kidding, i actually originally decided to collect this dataset in order to demonstrate basic signal processing concepts, such as filtering, fourier transform, auto-correlation, cross-correlation, etc..., (for a data analysis course i'm currently preparing).\n",
      "i wanted to demonstrate these concepts on signals that we all have intimate familiarity with and hope that this way these concepts will be better understood than with just made up signals.\n",
      "the weather is excellent for demonstrating these kinds of concepts as it contains periodic temporal structure with two very different periods (daily and yearly).\n",
      "content\n",
      "the dataset contains ~5 years of high temporal resolution (hourly measurements) data of various weather attributes, such as temperature, humidity, air pressure, etc.\n",
      "this data is available for 30 us and canadian cities, as well as 6 israeli cities.\n",
      "i've organized the data according to a common time axis for easy use.\n",
      "each attribute has it's own file and is organized such that the rows are the time axis (it's the same time axis for all files), and the columns are the different cities (it's the same city ordering for all files as well).\n",
      "additionally, for each city we also have the country, latitude and longitude information in a separate file.\n",
      "acknowledgements\n",
      "the dataset was aquired using weather api on the openweathermap website, and is available under the odbl license.\n",
      "inspiration\n",
      "weather data is both intrinsically interesting, and also potentially useful when correlated with other types of data.\n",
      "for example, wildfire spread is potentially related to weather conditions, demand for cabs is famously known to be correlated with weather conditions (here, here and here you can find nyc cab ride data), and use of city bikes is probably also correlated with weather in interesting ways (check out this austin dataset, this sf dataset, this montreal dataset, and this nyc dataset).\n",
      "traffic is also probably related to weather.\n",
      "another potentially interesting source of correlation is between weather and crime. here are a few crime datasets on kaggle of cities present in this weather dataset: chicago, philadelphia, los angeles, vancouver, austin, nyc\n",
      "there are many other potentially interesting connections between everyday life and the weather that we can explore together with the help of this dataset. have fun!\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "this data set contains monthly rainfall detail of 36 meteorological sub-divisions of india.\n",
      "content\n",
      "time period: 1901 - 2015\n",
      "granularity: monthly\n",
      "location: 36 meteorological sub-divisions in india\n",
      "rainfall unit: mm\n",
      "acknowledgements\n",
      "india meteorological department(imd) govt. of india has shared this dataset under govt. open data license - india.\n",
      "context\n",
      "for most football fans, may - july represents a lull period due to the lack of club football. what makes up for it, is the intense transfer speculation that surrounds all major player transfers today. their market valuations also lead to a few raised eyebrows, lately more than ever. i was curious to see how good a proxy popularity could be for ability, and the predictive power it would have in a model estimating a player's market value.\n",
      "content\n",
      "name: name of the player\n",
      "club: club of the player\n",
      "age : age of the player\n",
      "position : the usual position on the pitch\n",
      "position_cat :\n",
      "1 for attackers\n",
      "2 for midfielders\n",
      "3 for defenders\n",
      "4 for goalkeepers\n",
      "market_value : as on transfermrkt.com on july 20th, 2017\n",
      "page_views : average daily wikipedia page views from september 1, 2016 to may 1, 2017\n",
      "fpl_value : value in fantasy premier league as on july 20th, 2017\n",
      "fpl_sel : % of fpl players who have selected that player in their team\n",
      "fpl_points : fpl points accumulated over the previous season\n",
      "region:\n",
      "1 for england\n",
      "2 for eu\n",
      "3 for americas\n",
      "4 for rest of world\n",
      "nationality\n",
      "new_foreign : whether a new signing from a different league, for 2017/18 (till 20th july)\n",
      "age_cat\n",
      "club_id\n",
      "big_club: whether one of the top 6 clubs\n",
      "new_signing: whether a new signing for 2017/18 (till 20th july)\n",
      "inspiration\n",
      "to statistically analyse the beautiful game.\n",
      "context\n",
      "my uber drives (2016)\n",
      "here are the details of my uber drives of 2016. i am sharing this dataset for data science community to learn from the behavior of an ordinary uber customer.\n",
      "content\n",
      "geography: usa, sri lanka and pakistan\n",
      "time period: january - december 2016\n",
      "unit of analysis: drives\n",
      "total drives: 1,155\n",
      "total miles: 12,204\n",
      "dataset: the dataset contains start date, end date, start location, end location, miles driven and purpose of drive (business, personal, meals, errands, meetings, customer support etc.)\n",
      "acknowledgements & references\n",
      "users are allowed to use, download, copy, distribute and cite the dataset for their pet projects and training. please cite it as follows: “zeeshan-ul-hassan usmani, my uber drives dataset, kaggle dataset repository, march 23, 2017.”\n",
      "past research\n",
      "uber tlc foil response - the dataset contains over 4.5 million uber pickups in new york city from april to september 2014, and 14.3 million more uber pickups from january to june 2015 https://github.com/fivethirtyeight/uber-tlc-foil-response\n",
      "1.1 billion taxi pickups from new york - http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/\n",
      "what you can do with this data - a good example by yao-jen kuo - https://yaojenkuo.github.io/uber.html\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "• what is the average length of the trip?\n",
      "• average number of rides per week or per month?\n",
      "• total tax savings based on traveled business miles?\n",
      "• percentage of business miles vs personal vs. meals\n",
      "• how much money can be saved by a typical customer using uber, careem, or lyft versus regular cab service?\n",
      "septa - southeastern pennsylvania transportation authority\n",
      "the septa regional rail system consists of commuter rail service on 13 branches to more than 150 active stations in philadelphia, pennsylvania, and its suburbs and satellite cities.\n",
      "septa uses on-time performance (otp) to measure service reliability. otp identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time. however, by industry standard, a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.\n",
      "septa has established an annual goal of 91% for regional rail on-time performance. how well are they doing? is it even a meaningful measure?\n",
      "data description\n",
      "otp.csv\n",
      "train_id\n",
      "direction ('n' or 's' direction is demarcated as either northbound or southbound)^1\n",
      "origin (see map below - you'll see 'warminster', 'glenside',...'airport terminal..')\n",
      "next_station (think of this as the station stop, at timestamp)\n",
      "date\n",
      "status ('on time', '5 min', '10 min'. this is a status on train lateness. 999 is a suspended train)\n",
      "timestamp\n",
      "trainview.csv - gps train data (early release)\n",
      "most gps coordinates are based on track telemetry; however, cars are being equipped with gps units.\n",
      "train_id\n",
      "status\n",
      "next_station\n",
      "service\n",
      "dest\n",
      "lon\n",
      "lat\n",
      "source\n",
      "track_change\n",
      "track\n",
      "date\n",
      "timestamp0 first timestamp at coordinates.\n",
      "timestamp1 last timestamp at coordinates.\n",
      "you can look at the example here on how to track a single train.\n",
      "what to look for...\n",
      "ah, as a commuter you care more about the performance of the train(s) you plan to take. otp maybe 91% or above; but, if the train you take runs late, you'll spend extra time on the tracks. if it consistently runs late, maybe the schedule should be changed.\n",
      "look for patterns. for example, during the tuesday morning rush do some trains run consistently late? how long does it take to get to from point a to point b in the system? performance is very important to septa.\n",
      "below is a map of the system and station stops. this dataset contains data on the regional rail lines.\n",
      "septa train schedules can be found here. note different weekday, saturday, and sunday schedules.\n",
      "context\n",
      "this dataset is born from a test with the twitter streaming api to filter and collect data from this flow on a specific topic, in this case the french election.the script used to make this data collection is available on this github repository.\n",
      "since the 18th of march, the french election enter in the final straight line until the first poll the 23 april 2017 , the candidates for the position are:\n",
      "m. nicolas dupont-aignan\n",
      "mme marine le pen\n",
      "m. emmanuel macron\n",
      "m. benoît hamon\n",
      "mme nathalie arthaud\n",
      "m. philippe poutou\n",
      "m. jacques cheminade\n",
      "m. jean lassalle\n",
      "m. jean-luc mélenchon\n",
      "m. françois asselineau\n",
      "m. françois fillon\n",
      "the idea was to collect the data from the twitter api periodically. the acquisition process evolved as follows:\n",
      "versions 1, 2 and 3 every hour a python script listens to the twitter api stream for 10 minutes during 3 weeks.\n",
      "version 4+ the new versions will be based on a new data structure, and start after the validation by the french constitutional council on 18 march 2017 of the candidates.\n",
      "the data will be stored in a dbsqlite files(database_number of the week_number_block_weekday.sqlite format) and will be updated as often as i can (at least every week).\n",
      "after the first round (version 18+) i had to readjust the number of files per week and the 20 files kaggle limitation push me to reduce the number of files to upload (but you can join for your local analytics the version 17 + version 18+)\n",
      "example : illustration of the number of mentions of the different candidates\n",
      "i add to these databases a sqlite database that contains the informations from the google trends about the top 5 candidates.in thid database there is :\n",
      "a table that contains the overall interests by region\n",
      "a table that contains the interests by region for each candidate\n",
      "a table with the top25 associated queries for each candidate in top and rising ranking\n",
      "content\n",
      "in this dbsqlite file, you will find a data table that contains for every row:\n",
      "===============common===============\n",
      "the index of the line\n",
      "the language of the tweet\n",
      "for each candidate :mention_candidatename, if the candidate or his associated account has been called (0 or 1)\n",
      "the tweet\n",
      "the timestamp in milliseconds\n",
      "===============version 4+===============\n",
      "the day\n",
      "the hour (london timezone)\n",
      "the username of the user that made the tweet\n",
      "the username location (that he gives with his profile)\n",
      "if the tweet is a retweet or a quote (0 or 1)\n",
      "the username that has been retweeted\n",
      "the original tweet (the one retweeted or quoted)\n",
      "acknowledgements\n",
      "this election is gonna be intense.\n",
      "inspiration\n",
      "the first version of the dataset was just a test to collect the data and see the first pieces of work that the community can do with this dataset.the new versions are (i think and hope) adapted to do deep text analytics.\n",
      "content\n",
      "this database includes a record for each oil pipeline leak or spill reported to the pipeline and hazardous materials safety administration since 2010. these records include the incident date and time, operator and pipeline, cause of incident, type of hazardous liquid and quantity lost, injuries and fatalities, and associated costs.\n",
      "acknowledgements\n",
      "the oil pipeline accident reports were collected and published by the dot's pipeline and hazardous materials safety administration.\n",
      "context\n",
      "this dataset comes from a proof-of-concept study published in 1999 by golub et al. it showed how new cases of cancer could be classified by gene expression monitoring (via dna microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. these data were used to classify patients with acute myeloid leukemia (aml) and acute lymphoblastic leukemia (all).\n",
      "content\n",
      "golub et al \"molecular classification of cancer: class discovery and class prediction by gene expression monitoring\"\n",
      "there are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. these datasets contain measurements corresponding to all and aml samples from bone marrow and peripheral blood. intensity values have been re-scaled such that overall intensities for each chip are equivalent.\n",
      "acknowledgements\n",
      "molecular classification of cancer: class discovery and class prediction by gene expression\n",
      "science 286:531-537. (1999). published: 1999.10.14\n",
      "t.r. golub, d.k. slonim, p. tamayo, c. huard, m. gaasenbeek, j.p. mesirov, h. coller, m. loh, j.r. downing, m.a. caligiuri, c.d. bloomfield, and e.s. lander\n",
      "these datasets have been converted to a comma separated value files (csv).\n",
      "inspiration\n",
      "these datasets are great for classification problems. the original authors used the data to classify the type of cancer in each patient by their gene expressions.\n",
      "context\n",
      "brazilian politicians are entitled to refunds if they spend any of their money on an activity that is enabling them to \"better serve the people\".\n",
      "those expenses are public data, however, there is little monitoring/data analysis of it. a quick look at it shows a deputy with over 800 flights in one year. another deputy has over 140.000r$ expenses on mailing (old fashion mail) in one year.\n",
      "there are a lot of very suspicious data regarding the deputies expending behavior. can you help spot outliers and companies charging unusual amounts of money for a service?\n",
      "content\n",
      "data is public. it was collected from the official government website:\n",
      "http://www2.camara.leg.br/transparencia/cota-para-exercicio-da-atividade-parlamentar/dados-abertos-cota-parlamentar\n",
      "it was converted from xml to csv, filtered out irrelevant columns, and translated a few of the features to english.\n",
      "the uploaded data contains:\n",
      "u'deputy_name', u'political_party', u'deputy_state', u'company_name' u'company_id' u'refund_date'\n",
      "inspiration\n",
      "brazil is currently passing through thriving times.\n",
      "its political group has always been public involved in many scandals, but it is just now that a few brave men and women have started doing something about it. in 2016 we have had senators, former ministers and many others formally charged and arrested for their crimes.\n",
      "context\n",
      "i am currently working on summarizing chat context where it helps an agent in understanding previous context quickly. it interests me to apply the deep learning models to existing datasets and how they perform on them. i believe news articles are rich in grammar and vocabulary which allows us to gain greater insights.\n",
      "content\n",
      "the dataset consists of 4515 examples and contains author_name, headlines, url of article, short text, complete article. i gathered the summarized news from inshorts and only scraped the news articles from hindu, indian times and guardian. time period ranges from febrauary to august 2017.\n",
      "acknowledgements\n",
      "i would like to thank the authors of inshorts for their amazing work\n",
      "inspiration\n",
      "generating short length descriptions(headlines) from text(news articles).\n",
      "summarizing large amount of information which can be represented in compressed space\n",
      "purpose\n",
      "when i was working on the summarization task i didn't find any open source data-sets to work on, i believe there are people just like me who are working on these tasks and i hope it helps them.\n",
      "contributions\n",
      "it will be really helpful if anyone found nice insights from this data and can share their work. thankyou...!!!\n",
      "for those who are interested here is the link for the github code which includes the scripts for scraping. https://github.com/sunnysai12345/news_summary\n",
      "douban movie short comments dataset v2\n",
      "introduction\n",
      "douban movie is a chinese website that allows internet users to share their comments and viewpoints about movies. users are able to post short or long comments on movies and give them marks. this dataset contains more than 2 million short comments of 28 movies in douban movie website. it can be used on text classification, text clustering, sentiment analysis, semantic web construction and some other fields that relate to web mining or nlp (of chinese lol).\n",
      "metadata\n",
      "id the id of the comment (start from 0)\n",
      "movie_name_en the english name of the movie\n",
      "movie_name_cn the chinese name of the movie\n",
      "crawl_date the date that the data are crawled\n",
      "number the number of the comment\n",
      "username the username of the account\n",
      "date the date that the comment posted\n",
      "star the star that users give to the movie (from 1 to 5, 5 grades)\n",
      "comment the content of the comment\n",
      "like the count of \"like\" on the comment\n",
      "context\n",
      "goodreads book reviews dataset.\n",
      "content\n",
      "this data was collected duing oct (12-21) '17\n",
      "schema: root |-- bookid: string (nullable = false) |-- title: string (nullable = false) |-- author: string (nullable = false) |-- rating: string (nullable = false) |-- ratingscount: string (nullable = false) |-- reviewscount: string (nullable = false) |-- reviewername: string (nullable = true) |-- reviewerratings: string (nullable = true) |-- review: string (nullable = true)\n",
      "acknowledgements\n",
      "thank you, goodreads for the data. all the data here belongs to goodreads.\n",
      "inspiration\n",
      "this data was actually scrapped for webmining project.\n",
      "context\n",
      "in january 2017, pavic submitted a survey focused on smart city and collected data from 1076 people. this survey was fully anonymous and was aimed at improving the citizens' lives in the future smart city\n",
      "content\n",
      "the idea of the survey is to obtain a precise insight concerning the citizens' reactions to different recommendations in two different contexts. in clear, respondents were asked to choose among a set of 18 recommendations those that they would be most interested in if it were proposed in two different contexts: on a sunny and warm (20°c) saturday afternoon in spring (referred to as the \"sun\" context) and on a rainy and cold (8°c) saturday afternoon in winter (referred to as the \"rain\" context). the recommendations concerned various subjects : social or cultural events, discounts in restaurants, useful city information, etc. and people were asked in each context which they would like to receive as push notifications on their phones. for each context, respondents could give several or no responses.\n",
      "the following are the precise text of the questions submitted to the respondents : - for the \"sun\" dataset : a saturday in spring around 4pm with a comfy temperature of 20°c or 68°f. you are downtown in the city for the afternoon and your mobile application can send you personalized services/activities notifications in real-time. which of the following activities would you want to receive in your notifications ? (several may be chosen). - for the \"rain\" dataset : a rainy saturday in the winter around 4pm with a cool temperature of 8°c or 46°f. you are downtown in the city for the afternoon and your mobile application can send you personalized services/activities notifications in real-time. which of the following activities would you want to receive in your notifications ? (several may be chosen).\n",
      "inspiration\n",
      "these dataset could allow future applications both to simulate recommendation system algorithms, and to deduce clusters from the collected profiles.\n",
      "this data set is hacker news posts from the last 12 months (up to september 26 2016).\n",
      "it includes the following columns:\n",
      "title: title of the post (self explanatory)\n",
      "url: the url of the item being linked to\n",
      "num_points: the number of upvotes the post received\n",
      "num_comments: the number of comments the post received\n",
      "author: the name of the account that made the post\n",
      "created_at: the date and time the post was made (the time zone is eastern time in the us)\n",
      "one fun project suggestion is a model to predict the number of votes a post will attract.\n",
      "the scraper is written, so i can keep this up-to-date and add more historical data. i can also scrape the comments. just make the request in this dataset's forum.\n",
      "the is a fork of minimaxir's hn scraper (thanks minimaxir): https://github.com/minimaxir/get-all-hacker-news-submissions-comments\n",
      "context\n",
      "the world bank edstats all indicator query holds over 4,000 internationally comparable indicators that describe education access, progression, completion, literacy, teachers, population, and expenditures. the indicators cover the education cycle from pre-primary to vocational and tertiary education.\n",
      "content\n",
      "in addition to the above mentioned indicators, this dataset also holds learning outcome data from international and regional learning assessments (e.g. pisa, timss, pirls), equity data from household surveys, and projection/attainment data to 2050. for further information, please visit the edstats website.\n",
      "inspiration\n",
      "in your opinion, what are some of the more surprising indicators? are there any you would consider adding?\n",
      "which countries have the highest adult illiteracy rates? have they changed over time, and do rates vary based on age bracket?\n",
      "do school enrollment rates have an impact on adult illiteracy rates? if so, can you determine approximately how long a change in enrollment takes in order to impact illiteracy? does this change vary among countries, and if so, can you point to changes in policies or ngo efforts that might play a role?\n",
      "acknowledgements\n",
      "data was acquired from the world bank, and can be accessed in multiple formats here.\n",
      "content\n",
      "this report provides statistics for the number of illegal immigrants arrested or apprehended by the border patrol in each division (or sector) of the united states borders with canada, mexico, and caribbean islands; this data is a partial measure of the flow of people illegally entering the united states.\n",
      "acknowledgements\n",
      "data was compiled and published by the us border patrol on the customs and border protection webpage.\n",
      "context\n",
      "road accidents\n",
      "content\n",
      "dataset has been fetched from here and the files have been merged and cleaned to reach the final data attached. primarily captures road accidents in uk between 1979 and 2015 and has 70 features/columns and about 250k rows. also attached with it is an excel file with multiple tabs that can help one to understand the data.\n",
      "acknowledgements\n",
      "data has been fetched from open data platform uk and is being shared under open government licence. for more details refer to open data uk\n",
      "context:\n",
      "this dataset contains the number of international tourists arriving in brazil each month from 1989 to 2015.\n",
      "content:\n",
      "continent\n",
      "country\n",
      "state of arrival\n",
      "way in (by land, sea, river or air)\n",
      "year\n",
      "month\n",
      "count\n",
      "acknowledgements:\n",
      "i've downloaded this dataset from dados.gov.br, the brazilian open data portal, and tried to tidy it up a bit.\n",
      "context\n",
      "focuses on financial flows, trends in external debt, and other major financial indicators for developing and advanced economies (data from quarterly external debt statistics and quarterly public sector debt databases). includes over 200 time series indicators from 1970 to 2014, for most reporting countries, and pipeline data for scheduled debt service payments on existing commitments to 2022.\n",
      "content\n",
      "this dataset contains country names and indicator variables from 1970 until 2024. additional materials and detailed descriptions of the datasets can be downloaded from here.\n",
      "acknowledgement\n",
      "the original datasets and data dictionaries can be found here.\n",
      "inspiration\n",
      "few ideas for exploring the dataset:\n",
      "compare the current account balance across countries. is there a pattern associated with developing vs. advanced economies?\n",
      "how have the debt-related indicators changed over time? are these strongly associated with other financial indicators?\n",
      "database of tornado activity from 1950 to 2015\n",
      "created by national weather service and available at http://www.spc.noaa.gov/gis/svrgis/\n",
      "enhance understanding of where tornados happen, indicators of damage, and weather conditions associated with tornados (temp/el nino, la nina)\n",
      "metadata available at http://www.spc.noaa.gov/wcm/data/spc_severe_database_description.pdf\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "this data set contains yearly suicide detail of all the states/u.t of india by various parameters from 2001 to 2012.\n",
      "content\n",
      "time period: 2001 - 2012 granularity: yearly location: states and u.t's of india\n",
      "parameters:\n",
      "a) suicide causes b) education status c) by means adopted d) professional profile e) social status\n",
      "acknowledgements\n",
      "national crime records bureau (ncrb), govt of india has shared this dataset under govt. open data license - india. ncrb has also shared the historical data on their website\n",
      "context\n",
      "this dataset is a snapshot of most of the new news content published online over one week (august 24, 2017 through august 30, 2017).\n",
      "prepared by rohit kulkarni\n",
      "it includes approximately 1.4 million articles, with 20,000 news sources and 20+ languages.\n",
      "this dataset has just four fields (as per the column metadata):\n",
      "publish_time - earliest known time of the url appearing online in yyyymmddhhmm format, ist timezone\n",
      "feed_code - unique identifier for the publisher or domain\n",
      "source_url - url of the article\n",
      "headline_text - headline of the article (utf8, 20 possible languages)\n",
      "see the \"basic data exploration\" notebook for a quick look at the dataset contents.\n",
      "inspiration\n",
      "the sources include news feeds, news websites, government agencies, tech journals, company websites, blogs and wikipedia updates. the data has been collected by polling rss feeds and by crawling other large news aggregators.\n",
      "this 7 day slice was selected as there wasn't any downtime or internet outage during the interval. new news content is produced at this rate by publishers everyday, throughout the year.\n",
      "several other news datasets exploring other attributes, countries and topics can be seen on my profile.\n",
      "acknowledgements\n",
      "this dataset is free to use with the following citation:\n",
      "rohit kulkarni (2017), one week of global feeds [news csv dataset], doi:10.7910/dvn/ilat5b, retrieved from: [this url]\n",
      "context\n",
      "tsunami is a japanese word that translates to \"harbor wave\". it is a wave or a series of waves generated by an impulsive vertical displacement of the surface of the ocean or other body of water. tsunamis have been responsible for over 500,000 fatalities throughout the world — almost half from the 2004 indian ocean earthquake and tsunami!\n",
      "content\n",
      "the noaa/wds tsunami database is a listing of historical tsunami source events and runup locations throughout the world from 2000 b.c. to the present. the events were gathered from scientific and scholarly sources, regional and worldwide catalogs, tide gauge data, deep ocean sensor data, individual event reports, and unpublished works. there are currently over 2,000 source events in the database with event validities greater than one and over 13,000 runup locations where tsunami effects were observed.\n",
      "acknowledgements\n",
      "noaa's national centers for environmental information (ncei) and the world data service for geophysics compiled and published this tsunami database for tsunami warning centers, engineers, oceanographers, seismologists, and the general public.\n",
      "context\n",
      "the term \"astronaut\" derives from the greek words meaning \"space sailor\" and refers to all who have been launched as crew members aboard nasa spacecraft bound for orbit and beyond.\n",
      "content\n",
      "the national aeronautics and space administration (nasa) selected the first group of astronauts in 1959. from 500 candidates with the required jet aircraft flight experience and engineering training in addition to a height below 5 feet 11 inches, seven military men became the nation's first astronauts. the second and third groups chosen included civilians with extensive flying experience. by 1964, requirements had changed, and emphasis was placed on academic qualifications; in 1965, six scientist astronauts were selected from a group of 400 applicants who had a doctorate or equivalent experience in the natural sciences, medicine, or engineering. the group named in 1978 was the first of space shuttle flight crews and fourteen groups have been selected since then with a mix of pilots and mission specialists.\n",
      "there are currently 50 active astronauts and 35 management astronauts in the program; 196 astronauts have retired or resigned and 49 are deceased (as of april 2013).\n",
      "acknowledgements\n",
      "this dataset was published by the national aeronautics and space administration as the \"astronaut fact book\" (april 2013 edition). active astronauts' mission names and flight statistics were updated from the nasa website.\n",
      "inspiration\n",
      "which american astronaut has spent the most time in space? what university has produced the most astronauts? what subject did the most astronauts major in at college? have most astronauts served in the military? which branch? what rank did they achieve?\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 7 million products) that was created by extracting data from amazon.com.\n",
      "content\n",
      "this dataset has following fields:\n",
      "product_name\n",
      "manufacturer - the item manufacturer, as reported on amazon. some common \"manufacturers\", like disney, actually outsource their assembly line.\n",
      "price\n",
      "number_available_in_stock\n",
      "number_of_reviews\n",
      "number_of_answered_questions - amazon includes a question and answer service on all or most of its products. this field is a count of how many questions that were asked actually got answered.\n",
      "average_review_rating\n",
      "amazon_category_and_sub_category - a tree-based, >>-delimited categorization for the item in question.\n",
      "customers_who_bought_this_item_also_bought - references to other items that similar users bought. this is a recommendation engine component that played a big role in making amazon popular initially.\n",
      "description\n",
      "product_information\n",
      "product_description\n",
      "items_customers_buy_after_viewing_this_item\n",
      "customer_questions_and_answers - a string entry with all of the product's json question and answer pairs.\n",
      "customer_reviews - a string entry with all of the product's json reviews.\n",
      "sellers - a string entry with all of the product's json seller information (many products on amazon are sold by third parties).\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "this detailed dataset can be used to answer questions like:\n",
      "what types of toys are most popular on amazon?\n",
      "how dominant are brands in the amazon toy market?\n",
      "can you break down reviews to analyze their sentiment and contents?\n",
      "context\n",
      "this dataset is a record of every building or building unit (apartment, etc.) sold in the new york city property market over a 12-month period.\n",
      "content\n",
      "this dataset contains the location, address, type, sale price, and sale date of building units sold. a reference on the trickier fields:\n",
      "borough: a digit code for the borough the property is located in; in order these are manhattan (1), bronx (2), brooklyn (3), queens (4), and staten island (5).\n",
      "block; lot: the combination of borough, block, and lot forms a unique key for property in new york city. commonly called a bbl.\n",
      "building class at present and building class at time of sale: the type of building at various points in time. see the glossary linked to below.\n",
      "for further reference on individual fields see the glossary of terms. for the building classification codes see the building classifications glossary.\n",
      "note that because this is a financial transaction dataset, there are some points that need to be kept in mind:\n",
      "many sales occur with a nonsensically small dollar amount: $0 most commonly. these sales are actually transfers of deeds between parties: for example, parents transferring ownership to their home to a child after moving out for retirement.\n",
      "this dataset uses the financial definition of a building/building unit, for tax purposes. in case a single entity owns the building in question, a sale covers the value of the entire building. in case a building is owned piecemeal by its residents (a condominium), a sale refers to a single apartment (or group of apartments) owned by some individual.\n",
      "acknowledgements\n",
      "this dataset is a concatenated and slightly cleaned-up version of the new york city department of finance's rolling sales dataset.\n",
      "inspiration\n",
      "what can you discover about new york city real estate by looking at a year's worth of raw transaction records? can you spot trends in the market, or build a model that predicts sale value in the future?\n",
      "arabic handwritten characters dataset\n",
      "astract\n",
      "handwritten arabic character recognition systems face several challenges, including the unlimited variation in human handwriting and large public databases. in this work, we model a deep learning architecture that can be effectively apply to recognizing arabic handwritten characters. a convolutional neural network (cnn) is a special type of feed-forward multilayer trained in supervised mode. the cnn trained and tested our database that contain 16800 of handwritten arabic characters. in this paper, the optimization methods implemented to increase the performance of cnn. common machine learning methods usually apply a combination of feature extractor and trainable classifier. the use of cnn leads to significant improvements across different machine-learning classification algorithms. our proposed cnn is giving an average 5.1% misclassification error on testing data.\n",
      "context\n",
      "the motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of arabic handwritten character recognition. in recent years, arabic handwritten characters recognition with different handwriting styles as well, making it important to find and work on a new and advanced solution for handwriting recognition. a deep learning systems needs a huge number of data (images) to be able to make a good decisions.\n",
      "content\n",
      "the data-set is composed of 16,800 characters written by 60 participants, the age range is between 19 to 40 years, and 90% of participants are right-hand. each participant wrote each character (from ’alef’ to ’yeh’) ten times on two forms as shown in fig. 7(a) & 7(b). the forms were scanned at the resolution of 300 dpi. each block is segmented automatically using matlab 2016a to determining the coordinates for each block. the database is partitioned into two sets: a training set (13,440 characters to 480 images per class) and a test set (3,360 characters to 120 images per class). writers of training set and test set are exclusive. ordering of including writers to test set are randomized to make sure that writers of test set are not from a single institution (to ensure variability of the test set).\n",
      "in an experimental section we showed that the results were promising with 94.9% classification accuracy rate on testing images. in future work, we plan to work on improving the performance of handwritten arabic character recognition.\n",
      "acknowledgements\n",
      "ahmed el-sawy, mohamed loey, hazem el-bakry, arabic handwritten characters recognition using convolutional neural network, wseas, 2017 our proposed cnn is giving an average 5.1% misclassification error on testing data.\n",
      "inspiration\n",
      "creating the proposed database presents more challenges because it deals with many issues such as style of writing, thickness, dots number and position. some characters have different shapes while written in the same position. for example the teh character has different shapes in isolated position.\n",
      "benha university\n",
      "http://bu.edu.eg/staff/mloey\n",
      "https://mloey.github.io/\n",
      "context\n",
      "this dataset contains weather data for new delhi, india.\n",
      "content\n",
      "this data was taken out from wunderground with the help of their easy to use api. it contains various features such as temperature, pressure, humidity, rain, precipitation,etc.\n",
      "acknowledgements\n",
      "this data is owned by wunderground and although i ended up using noaa's data for my research, i thought that i'd share this data here as i haven't worked on hourly data yet and this might be of huge importance.\n",
      "inspiration\n",
      "the main target is to develop a prediction model accurate enough for predicting the weather. we can try something like predicting the weather in the next 24 hours like microsoft tried some time back.\n",
      "https://blogs.microsoft.com/next/2015/08/10/hows-the-weather-using-artificial-intelligence-for-better-answers/#sm.018l60051a9neka10is1m5qpi6u5y\n",
      "upon request from some users, i am uploading csv version.\n",
      "yes, there is already a dataset from manas\n",
      "however, i thought this dataset is different than that one.which includes player metadata, information about all the 11 players who participated in the match.\n",
      "thoughts :\n",
      "■ who are the valuable players for respective teams.\n",
      "■ who are more effective bowlers to bowl in the slog overs , is it spinners ?\n",
      "■ dhoni's strike rate against left-arm spinners in last five overs\n",
      "have fun with this dataset. files in the dataset include:\n",
      "1. ball_by_ball : includes ball by ball details of all the 577 matches.\n",
      "2. match : match metadata\n",
      "3. player : player metadata\n",
      "4. player_match : to know , who is the captain and keeper of the match , includes every player who take part in match even if player haven't get a chance to either bat or bowl.\n",
      "5. season : season wise details , orange cap , purple cap , man_of_the_series\n",
      "6. team : team name\n",
      "diagram\n",
      "context\n",
      "this data is from environmental protection administration, executive yuan, r.o.c. (taiwan).\n",
      "there is air quality data and meteorological monitoring data for research and analysis (only include northern taiwan 2015).\n",
      "content\n",
      "25 observation stations data in the 2015_air_quality_in_northern_taiwan.csv\n",
      "the columns in csv file are:\n",
      "time - the first column is observation time of 2015\n",
      "station - the second column is station name, there is 25 observation stations\n",
      "[banqiao, cailiao, datong, dayuan, guanyin, guting, keelung, linkou, longtan, pingzhen, sanchong, shilin, songshan, tamsui, taoyuan, tucheng, wanhua, wanli, xindian, xinzhuang, xizhi, yangming, yonghe, zhongli, zhongshan]\n",
      "items - from the third column to the last one\n",
      "item - unit - description\n",
      "so2 - ppb - sulfur dioxide\n",
      "co - ppm - carbon monoxide\n",
      "o3 - ppb - ozone\n",
      "pm10 - μg/m3 - particulate matter\n",
      "pm2.5 - μg/m3 - particulate matter\n",
      "nox - ppb - nitrogen oxides\n",
      "no - ppb - nitric oxide\n",
      "no2 - ppb - nitrogen dioxide\n",
      "thc - ppm - total hydrocarbons\n",
      "nmhc - ppm - non-methane hydrocarbon\n",
      "ch4 - ppm - methane\n",
      "uvb - uvi - ultraviolet index\n",
      "amb_temp - celsius - ambient air temperature\n",
      "rainfall - mm\n",
      "rh - % - relative humidity\n",
      "wind_speed - m/sec - the average of last ten minutes per hour\n",
      "wind_direc - degress - the average of last ten minutes per hour\n",
      "ws_hr - m/sec - the average of hour\n",
      "wd_hr - degress - the average of hour\n",
      "ph_rain - ph - acid rain\n",
      "rain_cond - μs/cm - conductivity of acid rain\n",
      "data mark\n",
      "# indicates invalid value by equipment inspection\n",
      "* indicates invalid value by program inspection\n",
      "x indicates invalid value by human inspection\n",
      "nr indicates no rainfall\n",
      "blank indicates no data\n",
      "license\n",
      "open government data license, version 1.0 http://data.gov.tw/license\n",
      "original dataset author : https://github.com/walkerkq\n",
      "from https://github.com/walkerkq/musiclyrics :\n",
      "50 years of pop music lyrics\n",
      "billboard has published a year-end hot 100 every december since 1958. the chart measures the performance of singles in the u.s. throughout the year. using r, i’ve combined the lyrics from 50 years of billboard year-end hot 100 (1965-2015) into one dataset for analysis. you can download that dataset here.\n",
      "the songs used for analysis were scraped from wikipedia’s entry for each billboard year-end hot 100 songs (e.g., 2014). this is the year-end chart, not weekly rankings. many artists have made the weekly chart but not the final year end chart. the final chart is calculated using an inverse point system based on the weekly billboard charts (100 points for a week at number one, 1 point for a week at number 100, etc).\n",
      "i used the xml and rcurl packages to scrape song and artist names from each wikipedia entry. i then used that list to scrape lyrics from sites that had predictable url strings (for example, metrolyrics.com uses metrolyrics.com/song-name-lyrics-artist-name.html). if the first site scrape failed, i moved onto the second, and so on. about 78.9% of the lyrics were scraped from metrolyics.com, 15.7% from songlyrics.com, 1.8% from lyricsmode.com. about 3.6% (187/5100) were unavailable.\n",
      "the dataset features 5100 observations with the features rank (1-100), song, artist, year, lyrics, and source. the artist feature is fairly standardized thanks to wikipedia, but there is still quite a bit of noise when it comes to artist collaborations (justin timberlake featuring timbaland, for example). if there were any errors in the lyrics that were scraped, such as spelling errors or derivatives like \"nite\" instead of \"night,\" they haven't been corrected.\n",
      "full analysis can be found here.\n",
      "walkerkq\n",
      "acknowledgements\n",
      "dataset is a mirror of : https://github.com/walkerkq/musiclyrics all credits to gathering it goes to https://github.com/walkerkq\n",
      "inspiration\n",
      "what makes a song's lyrics popular ?\n",
      "context\n",
      "there has been a lot of discussion of the ways in which the workforce for silicon valley tech companies differs from that of the united states as a whole. in particular, a lot of evidence suggests that tech workers (who tend to be more highly paid than workers in many other professions) are more likely to be white and male. this dataset will allow you to investigate the demographics for 23 silicon valley tech companies for yourself.\n",
      "contents\n",
      "this database contains eeo-1 reports filed by silicon valley tech companies. it was compiled by reveal from the center for investigative reporting.\n",
      "there are six columns in this dataset:\n",
      "company: company name\n",
      "year: for now, 2016 only\n",
      "race: possible values: \"american_indian_alaskan_native\", \"asian\", \"black_or_african_american\", \"latino\", \"native_hawaiian_or_pacific_islander\", \"two_or_more_races\", \"white\", \"overall_totals\"\n",
      "gender: possible values: \"male\", \"female\". non-binary gender is not counted in eeo-1 reports.\n",
      "job_category: possible values: \"administrative support\", \"craft workers\", \"executive/senior officials & mgrs\", \"first/mid officials & mgrs\", \"laborers and helpers\", \"operatives\", \"professionals\", \"sales workers\", \"service workers\", \"technicians\", \"previous_totals\", \"totals\"\n",
      "count: mostly integer values, but contains \"na\" for a no-data variable.\n",
      "acknowledgements:\n",
      "the eeo-1 database is licensed under the open database license (odbl) by reveal from the center for investigative reporting.\n",
      "you are free to copy, distribute, transmit and adapt the spreadsheet, so long as you:\n",
      "credit reveal (including this link if it’s distributed online);\n",
      "inform reveal that you are using the data in your work by emailing sinduja rangarajan at srangarajan@revealnews.org; and\n",
      "offer any new work under the same license.\n",
      "inspiration:\n",
      "how does each company’s workforce compare to the united states population as a whole? you can find county level diversity information here.\n",
      "which company is the most diverse? least diverse?\n",
      "context:\n",
      "global food price fluctuations can cause famine and large population shifts. price changes are increasingly critical to policymakers as global warming threatens to destabilize the food supply.\n",
      "content:\n",
      "over 740k rows of prices obtained in developing world markets for various goods. data includes information on country, market, price of good in local currency, quantity of good, and month recorded.\n",
      "acknowledgements:\n",
      "compiled by the world food program and distributed by hdx.\n",
      "inspiration:\n",
      "this data would be particularly interesting to pair with currency fluctuations, weather patterns, and/or refugee movements--do any price changes in certain staples predict population upheaval? do certain weather conditions influence market prices?\n",
      "license:\n",
      "released under cc by-igo.\n",
      "the 2014 killing of michael brown in ferguson, missouri, began the protest movement culminating in black lives matter and an increased focus on police accountability nationwide.\n",
      "since jan. 1, 2015, the washington post has been compiling a database of every fatal shooting in the us by a police officer in the line of duty. it's difficult to find reliable data from before this period, as police killings haven't been comprehensively documented, and the statistics on police brutality are much less available. as a result, a vast number of cases go unreported.\n",
      "the washington post is tracking more than a dozen details about each killing - including the race, age and gender of the deceased, whether the person was armed, and whether the victim was experiencing a mental-health crisis. they have gathered this information from law enforcement websites, local new reports, social media, and by monitoring independent databases such as \"killed by police\" and \"fatal encounters\". the post has also conducted additional reporting in many cases.\n",
      "there are four additional datasets. these are us census data on poverty rate, high school graduation rate, median household income, and racial demographics.\n",
      "source of census data: https://factfinder.census.gov/faces/nav/jsf/pages/community_facts.xhtml\n",
      "context\n",
      "the data have been organized in two different but related classification tasks.\n",
      "column_3c_weka.csv (file with three class labels)\n",
      "the first task consists in classifying patients as belonging to one out of three categories: normal (100 patients), disk hernia (60 patients) or spondylolisthesis (150 patients).\n",
      "column_2c_weka.csv (file with two class labels)\n",
      "for the second task, the categories disk hernia and spondylolisthesis were merged into a single category labelled as 'abnormal'. thus, the second task consists in classifying patients as belonging to one out of two categories: normal (100 patients) or abnormal (210 patients).\n",
      "content\n",
      "field descriptions:\n",
      "each patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):\n",
      "pelvic incidence\n",
      "pelvic tilt\n",
      "lumbar lordosis angle\n",
      "sacral slope\n",
      "pelvic radius\n",
      "grade of spondylolisthesis\n",
      "acknowledgements\n",
      "the original dataset was downloaded from uci ml repository:\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science\n",
      "files were converted to csv\n",
      "inspiration\n",
      "use these biomechanical features to classify patients according to their labels\n",
      "global historical climatology network-monthly (ghcn-m)\n",
      "context\n",
      "the global historical climatology network (ghcn) is an integrated database of climate summaries from land surface stations across the globe. this data set contains gridded mean temperature anomalies, or departures from a reference value or long-term average, from the global historical climatology network-monthly (ghcn-m) version 3.2.1 temperature data set. the gridded anomalies were produced from ghcn-m bias corrected data. each month of data consists of 2,592 gridded data points produced on a 5° by 5° basis for the entire globe (72 longitude by 36 latitude grid boxes).\n",
      "frequency: monthly\n",
      "period: 1880 to 2016\n",
      "content\n",
      "gridded data for every month from january 1880 to the most recent month is available. the data are temperature anomalies in degrees celsius. each gridded value was multiplied by 100 and written to file as an integer. missing values are represented by the value -9999. the data are formatted by year, month, latitude and longitude. there are 72 longitude grid values per line -- each grid is labeled as a concatenation of \"lon\", \"w\" or \"e\", then the degree. the latitude is captured in the \"lat\" field where the value indicates the lower bound of a grid cell (e.g. 85 indicates 85-90n whereas -90 indicates 85-90s). longitude values are written from 180°w to 180°e, and latitude values from 90°n to 90°s.\n",
      "this dataset permits the quantification of changes in the mean monthly temperature and precipitation for the earth's surface. changes in the observing system itself have been carefully removed to allow for the true climate variability at the earth's surface to be represented in the data.\n",
      "many surface weather stations undergo minor relocations through their history of observation. stations may also be subject to changes in instrumentation as measurement technology evolves. further, the land use/land cover in the vicinity of an observing site may also change with time. such modifications to an observing site have the potential to alter a thermometer's microclimate exposure characteristics and/or change the bias of measurements, the impact of which can be a systematic shift in the mean level of temperature readings that is unrelated to true climate variations. the process of removing such \"non-climatic\" artifacts in a climate time series is called homogenization. in version 3 of the ghcn-monthly temperature data, the apparent impacts of documented and undocumented inhomogeneities are detected and removed through automated pairwise comparisons of mean monthly temperature series as detailed in menne and williams [2009].\n",
      "inspiration\n",
      "this granular dataset permits extensive historical analysis of the earth’s climate to answer questions about climate change, including how different regions of the planet have been affected by changes in temperature over time. get started by forking the kernel mapping historical temperature anomalies with r.\n",
      "acknowledgements\n",
      "this data is a product of noaa's national centers for environmental information (ncei). it was compiled through the aggregation and analysis of many thousands of weather station records. the compete description of the processes and methods used may be found at https://www.ncdc.noaa.gov/ghcnm/v3.php.\n",
      "the global historical climatology network-monthly (ghcn-m) temperature dataset was first developed in the early 1990s (vose et al. 1992). a second version was released in 1997 following extensive efforts to increase the number of stations and length of the data record (peterson and vose, 1997). methods for removing inhomogeneities from the data record associated with non-climatic influences such as changes in instrumentation, station environment, and observing practices that occur over time were also included in the version 2 release (peterson and easterling, 1994; easterling and peterson 1995). since that time efforts have focused on continued improvements in dataset development methods including new quality control processes and advanced techniques for removing data inhomogeneities (menne and williams, 2009).\n",
      "license\n",
      "public domain license\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 14,92,992 restaurants) that was created by extracting data from tripadvisor.com.\n",
      "content\n",
      "this dataset has following fields:\n",
      "restaurant url\n",
      "name\n",
      "address\n",
      "phone\n",
      "city\n",
      "state\n",
      "country\n",
      "neighbourhood\n",
      "email id\n",
      "menu\n",
      "website\n",
      "latitude\n",
      "longitude\n",
      "about restaurant\n",
      "cuisine\n",
      "good for(suitable)\n",
      "price\n",
      "currency\n",
      "rating\n",
      "ranking\n",
      "deal(promotion)\n",
      "total review\n",
      "last reviewed\n",
      "recommended\n",
      "dining option\n",
      "award\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "the country-wise analyses of cuisine, rating, ranking, etc. can be performed.\n",
      "context\n",
      "the nyc department of finance collects data on every parking ticket issued in nyc (~10m per year!). this data is made publicly available to aid in ticket resolution and to guide policymakers.\n",
      "content\n",
      "there are four files, covering aug 2013-june 2017. the files are roughly organized by fiscal year (july 1 - june 30) with the exception of the initial dataset. the initial dataset also lacks 8 columns that are included in the other three datasets (although be warned that these additional data columns are used sparingly). see the dataset descriptions for exact details. columns include information about the vehicle ticketed, the ticket issued, location, and time.\n",
      "acknowledgements\n",
      "data was produced by nyc department of finance. fy2018 data is found here with updates every third week of the month.\n",
      "inspiration\n",
      "when are tickets most likely to be issued? any seasonality?\n",
      "where are tickets most commonly issued?\n",
      "what are the most common years and types of cars to be ticketed?\n",
      "context\n",
      "brazil has elections every two years, but alternating between two different types of elections, each type occurring every four years. there are the municipal elections, where mayors and city council members are elected (the last one occurred in 2016) and general elections where president, governors, senators and congressmen (regional and national) are elected (the last one occurred in 2014). brazil has 26 federal units plus the federal district. each one of these units (regions) elects its senators, congressmen and governors.\n",
      "for each federal unit, brazil's tse provides information on the donations declared by the three entities: candidates, parties and committees. the data comprises information describing every donation received. the donations can be divided in two categories with respect to the donor: they can come from legal persons (private citizens, identified by the cpf (cpf is an identification number used by the brazilian tax revenue office. it is roughly the brazilian analogue to a social security number. with the same purpose, companies are identified with a similar number called cnpj number) or from legal entities (i.e. companies, identified by the cnpj number). also, some entities can make donations among them (the party can give part of the money from a given donation to a candidate). in this type of transaction, the information on the original donor is also specified in the declarations. from now on, these type of donations will be referred to as non-original donations. apart from information concerning each brazilian federal unit separately, one can also obtain the information declared by the parties and committees at national level and for the presidential campaign (which has national and not regional scope).\n",
      "related paper:\n",
      "https://arxiv.org/pdf/1707.08826.pdf\n",
      "context\n",
      "this data set was created for use in the sberbank kaggle competition.\n",
      "content\n",
      "the data consists of three gis shapefiles one for each of the 3 major moscow ring roads; the mkad, ttk (or third ring) and sadovoe (or garden ring).\n",
      "acknowledgements\n",
      "the road shapefiles have been extracted from openstreetmap data, processed in qgis to extract only the roads of interest.\n",
      "openstreetmap license: https://www.openstreetmap.org/copyright\n",
      "inspiration\n",
      "with these files and the distances given in the sberbank dataset it should be possible to better understand the location of properties. with a better understanding of location it may be possible to improve the quality of the overall dataset which contains material amounts of missing or poorly coded data.\n",
      "overview\n",
      "the data are a time-series of fluorescence images measured of at ohsu lab of charles allen (https://www.ohsu.edu/xd/research/centers-institutes/oregon-institute-occupational-health-sciences/research/allen/).\n",
      "introduction\n",
      "we use a fluorescent protein as a reporter for the circadian clock gene period1. we are able to follow the expression of this gene in many neurons for several days to understand how the neural network in the suprachiasmatic nucleus synchronizes the circadian clock of individual neurons to produce a precise circadian rhythm. we analyze each image to determine the fluorescence intensity of each neuron over multiple circadian cycles.\n",
      "faq\n",
      "how where the images obtained: which animal and what staining?\n",
      "the images were taken from a transgenic mouse in which expression of the fluorescent protein venus is driven by the promoter for the circadian clock gene period 1.\n",
      "what is the anatomy of the images, and how are they oriented?\n",
      "the bright line is the third ventricle, which resides on the midline of the brain. the two bright regions on either side of the ventricle are the two portions of the suprachiasmatic nucleus (scn). below the ventricle and the scn is a dark horizontal band that represents the optic chasm.\n",
      "what is the bright vertical line in the top middle?\n",
      "the bright line is the third ventricle. pericytes that line the ventricle express the venus at very high levels. we don't know the function of the circadian clock in these cells.\n",
      "challenge\n",
      "currently we have to analyze each experiment by hand to follow an individual through a couple hundred images. this takes several days. this problem is going to get worse because we have just purchased a new automated microscope stage that will allow us to simultaneously image from four suprachiasmatic nuclei.\n",
      "preview\n",
      "ideas for analysis\n",
      "wavelets (pywavelets) following https://www.ncbi.nlm.nih.gov/pubmed/18931366\n",
      "questions\n",
      "do the cells move during the experiment?\n",
      "how regular is their signal?\n",
      "is the period 24 hours?\n",
      "do nearby cells oscillate together?\n",
      "do they form chunks or groups, over what range do they work?\n",
      "are there networks formed from time-precedence?\n",
      "facebook is becoming an essential tool for more than just family and friends. discover how cheltenham township (usa), a diverse community just outside of philadelphia, deals with major issues such as the bill cosby trial, everyday traffic issues, sewer i/i problems and lost cats and dogs. and yes, theft.\n",
      "communities work when they're connected and exchanging information. what and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?\n",
      "use any facebook public group\n",
      "you can leverage the examples here for any public facebook group. for an example of the source code used to collect this data, and a quick start docker image, take a look at the following project: facebook-group-scrape.\n",
      "data sources\n",
      "there are 4 csv files in the dataset, with data from the following 5 public facebook groups:\n",
      "unofficial cheltenham township\n",
      "elkins park happenings!\n",
      "free speech zone\n",
      "cheltenham lateral solutions\n",
      "cheltenham township residents\n",
      "post.csv\n",
      "these are the main posts you will see on the page. it might help to take a quick look at the page. commas in the msg field have been replaced with {comma}, and apostrophes have been replaced with {apost}.\n",
      "gid group id (5 different facebook groups)\n",
      "pid main post id\n",
      "id id of the user posting\n",
      "name user's name\n",
      "timestamp\n",
      "shares\n",
      "url\n",
      "msg text of the message posted.\n",
      "likes number of likes\n",
      "comment.csv\n",
      "these are comments to the main post. note, facebook postings have comments, and comments on comments.\n",
      "gid group id\n",
      "pid matches main post identifier in post.csv\n",
      "cid comment id.\n",
      "timestamp\n",
      "id id of user commenting\n",
      "name name of user commenting\n",
      "rid id of user responding to first comment\n",
      "msg message\n",
      "like.csv\n",
      "these are likes and responses. the two keys in this file (pid,cid) will join to post and comment respectively.\n",
      "gid group id\n",
      "pid matches main post identifier in post.csv\n",
      "cid matches comments id.\n",
      "response response such as like, angry etc.\n",
      "id the id of user responding\n",
      "name name of the user responding\n",
      "member.csv\n",
      "these are all the members in the group. some members never, or rarely, post or comment. you may find multiple entries in this table for the same person. the name of the individual never changes, but they change their profile picture. each profile picture change is captured in this table. facebook gives users a new id in this table when they change their profile picture.\n",
      "gid group id\n",
      "id id of the member\n",
      "name name of the member\n",
      "url url of the member\n",
      "context\n",
      "the dataset contains historical product demand for a manufacturing company with footprints globally. the company provides thousands of products within dozens of product categories. there are four central warehouses to ship products within the region it is responsible for. since the products are manufactured in different locations all over the world, it normally takes more than one month to ship products via ocean to different central warehouses. if forecasts for each product in different central with reasonable accuracy for the monthly demand for month after next can be achieved, it would be beneficial to the company in multiple ways. this dataset is all real-life data and products/warehouse and category information encoded.\n",
      "content\n",
      "product_code: the product name encoded. warehouse: warehouse name encoded. product_category: product category for each product_code encoded. date: the date customer needs the product. order_demand: single order qty.\n",
      "inspiration\n",
      "is it possible to make forecasts for thousands of products (some of them are highly variable in terms of monthly demand) for the the month after next?\n",
      "if you get richer your teeth could get worse (if you eat more sugar foods) or better (because of better health assistance or, even, more education and health-conciousness). these variables can be analysed with these data, downloaded from gapminder data:\n",
      "bad teeth per child (12 yr, who)\n",
      "gdp/capita (us$, inflation-adjusted, world bank)\n",
      "government health spending per person (us$, who)\n",
      "sugar comsumption per person (g per day, fao)\n",
      "literacy rate, adult total (% of people ages 15 and above, unesco)\n",
      "context\n",
      "why some countries are so different from the others?\n",
      "feel free to upvote :) autor: joni hoppen - linkedin - https://www.linkedin.com/in/joniarroba/\n",
      "content\n",
      "i have gathered manually most of the information at world bank, unicef and so on. some data were not there so i used k-nn to guess some values and have a full dataset that can be used of our data science community.\n",
      "information of each of the 65 variables were made available here http://bit.ly/2l2hjh3\n",
      "acknowledgements\n",
      "thanks www.aquare.la advanced analytics that came up with the idea of creating this dataset to test their vortx tool. also thanks to professionals involved in creating indexes and collecting them, this is such a great valuable work to help better see the world.\n",
      "inspiration\n",
      "what would be the best, way to equalize the world?\n",
      "context\n",
      "attached is a set of products in which we are trying to determine which products we should continue to sell, and which products to remove from our inventory. the file contains both historical sales data and active inventory, which can be discerned with the column titled \"file type\".\n",
      "we suspect that data science applied to the set--such as a decision tree analysis or logistic regression, or some other machine learning model---can help us generate a value (i.e., probability score) for each product, that can be used as the main determinant evaluating the inventory. each row in the file represents one product.\n",
      "it is important to note that we have many products in our inventory, and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year.\n",
      "content\n",
      "the file contains historical sales data (identified with the column titled file_type) along with current active inventory that is in need of evaluation (i.e., file type = \"active\"). the historical data shows sales for the past 6 months. the binary target (1 = sale, 0 = no sale in past six months) is likely the primary target that should drive the analysis.\n",
      "the other columns contain numeric and categorical attributes that we deem relevant to sales.\n",
      "note that some of the historical sales skus are also included in the active inventory.\n",
      "a few comments about the attributes included, as we realize we may have some attributes that are unnecessary or may need to be explained.\n",
      "sku_number: this is the unique identifier for each product.\n",
      "order: just a sequential counter. can be ignored.\n",
      "soldflag: 1 = sold in past 6 mos. 0 = not sold\n",
      "marketingtype = two categories of how we market the product. this should probably be ignored, or better yet, each type should be considered independently.\n",
      "new_release_flag = any product that has had a future release (i.e., release number > 1)\n",
      "inspiration\n",
      "(1) what is the best model to use that will provide us with a probability estimate of a sale for each sku? we are mainly interested in a relative unit that we can continuously update based on these attributes (and others that we add, as we are able).\n",
      "(2) is it possible to provide a scored file (i.e., a probability score for each sku in the file), and to provide an evaluation of the accuracy of the selected model?\n",
      "(3) what are the next steps we should take?\n",
      "thanks very much for any suggestions you may provide.\n",
      "context\n",
      "this is a dataset of devanagari script characters. it comprises of 92000 images [32x32 px] corresponding to 46 characters, consonants \"ka\" to \"gya\", and the digits 0 to 9. the vowels are missing.\n",
      "content\n",
      "the csv file is of the dimension 92000 * 1025. there are 1024 input features of pixel values in grayscale (0 to 255). the column \"character\" represents the devanagari character name corresponding to each image.\n",
      "acknowledgements\n",
      "this dataset was originally created by computer vision research group, nepal. [website archive] (https://web.archive.org/web/20160105230017/http://cvresearchnepal.com/wordpress/dhcd/)\n",
      "example script\n",
      "https://nbviewer.jupyter.org/github/rishianand9/devanagari-character-recognition/blob/master/dcrs.ipynb\n",
      "context\n",
      "the american time use survey (atus) is the nation’s first federally administered, continuous survey on time use in the united states. the goal of the survey is to measure how people divide their time among life’s activities.\n",
      "in atus, individuals are randomly selected from a subset of households that have completed their eighth and final month of interviews for the current population survey (cps). atus respondents are interviewed only one time about how they spent their time on the previous day, where they were, and whom they were with. the survey is sponsored by the bureau of labor statistics and is conducted by the u.s. census bureau.\n",
      "the major purpose of atus is to develop nationally representative estimates of how people spend their time. many atus users are interested in the amount of time americans spend doing unpaid, nonmarket work, which could include unpaid childcare, eldercare, housework, and volunteering. the survey also provides information on the amount of time people spend in many other activities, such as religious activities, socializing, exercising, and relaxing. in addition to collecting data about what people did on the day before the interview, atus collects information about where and with whom each activity occurred, and whether the activities were done for one’s job or business. demographic information—including sex, race, age, educational attainment, occupation, income, marital status, and the presence of children in the household—also is available for each respondent. although some of these variables are updated during the atus interview, most of this information comes from earlier cps interviews, as the atus sample is drawn from a subset of households that have completed month 8 of the cps.\n",
      "the user guide can be found here.\n",
      "content\n",
      "there are 8 datasets containing microdata from 2003-2015:\n",
      "respondent file: the respondent file contains information about atus respondents, including their labor force status and earnings.\n",
      "roster file: the roster file contains information about household members and nonhousehold children (under 18) of atus respondents. it includes information such as age and sex.\n",
      "activity file: the activity file contains information about how atus respondents spent their diary day. it includes information such as activity codes, activity start and stop times, and locations. because activity codes have changed somewhat between 2003 and 2015, this file uses activity codes that appear in the 2003-2015 atus coding lexicon (pdf).\n",
      "activity summary file: the activity summary file contains information about the total time each atus respondent spent doing each activity on the diary day. because activity codes have changed somewhat between 2003 and 2015, this file uses activity codes that appear in the 2003-2015 atus coding lexicon (pdf).\n",
      "who file: the who file includes codes that indicate who was present during each activity.\n",
      "cps 2003-2015 file: the atus-cps file contains information about each household member of all individuals selected to participate in atus. the information on the atus-cps file was collected 2 to 5 months before the atus interview.\n",
      "eldercare roster file: the atus eldercare roster file contains information about people for whom the respondent provided care. eldercare data have been collected since 2011.\n",
      "replicate weights file: the replicate weights file contains miscellaneous atus weights.\n",
      "the atus interview data dictionary can be found here.\n",
      "the atus current population survey (cps) data dictionary can be found here.\n",
      "the atus occupation and industry codes can be found here.\n",
      "the atus activity lexicon can be found here.\n",
      "acknowledgements\n",
      "the original datasets can be found here.\n",
      "inspiration\n",
      "how do daily activities differ by:\n",
      "labor force status\n",
      "income\n",
      "household composition\n",
      "geographical region\n",
      "disability status\n",
      "context\n",
      "cpj began compiling detailed records on journalist deaths in 1992. cpj applies strict journalistic standards when investigating a death. one important aspect of their research is determining whether a death was work-related. as a result, they classify deaths as \"motive confirmed\" or \"motive unconfirmed.\"\n",
      "content\n",
      "the dataset contains 18 variables:\n",
      "type: cpj classified deaths as motive confirmed or motive confirmed, as well as media workers\n",
      "date\n",
      "name\n",
      "sex\n",
      "country_killed\n",
      "organization\n",
      "nationality\n",
      "medium\n",
      "job\n",
      "coverage\n",
      "freelance\n",
      "local_foreign\n",
      "source_fire\n",
      "type_death\n",
      "impunity_for_murder\n",
      "taken_captive\n",
      "threatened\n",
      "tortured\n",
      "acknowledgements\n",
      "the original dataset can be found here.\n",
      "inspiration\n",
      "some ideas for exploring the dataset:\n",
      "what is the trend in journalist deaths over time and how does this differ by type of death, job, coverage, and country?\n",
      "are there differences by sex and/or nationality?\n",
      "context:\n",
      "cdc's division of population health provides cross-cutting set of 124 indicators that were developed by consensus and that allows states and territories and large metropolitan areas to uniformly define, collect, and report chronic disease data that are important to public health practice and available for states, territories and large metropolitan areas. in addition to providing access to state-specific indicator data, the cdi web site serves as a gateway to additional information and data resources.\n",
      "content:\n",
      "a variety of health-related questions were assessed at various times and places across the us over the past 15 years. data is provided with confidence intervals and demographic stratification.\n",
      "acknowledgements:\n",
      "data was compiled by the cdc.\n",
      "inspiration:\n",
      "any interesting trends in certain groups?\n",
      "any correlation between disease indicators and locality hospital spending?\n",
      "content\n",
      "the world religion project aims to provide detailed information about religious adherence worldwide since 1945. it contains data about the number of adherents by religion in each of the states in the international system for every half-decade period. some of the religions are divided into religious families, and the breakdown of adherents within a given religion into religious families is provided to the extent data are available.\n",
      "the project was developed in three stages. the first stage consisted of the formation of a religions tree. a religion tree is a systematic classification of major religions and of religious families within those major religions. to develop the religion tree we prepared a comprehensive literature review, the aim of which was to define a religion, to find tangible indicators of a given religion of religious families within a major religion, and to identify existing efforts at classifying world religions. the second stage consisted of the identification of major data sources of religious adherence and the collection of data from these sources according to the religion tree classification. this created a dataset that included multiple records for some states for a given point in time, yet contained multiple missing data for specific states, specific time periods, and specific religions. the third stage consisted of cleaning the data, reconciling discrepancies of information from different sources, and imputing data for the missing cases.\n",
      "acknowledgements\n",
      "the dataset was created by zeev maoz, university of california-davis, and errol henderson, pennsylvania state university, and published by the correlates of war project.\n",
      "content\n",
      "this dataset includes a record for every branch of chase bank in the united states, including the branch's name and number, date established as a bank office and (if applicable) acquired by jp morgan chase, physical location as street address, city, state, zip, and latitude and longitude coordinates, and the amount deposited at the branch (or the institution, for the bank's main office) between july 1 and june 30, 2016, in us dollars.\n",
      "acknowledgements\n",
      "the location data was scraped from the chase bank website. the deposit data was compiled from the federal deposit insurance corporation's annual summary of deposits reports.\n",
      "inspiration\n",
      "where did chase bank customers deposit the most money last year? which bank branch has seen the most growth in deposits? how did the bank network of branch locations grow over the past century? what city has the most bank branches per capita?\n",
      "general information\n",
      "common voice is a corpus of speech data read by users on the common voice website (http://voice.mozilla.org/), and based upon text from a number of public domain sources like user submitted blog posts, old books, movies, and other public speech corpora. its primary purpose is to enable the training and testing of automatic speech recognition (asr) systems.\n",
      "structure\n",
      "the corpus is split into several parts for your convenience. the subsets with “valid” in their name are audio clips that have had at least 2 people listen to them, and the majority of those listeners say the audio matches the text. the subsets with “invalid” in their name are clips that have had at least 2 listeners, and the majority say the audio does not match the clip. all other clips, ie. those with fewer than 2 votes, or those that have equal valid and invalid votes, have “other” in their name.\n",
      "the “valid” and “other” subsets are further divided into 3 groups:\n",
      "dev - for development and experimentation\n",
      "train - for use in speech recognition training\n",
      "test - for testing word error rate\n",
      "organization and conventions\n",
      "each subset of data has a corresponding csv file with the following naming convention:\n",
      "“cv-{type}-{group}.csv”\n",
      "here “type” can be one of {valid, invalid, other}, and “group” can be one of {dev, train, test}. note, the invalid set is not divided into groups.\n",
      "each row of a csv file represents a single audio clip, and contains the following information:\n",
      "filename - relative path of the audio file\n",
      "text - supposed transcription of the audio\n",
      "up_votes - number of people who said audio matches the text\n",
      "down_votes - number of people who said audio does not match text\n",
      "age - age of the speaker, if the speaker reported it\n",
      "teens: '< 19'\n",
      "twenties: '19 - 29'\n",
      "thirties: '30 - 39'\n",
      "fourties: '40 - 49'\n",
      "fifties: '50 - 59'\n",
      "sixties: '60 - 69'\n",
      "seventies: '70 - 79'\n",
      "eighties: '80 - 89'\n",
      "nineties: '> 89'\n",
      "gender - gender of the speaker, if the speaker reported it\n",
      "male\n",
      "female\n",
      "other\n",
      "accent - accent of the speaker, if the speaker reported it\n",
      "us: 'united states english'\n",
      "australia: 'australian english'\n",
      "england: 'england english'\n",
      "canada: 'canadian english'\n",
      "philippines: 'filipino'\n",
      "hongkong: 'hong kong english'\n",
      "indian: 'india and south asia (india, pakistan, sri lanka)'\n",
      "ireland: 'irish english'\n",
      "malaysia: 'malaysian english'\n",
      "newzealand: 'new zealand english'\n",
      "scotland: 'scottish english'\n",
      "singapore: 'singaporean english'\n",
      "southatlandtic: 'south atlantic (falkland islands, saint helena)'\n",
      "african: 'southern african (south africa, zimbabwe, namibia)'\n",
      "wales: 'welsh english'\n",
      "bermuda: 'west indies and bermuda (bahamas, bermuda, jamaica, trinidad)'\n",
      "the audio clips for each subset are stored as mp3 files in folders with the same naming conventions as it’s corresponding csv file. so, for instance, all audio data from the valid train set will be kept in the folder “cv-valid-train” alongside the “cv-valid-train.csv” metadata file.\n",
      "acknowledgments\n",
      "this dataset was compiled by michael henretty, tilman kamp, kelly davis & the common voice team, who included the following acknowledgments:\n",
      "we sincerely thank all of the people who donated their voice on the common voice website and app. you are the backbone of this project, and we thank you for making this possible!\n",
      "we also thank our community on discourse (https://discourse.mozilla-community.org/c/voice) and github (https://github.com/mozilla/voice-web), you have made this project better every step of the way.\n",
      "and special thanks to mycroft, snips.ai, mythic, tatoeba.org, bangor university, and sap for joining us on this journey. we look forward to working more with each of you.\n",
      "context\n",
      "there is a question in our mind that which language, skills, and experience should we add to our toolbox for getting a job in google. well, i think why not we find out the answer by analyzing the google jobs site. google published all of their jobs at https://careers.google.com/. so i scraped all of the job data from that site by going every job page using selenium. i only take job title, job location, job responsibilities, minimum and preferred qualifications for this dataset.\n",
      "content\n",
      "this dataset is collected using selenium by scraping all of the jobs text for google career site. about the column\n",
      "title: the title of the job\n",
      "category: category of the job\n",
      "location: location of the job\n",
      "responsibilities: responsibilities for the job\n",
      "minimum qualifications: minimum qualifications for the job\n",
      "preferred qualifications: preferred qualifications for the job\n",
      "acknowledgements\n",
      "this dataset is collected using selenium. this product uses the google career site but is not endorsed or certified by google career site.\n",
      "inspiration\n",
      "you can find most popular skills for google jobs\n",
      "create identical job posts\n",
      "most popular languages\n",
      "etc\n",
      "this dataset contains the entire contents of each major api data set published by the us energy information administration. that's everything from the hourly electricity consumption in the united states to natural gas futures contracts.\n",
      "this data has been lightly reprocessed from the eia's bulk download facility by converting each file from a zip of jsons into a single json with the series name as the keys to the specific time series. please note that there are thousands of time series in here, and many of them may still require additional cleaning to deal with odd date formats and so on. the file preview is unable to show a complete listing. you can usually find full details of a given time series in the 'description' field.\n",
      "context:\n",
      "sentiment analysis, the task of automatically detecting whether a piece of text is positive or negative, generally relies on a hand-curated list of words with positive sentiment (good, great, awesome) and negative sentiment (bad, gross, awful). this dataset contains both positive and negative sentiment lexicons for 81 languages.\n",
      "content:\n",
      "the sentiment lexicons in this dataset were generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them. the general intuition is that words which are closely linked on a knowledge graph probably have similar sentiment polarities. for this project, sentiments were generated based on english sentiment lexicons.\n",
      "this dataset contains sentiment lexicons for the following languages:\n",
      "afrikaans\n",
      "albanian\n",
      "arabic\n",
      "aragonese\n",
      "armenian\n",
      "azerbaijani\n",
      "basque\n",
      "belarusian\n",
      "bengali\n",
      "bosnian\n",
      "breton\n",
      "bulgarian\n",
      "catalan\n",
      "chinese\n",
      "croatian\n",
      "czech\n",
      "danish\n",
      "dutch\n",
      "esperanto\n",
      "estonian\n",
      "faroese\n",
      "finnish\n",
      "french\n",
      "galician\n",
      "georgian\n",
      "german\n",
      "greek\n",
      "gujarati\n",
      "haitian creole\n",
      "hebrew\n",
      "hindi\n",
      "hungarian\n",
      "icelandic\n",
      "ido\n",
      "indonesian\n",
      "interlingua\n",
      "irish\n",
      "italian\n",
      "kannada\n",
      "khmer\n",
      "kirghiz\n",
      "korean\n",
      "kurdish\n",
      "latin\n",
      "latvian\n",
      "lithuanian\n",
      "luxembourgish\n",
      "macedonian\n",
      "malay\n",
      "maltese\n",
      "marathi\n",
      "norwegian\n",
      "norwegian\n",
      "persian\n",
      "polish\n",
      "portuguese\n",
      "romanian\n",
      "romansh\n",
      "russian\n",
      "scottish\n",
      "serbian\n",
      "slovak\n",
      "slovene\n",
      "spanish\n",
      "swahili\n",
      "swedish\n",
      "tagalog\n",
      "tamil\n",
      "telugu\n",
      "thai\n",
      "turkish\n",
      "turkmen\n",
      "ukrainian\n",
      "urdu\n",
      "uzbek\n",
      "vietnamese\n",
      "volapük\n",
      "walloon\n",
      "welsh\n",
      "western frisian\n",
      "yiddish\n",
      "for more information and additional sentiment lexicons, please visit the project’s website.\n",
      "acknowledgements:\n",
      "this dataset was collected by yanqing chen and steven skiena. if you use it in your work, please cite the following paper:\n",
      "chen, y., & skiena, s. (2014). building sentiment lexicons for all major languages. in acl (2) (pp. 383-389).\n",
      "it is distributed here under the gnu general public license. note that this is the full gpl, which allows many free uses, but does not allow its incorporation into any type of distributed proprietary software, even in part or in translation. for commercial applications please contact the dataset creators.\n",
      "inspiration:\n",
      "these word lists contain many words with similar meanings. can you automatically detect which words are cognates?\n",
      "can you use these sentiment lexicons to reverse-engineer the knowledge graphs that generated them?\n",
      "top spotify tracks of 2017\n",
      "at the end of each year, spotify compiles a playlist of the songs streamed most often over the course of that year. this year's playlist (top tracks of 2017) included 100 songs. the question is: what do these top songs have in common? why do people like them?\n",
      "original data source: the audio features for each song were extracted using the spotify web api and the spotipy python library. credit goes to spotify for calculating the audio feature values.\n",
      "data description: there is one .csv file in the dataset. (featuresdf.csv) this file includes:\n",
      "spotify uri for the song\n",
      "name of the song\n",
      "artist(s) of the song\n",
      "audio features for the song (such as danceability, tempo, key etc.)\n",
      "a more detailed explanation of the audio features can be found in the metadata tab.\n",
      "exploring the data: some suggestions for what to do with the data:\n",
      "look for patterns in the audio features of the songs. why do people stream these songs the most?\n",
      "try to predict one audio feature based on the others\n",
      "see which features correlate the most\n",
      "context\n",
      "this data set was made from an html rip made by reddit user \"usheep\" who threatened to expose all the vendors on agora to the police if they did not meet his demands (sending him a small monetary amount ~few hundred dollars in exchange for him not leaking their info). most information about what happened to \"usheep\" and his threats is nonexistent. he posted the html rip and was never heard from again. agora shut down a few months after. it is unknown if this was related to \"usheep\" or not, but the raw html data remained.\n",
      "content\n",
      "this is a data parse of marketplace data ripped from agora (a dark/deep web) marketplace from the years 2014 to 2015. it contains drugs, weapons, books, services, and more. duplicate listings have been removed and prices have been averaged of any duplicates. all of the data is in a csv file and has over 100,000 unique listings.\n",
      "it is organized by:\n",
      "vendor: the seller\n",
      "category: where in the marketplace the item falls under\n",
      "item: the title of the listing\n",
      "description: the description of the listing\n",
      "price: cost of the item (averaged across any duplicate listings between 2014 and 2015)\n",
      "origin: where the item is listed to have shipped from\n",
      "destination: where the item is listed to be shipped to (blank means no information was provided, but mostly likely worldwide. i did not enter worldwide for any blanks however as to not make assumptions)\n",
      "rating: the rating of the seller (a rating of [0 deals] or anything else with \"deals\" in it means there is not concrete rating as the amount of deals is too small for a rating to be displayed)\n",
      "remarks: only remark options are blank, or \"average price may be skewed outliar > .5 btc found\" which is pretty self explanatory.\n",
      "acknowledgements\n",
      "though i got this data from a 3rd party, it seems as though it originally came from here: https://www.gwern.net/dnm-archives gwern branwen seems to have complied all of his dark net marketplace leaks and html rips and has a multitude of possible uses for the data at the link above. it is free for anyone to use as long as proper credit is given to the creator. i would be happy to parse more data if anyone would like to request a specific website and/or format.\n",
      "inspiration\n",
      "this data could be used to track drug dealers across different platforms. potentially find correlations between different drugs and from where/to they ship in the world to show correlations between types of drugs and where drug dealers that supply them are located. prices can estimate drug economies in certain regions of the world. similar listings from 2 different vendors can perhaps point to competition to corner a market, or even show that some vendors may work together to corner a market. there are quite a few opportunities to do some really great stuff to find correlations between illegal drugs, weapons, and more in order to curb the flow of dark net drug trade by identifying high risk regions or vendors. i can potentially do a new parse of other websites so you can find correlations across websites rather than just within agora.\n",
      "world development indicators provides a compilation of relevant, high-quality, and internationally comparable statistics about global development and the fight against poverty. it is intended to help policymakers, students, analysts, professors, program managers, and citizens find and use data related to all aspects of development, including those that help monitor progress toward the world bank group’s two goals of ending poverty and promoting shared prosperity.\n",
      "content\n",
      "this dataset includes indicators at both national and regional levels for: -agriculture & rural development -aid effectiveness -climate change -economy & growth -education -energy & mining -environment -external debt -financial sector -gender -health -infrastructure -labor & social protection -poverty -private sector -public sector -science & technology -social development -trade, urban development\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the world bank. please check their instance at http://data.worldbank.org/data-catalog/world-development-indicators for updates and related information.\n",
      "context\n",
      "i am a student exploring the possibility of making money in football betting. i am currently doing a literature review on modelling association football scores and trying to put together a machine learning system to use for my first betting campaign next season. what i have learned thus far is that outcomes of football events are partly deterministic and partly random. i do not know exactly how to go about implementing this in a machine learning system yet. i am also hoping to find useful features from this dataset.\n",
      "content\n",
      "the data here contains match statistics collected from whoscored.com europes top five leagues from 2012-2013 to 2016-2017 season. it contains just about all match statistics that anyone can ever hope for including but not limited to goals, corners, possession, ratings, coaches, lineups, and other relevent match statistics\n",
      "the features are simply just self explanatory and have been given long but meaningful names\n",
      "acknowledgement\n",
      "i collected the data from the whoscored.com website. i scraped it using beautifulsoup in python and just extracted the features i thought could have some use.\n",
      "inspiration\n",
      "this is just something i hope could become something but hey, it may be nothing. i am just interested to know the kind of insights that could be generated from this.\n",
      "context\n",
      "this is a list of the finishers of the boston marathon of 2015, 2016 and 2017.\n",
      "it's important to highlight that the boston marathon is the oldest marathon run in the us as it is the only marathon (other than olympic trails) that most of the participants have to qualify to participate.\n",
      "for the professional runners, it's a big accomplishment to win the marathon. for most of the other participants, it's an honor to be part of it.\n",
      "content\n",
      "it contains the name, age, gender, country, city and state (where available), times at 9 different stages of the race, expected time, finish time and pace, overall place, gender place and division place.\n",
      "decided to keep every year as a separate file, making it more manageable and easier to deal with it.\n",
      "acknowledgements\n",
      "data was scrapped from the official marathon website - http://registration.baa.org/2017/cf/public/iframe_resultssearch.cfm\n",
      "i have found that other people have done this kind of scraping, so, some of those ideas together with things i have learned in my quest to become a data scientist created the set.\n",
      "you can actually find the scraping notebooks at - https://github.com/rojour/boston_results . notebook it's not very clean yet, but i will get to it soon...\n",
      "inspiration\n",
      "i was a participant in the marathon 2016 and 2017 edition, as well as a data science student, so it was a natural curiosity.\n",
      "i have done a preliminary study of some fun facts. you can see the kernel here as well as in the github page listed above.\n",
      "already some people have created some fun analysis of the results (mostly of the first part - 2016) that was the first upload, but i am curious of what people may come up with... now that three years are available, it may spark the creative juices of some.\n",
      "i believe it's a simple, fun dataset that can be used by the new to play with, and by some veterans to get creative.\n",
      "dataset of all of the crimes in the dc metro police system ranging from theft, arson, assault, homicide, sex abuse, robbery, and burglary.\n",
      "data can be easily geocoded and mapped, trends can be extracted, and predictions can be made.\n",
      "would be interesting to combine with other datasets, i.e. changes in housing prices, history of construction sites etc. j an informal hypothesis would be: if the local government invests in fixing the sidewalks in a neighborhood, how much would the investment decrease crime levels on a block by block basis.\n",
      "raw data can be accessed from: http://crimemap.dc.gov/crimemapsearch.aspx#tabs-geoother\n",
      "the data is most easily accessed by downloading 1 ward at a time for the specific data range.\n",
      "context\n",
      "extraterrestrials, visitors, little green men, ufos, swap gas. what do they want? where do they come from? do they like cheeseburgers? this dataset will likely not help you answer these questions. it does contain over 80,000 records of ufo sightings dating back as far as 1949. with the latitude and longitude data it is possible to assess the global distribution of ufo sightings (patterns could aid in planetary defence if invasion proves to be imminent). the dates and times, along with the duration of the ufo's stay and description of the craft also lend themselves to predictions. can we find patterns in their arrival times and durations? do aliens work on weekends? help defend the planet and learn about your fellow earthlings (and when they are most likely to see et).\n",
      "content\n",
      "date_time - standardized date and time of sighting\n",
      "city - location of ufo sighting\n",
      "state/province - the us state or canadian province, appears blank for other locations\n",
      "country - country of ufo sighting\n",
      "ufo_shape - a one word description of the \"spacecraft\"\n",
      "length_of_encounter_seconds - standardized to seconds, length of the observation of the ufo\n",
      "described_duration _of_encounter - raw description of the length of the encounter (shows uncertainty to previous column)\n",
      "description - text description of the ufo encounter. warning column is messy, with some curation it could lend itself to some natural language processing and sentiment analysis.\n",
      "date_documented - when was the ufo sighting reported\n",
      "latitude - latitude\n",
      "longitude - longitude\n",
      "note there are missing data in the columns. i've left it as is because depending on what the user is interested in the missing values in any one column may or may not matter.\n",
      "acknowledgements\n",
      "i found these data here: https://github.com/planetsig/ufo-reports full credit to them for the curation, i added some column headers and just described what i've seen\n",
      "inspiration\n",
      "some great ways to use these data would be:\n",
      "a global plot of the locations of recorded ufo sightings.\n",
      "can the duration of the ufo visit be predicted from the other data?\n",
      "is there a pattern to the appearances? at certain times of day, on certain days of the week or days of the year? (i.e. are people on their way home from the pub more likely to see little green men?)\n",
      "are certain shapes of ufo more likely to be seen in different geographical regions.\n",
      "context\n",
      "a dataset of atp matches including individual statistics.\n",
      "content\n",
      "in these datasets there are individual csv files for atp tournament from 2000 to 2017.\n",
      "the numbers in the last columns are absolute values, using them you can calculate percentages.\n",
      "dataset legend\n",
      "all the match statistics are in absolute number format, you can convert to percentages using the total point number\n",
      "ace = absolute number of aces\n",
      "df = number of double faults\n",
      "svpt = total serve points\n",
      "1stin = 1st serve in\n",
      "1st won = points won on 1st serve\n",
      "2ndwon = points won on 2nd serve\n",
      "svgms = serve games\n",
      "bpsaved = break point saved\n",
      "bpfaced = break point faced\n",
      "acknowledgement\n",
      "thanks to jeff sackmann for the excellent work. be sure to visit his github profile\n",
      "https://github.com/jeffsackmann/tennis_atp\n",
      "inspiration\n",
      "this dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. i'm planning to add historical odds and injuries data as soon as i have the time to get them.\n",
      "context\n",
      "the united states census bureau conducts annual surveys to assess the finances of elementary and high schools. the attached csv file contains a summary of revenue and expenditure for the years 1992-2015, organized by state.\n",
      "content\n",
      "[elsec_main.csv] a comma-separated spreadsheet containing revenues and expenditures for all u.s. school districts, 1993-2015.\n",
      "state,enroll,name,yrdata,totalrev,tfedrev,tstrev,tlocrev,totalexp,tcurinst,tcurssvc,tcuronon,tcapout\n",
      "alabama,7568,autauga co sch dist,1995,31827,2821,21389,7617,27457,15228,7123,2575,2176\n",
      "alabama,19961,baldwin co sch dist,1995,93379,6655,55108,31616,87973,48750,22961,6927,6795\n",
      "[elsec_summary.csv] a comma-separated spreadsheet containing state summaries of revenues and expenditures, organized by year.\n",
      "state,year,enroll,total_revenue,federal_revenue,state_revenue,local_revenue,total_expenditure,instruction_expenditure,support_services_expenditure,other_expenditure,capital_outlay_expenditure\n",
      "alabama,1992,,2678885,304177,1659028,715680,2653798,1481703,735036,,174053\n",
      "alaska,1992,,1049591,106780,720711,222100,972488,498362,350902,,37451\n",
      "be warned, some data will be nan's (most notably, the 1992 records contain no data for enrollment).\n",
      "data was created from the spreadsheets in [elsec.zip] (taken from the u.s. census bureau site) using [chew_data.py] and [state_summary.py]. column names are documented in [school15doc.pdf].\n",
      "sources\n",
      "https://www.census.gov/programs-surveys/school-finances/data/tables.html\n",
      "changelog\n",
      "[v 0.2] added data from 1993-2001. data is now harvested from the main spreadsheets instead of the summary spreadsheets. data by school district is now available.\n",
      "[v 0.3] added 1992 data. added enrollment data for all years except 1992 (unavailable).\n",
      "[v 0.4] straightening a few things out as i play with the data in my own kernel. changed \"program_other_expenditure\" to \"other_expenditure\" and fixed chew_data.py to properly pull that information. removed \"non-elsec\" funding and \"program_current_expenditure\" columns.\n",
      "context\n",
      "this dataset is an aggregate of the screen-fixations from screen movements of starcraft 2 replay files.\n",
      "content\n",
      "this dataset contains 21 variables:\n",
      "gameid: unique id for each game\n",
      "leagueindex: 1-8 for bronze, silver, gold, diamond, master, grandmaster, professional leagues\n",
      "age: age of each player\n",
      "hoursperweek: hours spent playing per week\n",
      "totalhours: total hours spent playing\n",
      "apm: action per minute\n",
      "selectbyhotkeys: number of unit selections made using hotkeys per timestamp\n",
      "assigntohotkeys: number of units assigned to hotkeys per timestamp\n",
      "uniquehotkeys: number of unique hotkeys used per timestamp\n",
      "minimapattacks: number of attack actions on minimal per timestamp\n",
      "minimaprightclicks: number of right-clicks on minimal per timestamp\n",
      "numberofpacs: number of pacs per timestamp\n",
      "gapbetweenpacs: mean duration between pacs (milliseconds)\n",
      "actionlatency: mean latency from the onset of pacs to their first action (milliseconds)\n",
      "actionsinpac: mean number of actions within each pac\n",
      "totalmapexplored: number of 24x24 game coordinate grids viewed by player per timestamp\n",
      "workersmade: number of scvs, drones, probes trained per timestamp\n",
      "uniqueunitsmade: unique units made per timestamp\n",
      "complexunitsmade: number of ghosts, investors, and high templars trained per timestamp\n",
      "complexabilityused: abilities requiring specific targeting instructions used per timestamp\n",
      "maxtimestamp: time stamp of game's last recorded event\n",
      "inspiration\n",
      "questions worth exploring:\n",
      "how do the replay attributes differ by level of player expertise?\n",
      "what are significant predictors of a player's league?\n",
      "acknowledgements\n",
      "this dataset is from simon fraser university - summit and can be found here. you must give attribution to the work; you may not use this work for commercial purposes; you may not alter, transform, or build upon this work. any further uses require the permission of the rights holder.\n",
      "context\n",
      "is crime in america rising or falling? the answer is not as simple as politicians make it out to be because of how the fbi collects crime data from the country’s more than 18,000 police agencies. national estimates can be inconsistent and out of date, as the fbi takes months or years to piece together reports from those agencies that choose to participate in the voluntary program.\n",
      "to try to fill this gap, the marshall project collected and analyzed more than 40 years of data on the four major crimes the fbi classifies as violent — homicide, rape, robbery and assault — in 68 police jurisdictions with populations of 250,000 or greater. we obtained 2015 reports, which have yet to be released by the fbi, directly from 61 of them. we calculated the rate of crime in each category and for all violent crime, per 100,000 residents in the jurisdiction, based on the fbi’s estimated population for that year. we used the 2014 estimated population to calculate 2015 crime rates per capita.\n",
      "acknowledgements\n",
      "the crime data was acquired from the fbi uniform crime reporting program's \"offenses known and clearances by arrest\" database for the year in question, held at the national archives of criminal justice data. the data was compiled and analyzed by gabriel dance, tom meagher, and emily hopkins of the marshall project; the analysis was published as crime in context on 18 august 2016.\n",
      "context:\n",
      "the mass movement of uprooted people is a highly charged geopolitical issue. this data, gathered by the un high commissioner for refugees (unhcr), covers movement of displaced persons (asylum seekers, refugees, internally displaced persons (idp), stateless). also included are destination country responses to asylum petitions.\n",
      "content:\n",
      "this dataset includes 6 csv files covering:\n",
      "asylum monthly applications opened (asylum_seekers_monthly.csv)\n",
      "yearly progress through the refugee system (asylum_seekers.csv)\n",
      "refugee demographics (demographics.csv)\n",
      "yearly time series data on unhcr’s populations of concern (time_series.csv)\n",
      "yearly population statistics on refugees by residence and destination (persons_of_concern.csv)\n",
      "yearly data on resettlement arrivals, with or without unhcr assistance (resettlement.csv)\n",
      "acknowledgements:\n",
      "this dataset was gathered from unhcr. photo by ali tareq.\n",
      "inspiration:\n",
      "what are the most frequent destination countries for refugees? how has refugee flow changed? any trends that could predict future refugee patterns?\n",
      "context\n",
      "i wanted an easy way to share all the lending club data with others. unfortunately, the data on their site is fragmented into many smaller files. there is another lending club dataset on kaggle, but it hasn't been updated in years. it also doesn't include the rejected loans, which i put in here.\n",
      "i created a git repo for the code to create this data: https://github.com/nategeorge/preprocess_lending_club_data\n",
      "content\n",
      "the definitions for the fields are here, at the bottom of the page.\n",
      "unfortunately, there is a limit of 500mb for dataset files (so lame!), so i had to compress the files with gzip in the python pandas package.\n",
      "i cleaned the data a tiny bit: i removed %s from int_rate and revol_util, and deleted the url column.\n",
      "to load the data in python:\n",
      "import pandas as pd\n",
      "\n",
      "accept_df = pd.read_csv('../input/accepted_2007_to_2016.csv.gz', compression='gzip')\n",
      "reject_df = pd.read_csv('../input/rejected_2007_to_2016.csv.gz', compression='gzip')\n",
      "\n",
      "# too many columns to print the info summary out, so we need to force it\n",
      "print(accept_df.info(verbose=true, null_counts=true))\n",
      "in r:\n",
      "library(data.table)\n",
      "\n",
      "accepted_def &lt;- read.csv(gzfile('rejected_2007_to_2016.csv.gz'), na.strings='')\n",
      "acc_dt &lt;- as.data.table(accepted_def)\n",
      "rejected_def &lt;- read.csv(gzfile('accepted_2007_to_2016.csv.gz'), na.strings='')\n",
      "rej_dt &lt;- as.data.table(accepted_def)\n",
      "there are also separate csvs in the main input folder, but the only advantage over the lending club site is that the 2016 year is joined into one file instead of 4.\n",
      "inspiration\n",
      "i wanted to make this dataset easily available for others to use.\n",
      "many new websites and online tools have come into existence to support scholarly communication in all phases of the research workflow. to what extent researchers are using these and more traditional tools has been largely unknown. this 2015-2016 survey aimed to fill that gap.\n",
      "the survey captured information on tool usage for 17 research activities, stance towards open access and open science, and expectations of the most important development in scholarly communication. respondents’ demographics included research roles, country of affiliation, research discipline and year of first publication. the online survey employed an open, non-probability sample. a largely self-selected group of 20,663 researchers, librarians, editors, publishers and other groups involved in research took the survey, which was available in seven languages. the survey was open from may 10, 2015 to february 10, 2016.\n",
      "this data set contains:\n",
      "full raw (anonymized) and cleaned data files (csv, each file containing 20,663 records and 178 variables)\n",
      "variable lists for raw and cleaned data files (csv)\n",
      "readme file (txt)\n",
      "the dataset is also deposited in zenodo: http://dx.doi.org/10.5281/zenodo.49583\n",
      "the full description of survey methodology is in a data publication in f1000 research: http://dx.doi.org/10.12688/f1000research.8414.1\n",
      "more information on the project this survey is part of can be found here: http://101innovations.wordpress.com\n",
      "[edited to add] for quick visual exploration of the data, check out the interactive dashboard on silk: http://dashboard101innovations.silk.co/\n",
      "contact:\n",
      "jeroen bosman: http://orcid.org/0000-0001-5796-2727 / j.bosman@uu.nl\n",
      "bianca kramer: http://orcid.org/0000-0002-5965-6560 / b.m.r.kramer@uu.nl\n",
      "context\n",
      "this dataset includes information about gun-death in the us in the years 2012-2014.\n",
      "content\n",
      "the data includes data regarding the victim's age, sex, race, education, intent, time (month and year) and place of death, and whether or not police was at the place of death.\n",
      "acknowledgements\n",
      "i came across this thanks to fivethirtyeight's gun deaths in america project. the data originated from the cdc, and can be found here.\n",
      "context\n",
      "bible (or biblia in greek) is a collection of sacred texts or scriptures that jews and christians consider to be a product of divine inspiration and a record of the relationship between god and humans (wiki). and for data mining purpose, we could do many things using bible scriptures as for nlp, classification, sentiment analysis and other particular topics between data science and theology perspective.\n",
      "content\n",
      "here you will find the following bible versions in sql, sqlite, xml, csv, and json format:\n",
      "american standard-asv1901 (asv)\n",
      "bible in basic english (bbe)\n",
      "darby english bible (darby)\n",
      "king james version (kjv)\n",
      "webster's bible (wbt)\n",
      "world english bible (web)\n",
      "young's literal translation (ylt)\n",
      "each verse is accessed by a unique key, the combination of the book+chapter+verse id.\n",
      "example:\n",
      "genesis 1:1 (genesis chapter 1, verse 1) = 01001001 (01 001 001)\n",
      "exodus 2:3 (exodus chapter 2, verse 3) = 02002003 (02 002 003)\n",
      "the verse-id system is used for faster, simplified queries.\n",
      "for instance: 01001001 - 02001005 would capture all verses between genesis 1:1 through exodus 1:5.\n",
      "written simply:\n",
      "select * from bible.t_asv where id between 01001001 and 02001005\n",
      "coordinating tables\n",
      "there is also a number-to-book key (key_english table), a cross-reference list (cross_reference table), and a bible key containing meta information about the included translations (bible_version_key table). see below sql table layout. these tables work together providing you a great basis for a bible-reading and cross-referencing app. in addition, each book is marked with a particular genre, mapping in the number-to-genre key (key_genre_english table) and common abbreviations for each book can be looked up in the abbreviations list (key_abbreviations_english table). while its expected that your programs would use the verse-id system, book #, chapter #, and verse # columns have been included in the bible versions tables.\n",
      "a valuable cross-reference table\n",
      "a very special and valuable addition to these databases is the extensive cross-reference table. it was created from the project at http://www.openbible.info/labs/cross-references/. see .txt version included from http://www.openbible.info website. its extremely useful in bible study for discovering related scriptures. for any given verse, you simply query vid (verse id), and a list of rows will be returned. each of those rows has a rank (r) for relevance, start-verse (sv), and end verse (ev) if there is one.\n",
      "basic web interaction\n",
      "the web folder contains two php files. edit the first few lines of index.php to match your server's settings. place these in a folder on your webserver. the references search box can be multiple comma separated values. (i.e. john 3:16, rom 3:23, 1 jn 1:9, romans 10:9-10) you can also directly link to a verse by altering the uri: [http://localhost/index.php?b=john 3:16, rom 3:23, 1 jn 1:9, romans 10:9-10](http://localhost/index.php?b=john 3:16, rom 3:23, 1 jn 1:9, romans 10:9-10)\n",
      "bible-mysql.sql (mysql) is the main database and most feature-oriented due to contributions from developers. it is suggested you use that for most things, or at least convert the information from it.\n",
      "cross_references-mysql.sql (mysql) is the cross-reference table. it has been separated to become an optional feature. this is converted from the project at http://www.openbible.info/labs/cross-references/.\n",
      "bible-sqlite.db (sqlite) is a basic simplified database for simpler applications (includes cross-references too).\n",
      "cross_references.txt is the source cross-reference file obtained from http://www.openbible.info/labs/cross-references/\n",
      "in csv folder, you will find (same list order with the other formats):\n",
      "bible_version_key.csv\n",
      "key_abbreviations_english.csv\n",
      "key_english.csv\n",
      "key_genre_english.csv\n",
      "t_asv.csv, t_bbe.csv, t_dby.csv, t_wbt.csv, t_web.csv, t_ylt.csv\n",
      "acknowledgements\n",
      "in behalf of the original contributors (github)\n",
      "inspirations\n",
      "wordnet as an additional semantic resource for nlp\n",
      "this data set used in the coil 2000 challenge contains information on customers of an insurance company. the data consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. the data was collected to answer the following question: can you predict who would be interested in buying a caravan insurance policy and give an explanation why?\n",
      "acknowledgements\n",
      "disclaimer\n",
      "this dataset is owned and supplied by the dutch datamining company sentient machine research, and is based on real world business data. you are allowed to use this dataset and accompanying information for non commercial research and education purposes only. it is explicitly not allowed to use this dataset for commercial education or demonstration purposes. for any other use, please contact peter van der putten, info@smr.nl.\n",
      "this dataset has been used in the coil challenge 2000 datamining competition. for papers describing results on this dataset, see the tic 2000 homepage: http://www.wi.leidenuniv.nl/~putten/library/cc2000/\n",
      "please cite/acknowledge:\n",
      "p. van der putten and m. van someren (eds) . coil challenge 2000: the insurance company case. published by sentient machine research, amsterdam. also a leiden institute of advanced computer science technical report 2000-09. june 22, 2000.\n",
      "the data\n",
      "originally, this dataset was broken into two parts: the training set and the evaluation set. as this was a competition, the responses to the evaluation set were not given as part of the original release; they were, however, released after the end of the competition in a separate file. this dataset contains all three of these files, combined into one.\n",
      "the field origin in the caravan-insurance-challenge.csv file has the values train and test, corresponding to the training and evaluation sets, respectively. to simulate the original challenge, you can ignore the test rows, and test your model's prediction on those observations once you've trained only on the training set.\n",
      "each observation corresponds to a postal code. variables beginning with m refer to demographic statistics of the postal code, while variables beginning with p and a (as well as caravan, the target variable) refer to product ownership and insurance statistics in the postal code.\n",
      "the data file contains the following fields:\n",
      "origin: train or test, as described above\n",
      "mostype: customer subtype; see l0\n",
      "maanthui: number of houses 1 - 10\n",
      "mgemomv: avg size household 1 - 6\n",
      "mgemleef: avg age; see l1\n",
      "moshoofd: customer main type; see l2\n",
      "** percentages in each group, per postal code (see l3)**:\n",
      "mgodrk: roman catholic\n",
      "mgodpr: protestant ...\n",
      "mgodov: other religion\n",
      "mgodge: no religion\n",
      "mrelge: married\n",
      "mrelsa: living together\n",
      "mrelov: other relation\n",
      "mfalleen: singles\n",
      "mfgekind: household without children\n",
      "mfwekind: household with children\n",
      "moplhoog: high level education\n",
      "moplmidd: medium level education\n",
      "mopllaag: lower level education\n",
      "mberhoog: high status\n",
      "mberzelf: entrepreneur\n",
      "mberboer: farmer\n",
      "mbermidd: middle management\n",
      "mberarbg: skilled labourers\n",
      "mberarbo: unskilled labourers\n",
      "mska: social class a\n",
      "mskb1: social class b1\n",
      "mskb2: social class b2\n",
      "mskc: social class c\n",
      "mskd: social class d\n",
      "mhhuur: rented house\n",
      "mhkoop: home owners\n",
      "maut1: 1 car\n",
      "maut2: 2 cars\n",
      "maut0: no car\n",
      "mzfonds: national health service\n",
      "mzpart: private health insurance\n",
      "minkm30: income < 30.000\n",
      "mink3045: income 30-45.000\n",
      "mink4575: income 45-75.000\n",
      "mink7512: income 75-122.000\n",
      "mink123m: income >123.000\n",
      "minkgem: average income\n",
      "mkoopkla: purchasing power class\n",
      "** total number of variable in postal code (see l4)**:\n",
      "pwapart: contribution private third party insurance\n",
      "pwabedr: contribution third party insurance (firms) ...\n",
      "pwaland: contribution third party insurane (agriculture)\n",
      "ppersaut: contribution car policies\n",
      "pbesaut: contribution delivery van policies\n",
      "pmotsco: contribution motorcycle/scooter policies\n",
      "pvraaut: contribution lorry policies\n",
      "paanhang: contribution trailer policies\n",
      "ptractor: contribution tractor policies\n",
      "pwerkt: contribution agricultural machines policies\n",
      "pbrom: contribution moped policies\n",
      "pleven: contribution life insurances\n",
      "ppersong: contribution private accident insurance policies\n",
      "pgezong: contribution family accidents insurance policies\n",
      "pwaoreg: contribution disability insurance policies\n",
      "pbrand: contribution fire policies\n",
      "pzeilpl: contribution surfboard policies\n",
      "pplezier: contribution boat policies\n",
      "pfiets: contribution bicycle policies\n",
      "pinboed: contribution property insurance policies\n",
      "pbystand: contribution social security insurance policies\n",
      "awapart: number of private third party insurance 1 - 12\n",
      "awabedr: number of third party insurance (firms) ...\n",
      "awaland: number of third party insurance (agriculture)\n",
      "apersaut: number of car policies\n",
      "abesaut: number of delivery van policies\n",
      "amotsco: number of motorcycle/scooter policies\n",
      "avraaut: number of lorry policies\n",
      "aaanhang: number of trailer policies\n",
      "atractor: number of tractor policies\n",
      "awerkt: number of agricultural machines policies\n",
      "abrom: number of moped policies\n",
      "aleven: number of life insurances\n",
      "apersong: number of private accident insurance policies\n",
      "agezong: number of family accidents insurance policies\n",
      "awaoreg: number of disability insurance policies\n",
      "abrand: number of fire policies\n",
      "azeilpl: number of surfboard policies\n",
      "aplezier: number of boat policies\n",
      "afiets: number of bicycle policies\n",
      "ainboed: number of property insurance policies\n",
      "abystand: number of social security insurance policies\n",
      "caravan: number of mobile home policies 0 - 1\n",
      "keys (l1 - l4)\n",
      "l0: customer subtype\n",
      "1: high income, expensive child\n",
      "2: very important provincials\n",
      "3: high status seniors\n",
      "4: affluent senior apartments\n",
      "5: mixed seniors\n",
      "6: career and childcare\n",
      "7: dinki's (double income no kids)\n",
      "8: middle class families\n",
      "9: modern, complete families\n",
      "10: stable family\n",
      "11: family starters\n",
      "12: affluent young families\n",
      "13: young all american family\n",
      "14: junior cosmopolitan\n",
      "15: senior cosmopolitans\n",
      "16: students in apartments\n",
      "17: fresh masters in the city\n",
      "18: single youth\n",
      "19: suburban youth\n",
      "20: etnically diverse\n",
      "21: young urban have-nots\n",
      "22: mixed apartment dwellers\n",
      "23: young and rising\n",
      "24: young, low educated\n",
      "25: young seniors in the city\n",
      "26: own home elderly\n",
      "27: seniors in apartments\n",
      "28: residential elderly\n",
      "29: porchless seniors: no front yard\n",
      "30: religious elderly singles\n",
      "31: low income catholics\n",
      "32: mixed seniors\n",
      "33: lower class large families\n",
      "34: large family, employed child\n",
      "35: village families\n",
      "36: couples with teens 'married with children'\n",
      "37: mixed small town dwellers\n",
      "38: traditional families\n",
      "39: large religous families\n",
      "40: large family farms\n",
      "41: mixed rurals\n",
      "l1: average age keys:\n",
      "1: 20-30 years 2: 30-40 years 3: 40-50 years 4: 50-60 years 5: 60-70 years 6: 70-80 years\n",
      "l2: customer main type keys:\n",
      "1: successful hedonists\n",
      "2: driven growers\n",
      "3: average family\n",
      "4: career loners\n",
      "5: living well\n",
      "6: cruising seniors\n",
      "7: retired and religeous\n",
      "8: family with grown ups\n",
      "9: conservative families\n",
      "10: farmers\n",
      "l3: percentage keys:\n",
      "0: 0%\n",
      "1: 1 - 10%\n",
      "2: 11 - 23%\n",
      "3: 24 - 36%\n",
      "4: 37 - 49%\n",
      "5: 50 - 62%\n",
      "6: 63 - 75%\n",
      "7: 76 - 88%\n",
      "8: 89 - 99%\n",
      "9: 100%\n",
      "l4: total number keys:\n",
      "0: 0\n",
      "1: 1 - 49\n",
      "2: 50 - 99\n",
      "3: 100 - 199\n",
      "4: 200 - 499\n",
      "5: 500 - 999\n",
      "6: 1000 - 4999\n",
      "7: 5000 - 9999\n",
      "8: 10,000 - 19,999\n",
      "9: >= 20,000\n",
      "dataset with the text of 10% of questions and answers from the stack overflow programming q&a website.\n",
      "this is organized as three tables:\n",
      "questions contains the title, body, creation date, closed date (if applicable), score, and owner id for all non-deleted stack overflow questions whose id is a multiple of 10.\n",
      "answers contains the body, creation date, score, and owner id for each of the answers to these questions. the parentid column links back to the questions table.\n",
      "tags contains the tags on each of these questions\n",
      "datasets of all r questions and all python questions are also available on kaggle, but this dataset is especially useful for analyses that span many languages.\n",
      "example projects include:\n",
      "identifying tags from question text\n",
      "predicting whether questions will be upvoted, downvoted, or closed based on their text\n",
      "predicting how long questions will take to answer\n",
      "license\n",
      "all stack overflow user contributions are licensed under cc-by-sa 3.0 with attribution required.\n",
      "about this data\n",
      "this is a list of 1,000 hotels and their reviews provided by datafiniti's business database. the dataset includes hotel location, name, rating, review data, title, username, and more.\n",
      "what you can do with this data\n",
      "you can use this data to compare hotel reviews on a state-by-state basis; experiment with sentiment scoring and other natural language processing techniques. the review data lets you correlate keywords in the review text with ratings. e.g.:\n",
      "what are the bottom and top states for hotel reviews by average rating?\n",
      "what is the correlation between a state’s population and their number of hotel reviews?\n",
      "what is the correlation between a state’s tourism budget and their number of hotel reviews?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "content\n",
      "the human development index (hdi) is a summary measure of achievements in key dimensions of human development: a long and healthy life, access to knowledge, and a decent standard of living. the hdi is the geometric mean of normalized indices for each of the three dimensions. the health dimension is assessed by life expectancy at birth, the education dimension is measured by mean of years of education for adults aged 25 years and more and expected years of education for children, and the standard of living dimension is measured by gross national income per capita. the inequality-adjusted human development index (ihdi) adjusts the hdi for inequality in the distribution of each dimension across the population.\n",
      "the gender development index (gdi) measures gender inequalities in achievement in three basic dimensions of human development: health, measured by female and male life expectancy at birth; education, measured by female and male expected years of education for children and female and male mean years of education for adults ages 25 and older; and command over economic resources, measured by female and male estimated earned income. the gender inequality index (gii) reflects gender-based disadvantage in three dimensions—reproductive health, empowerment, and the labour market—for as many countries as data of reasonable quality allow. it shows the loss in potential human development due to inequality between female and male achievements in these dimensions.\n",
      "the multidimensional poverty index (mpi) identifies multiple deprivations at the household level in education, health, and standard of living as indicators of poverty. it uses micro data from household surveys, and — unlike the ihdi — all the indicators needed to construct the measure must come from the same survey.\n",
      "context\n",
      "the ecological footprint measures the ecological assets that a given population requires to produce the natural resources it consumes (including plant-based food and fiber products, livestock and fish products, timber and other forest products, space for urban infrastructure) and to absorb its waste, especially carbon emissions. the footprint tracks the use of six categories of productive surface areas: cropland, grazing land, fishing grounds, built-up (or urban) land, forest area, and carbon demand on land.\n",
      "a nation’s biocapacity represents the productivity of its ecological assets, including cropland, grazing land, forest land, fishing grounds, and built-up land. these areas, especially if left unharvested, can also absorb much of the waste we generate, especially our carbon emissions.\n",
      "both the ecological footprint and biocapacity are expressed in global hectares — globally comparable, standardized hectares with world average productivity.\n",
      "if a population’s ecological footprint exceeds the region’s biocapacity, that region runs an ecological deficit. its demand for the goods and services that its land and seas can provide — fruits and vegetables, meat, fish, wood, cotton for clothing, and carbon dioxide absorption — exceeds what the region’s ecosystems can renew. a region in ecological deficit meets demand by importing, liquidating its own ecological assets (such as overfishing), and/or emitting carbon dioxide into the atmosphere. if a region’s biocapacity exceeds its ecological footprint, it has an ecological reserve.\n",
      "acknowledgements\n",
      "the ecological footprint measure was conceived by mathis wackernagel and william rees at the university of british columbia. ecological footprint data was provided by the global footprint network.\n",
      "inspiration\n",
      "is your country running an ecological deficit, consuming more resources than it can produce per year? which countries have the greatest ecological deficits or reserves? do they consume less or produce more than the average country? when will earth overshoot day, the day on the calendar when humanity has used one year of natural resources, occur in 2017?\n",
      "context\n",
      "coming soon\n",
      "content\n",
      "coming soon\n",
      "acknowledgements\n",
      "this data is taken from coinmarketcap and it is free to use the data. https://coinmarketcap.com/\n",
      "warning\n",
      "実際の取引にこの情報を使うときは十分ご注意ください。弊社およびコミュニティメンバーは損失の責任を取ることができません。\n",
      "zip code data show selected income and tax items classified by state, zip code, and size of adjusted gross income. data are based on individual income tax returns filed with the irs. the data include items, such as:\n",
      "number of returns, which approximates the number of households\n",
      "number of personal exemptions, which approximates the population\n",
      "adjusted gross income\n",
      "wages and salaries\n",
      "dividends before exclusion\n",
      "interest received\n",
      "content\n",
      "for details of the exact fields available, please see the field_definitions.csv. please note that the exact fields available can change from year to year, this definitions file was generated by retaining only the most recent year's entry from the years which had pdf manuals. the associated irs form numbers are the most likely to change over time.\n",
      "acknowledgements\n",
      "this data was generated by the internal revenue service.\n",
      "description\n",
      "over 8 million github issue titles and descriptions from 2017. prepared from instructions at how to create data products that are magical using sequence-to-sequence models.\n",
      "original source\n",
      "the data was adapted from github data accessible from github archive. the constructocat image is from https://octodex.github.com/constructocat-v2.\n",
      "license\n",
      "mit license\n",
      "copyright (c) 2018 david shinn\n",
      "permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"software\"), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\n",
      "the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.\n",
      "the software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.\n",
      "context\n",
      "favicons are the (usually tiny) image files that browsers may use to represent websites in tabs, in the url bar, or for bookmarks. kaggle, for example, uses an image of a blue lowercase \"k\" as its favicon. this dataset contains about 360,000 favicons from popular websites.\n",
      "content and acknowledgements\n",
      "these favicons were scraped in july 2016. i wrote a crawler that went through alexa's top 1 million sites, and made a request for 'favicon.ico' at the site root. if i got a 200 response code, i saved the result as ${site_url}.ico. for domains that were identical but for the tld (e.g. google.com, google.ca, google.jp...), i scraped only one favicon. my scraping/cleaning code is on github here.\n",
      "of 1m sites crawled, 540k responded with a 200 code. the dataset has 360k images, which were the remains after filtering out:\n",
      "empty files (-140k)\n",
      "non-image files, according to the file command (-40k). these mostly had type html, ascii, or utf-*.\n",
      "corrupt/malformed image files - i.e. those that were sufficiently messed up that imagemagick failed to parse them. (-1k)\n",
      "the remaining files are exactly as i received them from the site. they are mostly ico files, with the most common sizes being 16x16, 32x32, and 48x48. but there's a long tail of more exotic formats and sizes (there is at least one person living among us who thought that 88x31 was a fine size for a favicon).\n",
      "the favicon files are divided among 6 zip files, full-0.zip, full-1.zip... full-5.zip. (if you wish to download the full dataset as a single tarball, you can do so from the internet archive)\n",
      "favicon_metadata.csv is a csv file with one row per favicon in the dataset. the split_index says which of the zip files the image landed in. for an example of loading and interacting with particular favicons in a kernel context, check out the favicon helper functions kernel.\n",
      "as mentioned above, the full dataset is a dog's breakfast of different file formats and dimensions. i've created 'standardized' subsets of the data that may be easier to work with (particularly for machine learning applications, where it's necessary to have fixed dimensions).\n",
      "16_16.tar.gz is a tarball containing all 16x16 favicons in the dataset, converted to png. it has 290k images. ico is a container format, and many of the ico files in the raw dataset contain several versions of the same favicon at different resolutions. 16x16 favicons that were stuffed together in an ico file with images of other sizes are included in this set. but i did no resizing - if a favicon has no 'native' 16x16 version, it isn't in this set.\n",
      "16_16_distinct.tar.gz is identical to the above, but with 70k duplicate or near-duplicate images removed. there are a small number of commonly repeated favicons like the blogger \"b\" that occur thousands of times, which could be an annoyance depending on the use case - e.g. a generative model might get stuck in a local maximum of spitting out blogger bs.\n",
      "alexa's top 1-million list includes 'adult' sites, so some urls and favicons may be nsfw or offensive. (it's pretty hard to make a credible depiction of nudity in 256 pixels, but there are some occasional attempts.)\n",
      "inspiration\n",
      "i hope this dataset might be especially useful for small-scale deep learning experiments. scaling photographs down to 16x16 would render many of them unintelligible, but these favicons were born tiny. the 16_16 fold has more instances than mnist, and the images are even smaller! (though, unlike mnist, most of the images in this dataset are not grayscale.)\n",
      "if you liked this, you should also check out the recently released large logo dataset. they've currently made available 550k favicons resized to 32x32. their data was collected more recently, and their scraping process was more robust, so their dataset should probably be preferred (though you might still want to use this one if you need the raw favicon files, or if you prefer to use 16x16 non-resized images).\n",
      "context\n",
      "some camera enthusiast went and described 1,000 cameras based on 13 properties!\n",
      "content\n",
      "row one describes the datatype for each column and can probably be removed.\n",
      "the 13 properties of each camera:\n",
      "model\n",
      "release date\n",
      "max resolution\n",
      "low resolution\n",
      "effective pixels\n",
      "zoom wide (w)\n",
      "zoom tele (t)\n",
      "normal focus range\n",
      "macro focus range\n",
      "storage included\n",
      "weight (inc. batteries)\n",
      "dimensions\n",
      "price\n",
      "acknowledgements\n",
      "these datasets have been gathered and cleaned up by petra isenberg, pierre dragicevic and yvonne jansen. the original source can be found here.\n",
      "this dataset has been converted to csv.\n",
      "context:\n",
      "as the price of installing solar has gotten less expensive, more homeowners are turning to it as a possible option for decreasing their energy bill. we want to make installing solar panels easy and understandable for anyone. project sunroof puts google's expansive data in mapping and computing resources to use, helping calculate the best solar plan for you.\n",
      "content:\n",
      "see metadata for indepth description. data is at census-tract level. project sunroof computes how much sunlight hits your roof in a year. it takes into account: google's database of imagery and maps 3d modeling of your roof shadows cast by nearby structures and trees all possible sun positions over the course of a year historical cloud and temperature patterns that might affect solar energy production\n",
      "acknowledgements:\n",
      "data was compiled by google project sunroof. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too.\n",
      "inspiration:\n",
      "which tracts have the highest potential possible coverage? carbon offsets?\n",
      "which tracts have the highest estimated solar panel utilization? as a percent of carbon offsets?\n",
      "if you want more energy data, check out 30 years of european wind generation and 30 years of european solar generation.\n",
      "context\n",
      "on the morning of 10 january 2017, opplysningsrådet for veitrafikken (ofv), norwegian road association, held a business breakfast for its member organizations, where they presented the annual presentation under the title \"car year 2016. status and trends\" (bilåret 2016 – status og trender). among the highlights for the year, ofv reported all-time-high sales of electric cars, with fully electric and plug-in hybrid cars accounting for 40,2% of all new car sales (compare to 7.4% for sweden and 3.6% for denmark). no other country in the world has this level of popularity of battery-equipped vehicles! in november 2016, 12 out of 15 most popular cars sold in norway were either hybrids of fully electric vehicles with bwm-i3 snapping the title as the most popular car in norway, ahead of undisputed leader of the last decade vw golf (including egolf), according to bilnorge.no. among 10 most popular cars for the year, ofv reported, there was only one(!) fossil fuel vehicle.\n",
      "ofv makes annual forecast of new passenger car sales. short summary of their methodology:\n",
      "based on ofv statistics over several years\n",
      "taking into account the actual monthly figures for the last four years\n",
      "actual same-month sales for the previous year is combined with the average for the eight previous months, weighed by the month's proportion in a year, adjusted by year's actual sales compared with those of the last year.\n",
      "ofv forecast for 2016 was 157 500 new passenger cars. actual sales were 154 603 cars. applying the same model for 2017, ofv forecasts 152 400 new passenger cars to be sold in norway.\n",
      "content\n",
      "dataset includes two tables:\n",
      "1) monthly sales of new passenger cars by make (manufacturer brand) - norway_new_car_sales_by_make.csv\n",
      "year - year of sales\n",
      "month - month of sales\n",
      "make - car make (e.g. volkswagen, toyota, tesla)\n",
      "quantity - number of units sold\n",
      "pct - percent share in monthly total\n",
      "2) monthly summary of top-20 most popular models (by make and model) - norway_new_car_sales_by_model.csv\n",
      "year - year of sales\n",
      "month - month of sales\n",
      "make - car make (e.g. volkswagen, toyota, tesla)\n",
      "model - car model (e.g. bmw-i3, volkswagen golf, tesla s75)\n",
      "quantity - number of units sold\n",
      "pct - percent share in monthly total\n",
      "3) summary stats for car sales in norway by month - norway_new_car_sales_by_month.csv\n",
      "year - year of sales\n",
      "month - month of sales\n",
      "quantity - total number of units sold\n",
      "quantity_yoy - change yoy in units\n",
      "import - total number of units imported (used cars)\n",
      "import_yoy - change yoy in units\n",
      "used - total number of units owner changes inside the country (data available from 2012)\n",
      "used_yoy - change yoy in units\n",
      "avg_co2 - average co2 emission of all cars sold in a given month (in g/km)\n",
      "bensin_co2 - average co2 emission of bensin-fueled cars sold in a given month (in g/km)\n",
      "diesel_co2 - average co2 emission of diesel-fueled cars sold in a given month (in g/km)\n",
      "quantity_diesel - number of diesel-fueled cars sold in the country in a given month\n",
      "diesel_share - share of diesel cars in total sales (quantity_diesel / quantity)\n",
      "diesel_share_ly - share of diesel cars in total sales a year ago\n",
      "quantity_hybrid - number of new hybrid cars sold in the country (both phev and bv)\n",
      "quantity_electric - number of new electric cars sold in the country (zero emission vehicles)\n",
      "import_electric - number of used electric cars imported to the country (zero emission vehicles)\n",
      "note: the numbers on sales of hybrid and electric cars is unavailable prior to 2011.\n",
      "data is complied from monthly tables published on ofv website (example here). additional datapoints added from summary tables published on dinside.no\n",
      "acknowledgements\n",
      "opplysningsrådet for veitrafikken (ofv) is a politically independent membership organization that works to get politicians and authorities to build safer and more efficient roads in norway. the organization has about 60 members, representing different types of road users. members are leading players in road safety, car owner associations, public transportation companies, shippers, car dealers, oil companies, banking, finance and insurance, road builders and general contractors.\n",
      "site: http://www.ofvas.no and http://www.ofv.no\n",
      "monthly summary statistics and market news: http://www.dinside.no/emne/bilsalget and http://statistikk.ofv.no/ofv_bilsalg_small.asp\n",
      "detailed sales per model: http://www.ofvas.no/co2-utslippet/category406.html (using http://www.newocr.com/)\n",
      "inspiration\n",
      "1) how did norway get here? when did they start on the journey towards electric-powered vehicles and what might have contributed? 2) did you now that until recently (september 2016), norway has been second most important market for tesla motors (after us)? 3) can you beat the forecast accuracy of ofv for 2016 and produce a better estimate for 2017?\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "to better understand the imports and exports by india and how it changed in 3 years.\n",
      "content\n",
      "import and export data available by principle commodity and country wise for 3 years from apr'2014 to mar'2017.\n",
      "column descriptions\n",
      "pc_code: integer, principal commodity code\n",
      "pc: string, principal commodity name\n",
      "unit: string, measurement of quantity\n",
      "country_code: integer, country code\n",
      "country_name: string, country name\n",
      "quantity: integer, quantify of export or import\n",
      "value: integer, monetary valeu of the quantity (in million usd)\n",
      "acknowledgements\n",
      "ministry of commerce and industry, govt of india has published these datasets in open govt data platform india portal under govt. open data license - india.\n",
      "inspiration\n",
      "some of questions i would like to be answered are\n",
      "top countries by growth percentage.\n",
      "top commodity by quantity or value.\n",
      "yoy growth of export and import.\n",
      "context\n",
      "according to the oregonian hundreds of national guard armories across the u.s. may have been contaminated with lead from indoor firing ranges. it was reported that areas populated by children under 7 years of age should have less than 40 micrograms of lead per square foot.\n",
      "content\n",
      "the oregonian collected over 23,000 pages of public records following a freedom of information act request. the dataset covers armory inspections conducted since 2012 and may facilitate investigation of lead contamination in the u.s.\n",
      "acknowledgements\n",
      "the data assembly process is described by melissa lewis here.\n",
      "inspiration\n",
      "this dataset can be used to conduct research in the realm of public health. it will be especially useful if 1) you know about health effects of exposure to lead in relatively short terms periods; 2) you are able to find relevant health data to conduct a study on lead poisoning.\n",
      "context\n",
      "cartolafc is the most popular fantasy football in brazil. before each round of the brazilian football league, players choose which athletes they want for their teams, and they score points based on their real-life performances.\n",
      "content\n",
      "data is divided in 7 kinds of files:\n",
      "athletes (atletas)\n",
      "\"atleta_id\": id,\n",
      "\"nome\": athlete's full name,\n",
      "\"apelido\": athlete's nickname\n",
      "clubs (clubes)\n",
      "\"id\": id,\n",
      "\"nome\": club's name,\n",
      "\"abreviacao\": name abbreviation,\n",
      "\"slug\": used for some api calls\n",
      "matches (partidas)\n",
      "\"rodada_id\": current round,\n",
      "\"clube_casa_id\": home team id,\n",
      "\"clube_visitante_id\": away team id,\n",
      "\"clube_casa_posicao\": home team's position on the league,\n",
      "\"clube_visitante_posicao\": away team's position on the league,\n",
      "\"aproveitamento_mandante\": home team's outcome on the last five matches (d: loss, e: draw, v: victory),\n",
      "\"aproveitamento_visitante\": away team's outcome on the last five matches (d: loss, e: draw, v: victory),\n",
      "\"placar_oficial_mandante\": home team's score,\n",
      "\"placar_oficial_visitante\": away team's score,\n",
      "\"partida_data\": match date,\n",
      "\"local\": stadium,\n",
      "\"valida\": match valid for scoring\n",
      "scouts\n",
      "\"atleta_id\": reference to athlete,\n",
      "\"rodada_id\": current round,\n",
      "\"clube_id\": reference to club,\n",
      "\"posicao_id\": reference to position,\n",
      "\"status_id\": reference to status,\n",
      "\"pontos_num\": points scored on current round,\n",
      "\"preco_num\": current price,\n",
      "\"variacao_num\": price variation from previous round,\n",
      "\"media_num\": average points per played round,\n",
      "\"jogos_num\": number of matches played,\n",
      "\"fs\": suffered fouls,\n",
      "\"pe\": missed passes,\n",
      "\"a\": assistances,\n",
      "\"ft\": shots on the post,\n",
      "\"fd\": defended shots,\n",
      "\"ff\": shots off target,\n",
      "\"g\": goals,\n",
      "\"i\": offsides,\n",
      "\"pp\": missed penalties,\n",
      "\"rb\": successful tackes,\n",
      "\"fc\": fouls commited,\n",
      "\"gc\": own goals,\n",
      "\"ca\": yellow cards,\n",
      "\"cv\": red cards,\n",
      "\"sg\": clean sheets (only defenders),\n",
      "\"dd\": difficult defenses (only goalies),\n",
      "\"dp\": defended penalties (only goalies),\n",
      "\"gs\": suffered goals (only goalies)\n",
      "positions (posicoes)\n",
      "\"id\": id,\n",
      "\"nome\": name,\n",
      "\"abreviacao\": abbreviation\n",
      "status\n",
      "\"id\": id,\n",
      "\"nome\": name\n",
      "points (pontuacao)\n",
      "\"abreviacao\": abbreviation,\n",
      "\"nome\": name,\n",
      "\"pontuacao\": points earned for respective scout\n",
      "acknowledgements\n",
      "the datasets from 2014 to 2016 were taken from here: https://github.com/thevtm/cartolafcdados.\n",
      "data from 2017 until round 11 was taken from this repo: https://github.com/henriquepgomide/cartola.\n",
      "from 2017 round 12 and on, i've been extracting the data from cartolafc's api (which is not officially public).\n",
      "inspiration\n",
      "it would be interesting to see analyses on which factors make an athlete or team more likely to score points, and also predictive models for future scores.\n",
      "falls among the elderly is an important health issue. fall detection and movement tracking are therefore instrumental in addressing this issue. this paper responds to the challenge of classifying different movements as a part of a system designed to fulfill the need for a wearable device to collect data for fall and near-fall analysis. four different fall trajectories (forward, backward, left and right), three normal activities (standing, walking and lying down) and near-fall situations are identified and detected.\n",
      "falls are a serious public health problem and possibly life threatening for people in fall risk groups. we develop an automated fall detection system with wearable motion sensor units fitted to the subjects’ body at six different positions. each unit comprises three tri-axial devices (accelerometer, gyroscope, and magnetometer/compass). fourteen volunteers perform a standardized set of movements including 20 voluntary falls and 16 activities of daily living (adls), resulting in a large dataset with 2520 trials. to reduce the computational complexity of training and testing the classifiers, we focus on the raw data for each sensor in a 4 s time window around the point of peak total acceleration of the waist sensor, and then perform feature extraction and reduction.\n",
      "we successfully distinguish falls from adls using six machine learning techniques (classifiers): the k-nearest neighbor (k-nn) classifier, least squares method (lsm), support vector machines (svm), bayesian decision making (bdm), dynamic time warping (dtw), and artificial neural networks (anns). we compare the performance and the computational complexity of the classifiers and achieve the best results with the k-nn classifier and lsm, with sensitivity, specificity, and accuracy all above 95%. these classifiers also have acceptable computational requirements for training and testing. our approach would be applicable in real-world scenarios where data records of indeterminate length, containing multiple activities in sequence, are recorded.\n",
      "if you are using this dataset don't forget to cite\n",
      "özdemir, ahmet turan, and billur barshan. “detecting falls with wearable sensors using machine learning techniques.” sensors (basel, switzerland) 14.6 (2014): 10691–10708. pmc. web. 23 apr. 2017.\n",
      "context:\n",
      "the open access series of imaging studies (oasis) is a project aimed at making mri data sets of the brain freely available to the scientific community. by compiling and freely distributing mri data sets, we hope to facilitate future discoveries in basic and clinical neuroscience. oasis is made available by the washington university alzheimer’s disease research center, dr. randy buckner at the howard hughes medical institute (hhmi)( at harvard university, the neuroinformatics research group (nrg) at washington university school of medicine, and the biomedical informatics research network (birn).\n",
      "content:\n",
      "cross-sectional mri data in young, middle aged, nondemented and demented older adults: this set consists of a cross-sectional collection of 416 subjects aged 18 to 96. for each subject, 3 or 4 individual t1-weighted mri scans obtained in single scan sessions are included. the subjects are all right-handed and include both men and women. 100 of the included subjects over the age of 60 have been clinically diagnosed with very mild to moderate alzheimer’s disease (ad). additionally, a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session.\n",
      "longitudinal mri data in nondemented and demented older adults: this set consists of a longitudinal collection of 150 subjects aged 60 to 96. each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. for each subject, 3 or 4 individual t1-weighted mri scans obtained in single scan sessions are included. the subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate alzheimer’s disease. another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.\n",
      "acknowledgements:\n",
      "when publishing findings that benefit from oasis data, please include the following grant numbers in the acknowledgements section and in the associated pubmed central submission: p50 ag05681, p01 ag03991, r01 ag021910, p20 mh071616, u24 rr0213\n",
      "inspiration:\n",
      "can you predict dementia? alzheimer’s?\n",
      "project description:\n",
      "1) data background\n",
      "in the data mining class, we had the opportunity to analyze data by performing data mining algorithms to a dataset. our dataset is from office of foreign labor certification (oflc). oflc is a division of the u.s. department of labor. the main duty of oflc is to assist the secretary of labor to enforce part of the immigration and nationality act (ina), which requires certain labor conditions exist before employers can hire foreign workers. h-1b is a visa category in the united states of america under the ina, section 101(a)(15)(h) which allows u.s. employers to employ foreign workers. the first step employer must take to hire a foreign worker is to file the labor condition application. in this project, we will analyze the data from the labor condition application.\n",
      "1.1) introduction to h1b dataset\n",
      "the h-1b dataset selected for this project contains data from employer’s labor condition application and the case certification determinations processed by the office of foreign labor certification (oflc) where the date of the determination was issues on or after october 1, 2016 and on or before june 30, 2017.\n",
      "the labor condition application (lca) is a document that a perspective h-1b employer files with u.s. department of labor employment and training administration (doleta) when it seeks to employ non-immigrant workers at a specific job occupation in an area of intended employment for not more than three years.\n",
      "1.2) goal of the project\n",
      "our goal for this project is to predict the case status of an application submitted by the employer to hire non-immigrant workers under the h-1b visa program. employer can hire non-immigrant workers only after their lca petition is approved. the approved lca petition is then submitted as part of the petition for a non-immigrant worker application for work authorizations for h-1b visa status.\n",
      "we want to uncover insights that can help employers understand the process of getting their lca approved. we will use weka software to run data mining algorithms to understand the relationship between attributes and the target variable.\n",
      "2)dataset information:\n",
      "a) source: office of foreign labor certification, u.s. department of labor employment and training administration\n",
      "b) list link: https://www.foreignlaborcert.doleta.gov/performancedata.cfm\n",
      "c) dataset type: record – transaction data\n",
      "d) number of attributes: 40\n",
      "e) number of instances: 528,147\n",
      "f) date created: july 2017\n",
      "3) attribute list:\n",
      "the detailed description of each attribute below is given in the record layout file available in the zip folder h1b disclosure dataset files.\n",
      "the h-1b dataset from oflc contained 40 attributes and 528,147 instances. the attributes are in the table below. the attributes highlighted bold were removed during the data cleaning process.\n",
      "1) case_number\n",
      "2)case_submitted\n",
      "3)decision_date\n",
      "4)visa_class\n",
      "5)employment_start_date\n",
      "6)employment_end_date\n",
      "7)employer_name\n",
      "8)employer_address\n",
      "9)employer_city\n",
      "10)employer_state\n",
      "11)employer_postal_code\n",
      "12)employer_country\n",
      "13)employer_province\n",
      "14)employer_phone\n",
      "15)employer_phone_ext\n",
      "16)agent_attorney_name\n",
      "17)agent_attorney_city\n",
      "18)agent_attorney_state\n",
      "19)job_title\n",
      "20)soc_code\n",
      "21)soc_name\n",
      "22)naics_code\n",
      "23)total_workers\n",
      "24)full_time_position\n",
      "25)prevailing_wage\n",
      "26)pw_unit_of_pay\n",
      "27)pw_source\n",
      "28)pw_source_year\n",
      "29)pw_source_other\n",
      "30)wage_rate_of_pay_from\n",
      "31)wage_rate_of_pay_to\n",
      "32)wage_unit_of_pay\n",
      "33)h-1b_dependent\n",
      "34) willful_violator\n",
      "35) worksite_city\n",
      "36)worksite_county\n",
      "37)worksite_state\n",
      "38)worksite_postal_code\n",
      "39)original_cert_date\n",
      "40)case_status* - __class attribute - to be predicted\n",
      "3.1) class attribute\n",
      "for the h-1b dataset our class attribute is ‘case_status’. there are 4 categories of case status. the values of case_status attributes are:\n",
      "1) certified\n",
      "2) certified_withdrawn\n",
      "3) withdrawn\n",
      "4) denied\n",
      "certified means the lca of an employer was approved. certified withdrawn means the case was withdrawn after it was certified by oflc. withdrawn means the case was withdrawn by the employer. denied means the case was denied oflc.\n",
      "context\n",
      "invasive ductal carcinoma (idc) is the most common subtype of all breast cancers. to assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the idc. as a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of idc inside of a whole mount slide.\n",
      "content\n",
      "the original dataset consisted of 162 whole mount slide images of breast cancer (bca) specimens scanned at 40x. from that, 277,524 patches of size 50 x 50 were extracted (198,738 idc negative and 78,786 idc positive). each patch’s file name is of the format: u_xx_yy_classc.png — > example 10253_idx5_x1351_y1101_class0.png . where u is the patient id (10253_idx5), x is the x-coordinate of where this patch was cropped from, y is the y-coordinate of where this patch was cropped from, and c indicates the class where 0 is non-idc and 1 is idc.\n",
      "acknowledgements\n",
      "the original files are located here: http://gleason.case.edu/webdata/jpi-dl-tutorial/idc_regular_ps50_idx5.zip citation: https://www.ncbi.nlm.nih.gov/pubmed/27563488 and http://spie.org/publications/proceedings/paper/10.1117/12.2043872\n",
      "inspiration\n",
      "breast cancer is the most common form of cancer in women, and invasive ductal carcinoma (idc) is the most common form of breast cancer. accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.\n",
      "context\n",
      "patients with liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles and drugs. this dataset was used to evaluate prediction algorithms in an effort to reduce burden on doctors.\n",
      "content\n",
      "this data set contains 416 liver patient records and 167 non liver patient records collected from north east of andhra pradesh, india. the \"dataset\" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). this data set contains 441 male patient records and 142 female patient records.\n",
      "any patient whose age exceeded 89 is listed as being of age \"90\".\n",
      "columns:\n",
      "age of the patient\n",
      "gender of the patient\n",
      "total bilirubin\n",
      "direct bilirubin\n",
      "alkaline phosphotase\n",
      "alamine aminotransferase\n",
      "aspartate aminotransferase\n",
      "total protiens\n",
      "albumin\n",
      "albumin and globulin ratio\n",
      "dataset: field used to split the data into two sets (patient with liver disease, or no disease)\n",
      "acknowledgements\n",
      "this dataset was downloaded from the uci ml repository:\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "inspiration\n",
      "use these patient records to determine which patients have liver disease and which ones do not.\n",
      "context\n",
      "the affordable care act (aca) is the name for the comprehensive health care reform law and its amendments which addresses health insurance coverage, health care costs, and preventive care. the law was enacted in two parts: the patient protection and affordable care act was signed into law on march 23, 2010 by president barack obama and was amended by the health care and education reconciliation act on march 30, 2010.\n",
      "content\n",
      "this dataset provides health insurance coverage data for each state and the nation as a whole, including variables such as the uninsured rates before and after obamacare, estimates of individuals covered by employer and marketplace healthcare plans, and enrollment in medicare and medicaid programs.\n",
      "acknowledgements\n",
      "the health insurance coverage data was compiled from the us department of health and human services and us census bureau.\n",
      "inspiration\n",
      "how has the affordable care act changed the rate of citizens with health insurance coverage? which states observed the greatest decline in their uninsured rate? did those states expand medicaid program coverage and/or implement a health insurance marketplace? what do you predict will happen to the nationwide uninsured rate in the next five years?\n",
      "content\n",
      "the united states census count (also known as the decennial census of population and housing) is a count of every resident of the us. the census occurs every 10 years and is conducted by the united states census bureau. census data is publicly available through the census website, but much of the data is available in summarized data and graphs. the raw data is often difficult to obtain, is typically divided by region, and it must be processed and combined to provide information about the nation as a whole. the united states census dataset includes nationwide population counts from the 2000 and 2010 censuses. data is broken out by gender, age and location using zip code tabular areas (zctas) and geoids. zctas are generalized representations of zip codes, and often, though not always, are the same as the zip code for an area. geoids are numeric codes that uniquely identify all administrative, legal, and statistical geographic areas for which the census bureau tabulates data. geoids are useful for correlating census data with other censuses and surveys.\n",
      "dataset description\n",
      "| geo_id | string | geo code | |-------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | minimum_age | integer | the minimum age in the age range. if null, this indicates the row as a total for male, female, or overall population. | | maximum_age | integer | the maximum age in the age range. if null, this indicates the row as having no maximum (such as 85 and over) or the row is a total of the male, female, or overall population. | | gender | string | male or female. if empty, the row is a total population summary. | | population | integer | the total count of the population for this segment. |\n",
      "acknowledgements\n",
      "this dataset was created by the united states census bureau.\n",
      "use this dataset with bigquery\n",
      "you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too: https://cloud.google.com/bigquery/public-data/international-census.\n",
      "context\n",
      "an asteroid's orbit is computed by finding the elliptical path about the sun that best fits the available observations of the object. that is, the object's computed path about the sun is adjusted until the predictions of where the asteroid should have appeared in the sky at several observed times match the positions where the object was actually observed to be at those same times. as more and more observations are used to further improve an object's orbit, we become more and more confident in our knowledge of where the object will be in the future.\n",
      "when the discovery of a new near earth asteroid is announced by the minor planet center, sentry automatically prioritizes the object for an impact risk analysis. if the prioritization analysis indicates that the asteroid cannot pass near the earth or that its orbit is very well determined, the computationally intensive nonlinear search for potential impacts is not pursued. if, on the other hand, a search is deemed necessary then the object is added to a queue of objects awaiting analysis. its position in the queue is determined by the estimated likelihood that potential impacts may be found.\n",
      "content\n",
      "sentry is a highly automated collision monitoring system that continually scans the most current asteroid catalog for possibilities of future impact with earth over the next 100 years. this dataset includes the sentry system's list of possible asteroid impacts with earth and their probability, in addition to a list of all known near earth asteroids and their characteristics.\n",
      "acknowledgements\n",
      "the asteroid orbit and impact risk data was collected by nasa's near earth object program at the jet propulsion laboratory (california institute of technology).\n",
      "inspiration\n",
      "during which year is earth at the highest risk of an asteroid impact? how do asteroid impact predictions change over time? which possible asteroid impact would be the most devastating, given the asteroid's size and speed?\n",
      "context:\n",
      "the environmental protection agency (epa) creates air quality trends using measurements from monitors located across the country. all of this data comes from epa’s air quality system (aqs). data collection agencies report their data to the epa via this system and it calculates several types of aggregate (summary) data for epa internal use.\n",
      "content:\n",
      "field descriptions:\n",
      "state code: the fips code of the state in which the monitor resides.\n",
      "county code: the fips code of the county in which the monitor resides.\n",
      "site num:a unique number within the county identifying the site.\n",
      "parameter code: the aqs code corresponding to the parameter measured by the monitor.\n",
      "poc: this is the “parameter occurrence code” used to distinguish different instruments that measure the same parameter at the same site.\n",
      "latitude: the monitoring site’s angular distance north of the equator measured in decimal degrees.\n",
      "longitude: the monitoring site’s angular distance east of the prime meridian measured in decimal degrees.\n",
      "datum: the datum associated with the latitude and longitude measures.\n",
      "parameter name: the name or description assigned in aqs to the parameter measured by the monitor. parameters may be pollutants or non-pollutants.\n",
      "sample duration: the length of time that air passes through the monitoring device before it is analyzed (measured). so, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). for continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).\n",
      "pollutant standard:a description of the ambient air quality standard rules used to aggregate statistics. (see description at beginning of document.)\n",
      "metric used: the base metric used in the calculation of the aggregate statistics presented in the remainder of the row. for example, if this is daily maximum, then the value in the mean column is the mean of the daily maximums.\n",
      "method name: a short description of the processes, equipment, and protocols used in gathering and measuring the sample.\n",
      "year: the year the annual summary data represents.\n",
      "units of measure: the unit of measure for the parameter. qad always returns data in the standard units for the parameter. submitters are allowed to report data in any unit and epa converts to a standard unit so that we may use the data in calculations.\n",
      "event type: indicates whether data measured during exceptional events are included in the summary. a wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. no events means no events occurred. events included means events occurred and the data from them is included in the summary. events excluded means that events occurred but data form them is excluded from the summary. concurred events excluded means that events occurred but only epa concurred exclusions are removed from the summary. if an event occurred for the parameter in question, the data will have multiple records for each monitor.\n",
      "observation count: the number of observations (samples) taken during the year.\n",
      "observation percent: the percent representing the number of observations taken with respect to the number scheduled to be taken during the year. this is only calculated for monitors where measurements are required (e.g., only certain parameters).\n",
      "completeness indicator: an indication of whether the regulatory data completeness criteria for valid summary data have been met by the monitor for the year. y means yes, n means no or that there are no regulatory completeness criteria for the parameter.\n",
      "valid day count: the number of days during the year where the daily monitoring criteria were met, if the calculation of the summaries is based on valid days.\n",
      "required day count: the number of days during the year which the monitor was scheduled to take samples if measurements are required.\n",
      "exceptional data count: the number of data points in the annual data set affected by exceptional air quality events (things outside the norm that affect air quality).\n",
      "null data count: the count of scheduled samples when no data was collected and the reason for no data was reported.\n",
      "primary exceedance count: the number of samples during the year that exceeded the primary air quality standard.\n",
      "secondary exceedance count: the number of samples during the year that exceeded the secondary air quality standard.\n",
      "certification indicator: an indication whether the completeness and accuracy of the information on the annual summary record has been certified by the submitter. certified means the submitter has certified the data (due may 01 the year after collection). certification not required means that the parameter does not require certification or the deadline has not yet passed. uncertified (past due) means that certification is required but is overdue. requested but not yet concurred means the submitter has completed the process, but epa has not yet acted to certify the data. requested but denied means the submitter has completed the process, but epa has denied the request for cause. was certified but data changed means the data was certified but data was replaced and the process has not been repeated.\n",
      "num obs below mdl: the number of samples reported during the year that were below the method detection limit (mdl) for the monitoring instrument. sometimes these values are replaced by 1/2 the mdl in summary calculations.\n",
      "arithmetic mean: the average (arithmetic mean) value for the year.\n",
      "arithmetic standard dev: the standard deviation about the mean of the values for the year.\n",
      "1st max value: the highest value for the year.\n",
      "1st max datetime: the date and time (on a 24-hour clock) when the highest value for the year (the previous field) was taken.\n",
      "2nd max value: the second highest value for the year.\n",
      "2nd max datetime: the date and time (on a 24-hour clock) when the second highest value for the year (the previous field) was taken.\n",
      "3rd max value: the third highest value for the year.\n",
      "3rd max datetime: the date and time (on a 24-hour clock) when the third highest value for the year (the previous field) was taken.\n",
      "4th max value: the fourth highest value for the year.\n",
      "4th max datetime: the date and time (on a 24-hour clock) when the fourth highest value for the year (the previous field) was taken.\n",
      "1st max non overlapping value: for 8-hour co averages, the highest value of the year.\n",
      "1st no max datetime: the date and time (on a 24-hour clock) when the first maximum non overlapping value for the year (the previous field) was taken.\n",
      "2nd max non overlapping value: for 8-hour co averages, the second highest value of the year that does not share any hours with the 8-hour period of the first max non overlapping value.\n",
      "2nd no max datetime: the date and time (on a 24-hour clock) when the second maximum non overlapping value for the year (the previous field) was taken.\n",
      "99th percentile: the value from this monitor for which 99 per cent of the rest of the measured values for the year are equal to or less than.\n",
      "98th percentile: the value from this monitor for which 98 per cent of the rest of the measured values for the year are equal to or less than.\n",
      "95th percentile: the value from this monitor for which 95 per cent of the rest of the measured values for the year are equal to or less than.\n",
      "90th percentile: the value from this monitor for which 90 per cent of the rest of the measured values for the year are equal to or less than.\n",
      "75th percentile: the value from this monitor for which 75 per cent of the rest of the measured values for the year are equal to or less than.\n",
      "50th percentile: the value from this monitor for which 50 per cent of the rest of the measured values for the year are equal to or less than (i.e., the median).\n",
      "10th percentile: the value from this monitor for which 10 per cent of the rest of the measured values for the year are equal to or less than.\n",
      "local site name: the name of the site (if any) given by the state, local, or tribal air pollution control agency that operates it.\n",
      "address: the approximate street address of the monitoring site.\n",
      "state name: the name of the state where the monitoring site is located.\n",
      "county name: the name of the county where the monitoring site is located.\n",
      "city name: the name of the city where the monitoring site is located. this represents the legal incorporated boundaries of cities and not urban areas.\n",
      "cbsa name: the name of the core bases statistical area (metropolitan area) where the monitoring site is located.\n",
      "date of last change: the date the last time any numeric values in this record were updated in the aqs data system.\n",
      "acknowledgements:\n",
      "these data come from the epa. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on google bigquery, too: https://cloud.google.com/bigquery/public-data/epa\n",
      "inspiration:\n",
      "within these data are tons of ways for you to learn about air pollution and how it can affect our health and environment. you can also compare key air emissions to gross domestic product, vehicle miles traveled, population, and energy consumption back to 1970. best of all, you can check out air trends where you live!\n",
      "content\n",
      "this report lists each failure of a commercial bank, savings association, and savings bank since the establishment of the fdic in 1933. each record includes the institution name and fin number, institution and charter types, location of headquarters (city and state), effective date, insurance fund and certificate number, failure transaction type, total deposits and total assets last reported prior to failure (in thousands of dollars), and the estimated cost of resolution. data on estimated losses are not available for fdic insured failures prior to 1986 or for fslic insured failures from 1934-88.\n",
      "acknowledgements\n",
      "the bank failure report was downloaded from the fdic website.\n",
      "inspiration\n",
      "what type of banking institution is the most likely to fail? how have bank failure rates changed over time? what commercial bank failure cost the federal government the most to resolve?\n",
      "most existing machine learning classifiers are highly vulnerable to adversarial examples. an adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. in many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.\n",
      "adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.\n",
      "to accelerate research on adversarial examples, google brain is organizing competition on adversarial examples and defenses within the nips 2017 competition track. this dataset contains the development images for this competition.\n",
      "the competition on adversarial examples and defenses consist of three sub-competitions:\n",
      "non-targeted adversarial attack. the goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.\n",
      "targeted adversarial attack. the goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.\n",
      "defense against adversarial attack. the goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.\n",
      "in each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. in the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.\n",
      "the price paid data includes information on all registered property sales in england and wales that are sold for full market value. address details have been truncated to the town/city level.\n",
      "you might also find the hm land registry transaction records to be a useful supplement to this dataset: https://www.kaggle.com/hm-land-registry/uk-land-registry-transactions\n",
      "the available fields are as follows:\n",
      "transaction unique identifier a reference number which is generated automatically recording each published sale. the number is unique and will change each time a sale is recorded.\n",
      "price sale price stated on the transfer deed.\n",
      "date of transfer date when the sale was completed, as stated on the transfer deed.\n",
      "property type d = detached, s = semi-detached, t = terraced, f = flats/maisonettes, o = other note that: - we only record the above categories to describe property type, we do not separately identify bungalows. - end-of-terrace properties are included in the terraced category above. - ‘other’ is only valid where the transaction relates to a property type that is not covered by existing values.\n",
      "old/new indicates the age of the property and applies to all price paid transactions, residential and non-residential. y = a newly built property, n = an established residential building\n",
      "duration relates to the tenure: f = freehold, l= leasehold etc. note that hm land registry does not record leases of 7 years or less in the price paid dataset.\n",
      "town/city\n",
      "district\n",
      "county\n",
      "ppd category type indicates the type of price paid transaction. a = standard price paid entry, includes single residential property sold for full market value. b = additional price paid entry including transfers under a power of sale/repossessions, buy-to-lets (where they can be identified by a mortgage) and transfers to non-private individuals. note that category b does not separately identify the transaction types stated. hm land registry has been collecting information on category a transactions from january 1995. category b transactions were identified from october 2013.\n",
      "record status - monthly file only indicates additions, changes and deletions to the records.(see guide below). a = addition c = change d = delete.\n",
      "note that where a transaction changes category type due to misallocation (as above) it will be deleted from the original category type and added to the correct category with a new transaction unique identifier.\n",
      "this data was kindly released by hm land registry under the open government license 3.0. you can find their current release here.\n",
      "data produced by hm land registry © crown copyright 2017.\n",
      "context\n",
      "humans (and many other animals) have the ability to reduce or suppress their brains' responses to sensory consequences that are a result of their own actions. the nervous system accomplishes this with a corollary discharge forward model system in which an \"efference copy\" of an impending motor plan is transmitted from motor to sensory cortex where it generates a \"corollary discharge\" representation of the expected sensory consequences of the imminent motor act. for example, when you move your eyes from left to right, your brain knows the environment is not shifting. when you speak, your auditory cortex has a reduced response to the expected sound of your voice.\n",
      "schizophrenia is a chronic mental illness that affects about 1% of people across the globe. one possible explanation for some of the symptoms of schizophrenia is that one or more problems with the corollary discharge process in the nervous system makes it difficult for patients to differentiate between internally and externally generated stimuli. therefore, studying this process and its relationship to symptoms in the illness might allow us to better understand abnormal brain processes in patients with this diagnosis.\n",
      "in a previously published eeg experiment (full report), we used a simple button pressing task in which subjects either (1) pressed a button to immediately generated a tone, (2) passively listened to the same tone, or (3) pressed a button without generating a tone to study the corollary discharge in people with schizophrenia and comparison controls. we found that comparison controls suppressed the n100, a negative deflection in eeg brain wave 100 milliseconds after the onset of a sound, when they pressed a button to generate a tone compared to passive playback, but patients with schizophrenia did not. this data set is a larger sample replication of that previous study. specifically, eeg data from 22 controls and 36 patients with schizophrenia have been combined with 10 controls and 13 patients from the previous report.\n",
      "methods\n",
      "due to the size of the raw eeg data, some pre-processing was done prior to upload. eeg data acquisition parameters and the experimental task was identical to that described in our paper. however, pre-processing differed. all individual subject data had at least the following data processing steps applied, in this order:\n",
      "re-reference to averaged ear lobes\n",
      "0.1 hz high-pass filter\n",
      "interpolation of outlier channels in the continuous eeg data (outliers defined as in this paper)\n",
      "chop continous data into single trial epochs 1.5 seconds before and after task events (3s total)\n",
      "baseline correction -100 to 0ms\n",
      "canonical correlation analysis to remove muscle and high-frequency white noise artifacts\n",
      "rejection of outlier single trials (outliers defined as in this paper)\n",
      "removal of outlier components from a spatial independent components analysis (outliers defined as in this paper)\n",
      "interpolation of outlier channels within single trials (outliers defined as in this paper)\n",
      "derived data includes event-related potential (erp) averages for 9 electrode sites analyzed in our previous report, including fz, fcz, cz, fc3, fc4, c3, c4, cp3, cp4 (pictured below):\n",
      "the erps are calculated by averaging across trials for every sample in the time series, separately for each subject, electrode, and condition.\n",
      "content\n",
      "the single trial data from all 64 channels are too large to be uploaded for all 81 subjects, but those interested in that type of data will find one subject (subject 21 in 21.csv) among the data files. this includes all his data after the pre-processing step 9 listed above.\n",
      "for those interested in comparing patients with schizophrenia to control subjects, the erpdata.csv file contains the averaged, erp time series for all subjects, conditions, and the 9 electrodes mentioned above. these data, along with the subject information in demographic.csv could be used to replicate the analyses in our prior report.\n",
      "for those interested in single trial categorization/prediction like the grasp-and-lift challenge or the face decoding challenge, the mergedtrialdata.csv contains summary measurements from nearly 24,000 individual trials (all subjects and conditions are included).\n",
      "acknowledgements\n",
      "funding for the study procedures, initial analyses and publications came from the national institute of mental health. please see grant info for additional details, and cite this nimh project number (r01mh058262) in any work related to these data. all study participants gave written, informed consent to participate in this study, which received institutional review board approval.\n",
      "mexican cuisine is often the best food option is southern california. and the burrito is the hallmark of delicious taco shop food: tasty, cheap, and filling. appropriately, an effort was launched to critique burritos across the county and make this data open to the lay burrito consumer. at this time, the data set contains ratings from over 200 burritos from around 50 restaurants.\n",
      "there are 10 core dimensions of the san diego burrito. * volume * tortilla quality *temperature * meat quality * non-meat filling quality * meat-to-filling ratio * uniformity * salsa quality * flavor synergy * wrap integrity\n",
      "all of these measures (except for volume) are rated on a scale from 0 to 5, 0 being terrible, and 5 being optimal. other information available for each burrito includes an overall rating, cost, yelp rating of the restaurant, and more.\n",
      "more information about the data set, as well as a link to the continuously updated dataset, can be found here.\n",
      "context\n",
      "metal-archives.com (ma for short) is an encyclopedia website which includes information of nearly all heavy bands and albums on the earth. this information is collected and submitted by metalheads from all around the world. this dataset includes all \"death metal\" bands and albums on ma (by nov. 2016). it's the search result by genre key word \"death metal\" which includes all bands which contain phrase \"death metal\" in their genre. (e.g. \"technical death metal\", \"brutal death metal\", \"melodic death metal\" ... )\n",
      "the banner of dataset is the cover art of new jersey-based death metal band disma's debut full-length album \"towards the megalith\" (2011, profound lore records). it's beautiful but not quite typical for this dataset's theme. but 1900+ resolution picture about death metal is rare, so i've chosen this one.\n",
      "content\n",
      "there are three csv files included in the dataset:\n",
      "bands.csv contains 37,723 bands. each record is consisted of 8 fields:\n",
      "id: sequential integer id.\n",
      "name: the band's name which can contains non-english character, punctuations, numbers and other weird characters.\n",
      "country: country the band is from. \"international\" means the members of the band are from multiple countries.\n",
      "status: band's current activity-status: 'unknown', 'split-up', 'active', 'changed name', 'on hold' and 'disputed'.\n",
      "from_in: the year in which the band formed.\n",
      "genre: the description of the band's genre. it's irregular, so you'd better not deem it as category but short text.\n",
      "theme: the description of the band's lyric theme.\n",
      "active: the time-span in which the band is active.\n",
      "albums.csv contains 28,069 albums. each record is consisted of 4 fields:\n",
      "id: sequential integer id.\n",
      "band: foreign key to band's id in bands.csv.\n",
      "title: album title.\n",
      "year: the album's release year.\n",
      "reviews.csv contains 21,510 reviews. each record is consisted of 5 fields:\n",
      "id: sequential integer id.\n",
      "album: foreign key to album's id in albums.csv.\n",
      "title: the review's title.\n",
      "score: the score for that album. float number from 0.0 to 1.0 (from negative to positive).\n",
      "content: the review's text.\n",
      "notice\n",
      "this dataset only contains full-length studio albums (excluding eps, singles, live albums, split albums and others).\n",
      "all commas in dataset are replaced by \"|\" to make comma available for fields separator.\n",
      "na value is \"n/a\".\n",
      "all text is in utf-8 encoding.\n",
      "acknowledgements\n",
      "metal-archives! \\m/\n",
      "inspiration\n",
      "statistical analysis\n",
      "emotional analysis (reviews)\n",
      "genre classification (by nlp of title and/or theme)\n",
      "score prediction (by nlp of review's content)\n",
      "context\n",
      "these datasets were created for a college course work. it was an opportunity to test deep learning capabilities for computer vision in a very restricted problem.\n",
      "the idea is to explore a classification problem for a single coin and a regression problem for a group of coins, trying to count how much money they sum. you can see my initial approach here.\n",
      "content\n",
      "there are two datasets. one for classification and another for regression. the first contains 3000 images with just a single coin in it. the second contains the first one and another 3000 images with two or more coins present in each example.\n",
      "in the classification problem there are five classes: 5, 10, 25, 50 and 100 cents. for regression, there are examples from 5 to 175 cents.\n",
      "every file contains its value in money as its filename. for example: 5_1477146780.jpg contains a single 5 cents coin. if there's only one coin, it's the coin value itself. in the 80_1477851090.jpg you're going to find enough coins to sum 80 cents. an example (90_1477854720.jpg):\n",
      "different coins from each type were used to make it more interesting. my fingers appear in some images!\n",
      "i tried to keep the distance, illumination and background constant for all of them, but some differences will be noticed, specially in the illumination. changing the coin position has an great impact in how the light reflects over it. the structure used to take the pictures (in fact, a second light source was added):\n",
      "inspiration\n",
      "a model that can sum coins and tell how much money we have in a group of coins could be used for people with vision disabilities. can deep learning count, classify and sum in a single model? should we split the problem into segmentation, classification and then sum it? what can be done with this amount of data? can we achieve a good generalization and predict sums beyond the dataset greatest value?\n",
      "citation\n",
      "if you want to use this dataset for any purpose contemplated by its license, add the reference:\n",
      "moneda, l. (2016) brazilian coins dataset. retrieved from: http://lgmoneda.github.io/\n",
      "acknowledgment\n",
      "i'd like to thanks luciana harada and rafael de souza, my group in the college course that generated these datasets.\n",
      "current population survey - august 2016\n",
      "context\n",
      "the current population survey (cps) is one of the oldest, largest, and most well-recognized surveys in the united states. it is immensely important, providing information on many of the things that define us as individuals and as a society – our work, our earnings, and our education.\n",
      "frequency: monthly\n",
      "period: august 2016\n",
      "content\n",
      "in addition to being the primary source of monthly labor force statistics, the cps is used to collect data for a variety of other studies that keep the nation informed of the economic and social well-being of its people. this is done by adding a set of supplemental questions to the monthly basic cps questions. supplemental inquiries vary month to month and cover a wide variety of topics such as child support, volunteerism, health insurance coverage, and school enrollment. supplements are usually conducted annually or biannually, but the frequency and recurrence of a supplement depend completely on what best meets the needs of the supplement’s sponsor.\n",
      "data dictionary: http://thedataweb.rm.census.gov/pub/cps/basic/201501-/january_2015_record_layout.txt\n",
      "acknowledgements\n",
      "the current population survey (cps) is administered, processed, researched and disseminated by the u.s. census bureau on behalf of the bureau of labor statistics (bls).\n",
      "severe weather data inventory\n",
      "context\n",
      "the severe weather data inventory (swdi) is an integrated database of severe weather records for the united states. severe weather is a phenomenon that risks the physical well-being of people and property. in fact, the frozen precipitation resulting from fast updrafts during strong thunderstorms can lead to serious damage and harm. each year, the u.s. sees approximately $1 billion in property and crop damage due to severe weather incidents.\n",
      "frequency: event-level\n",
      "period: 2015\n",
      "content\n",
      "the records in swdi come from a variety of sources in the national climatic data center archive and cover a number of weather phenomena. this extract from 2015 covers hail detections including the probability of a weather event as well as the size and severity of hail -- all of which help understand potential damage to property and injury to people. records are event-level records. individual storm cells with a high probability of yielding hail are included in this dataset -- a total of n = 10,824,080.\n",
      "inspiration\n",
      "think about the geospatial and spatial statistical techniques that can be applied to this data to uncover patterns in storms.\n",
      "how often does serious severe weather happen?\n",
      "where do these severe weather events normally occur?\n",
      "what correlations exist between severe weather and other environmental phenomena?\n",
      "acknowledgements\n",
      "this data is a product of noaa's national centers for environmental information (ncei). the dataset is generated by a variety of products that have been submitted to noaa's weather and climate archives at ncei. the datasets and methods are described at http://www.ncdc.noaa.gov/swdi/.\n",
      "swdi provides a uniform way to access data from a variety of sources, but it does not provide any additional quality control beyond the processing which took place when the data were archived. the data sources in swdi will not provide complete severe weather coverage of a geographic region or time period, due to a number of factors (eg, reports for a location or time period not provided to noaa). the absence of swdi data for a particular location and time should not be interpreted as an indication that no severe weather occurred at that time and location. furthermore, much of the data in swdi is automatically derived from radar data and represents probable conditions for an event, rather than a confirmed occurrence.\n",
      "license\n",
      "public domain license\n",
      "a dataset of stack overflow programming questions. for each question, it includes:\n",
      "question id\n",
      "creation date\n",
      "closed date, if applicable\n",
      "score\n",
      "owner user id\n",
      "number of answers\n",
      "tags\n",
      "this dataset is ideal for answering questions such as:\n",
      "the increase or decrease in questions in each tag over time\n",
      "correlations among tags on questions\n",
      "which tags tend to get higher or lower scores\n",
      "which tags tend to be asked on weekends vs weekdays\n",
      "this dataset was extracted from the stack overflow database at 2016-10-13 18:09:48 utc and contains questions up to 2016-10-12. this includes 12583347 non-deleted questions, and 3654954 deleted ones.\n",
      "this is all public data within the stack exchange data dump, which is much more comprehensive (including question and answer text), but also requires much more computational overhead to download and process. this dataset is designed to be easy to read in and start analyzing. similarly, this data can be examined within the stack exchange data explorer, but this offers analysts the chance to work with it locally using their tool of choice.\n",
      "note that for space reasons only non-deleted questions are included in the sqllite dataset, but the csv.gz files include deleted questions as well (with an additional deletiondate file).\n",
      "see the github repo for more.\n",
      "context\n",
      "each day, backblaze takes a snapshot of each operational hard drive that includes basic hard drive information (e.g., capacity, failure) and s.m.a.r.t. statistics reported by each drive. this dataset contains data from the first two quarters in 2016.\n",
      "content\n",
      "this dataset contains basic hard drive information and 90 columns or raw and normalized values of 45 different s.m.a.r.t. statistics. each row represents a daily snapshot of one hard drive.\n",
      "date: date in yyyy-mm-dd format\n",
      "serial_number: manufacturer-assigned serial number of the drive\n",
      "model: manufacturer-assigned model number of the drive\n",
      "capacity_bytes: drive capacity in bytes\n",
      "failure: contains a “0” if the drive is ok. contains a “1” if this is the last day the drive was operational before failing.\n",
      "90 variables that begin with 'smart': raw and normalized values for 45 different smart stats as reported by the given drive\n",
      "inspiration\n",
      "some items to keep in mind as you process the data:\n",
      "s.m.a.r.t. statistic can vary in meaning based on the manufacturer and model. it may be more informative to compare drives that are similar in model and manufacturer\n",
      "some s.m.a.r.t. columns can have out-of-bound values\n",
      "when a drive fails, the 'failure' column is set to 1 on the day of failure, and starting the day after, the drive will be removed from the dataset. each day, new drives are also added. this means that total number of drives each day may vary.\n",
      "s.m.a.r.t. 9 is the number of hours a drive has been in service. to calculate a drive's age in days, divide this number by 24.\n",
      "given the hints above, below are a couple of questions to help you explore the dataset:\n",
      "what is the median survival time of a hard drive? how does this differ by model/manufacturer?\n",
      "can you calculate the probability that a hard drive will fail given the hard drive information and statistics in the dataset?\n",
      "acknowledgement\n",
      "the original collection of data can be found here. when using this data, backblaze asks that you cite backblaze as the source; you accept that you are solely responsible for how you use the data; and you do not sell this data to anyone.\n",
      "context\n",
      "since 2008, guests and hosts have used airbnb to travel in a more unique, personalized way. as part of the airbnb inside initiative, this dataset describes the listing activity of homestays in seattle, wa.\n",
      "content\n",
      "the following airbnb activity is included in this seattle dataset: * listings, including full descriptions and average review score * reviews, including unique id for each reviewer and detailed comments * calendar, including listing id and the price and availability for that day\n",
      "inspiration\n",
      "can you describe the vibe of each seattle neighborhood using listing descriptions?\n",
      "what are the busiest times of the year to visit seattle? by how much do prices spike?\n",
      "is there a general upward trend of both new airbnb listings and total airbnb visitors to seattle?\n",
      "for more ideas, visualizations of all seattle datasets can be found here.\n",
      "acknowledgement\n",
      "this dataset is part of airbnb inside, and the original source can be found here.\n",
      "content\n",
      "the motor vehicle collision database includes the date and time, location (as borough, street names, zip code and latitude and longitude coordinates), injuries and fatalities, vehicle number and types, and related factors for all 65,500 collisions in new york city during 2015 and 2016.\n",
      "acknowledgements\n",
      "the vehicle collision data was collected by the nypd and published by nyc opendata.\n",
      "the dataset contains the following variables: water consumption per user (cubic meters) from 2009 to 2016, land use, type of user (e.g. industrial, housing, public infrastructure, etc.), zip code, and others. the challenge is to treat nas in a way that do not distort the overall dataset. you should also check whether there are any missing values. if so, can you ﬁll them in, and do you understand why they are missing? this dataset is property of a local water provider called aguah and its part of a research developed between 2014 and 2016.\n",
      "the marvel universe\n",
      "marvel comics, originally called timely comics inc., has been publishing comic books for several decades. \"the golden age of comics\" name that was given due to the popularity of the books during the first years, was later followed by a period of decline of interest in superhero stories due to world war ref. in 1961, marvel relaunched its superhero comic books publishing line. this new era started what has been known as the marvel age of comics. characters created during this period such as spider-man, the hulk, the fantastic four, and the x-men, together with those created during the golden age such as captain america, are known worldwide and have become cultural icons during the last decades. later, marvel's characters popularity has been revitalized even more due to the release of several recent movies which recreate the comic books using spectacular modern special effects. nowadays, it is possible to access the content of the comic books via a digital platform created by marvel, where it is possible to subscribe monthly or yearly to get access to the comics. more information about the marvel universe can be found here.\n",
      "content\n",
      "the dataset contains heroes and comics, and the relationship between them. the dataset is divided into three files:\n",
      "nodes.csv: contains two columns (node, type), indicating the name and the type (comic, hero) of the nodes.\n",
      "edges.csv: contains two columns (hero, comic), indicating in which comics the heroes appear.\n",
      "hero-edge.csv: contains the network of heroes which appear together in the comics. this file was originally taken from http://syntagmatic.github.io/exposedata/marvel/\n",
      "past research (acknowledgements)\n",
      "the marvel comics character collaboration graph was originally constructed by cesc rosselló, ricardo alberich, and joe miro from the university of the balearic islands. they compare the characteristics of this universe to real-world collaboration networks, such as the hollywood network, or the one created by scientists who work together in producing research papers. their original sources can be found here. with this dataset, the authors published the paper titled: \"marvel universe looks almost like a real social network\".\n",
      "the original mnist image dataset of handwritten digits is a popular benchmark for image-based machine learning methods but researchers have renewed efforts to update it and develop drop-in replacements that are more challenging for computer vision and original for real-world applications. as noted in one recent replacement called the fashion-mnist dataset, the zalando researchers quoted the startling claim that \"most pairs of mnist digits (784 total pixels per sample) can be distinguished pretty well by just one pixel\". to stimulate the community to develop more drop-in replacements, the sign language mnist is presented here and follows the same csv format with labels and pixel values in single rows. the american sign language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding j and z which require motion).\n",
      "the dataset format is patterned to match closely with the classic mnist. each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter a-z (and no cases for 9=j or 25=z because of gesture motions). the training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard mnist but otherwise similar with a header row of label, pixel1,pixel2....pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. the original hand gesture image data represented multiple users repeating the gesture against different backgrounds. the sign language mnist data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. to create new data, an image pipeline was used based on imagemagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. the modification and expansion strategy was filters ('mitchell', 'robidoux', 'catrom', 'spline', 'hermite'), along with 5% random pixelation, +/- 15% brightness/contrast, and finally 3 degrees rotation. because of the tiny size of the images, these modifications effectively alter the resolution and class separation in interesting, controllable ways.\n",
      "this dataset was inspired by the fashion-mnist 2 and the machine learning pipeline for gestures by sreehari 4.\n",
      "a robust visual recognition algorithm could provide not only new benchmarks that challenge modern machine learning methods such as convolutional neural nets but also could pragmatically help the deaf and hard-of-hearing better communicate using computer vision applications. the national institute on deafness and other communications disorders (nidcd) indicates that the 200-year-old american sign language is a complete, complex language (of which letter gestures are only part) but is the primary language for many deaf north americans. asl is the leading minority language in the u.s. after the \"big four\": spanish, italian, german, and french. one could implement computer vision in an inexpensive board computer like raspberry pi with opencv, and some text-to-speech to enabling improved and automated translation applications.\n",
      "context:\n",
      "there has been increased interest among the public about the environment and living conditions in india. especially, after since many manufacturing units are being planned, people are worried about how it will affect the underground water quality and the environment. government of india, under the ministry of drinking water and sanitation has released the water quality affected data for 2009, 2010, 2011 and 2012. the objective here is to analyze this data alongside with forest, industries, habitation, and development projects data in the same area (panchayat) to figure out whether there is any connection between the development effort and the quality of water getting affected. this effort will identify such associations and create awareness such that people and the govt. can act in time to avoid further deterioration of the water resources.\n",
      "content:\n",
      "currently, there is this data set of areas with affected water quality for the years 2009, 2010, 2011 and 2012. further datasets are expected for subsequent years. these datasets identify the state, district and specific localities in which water quality degradation has been reported in that particular year. focus should be on the area (panchayat/village) rather than the district or the state as a whole, and observations should be made if there are any associations between the other sets of data available for the same area (from industrial, habitation, manufacturing and other sources, which i intend to add here also).\n",
      "acknowledgements:\n",
      "my deep gratitude to the government of india for making this data available through the open data initiative.\n",
      "inspiration:\n",
      "let's explore if there are any repetitive patterns of water quality degradation in the same area for multiple years.\n",
      "as a whole, which areas in india has a lot of water quality degradation issues over the years (heat maps)\n",
      "which chemical is predominantly present in most of the water quality issues (heat maps). and why (from the associations with other developmental data like industry, manufacturing, development initiatives, housing, habitation, etc.)\n",
      "as a whole, for the country, is the water quality degrading or upgrading (number of instances reported of water quality getting affected)?\n",
      "let's explore if there are any associations between the water quality data and the other developmental data. if there is, then what is the extent (visualisation) and how can we address it (prescriptive).\n",
      "let's predict if there are going to be water quality issues in areas that are not affected right now based on the developmental and water quality data that is available right now. prevention is better than cure!\n",
      "it would be great if we could have water quality and industrial/development experts in this analysis, so that they can contribute their valuable inputs!\n",
      "context\n",
      "this is real real-time bidding data that is used to predict if an advertiser should bid for a marketing slot e.g. a banner on a webpage. explanatory variables are things like browser, operation system or time of the day the user is online, marketplace his identifiers were traded on earlier, etc. the column 'convert' is 1, when the person clicked on the ad, and 0 if this is not the case.\n",
      "content\n",
      "unfortunately, the data had to be anonymized, so you basically can't do a lot of feature engineering. i just applied pca and kept 0.99 of the linear explanatory power. however, i think it's still really interesting data to just test your general algorithms on imbalanced data. ;)\n",
      "inspiration\n",
      "since it's heavily imbalanced data, it doesn't make sense to train for accuracy, but rather try to get obtain a good auc, f1score, mcc or recall rate, by cross-validating your data. it's interesting to compare different models (logistic regression, decision trees, svms, ...) over these metrics and see the impact that your split in train:test data has on the data.\n",
      "it might be good strategy to follow these tactics to combat imbalanced classes.\n",
      "this dataset contains information on player reconnaissance in over 500 professional-level starcraft games. from the perspective of one player (the terran), it contains information on how many enemy (protoss) units the player has observed, can observe, has seen destroyed, etc., along with an overall measure of how much enemy territory the player can see.\n",
      "acknowledgements\n",
      "this dataset was downloaded from this webpage. it was the basis for the following paper:\n",
      "hostetler, j., dereszynski, e., dietterich, t., and fern, a. (2012). inferring strategies from limited reconnaissance in real-time strategy games. proc. 28th conference on uncertainty in artificial intelligence (uai 2012) (to appear).\n",
      "the data\n",
      "games are divided into 30 second chunks, with the first 7 minutes of each game being represented in this dataset. values of variables at any given time cycle represent their values over the entire chunk that ends at that time.\n",
      "this dataset contains the following fields:\n",
      "game: a unique identifier for the game being played\n",
      "cycle: the cycle (in game frames, which are typically 24 fps)\n",
      "unit: the enemy unit that this row gives info for\n",
      "losses: how many of this enemy unit were lost during this time chunk?\n",
      "observable-units: how many of this enemy unit could the player see during this time chunk?\n",
      "observed-losses: how many of this enemy did the player observe being lost during this time chunk?\n",
      "production: how many of this enemy unit became observable (i.e., was produced, or -- in the case of buildings -- was under construction) during this time chunk?\n",
      "scouting: how many of this enemy unit did the player scout during this time chunk?\n",
      "vision: what proportion of the total enemy territory could the player observe during this time chunk? -- note that vision appears once per unit; however, the vision variable is not linked to any one unit. its value spans the time chunk, and is identical in every row that represents a given time chunk\n",
      "context\n",
      "i have the idea to build a virtual companion, capable of holding long and interesting conversations. but the lack of a good technique, and good datasets apparently is holding the advances of ai in that sense. chatbots don't have personality, nor context awareness, and datasets used to train them are just pair of question/answers, or it conversations. this dataset is being built using rdany bot for telegram, kik and messenger.\n",
      "if you want to see this dataset grow, please use and share it.\n",
      "telegram: https://t.me/rdanybot\n",
      "kik: https://kik.me/rdanybot\n",
      "messenger: https://m.me/rdanybot\n",
      "you can also support the development on patreon: https://www.patreon.com/rdanybot\n",
      "this bot have a personality:\n",
      "candid\n",
      "true\n",
      "fun\n",
      "optimistic\n",
      "empathic\n",
      "gender neutral\n",
      "likes art\n",
      "and knows a very limited word, its room, wikipedia, and a schematic view of the world. and speaks spanish (native), english (with some errors), and other languages (using automatic translation). you can learn more about it on rdany's telegram channel: https://t.me/rdany\n",
      "content\n",
      "the dataset consists on 157 anonymized (modified personal information) conversations between a human and other human acting as a companion bot. each conversation and messages are labeled with hashed ids.\n",
      "{\n",
      "    \"2059a7bf16436f39b3e713f7b5fe756776cd5e5a601186a1ba17c017027781d9\": [\n",
      "        {\n",
      "            \"date\": 0,\n",
      "            \"hashed_message_id\": \"0fd2e5cf87ae0f148db113f06ab746e0c76a55de04819e1a45c9454a34ba8a97\",\n",
      "            \"source\": \"human\",\n",
      "            \"text\": \"[start]\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": 108,\n",
      "            \"hashed_message_id\": \"a8c5a80334c8177f07913a192f048d05cd5ad5cc77752eb0abb8d0705eccedfb\",\n",
      "            \"source\": \"human\",\n",
      "            \"text\": \"hello\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": 15097,\n",
      "            \"hashed_message_id\": \"73e9765c0d0eab4dfd6f9be2d665e32cc97c5fc3e0fd9c2d12ef920d18ecf349\",\n",
      "            \"source\": \"robot\",\n",
      "            \"text\": \"hi! how are you?!\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "date: seconds since first message\n",
      "hashed_message_id: message id\n",
      "source: human if the message is from the user, and robot if is from rdany\n",
      "text: text of the message, or \"[start]\" for the start command, or \"[photo]\", \"[document]\", \"[audio]\", \"[voice]\", \"[unknown]\" for other types of messages.\n",
      "acknowledgements\n",
      "thanks to all the amazing people that spent time speaking to a crazy human pretending to be a robot :)\n",
      "inspiration\n",
      "can you train your own virtual companion that says hello using this dataset?\n",
      "context\n",
      "this dataset was collected by me from car sale advertisements for study/practice purposes in 2016. though there is couple well known car features datasets they seems quite simple and outdated. car topic is really interesting. but i wanted to practice with real raw data which has all inconvenient moments (as na’s for example).\n",
      "this dataset contains data for more than 9.5k cars sale in ukraine. most of them are used cars so it opens the possibility to analyze features related to car operation. at the end of the day i look at this data as a subset from all ukrainian car fleet.\n",
      "content\n",
      "dataset contains 9576 rows and 10 variables with essential meanings:\n",
      "car: manufacturer brand\n",
      "price: seller’s price in advertisement (in usd)\n",
      "body: car body type\n",
      "mileage: as mentioned in advertisement (‘000 km)\n",
      "engv: rounded engine volume (‘000 cubic cm)\n",
      "engtype: type of fuel (“other” in this case should be treated as na)\n",
      "registration: whether car registered in ukraine or not\n",
      "year: year of production\n",
      "model: specific model name\n",
      "drive: drive type\n",
      "data has gaps, so be careful and check for na’s. i tried to check and drop repeated offers, but theoretically duplications are possible.\n",
      "inspiration\n",
      "data will be handy to study and practice different models and approaches. as a further step you can compare patters in ukrainian market to your own domestic car market characteristics.\n",
      "content\n",
      "the dataset contains a record of each reported wildlife strike of a military, commercial, or civil aircraft between 1990 and 2015. each row contains the incident date, aircraft operator, aircraft make and model, engine make and model, airport name and location, species name and quantity, and aircraft damage.\n",
      "acknowledgements\n",
      "the wildlife strike database was compiled from reports received from airports, airlines, and pilots and published by the federal aviation association.\n",
      "context\n",
      "the museum of modern art (moma) acquired its first artworks in 1929, the year it was established. today, the museum’s evolving collection contains almost 200,000 works from around the world spanning the last 150 years. the collection includes an ever-expanding range of visual expression, including painting, sculpture, printmaking, drawing, photography, architecture, design, film, and media and performance art.\n",
      "content\n",
      "moma is committed to helping everyone understand, enjoy, and use our collection. the museum’s website features 72,706 artworks from 20,956 artists. the artworks dataset contains 130,262 records, representing all of the works that have been accessioned into moma’s collection and cataloged in our database. it includes basic metadata for each work, including title, artist, date, medium, dimensions, and date acquired by the museum. some of these records have incomplete information and are noted as “not curator approved.” the artists dataset contains 15,091 records, representing all the artists who have work in moma's collection and have been cataloged in our database. it includes basic metadata for each artist, including name, nationality, gender, birth year, and death year.\n",
      "inspiration\n",
      "which artist has the most works in the museum collection or on display? what is the largest work of art in the collection? how many pieces in the collection were made during your birth year? what gift or donation is responsible for the most artwork in the collection?\n",
      "this dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output.\n",
      "the overall scope of emhires is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in europe and not to mime the actual evolution of solar power production in the latest decades. for this reason, the hourly solar power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the solar installed capacity. thus, the installed capacity considered is fixed as the one installed at the end of 2015. for this reason, data from emhires should not be compared with actual power generation data other than referring to the reference year 2015.\n",
      "content\n",
      "the data is available at both the national level and the nuts 2 level. the nuts 2 system divides the eu into 276 statistical units.\n",
      "please see the manual for the technical details of how these estimates were generated.\n",
      "this product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. please don't use it commercially.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the european commission's stetis program. you can find the original dataset here.\n",
      "inspiration\n",
      "how clean is the dataset? older solar estimates used to contain impossible values around sunset (ie more energy than the sun releases) or negative sunlight.\n",
      "what does a typical year look like? one common approach is to stitch together 12 months of raw data, using the 12 most typical months per this iso standard.\n",
      "if you like\n",
      "if you like this dataset, you might also enjoy: - 30 years of european wind - google's project sunroof\n",
      "context:\n",
      "some words, like “the” or “and” in english, are used a lot in speech and writing. for most natural language processing applications, you will want to remove these very frequent words. this is usually done using a list of “stopwords” which has been complied by hand.\n",
      "content:\n",
      "this dataset contains a list of stopwords for the following languages (languages which are not from the indo-european language family have been starred):\n",
      "english\n",
      "french\n",
      "german\n",
      "italian\n",
      "spanish\n",
      "portuguese\n",
      "finnish*\n",
      "swedish\n",
      "arabic*\n",
      "russian\n",
      "hungarian\n",
      "bulgarian\n",
      "romanian\n",
      "czech\n",
      "polish\n",
      "persian/farsi\n",
      "hindi\n",
      "marathi\n",
      "bengali\n",
      "acknowledgements:\n",
      "this dataset is copyright (c) 2005, jacques savoy and distributed under the bsd license. more information can be found here.\n",
      "inspiration:\n",
      "this dataset is mainly helpful for use during nlp analysis, however there may some interesting insights to be found in the data.\n",
      "what qualities do stopwords share across languages? given a novel language, could you predict what its stopwords should be?\n",
      "what stopwords are shared across languages?\n",
      "often, related languages will have words with the same meaning and similar spellings. can you automatically identify any of these pairs of words?\n",
      "you may also like:\n",
      "stopword lists for 9 african languages\n",
      "context\n",
      "this data set deals with the financial distress prediction for a sample of companies.\n",
      "content\n",
      "first column: company represents sample companies.\n",
      "second column: time shows different time periods that data belongs to. time series length varies between 1 to 14 for each company.\n",
      "third column: the target variable is denoted by \"financial distress\" if it is greater than -0.50 the company should be considered as healthy (0). otherwise, it would be regarded as financially distressed (1).\n",
      "fourth column to the last column: the features denoted by x1 to x83, are some financial and non-financial characteristics of the sampled companies. these features belong to the previous time period, which should be used to predict whether the company will be financially distressed or not (classification). feature x80 is categorical variable.\n",
      "for example, company 1 is financially distressed at time 4 but company 2 is still healthy at time 14.\n",
      "this data set is imbalanced (there are 136 financially distressed companies against 286 healthy ones i.e., 136 firm-year observations are financially distressed while 3546 firm-year observations are healthy) and skewed, so f-score should be employed as the performance evaluation criterion.\n",
      "it should be noted that 30% of this data set should be randomly assigned as hold-out test set so the remaining 70% is used for feature selection and model selection i.e., train set.\n",
      "note: 1- this data could be viewed as a classification problem. 2- this data could also be considered as a regression problem and then the result will be converted into a classification. 3- this data could be regarded as a multivariate time series classification.\n",
      "inspiration\n",
      "which features are most indicative of financial distress?\n",
      "what types of machine learning models perform best on this dataset?\n",
      "context\n",
      "this dataset includes taxi trips for 2016, reported to the city of chicago in its role as a regulatory agency. to protect privacy but allow for aggregate analyses, the taxi id is consistent for any given taxi medallion number but does not show the number, census tracts are suppressed in some cases, and times are rounded to the nearest 15 minutes. due to the data reporting process, not all trips are reported but the city believes that most are. see http://digital.cityofchicago.org/index.php/chicago-taxi-data-released for more information about this dataset and how it was created.\n",
      "content\n",
      "please see the data dictionary for details of specific fields. we also shrunk the original files by roughly two thirds by dropping redundant columns and remapping several others to use shorter ids. for example, the taxi_id column used to be a 128 character string. we’ve replaced it with an integer containing at most four digits.\n",
      "the redundant columns were unique_key, pickup_location, and dropoff_location. the remapped columns were taxi_id, company, pickup_census_tract, dropoff_census_tract, pickup_latitude, pickup_longitude, dropoff_latitude, and dropoff_longitude. the original versions of those columns can be unpacked using the column_remapping.json.\n",
      "acknowledgements\n",
      "this dataset was kindly made publically available by the city of chicago at: https://data.cityofchicago.org/transportation/taxi-trips/wrvz-psew\n",
      "please note that this site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the city of chicago. the city of chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. the data provided at this site is subject to change at any time. it is understood that the data provided at this site is being used at one’s own risk.\n",
      "inspiration\n",
      "how centralized is chicago? in other words, what portion of trips are to or from downtown?\n",
      "chicago has an extensive metro system. are taxis competing with the trains by covering similar routes or supplementing public transit by getting people to and from train stations?\n",
      "use this dataset with bigquery\n",
      "you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too: https://cloud.google.com/bigquery/public-data/chicago-taxi. bigquery hosts the full version of this dataset, which extends from 2013 through the present.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 615,000 hotels) that was created by extracting data from makemytrip.com, a travel portal in india. the complete dataset is available on datastock, a web data repository with historical records from several industries.\n",
      "content\n",
      "this dataset has following fields:\n",
      "area\n",
      "city\n",
      "country\n",
      "crawl_date\n",
      "highlight_value\n",
      "hotel_overview\n",
      "hotel_star_rating\n",
      "image_urls\n",
      "in_your_room\n",
      "is_value_plus\n",
      "latitude\n",
      "longitude\n",
      "mmt_holidayiq_review_count\n",
      "mmt_location_rating\n",
      "mmt_review_count\n",
      "mmt_review_rating\n",
      "mmt_review_score\n",
      "mmt_traveller_type_review_count\n",
      "mmt_tripadvisor_count\n",
      "pageurl\n",
      "property_address\n",
      "property_id\n",
      "property_name\n",
      "property_type\n",
      "qts\n",
      "query_time_stamp\n",
      "room_types\n",
      "site_review_count\n",
      "site_review_rating\n",
      "sitename\n",
      "state\n",
      "traveller_rating\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of the reviews, ratings and property description can be performed.\n",
      "context\n",
      "the british national health service releases data covering every public sector prescription made in the country. this covers a single year of that data.\n",
      "content\n",
      "covering all general practices in england, the data includes figures on the number of prescription items that are dispensed each month and information relating to costs.\n",
      "for each gp practice, the total number of items that were prescribed and then dispensed is shown. the total net ingredient cost and the total actual cost of these items is shown.\n",
      "chemical level\n",
      "all prescribed and dispensed medicines (by chemical name), dressings and appliances (at section level) are listed for each gp practice.\n",
      "presentation level\n",
      "all prescribed and dispensed medicines, dressings and appliances are listed at presentation level, for each gp practice. (presentation level gives the individual drug name, the form, and strength or size accordingly). the total quantity of drugs dispensed (in terms of number of tablets or millilitres, for example) is shown. this data does not list each individual prescription and does not contain any patient identifiable data.\n",
      "the data have been edited from their original version. during the data preparation process i:\n",
      "dropped obsolete and redundant columns.\n",
      "normalized the bnf (british national formulary) codes, bnf names, and practice codes. these steps reduced the total file size by roughly 75%, at the cost of requiring one table join to access some of the data.\n",
      "for further details, please see:\n",
      "faq\n",
      "glossary of terms\n",
      "acknowledgements\n",
      "this dataset was kindly released by the united kingdom's national health service under their government open data license v3. you can find this and other datasets at their open data site.\n",
      "inspiration\n",
      "what trends can you see in the data? for example, can you identify the onset of winter based on the types of drugs being prescribed?\n",
      "the bnf name entries contain dosage data that i haven't yet cleaned and extracted. can you unpack that field into item dispensed, units, and dosage? if so, let me know in the forums and i'll add it to the dataset!\n",
      "per this blog from oxford, the raw bnf codes contain quite a bit of information about a drug's function. can you find a source of open data for translating these codes? it's probable that one exists somewhere at https://www.nhsbsa.nhs.uk/nhs-prescription-services.\n",
      "context:\n",
      "every year since 1947, representatives of un member states gather at the annual sessions of the united nations general assembly. the centrepiece of each session is the general debate. this is a forum at which leaders and other senior officials deliver statements that present their government’s perspective on the major issues in world politics. these statements are akin to the annual legislative state-of-the-union addresses in domestic politics. this dataset, the un general debate corpus (ungdc), includes the corpus of texts of general debate statements from 1970 (session 25) to 2016 (session 71).\n",
      "content:\n",
      "this dataset includes the text of each country’s statement from the general debate, separated by country, session and year and tagged for each. the text was scanned from pdfs of transcripts of the un general sessions. as a result, the original scans included page numbers in the text from ocr (optical character recognition) scans, which have been removed. this dataset only includes english.\n",
      "acknowledgements:\n",
      "this dataset was prepared by alexander baturo, niheer dasandi, and slava mikhaylov, and is presented in the paper \"understanding state preferences with text as data: introducing the un general debate corpus\" research & politics, 2017.\n",
      "inspiration:\n",
      "this dataset includes over forty years of data from different countries, which allows for the exploration of differences between countries and over time. this allows you to ask both country-specific and longitudinal questions. some questions that might be interesting:\n",
      "how has the sentiment of each country’s general debate changed over time?\n",
      "what topics have been more or less popular over time and by region?\n",
      "can you build a classifier which identifies which country a given text is from?\n",
      "are there lexical or syntactic changes over time or differences between region?\n",
      "how does the latitude of a country affect lexical complexity?\n",
      "utility data\n",
      "the data is extracted from geonames, a very exhaustive list of worldwide toponyms. it can be joined with datasets containing geographic fields to facilitate geospatial analysis including mapping.\n",
      "this datapackage only lists cities above 15,000 inhabitants. each city is associated with its country and subcountry to reduce the number of ambiguities. subcountry can be the name of a state (e.g., in united kingdom or the united states of america) or the major administrative section (e.g., ''region'' in france''). see admin1 field on geonames website for further info about subcountry.\n",
      "notice that:\n",
      "some cities like vatican city or singapore are a whole state so they don't belong to any subcountry. therefore subcountry is n/a.\n",
      "there is no guaranty that a city has a unique name in a country and subcountry (at the time of writing, there are about 60 ambiguities). but for each city, the source data primary key geonameid is provided.\n",
      "preparation\n",
      "you can run the script yourself to update the data and publish them to github/kaggle: see scripts readme\n",
      "acknowledgments and license\n",
      "all data is licensed under the creative common attribution license as is the original data from geonames. this means you have to credit geonames when using the data. and while no credit is formally required a link back or credit to lexman and the open knowledge foundation is much appreciated. this dataset description is reproduced here from its original source with slight modifications.\n",
      "routes database\n",
      "as of january 2012, the openflights/airline route mapper route database contains 59036 routes between 3209 airports on 531 airlines spanning the globe.\n",
      "content\n",
      "the data is iso 8859-1 (latin-1) encoded.\n",
      "each entry contains the following information:\n",
      "airline 2-letter (iata) or 3-letter (icao) code of the airline.\n",
      "airline id unique openflights identifier for airline (see airline).\n",
      "source airport 3-letter (iata) or 4-letter (icao) code of the source airport.\n",
      "source airport id unique openflights identifier for source airport (see airport)\n",
      "destination airport 3-letter (iata) or 4-letter (icao) code of the destination airport.\n",
      "destination airport id unique openflights identifier for destination airport (see airport)\n",
      "codeshare \"y\" if this flight is a codeshare (that is, not operated by airline, but another carrier), empty otherwise.\n",
      "stops number of stops on this flight (\"0\" for direct)\n",
      "equipment 3-letter codes for plane type(s) generally used on this flight, separated by spaces\n",
      "the special value \\n is used for \"null\" to indicate that no value is available.\n",
      "notes:\n",
      "routes are directional: if an airline operates services from a to b and from b to a, both a-b and b-a are listed separately.\n",
      "routes where one carrier operates both its own and codeshare flights are listed only once.\n",
      "acknowledgements\n",
      "this dataset was downloaded from openflights.org under the open database license. this is an excellent resource and there is a lot more on their website, so check them out!\n",
      "context\n",
      "this data set can be paired with the shot logs data set from the same season.\n",
      "content\n",
      "full players stats from the 2014-2015 season + personal details such as height. weight, etc.\n",
      "the data was scraped and copied from: http://www.basketball-reference.com/teams/ and http://stats.nba.com/leaders#!?season=2014-15&seasontype=regular%20season&statcategory=min&cf=min*g*2&permode=totals\n",
      "emnist\n",
      "the emnist dataset is a set of handwritten character digits derived from the nist special database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the mnist dataset. further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1.\n",
      "format\n",
      "there are six different splits provided in this dataset and each are provided in two formats:\n",
      "binary (see emnist_source_files.zip)\n",
      "csv (combined labels and images)\n",
      "each row is a separate image\n",
      "785 columns\n",
      "first column = class_label (see mappings.txt for class label definitions)\n",
      "each column after represents one pixel value (784 total for a 28 x 28 image)\n",
      "byclass and bymerge datsets\n",
      "the full complement of the nist special database 19 is available in the byclass and bymerge splits. these two datasets have the same image information but differ in the number of images in each class. both datasets have an uneven number of images per class and there are more digits than letters. the number of letters roughly equate to the frequency of use in the english language.\n",
      "train: 697,932\n",
      "test: 116,323\n",
      "total: 814,255\n",
      "classes: byclass 62 (unbalanced) / bymerge 47 (unbalanced)\n",
      "balanced dataset\n",
      "the emnist balanced dataset is meant to address the balance issues in the byclass and bymerge datasets. it is derived from the bymerge dataset to reduce mis-classification errors due to capital and lower case letters and also has an equal number of samples per class. this dataset is meant to be the most applicable.\n",
      "train: 112,800\n",
      "test: 18,800\n",
      "total: 131,600\n",
      "classes: 47 (balanced)\n",
      "letters datasets\n",
      "the emnist letters dataset merges a balanced set of the uppercase and lowercase letters into a single 26-class task.\n",
      "train: 88,800\n",
      "test: 14,800\n",
      "total: 103,600\n",
      "classes: 37 (balanced)\n",
      "digits and mnist datsets\n",
      "the emnist digits and emnist mnist dataset provide balanced handwritten digit datasets directly compatible with the original mnist dataset.\n",
      "train: digits 240,000 / mnist 60,000\n",
      "test: digits 40,000 / mnist 10,000\n",
      "total: digits 280,000 / mnist 70,000\n",
      "classes: 47 (balanced)\n",
      "visual breakdown of emnist datasets\n",
      "please refer to the emnist paper for details on the structure of the dataset https://arxiv.org/abs/1702.05373v1.\n",
      "acknowldgements\n",
      "cohen, g., afshar, s., tapson, j., & van schaik, a. (2017). emnist: an extension of mnist to handwritten letters.\n",
      "dataset retrieved from https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
      "gregory cohen, saeed afshar, jonathan tapson, and andre van schaik\n",
      "the marcs institute for brain, behaviour and development\n",
      "western sydney university\n",
      "penrith, australia 2751\n",
      "context\n",
      "this is probably the dumbest dataset on kaggle. the whole point is, however, to provide a common dataset for linear regression. although such a dataset can easily be generated in excel with random numbers, results would not be comparable.\n",
      "content\n",
      "the training dataset is a csv file with 700 data pairs (x,y). the x-values are numbers between 0 and 100. the corresponding y-values have been generated using the excel function norminv(rand(), x, 3). consequently, the best estimate for y should be x. the test dataset is a csv file with 300 data pairs.\n",
      "acknowledgements\n",
      "thank you, dan bricklin and bob frankston for inventing the first spreadsheet.\n",
      "inspiration\n",
      "i hope this dataset will encourage all newbies to enter the world of machine learning, possibly starting with a simple linear regression.\n",
      "data license\n",
      "obviously, data is free.\n",
      "context:\n",
      "orcid provides a persistent digital identifier that distinguishes you from every other researcher and, through integration in key research workflows such as manuscript and grant submission, supports automated linkages between you and your professional activities ensuring that your work is recognized. find out more.\n",
      "content:\n",
      "this data is a subset of the entire orcid collection. the subset here was produced by john bohannon. you can see his excellent ipython notebook and the entire (300gb!) orcid archives here.\n",
      "the data covers ~742k unique researchers and includes:\n",
      "orcid_id\n",
      "phd_year\n",
      "country_2016\n",
      "earliest_year\n",
      "earliest_country\n",
      "has_phd\n",
      "phd_country\n",
      "has_migrated\n",
      "acknowledgements:\n",
      "bohannon j, doran k (2017) introducing orcid. science 356(6339) 691-692. http://dx.doi.org/10.1126/science.356.6339.691\n",
      "additionally, please cite the dryad data package:\n",
      "bohannon j, doran k (2017) data from: introducing orcid. dryad digital repository. http://dx.doi.org/10.5061/dryad.48s16\n",
      "inspiration:\n",
      "where do most researchers move to?\n",
      "what countries experience the largest ‘brain drain’? as a % of population?\n",
      "can you predict researcher migration?\n",
      "context\n",
      "as a former transportation student i know how the weather can influence traffic. both the increase of traffic, as well as the decrease of road conditions increases the travel time.\n",
      "content\n",
      "weather data collected from the national weather service. it contains the first six months of 2016, for a weather station in central park. it contains for each day the minimum temperature, maximum temperature, average temperature, precipitation, new snow fall, and current snow depth. the temperature is measured in fahrenheit and the depth is measured in inches. t means that there is a trace of precipitation.\n",
      "acknowledgements\n",
      "the data was retrieved on 20th of july, 2017 on the website http://w2.weather.gov/climate/xmacis.php?wfo=okx.\n",
      "context\n",
      "this dataset contains a randomized sample of roughly one quarter of all stories and comments from hacker news from its launch in 2006. hacker news is a social news website focusing on computer science and entrepreneurship. it is run by paul graham's investment fund and startup incubator, y combinator. in general, content that can be submitted is defined as \"anything that gratifies one's intellectual curiosity\".\n",
      "content\n",
      "each story contains a story id, the author that made the post, when it was written, and the number of points the story received.\n",
      "please note that the text field includes profanity. all texts are the author’s own, do not necessarily reflect the positions of kaggle or hacker news, and are presented without endorsement.\n",
      "acknowledgements\n",
      "this dataset was kindly made publicly available by hacker news under the mit license.\n",
      "inspiration\n",
      "recent studies have found that many forums tend to be dominated by a very small fraction of users. is this true of hacker news?\n",
      "hacker news has received complaints that the site is biased towards y combinator startups. do the data support this?\n",
      "is the amount of coverage by hacker news predictive of a startup’s success?\n",
      "use this dataset with bigquery\n",
      "you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data in bigquery, too: https://cloud.google.com/bigquery/public-data/hacker-news\n",
      "the bigquery version of this dataset has roughly four times as many articles.\n",
      "context\n",
      "pantheon is a project celebrating the cultural information that endows our species with these fantastic capacities. to celebrate our global cultural heritage we are compiling, analyzing and visualizing datasets that can help us understand the process of global cultural development. dive in, visualize, and enjoy.\n",
      "content\n",
      "the pantheon 1.0 data measures the global popularity of historical characters using two measures. the simpler of the two measures, which we denote as l, is the number of different wikipedia language editions that have an article about a historical character. the more sophisticated measure, which we name the historical popularity index (hpi) corrects l by adding information on the age of the historical character, the concentration of page views among different languages, the coefficient of variation in page views, and the number of page views in languages other than english.\n",
      "for annotations of specific values visit the column metadata in the /data tab. a more comprehensive breakdown is available on the parthenon website.\n",
      "acknowledgements\n",
      "pantheon is a project developed by the macro connections group at the massachusetts institute of technology media lab. for more on the dataset and to see visualizations using it, visit its landing page on the mit website.\n",
      "inspiration\n",
      "which historical figures have a biography in the most languages? who received the most wikipedia page views? which occupations or industries are the most popular? what country has the most individuals with a historical popularity index over twenty?\n",
      "inception-v3 is trained for the imagenet large visual recognition challenge using the data from 2012. this is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"zebra\", \"dalmatian\", and \"dishwasher\".\n",
      "here's code on github to train inception-v3\n",
      "context\n",
      "this corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:\n",
      "220,579 conversational exchanges between 10,292 pairs of movie characters\n",
      "involves 9,035 characters from 617 movies\n",
      "in total 304,713 utterances\n",
      "movie metadata included:\n",
      "genres\n",
      "release year\n",
      "imdb rating\n",
      "number of imdb votes\n",
      "imdb rating\n",
      "character metadata included:\n",
      "gender (for 3,774 characters)\n",
      "position on movie credits (3,321 characters)\n",
      "content\n",
      "in all files the original field separator was \" +++$+++ \" and have been converted to tabs (\\t). additionally, the original file encoding was iso-8859-2. it's possible that the field separator conversion and decoding may have left some artifacts.\n",
      "movie_titles_metadata.txt\n",
      "contains information about each movie title\n",
      "fields:\n",
      "movieid,\n",
      "movie title,\n",
      "movie year,\n",
      "imdb rating,\n",
      "no. imdb votes,\n",
      "genres in the format ['genre1','genre2',é,'genren']\n",
      "movie_characters_metadata.txt\n",
      "contains information about each movie character\n",
      "fields:\n",
      "characterid\n",
      "character name\n",
      "movieid\n",
      "movie title\n",
      "gender (\"?\" for unlabeled cases)\n",
      "position in credits (\"?\" for unlabeled cases)\n",
      "movie_lines.txt\n",
      "contains the actual text of each utterance\n",
      "fields:\n",
      "lineid\n",
      "characterid (who uttered this phrase)\n",
      "movieid\n",
      "character name\n",
      "text of the utterance\n",
      "movie_conversations.txt\n",
      "the structure of the conversations\n",
      "fields\n",
      "characterid of the first character involved in the conversation\n",
      "characterid of the second character involved in the conversation\n",
      "movieid of the movie in which the conversation occurred\n",
      "list of the utterances that make the conversation, in chronological order: ['lineid1','lineid2',é,'lineidn'] has to be matched with movie_lines.txt to reconstruct the actual content\n",
      "raw_script_urls.txt\n",
      "the urls from which the raw sources were retrieved\n",
      "acknowledgements\n",
      "this corpus comes from the paper, \"chameleons in imagined conversations: a new approach to understanding coordination of linguistic style in dialogs\" by cristian danescu-niculescu-mizil and lillian lee.\n",
      "the paper and up-to-date data can be found here: http://www.cs.cornell.edu/~cristian/cornell_movie-dialogs_corpus.html\n",
      "please see the readme for more information on the authors' collection procedures.\n",
      "the file formats were converted to tsv and may contain a few errors\n",
      "inspiration\n",
      "what are all of these imaginary people talking about? are they representative of how real people communicate?\n",
      "can you identify themes in movies from certain writers or directors?\n",
      "how does the dialog change between characters?\n",
      "airline database\n",
      "as of january 2012, the openflights airlines database contains 5888 airlines. some of the information is public data and some is contributed by users.\n",
      "content\n",
      "the data is iso 8859-1 (latin-1) encoded.\n",
      "each entry contains the following information: - airline id unique openflights identifier for this airline. - name name of the airline. - alias alias of the airline. for example, all nippon airways is commonly known as \"ana\". - iata 2-letter iata code, if available. - icao 3-letter icao code, if available. - callsign airline callsign. - country country or territory where airline is incorporated. - active \"y\" if the airline is or has until recently been operational, \"n\" if it is defunct. this field is not reliable: in particular, major airlines that stopped flying long ago, but have not had their iata code reassigned (eg. ansett/an), will incorrectly show as \"y\".\n",
      "the special value \\n is used for \"null\" to indicate that no value is available. this is from a mysql database where \\n is used for null.\n",
      "notes: airlines with null codes/callsigns/countries generally represent user-added airlines. since the data is intended primarily for current flights, defunct iata codes are generally not included. for example, \"sabena\" is not listed with a sn iata code, since \"sn\" is presently used by its successor brussels airlines.\n",
      "acknowledgements\n",
      "this dataset was downloaded from openflights.org under the open database license. this is an excellent resource and there is a lot more on their website, so check them out!\n",
      "the dataset contains a broad set of macroeconomic and financial data for the uk stretching back in some cases to the c13th and with one or two benchmark estimates available for 1086, the year of the domesday book. the dataset was originally called the 'three centuries of macroeconomic data' spreadsheet but has now been renamed given its broader coverage. version 3 of the dataset has now been updated to 2016.\n",
      "content\n",
      "the excel file contains the original data. it contains hundreds of time series, while the csv is an extract of several dozen headline time series.\n",
      "if you would like to see more of the data made available in csv format; please let me know what you would like extracted and i'll be happy to add it. please see excel_sheet_names.csv for details of what other data has yet to be unpacked.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the bank of england. you can find the original dataset here.\n",
      "inspiration\n",
      "which metrics give similar answers about when the industrial revolution began? how clear is the cutoff point?\n",
      "if you like\n",
      "if you enjoyed this dataset, you might also like the allen-unger global commodity prices dataset, which provides historic commodity prices from locations around the world.\n",
      "what i talk about when i talk about catalonia\n",
      "this is my grain of sand to help in the catalonia independence crisis. the iberian media has been a key driver to incubate disaffection between catalonia and spain. for example, a leading newspaper tweeted boycott and catalonia 9 times in the last month. in this dataset we analyze:\n",
      "tweets\n",
      "topics of tweets and\n",
      "sentiment differentials in news\n",
      "context\n",
      "the dramatic catalonia independence crisis offers a unique opportunity to analyze bias in news reporting as opinions on the issue are quite polarized (see #catalanreferendum on twitter). in this dataset, we compare how different newspapers (nyt, washington-post, bloomberg...) have reported one singular specific event in the saga: the reply of the spanish government to m.h. puigdemont speech of october 11th of 2017. for each of the 30 newspapers considered, the most popular news article that reported this news is represented as a row in the dataset. why this news? the spanish government, published a pdf called (\"requerimiento\") which was faxed to puigdemont. the document requires that puigdemont reply in five days a clarification of the meaning of his speech. this \"clean\" news offers a rare example where the news is about a written document rather than a speech or an action (usually subjected to more interpretations and biases)\n",
      "content\n",
      "news_...csv each row contains the news article and its translation to english.\n",
      "all3.csv contains 100k tweets.\n",
      "acknowledgements\n",
      "all the journalists who made this dataset possible. thanks to @datacanary for helping make the visualizations better!\n",
      "inspiration\n",
      "i always thought that sentiment analysis was a useless topic, but here there is a chance to use an objective measure to show how polarized reporting has become, (even if sentiment does not account for fakenews, nuances or sarcasm). the linear regressions shows that news written in spanish language are less positive about the event than the global mean. in other words, sentiment seems strongly biased by language. bias by location of the newspapers is also analyzed.\n",
      "disclaimer\n",
      "note that the 'bing' scale is used. other scales such afinn might yield different results.\n",
      "in 2015, 30,092 people were killed in the us in traffic accidents. this presents a 7.2% rise in fatalities over 2014, a startling change to a 50 year trend of declining fatality rates.\n",
      "because of this, the us government has publicly released this data and posted a call to action to try and investigate the data, to try and gain insights into why this is happening.\n",
      "i am posting the data here in raw format. there are very many files, so i will try to create a more cohesive dataset and add it here in the future. for now i am just uploading the *_aux.csv files, which contain the most data. i look forward to seeing what visualisations you guys can make from this!\n",
      "the data is compiled by the national highway traffic safety administration, and was released in this blog post.\n",
      "this dataset includes county-level data from the 2016 us presidential election.\n",
      "data are from michael w. kearney's github page, by way of max galka's county-level results map on metrocosm.com.\n",
      "disclaimer\n",
      "this is a data set of mine that i though might be enjoyable to the community. it's concerning next generation sequencing and transcriptomics. i used several raw datasets, that are public, but the processing to get to this dataset is extensive. this is my first contribution to kaggle, so be nice, and let me know how i can improve the experience. ngs machines are combined the biggest data producer worldwide. so why not add some (more? ) to kaggle.\n",
      "a look into yeast transcriptomics\n",
      "background\n",
      "yeasts ( in this case saccharomyces cerevisiae) are used in the production of beer, wine, bread and a whole lot of biotech applications such as creating complex pharmaceuticals. they are living eukaryotic organisms (meaning quite complex). all living organisms store information in their dna, but action within a cell is carried out by specific proteins. the path from dna to protein (from data to action) is simple. a specific region on the dna gets transcribed to mrna, that gets translated to proteins. common assumption says that the translation step is linear, more mrna means more protein. cells actively regulate the amount of protein by the amount of mrna it creates. the expression of each gene depends on the condition the cell is in (starving, stressed etc..) modern methods in biology show us all mrna that is currently inside a cell. assuming the linearity of the process, we can get more protein the more specific mrna is available to a cell. making mrna an excellent marker for what is actually happening inside a cell. it is important to consider that mrna is fragile. it is actively replenished only when it is needed. both mrna and proteins are expensive for a cell to produce .\n",
      "yeasts are good model organisms for this, since they only have about 6000 genes. they are also single cells which is more homogeneous, and contain few advanced features (splice junctions etc.)\n",
      "( all of this is heavily simplified, let me know if i should go into more details )\n",
      "the data\n",
      "files\n",
      "the following files are provided sc_expression.csv expression values for each gene over the available conditions **labels_cc.csv ** labels for the individual genes , their status and where known intracellular localization ( see below)\n",
      "maybe this would be nice as a little competition, i'll see how this one is going before i'll upload the other label files. please provide some feedback on the presentation, and whatever else you would want me to share.\n",
      "background\n",
      "i used 92 samples from various openly available raw datasets, and ran them through a modern rnaseq pipeline. spanning a range of different conditions (i hid the raw names). the conditions covered stress conditions, temperature and heavy metals, as well as growth media changes and the deletion of specific genes. originally i had 150 sets, 92 are of good enough quality. evaluation was done on gene level. each gene got it's own row, samples are columns (some are in replicates over several columns) . expression levels were normalized with by tpm (transcripts per million), a default normalization procedure. raw counts would have been integers, normalized they are floats.\n",
      "analysis and labels\n",
      "genes\n",
      "the function of individual genes is a matter of dispute. clearly living cells are complex. the inner machinations of cells are not visible. gene functionality is commonly inferred indirectly by removing a gene, and test the cells behavior. this is time consuming and not very precise. as you can see in the dataset, there is still much to be done to fully understand even single cell yeasts.\n",
      "the provided dataset is allows for a different approach to functional classification of genes. the label files contained in the set correspond a gene to a specific label. the classification is based on the official gene onthology associations classification. i simplified the nomenclature. gene functionality is usually given in a hierarchical structure. [inside cell --> cytoplasma --> associated to complex a ... ] i'm only keeping high level associations, and using readable terms instead of go terms. i'll extend if people are interested.\n",
      "labels\n",
      "cc labels concern cellular component.\n",
      "where the gene is within a cell. goes into details of found associations. the term 'cellular_component' should be seen as e.g the label 'cellular_component' is synonymous with 'unknown location' . cc is the easiest label to attach to a gene. it is the one that can be studied the easiest. still there are many genes missing.\n",
      "mf labels concern molecular function. what is the gene doing. [upcoming] bp labels concern biological processes. what is the genes involvement. [upcoming]\n",
      "the core interest here is whether it is possible to improve the genes classification by modeling the data. a common assumption says that genes that are expressed in the same conditions have functional relations. there are a bunch of possible applications out there, many of which are limited by our current state of knowledge on the complex systems we observe, or fail to do so. bringing biology into the realm of data science is an ongoing effort. having a better insight into the data might very well help.\n",
      "note\n",
      "the dataset is real, and therefore noisy the labels are incomplete even though i'm using the current state of the art. that is how much is known. using expression levels for classification was already attempted by softwares like spell (serial pattern of expression levels locator).\n",
      "acknowledgements\n",
      "i guess i own the dataset. it is a by product of another project of mine. if someone is interested in publishing this, contact me.\n",
      "inspiration\n",
      "unraveling genetic mechanisms is a complex but rewarding task. humans and yeast are quite similar in many ways. so apart from the fact that we use it for food and medicine, we might actually use knowledge gained from yeast eventually for studying diseases.\n",
      "again, any feedback is welcome, enjoy, ce\n",
      "context\n",
      "match details of over 6000 t20 matches, including innings scores and results.\n",
      "content\n",
      "the first 3 columns show the original data that was scraped. the remaining columns are individual data points extracted from these columns.\n",
      "• match_details: summary of match including stage of tournament (if applicable), home/away teams, venue and date of match.\n",
      "• result: summary of final result. includes ties (and any winners as a result of bowl-outs/super overs etc.), no results and abandoned matches.\n",
      "• scores: summary of scores of both innings. includes scores even if match ends in a no result. blank indicates that match was abandoned without a ball bowled.\n",
      "• date: date of match in standard date format, dd/mm/yyyy. if match goes to reserve day, this date is used.\n",
      "• venue: city of match. can be assumed to be main stadium within city. if more than one stadium in a city, it is usually labelled.\n",
      "• round: stage within tournament, e.g. final, semi-final, group stage etc. also, includes 1st, 2nd t20i etc. for bilateral series.\n",
      "• home: home or designated home team.\n",
      "• away: away or designated away team.\n",
      "• winner: winner of match including any winners by any method to determine a winner after a tie.\n",
      "• win_by_runs: number of runs team batting first wins by.\n",
      "• win_by_wickets: number of wickets team batting second wins by.\n",
      "• balls_remaining: number of balls remaining for team batting second after win.\n",
      "• innings1: team batting first\n",
      "• innings1_runs: first innings score\n",
      "• innings1_wickets: first innings wickets\n",
      "• innings1_overs_batted: actual length of first innings\n",
      "• innings1_overs: maximum length of first innings\n",
      "• innings2: team batting second\n",
      "• innings2_runs: second innings score\n",
      "• innings2_wickets: second innings wickets\n",
      "• innings2_overs_batted: actual length of second innings\n",
      "• innings2_overs: maximum length of second innings\n",
      "• d/l method: 1 means that the d/l method (or vjb method) was used to determine winner.\n",
      "• target: rain-adjusted target. if blank, target is first innings score plus 1, as normal.\n",
      "new: all t20 series added.\n",
      "please let me know if you spot any mistakes!\n",
      "context\n",
      "this data represents the top arrest charge of those processed at baltimore's central booking & intake facility. this data does not contain those who have been processed through juvenile booking.\n",
      "content\n",
      "the data set was created on october 18, 2011. the data set was last updated on november 18, 2016. it is updated on a monthly basis.\n",
      "metadata\n",
      "arrest-id\n",
      "age\n",
      "sex\n",
      "race\n",
      "arrestdate\n",
      "arresttime\n",
      "arrestlocation\n",
      "incidentoffense\n",
      "incidentlocation\n",
      "charge\n",
      "chargedescription\n",
      "district\n",
      "post\n",
      "neighborhood\n",
      "location1(location coordinates)\n",
      "past research\n",
      "i have done my own analysis on the data which can be found on the following github repository. feel free to give any suggestions regarding the data.\n",
      "github link\n",
      "inspiration\n",
      "how arrests vary across different gender, race, age ?\n",
      "which area in baltimore has most number of arrests made ?\n",
      "what are the top offences and/or charges made while making arrests ?\n",
      "acknowledgements\n",
      "the data is hosted on:\n",
      "data set source\n",
      "baltimore police depratment's website:\n",
      "baltimore police department\n",
      "context\n",
      "this dataset was assembled to investigate the possibility of predicting congressional election results by campaign finance reports from the period leading up to the election.\n",
      "content\n",
      "each row represents a candidate, with information on their campaign including the state, district, office, total contributions, total expenditures, etc. the content is specific to the year leading up to the 2016 election: (1/1/2015 through 10/19/2016).\n",
      "acknowledgements\n",
      "campaign finance information came directly from fec.gov. election results and vote totals for house races were taken from cnn's election results page.\n",
      "inspiration\n",
      "how much of an impact does campaign spending and fundraising have on an election? is the impact greater in certain areas? given this dataset, to what degree of accuracy could we have predicted the election results?\n",
      "context\n",
      "the federal housing finance agency house price index (hpi) is a broad measure of the movement of single-family house prices. the hpi is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancings on the same properties. the technical methodology for devising the index, collection, and publishing the data is at: http://www.fhfa.gov/policyprogramsresearch/research/paperdocuments/1996-03_hpi_techdescription_n508.pdf\n",
      "content\n",
      "contains monthly and quarterly time series from january 1991 to august 2016 for the u.s., state, and msa categories. analysis variables are the aggregate non-seasonally adjusted value and seasonally adjusted index values. the index value is 100 beginning january 1991.\n",
      "acknowledgements\n",
      "this data is found on data.gov\n",
      "inspiration\n",
      "can this data be combined with the corresponding census growth projections either at the state or msa level to forecast 24 months out the highest and lowest home index values?\n",
      "faostat provides access to over 3 million time-series and cross sectional data relating to food and agriculture. the full fao data can be found in the large zipfile, while a (somewhat out of date) summary of faostat is in the top level csv files. faostat contains data for 200 countries and more than 200 primary products and inputs in its core data set. the national version of faostat, countrystat, is being implemented in about 20 countries and three regions. it offers a two-way bridge amongst sub-national, national, regional and international statistics on food and agriculture.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nation on the undata site. you can find the original dataset here.\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "introduction\n",
      "flaredown is an app that helps patients of chronic autoimmune and invisible illnesses improve their symptoms by avoiding triggers and evaluating their treatments. each day, patients track their symptom severity, treatments and doses, and any potential environmental triggers (foods, stress, allergens, etc) they encounter.\n",
      "about the data\n",
      "instead of coupling symptoms to a particular illness, flaredown asks users to create their unique set of conditions, symptoms and treatments (“trackables”). they can then “check-in” each day and record the severity of symptoms and conditions, the doses of treatments, and “tag” the day with any unexpected environmental factors.\n",
      "user: includes an id, age, sex, and country.\n",
      "condition: an illness or diagnosis, for example rheumatoid arthritis, rated on a scale of 0 (not active) to 4 (extremely active).\n",
      "symptom: self-explanatory, also rated on a 0–4 scale.\n",
      "treatment: anything a patient uses to improve their symptoms, along with an optional dose, which is a string that describes how much they took during the day. for instance “3 x 5mg”.\n",
      "tag: a string representing an environmental factor that does not occur every day, for example “ate dairy” or “rainy day”.\n",
      "food: food items were seeded from the publicly-available usda food database. users have also added many food items manually.\n",
      "weather: weather is pulled automatically for the user's postal code from the dark sky api. weather parameters include a description, precipitation intensity, humidity, pressure, and min/max temperatures for the day.\n",
      "if users do not see a symptom, treatment, tag, or food in our database (for instance “abdominal pain” as a symptom) they may add it by simply naming it. this means that the data requires some cleaning, but it is patient-centered and indicates their primary concerns.\n",
      "suggested questions\n",
      "does x treatment affect y symptom positively/negatively/not at all? what are the most strongly-correlated symptoms and treatments?\n",
      "are there subsets within our current diagnoses that could more accurately represent symptoms and predict effective treatments?\n",
      "can we reliably predict what triggers a flare for a given user or all users with a certain condition?\n",
      "could we recommend treatments more effectively based on similarity of users, rather than specific symptoms and conditions? (netflix recommendations for treatments)\n",
      "can we quantify a patient’s level of disease activity based on their symptoms? how different is it from our existing measures?\n",
      "can we predict which symptom should be treated to have the greatest effect on a given illness?\n",
      "how accurately can we guess a condition based on a user’s symptoms?\n",
      "can we detect new interactions between treatments?\n",
      "please email logan@flaredown.com if you have questions about the project\n",
      "context: bdrw is a real-world image dataset for developing machine learning and vision algorithms with minimal requirement on data pre-processing and formatting to identify digits of the decimal number system appearing in bengali script. it can be seen as similar in flavor to svhn (e.g., the images are of small cropped digits), but incorporates higher visual heterogeneity and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). bdrw is obtained from numbers appearing in photographs, printed materials, sign boards, wall writings, calendar or book pages, etc.\n",
      "file: bdrw_train.zip (contains bdrw_train_1.zip, bdrw_train_2.zip)\n",
      "the data in the two zip files are to be used together and together contain a set of .jpg images of different sized which are cropped from different photographs, magazine prints, wall writing images, etc. each image represents a digit from the decimal number system written in bengali (https://en.wikipedia.org/wiki/bengali_numerals). the file labels.xls contains the number represented in each image which can be used as the ground truth labels for training a learning based system to recognize the bengali numbers.\n",
      "inspiration: this dataset is released for a machine vision challenge being hosted at ieee techsym 2016. the challenge will also include a testing set which includes samples not present in the training set released here and would be released after the challenge is closed.\n",
      "context\n",
      "the philippine statistics authority (psa) spearheads the conduct of the family income and expenditure survey (fies) nationwide. the survey, which is undertaken every three (3) years, is aimed at providing data on family income and expenditure, including, among others, levels of consumption by item of expenditure, sources of income in cash, and related information affecting income and expenditure levels and patterns in the philippines.\n",
      "content\n",
      "inside this data set is some selected variables from the latest family income and expenditure survey (fies) in the philippines. it contains more than 40k observations and 60 variables which is primarily comprised of the household income and expenditures of that specific household\n",
      "acknowledgements\n",
      "the philippine statistics authority for providing the publisher with their raw data\n",
      "inspiration\n",
      "socio-economic classification models in the philippines has been very problematic. in fact, not one sec model has been widely accepted. government bodies uses their own sec models and private research entities uses their own. we all know that household income is the greatest indicator of one's socio-economic classification that's why the publisher would like to find out the following:\n",
      "1) best model in predicting household income 2) key drivers of household income, we want to make the model as sparse as possible 3) some exploratory analysis in the data would also be useful\n",
      "context\n",
      "while exploring the aerial bombing operations of world war two dataset (https://www.kaggle.com/usaf/world-war-ii), and recalling that the d-day landings were nearly postponed due to poor weather, i sought out weather reports from the period to compare with missions in the bombing operations dataset.\n",
      "content\n",
      "the dataset contains information on weather conditions recorded on each day at various weather stations around the world. information includes precipitation, snowfall, temperatures, wind speed and whether the day included thunder storms or other poor weather conditions.\n",
      "acknowledgements\n",
      "the data are taken from the united states national oceanic and atmospheric administration (https://www.kaggle.com/noaa) national centres for environmental information website: https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/world-war-ii-era-data\n",
      "inspiration\n",
      "this dataset is mostly to assist with the analysis of the aerial bombing operations dataset, also hosted on kaggle.\n",
      "context:\n",
      "being able to automatically answer questions accurately remains a difficult problem in natural language processing. this dataset has everything you need to try your own hand at this task. can you correctly generate the answer to questions given the wikipedia article text the question was originally generated from?\n",
      "content:\n",
      "there are three question files, one for each year of students: s08, s09, and s10, as well as 690,000 words worth of cleaned text from wikipedia that was used to generate the questions.\n",
      "the \"question_answer_pairs.txt\" files contain both the questions and answers. the columns in this file are as follows:\n",
      "articletitle is the name of the wikipedia article from which questions and answers initially came.\n",
      "question is the question.\n",
      "answer is the answer.\n",
      "difficultyfromquestioner is the prescribed difficulty rating for the question as given to the question-writer.\n",
      "difficultyfromanswerer is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4.\n",
      "articlefile is the name of the file with the relevant article\n",
      "questions that were judged to be poor were discarded from this data set.\n",
      "there are frequently multiple lines with the same question, which appear if those questions were answered by multiple individuals.\n",
      "acknowledgements:\n",
      "these data were collected by noah smith, michael heilman, rebecca hwa, shay cohen, kevin gimpel, and many students at carnegie mellon university and the university of pittsburgh between 2008 and 2010. it is released here under cc by_sa 3.0. please cite this paper if you write any papers involving the use of the data above:\n",
      "smith, n. a., heilman, m., & hwa, r. (2008, september). question generation as a competitive undergraduate course project. in proceedings of the nsf workshop on the question generation shared task and evaluation challenge.\n",
      "you may also like:\n",
      "question-answer jokes: jokes of the question-answer form from reddit's r/jokes\n",
      "stanford question answering dataset: new reading comprehension dataset on 100,000+ question-answer pairs\n",
      "question pairs dataset: can you identify duplicate questions?\n",
      "in 2010, kaggle launched its first competition, which was won by jure zbontar, who used a simple linear model. since then a lot has changed. we've seen the rebirth of neural networks, the rise of python, the creation of powerful libraries like xgboost, keras and tensorflow.\n",
      "this is data set is a dump of all winners' posts from the kaggle blog starting with jure zbontar. it allows us to track trends in the techniques, tools and libraries that win competitions.\n",
      "this is a simple dump. if there's demand, i can upload more detail (including comments and tags).\n",
      "context\n",
      "on the data team at stack overflow, we spend a lot of time and energy thinking about tech ecosystems and how technologies are related to each other. one way to get at this idea of relationships between technologies is tag correlations, how often technology tags at stack overflow appear together relative to how often they appear separately. one place we see developers using tags at stack overflow is on their developer stories, or professional profiles/cvs/resumes. if we are interested in how technologies are connected and how they are used together, developers' own descriptions of their work and careers is a great place to get that.\n",
      "content\n",
      "a network of technology tags from developer stories on the stack overflow online developer community website.\n",
      "this is organized as two tables:\n",
      "stack_network_links contains links of the network, the source and target tech tags plus the value of the the link between each pair stack_network_nodes contains nodes of the network, the name of each node, which group that node belongs to (calculated via a cluster walktrap), and a node size based on how often that technology tag is used\n",
      "acknowledgements\n",
      "all stack overflow user contributions are licensed under cc-by-sa 3.0 with attribution required.\n",
      "nasa tracks about 15,000 near-earth objects -- small solar system bodies whose orbits bring them less than 1.3 au from the sun (i.e., within 130% of the the average distance between the earth and the sun). of these 15,000, 160 are comets. this dataset provides orbital data for these comets.\n",
      "the data\n",
      "notes on time and space\n",
      "timing information for each of these comets is given in barycentric dynamical time, or tdb. this is, very roughly, the number of days since january 1st, 4713 bc (see the wikipedia article on julian day for more info). check out those wikipedia articles for details.\n",
      "for information on inclination, argument, and longitude of the ascending node, look at this article.\n",
      "the non-gravitational forces are effects that accelerate or decelerate the comet, such as jets of gas.\n",
      "this dataset contains the following fields:\n",
      "object: the name of the comet\n",
      "epoch: the epoch for the comet, in tdb\n",
      "tp: time of perihelion passage, in tdb; this is the time when the comet was closest to the sun\n",
      "e: the orbital eccentricity of the comet\n",
      "i: inclination of the orbit with respect to the ecliptic plane and the equinox of j2000 (j2000-ecliptic), in degrees\n",
      "w: argument of perihelion (j2000-ecliptic), in degrees\n",
      "node: longitude of the ascending node (j2000-ecliptic), in degrees\n",
      "q: comet's distance at perihelion, in au\n",
      "q: comet's distance at aphelion, in au\n",
      "p: orbital period, in julian years\n",
      "a1: non-gravitational force parameter a1\n",
      "a2: non-gravitational force parameter a2\n",
      "a3: non-gravitational force parameter a3\n",
      "moid (au): minimum orbit intersection distance (the minimum distance between the osculating orbits of the neo and the earth)\n",
      "ref: orbital solution reference\n",
      "what should we try?\n",
      "what can we do with this dataset? - plot the comets' orbits - combine with earth's orbital data to predict close approaches\n",
      "acknowledgements\n",
      "this dataset was downloaded from the nasa data portal.\n",
      "context\n",
      "open payments is a national disclosure program created by the affordable care act (aca) and managed by centers for medicare & medicaid services (cms). the purpose of the program is to promote transparency into the financial relationships between pharmaceutical and medical device industries, and physicians and teaching hospitals. the financial relationships may include consulting fees, research grants, travel reimbursements, and payments from industry to medical practitioners.\n",
      "content\n",
      "there are 3 datasets that represent 3 different payment types:\n",
      "general payments: payments not made in connection with a research agreement. this dataset contains 65 variables.\n",
      "research payments: payments made in connection with a research agreement. this dataset contains 166 variables.\n",
      "physician ownership or investment interest: information about physicians who hold ownership or investment interest in the manufacturer/gpo or who have an immediate family member holding such interest. this dataset contains 29 variables.\n",
      "deleted/removed records: contains any deleted/removed records.\n",
      "a comprehensive methodology overview and data dictionary for each dataset can be found here.\n",
      "acknowledgements\n",
      "the original datasets can be found here.\n",
      "inspiration\n",
      "using the general payments dataset, can you determine any trends in the total amount of payment to hospitals and physicians across the medical specialties or by the form/nature of the payments?\n",
      "according to the research payments dataset, which area(s) of research or the type of drug/medical device receive the most amount of payment?\n",
      "international financial statistics (ifs) is a standard source of international statistics on all aspects of international and domestic finance. it reports, for most countries of the world, current data needed in the analysis of problems of international payments and of inflation and deflation, i.e., data on exchange rates, international liquidity, international banking, money and banking, interest rates, prices, production, international transactions, government accounts, and national accounts. last update in undata: 14 may 2010 if you need more current data, the imf has made their current database available for bulk download for personal use.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nations on the undata site. you can find the original dataset here.\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "context\n",
      "this dataset includes a log of all physical item checkouts from seattle public library. the dataset begins with checkouts occurring in april 2005, and is regularly updated. renewals are not included.\n",
      "content\n",
      "the dataset contains several types of files:\n",
      "checkout records hold the raw data. i've dropped several columns from these files in order to shrink the total dataset size down from a couple of dozen gigabytes; they can be rebuilt by merging with the library collection inventory.\n",
      "the data dictionary allows you to decode the 'itemtype' column from the checkout records.\n",
      "the library collection inventory is a dataset in its own right and stores important metadata about each title, such as the author and subjects.\n",
      "inspiration\n",
      "can you predict which books will be checked out in the coming month? spl posts fresh data every month, so you can check your forecast by downloading the new data from them.\n",
      "with a bit of imagination, this can be a great dataset for logistics modeling. make some assumptions about each location's storage capacity and where a book was checked out from and you've got a warehouse resource allocation problem.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the seattle public library. you can find the original copies of the three component datasets at the following links: - collection data - data dictionary - checkout records\n",
      "disclaimer\n",
      "the data made available here has been modified for use from its original source, which is the city of seattle. neither the city of seattle nor the office of the chief technology officer (octo) makes any claims as to the completeness, timeliness, accuracy or content of any data contained in this application; makes any representation of any kind, including, but not limited to, warranty of the accuracy or fitness for a particular use; nor are any such warranties to be implied or inferred with respect to the information or data furnished herein. the data is subject to change as modifications and updates are complete. it is understood that the information contained in the web feed is being used at one's own risk.\n",
      "for the complete terms of use, please see the city of seattle data policy.\n",
      "context\n",
      "collection of chat messages in night urban city between boys and girls.\n",
      "content\n",
      "data set of messages (more than 1 million of rows) in russian language from teenager population taken in period from 2012 to 2016 inclusive\n",
      "acknowledgements\n",
      "all personal info in the message' body were taken from public web source, and, though, are free of use.\n",
      "inspiration\n",
      "this dataset can be used to classify chat messages as male / female.\n",
      "key objectives\n",
      "extract phone numbers from messages. all phone numbers are located in ukraine and belongs to one from next operators\n",
      "+380 50\n",
      "+380 95\n",
      "+380 66\n",
      "+380 99\n",
      "+380 63\n",
      "+380 73\n",
      "+380 93\n",
      "+380 68\n",
      "+380 67\n",
      "+380 96\n",
      "+380 97\n",
      "+380 98\n",
      "classify chat messages by gender (male/female)\n",
      "context\n",
      "since industrialization, there has been an increasing concern about environmental pollution. as mentioned in the who report 7 million premature deaths annually linked to air pollution , air pollution is the world's largest single environmental risk. moreover as reported in the ny times article, india’s air pollution rivals china’s as world’s deadliest it has been found that india's air pollution is deadlier than even china's.\n",
      "using this dataset, one can explore india's air pollution levels at a more granular scale.\n",
      "content\n",
      "this data is combined(across the years and states) and largely clean version of the historical daily ambient air quality data released by the ministry of environment and forests and central pollution control board of india under the national data sharing and accessibility policy (ndsap).\n",
      "visualization of the mean rspm values over the years\n",
      "inspiration\n",
      "can we detect local trends? can we relate the air quality changes to changes in environmental policy in india?\n",
      "acknowledgements\n",
      "vishal subbiah (data downloading)\n",
      "context\n",
      "this dataset complements https://github.com/vmalyi/run-or-walk project which aims to detect whether the person is running or walking based on deep neural network and sensor data collected from ios device.\n",
      "this dataset has been accumulated with help of \"data collection\" ios app specially developed for this purpose: https://github.com/vmalyi/run-or-walk/tree/master/ios_app_data_collection.\n",
      "please note that this app is not available in the appstore yet.\n",
      "content\n",
      "currently, the dataset contains a single file which represents 88588 sensor data samples collected from accelerometer and gyroscope from iphone 5c in 10 seconds interval and ~5.4/second frequency. this data is represented by following columns (each column contains sensor data for one of the sensor's axes):\n",
      "acceleration_x\n",
      "acceleration_y\n",
      "acceleration_z\n",
      "gyro_x\n",
      "gyro_y\n",
      "gyro_z\n",
      "there is an activity type represented by \"activity\" column which acts as label and reflects following activities:\n",
      "\"0\": walking\n",
      "\"1\": running\n",
      "apart of that, the dataset contains \"wrist\" column which represents the wrist where the device was placed to collect a sample on:\n",
      "\"0\": left wrist\n",
      "\"1\": right wrist\n",
      "additionally, the dataset contains \"date\", \"time\" and \"username\" columns which provide information about the exact date, time and user which collected these measurements.\n",
      "context\n",
      "the data was scraped from several websites in czech republic and germany over a period of more than a year. originally i wanted to build a model for estimating whether a car is a good buy or a bad buy based on the posting. but i was unable to create a model i could be satisfied with and now have no use for this data. i'm a great believer in open data, so here goes.\n",
      "content\n",
      "the scrapers were tuned slowly over the course of the year and some of the sources were completely unstructured, so as a result the data is dirty, there are missing values and some values are very obviously wrong (e.g. phone numbers scraped as mileage etc.)\n",
      "there are roughly 3,5 million rows and the following columns:\n",
      "maker - normalized all lowercase\n",
      "model - normalized all lowercase\n",
      "mileage - in km\n",
      "manufacture_year\n",
      "engine_displacement - in ccm\n",
      "engine_power - in kw\n",
      "body_type - almost never present, but i scraped only personal cars, no motorcycles or utility vehicles\n",
      "color_slug - also almost never present\n",
      "stk_year - year of the last emission control\n",
      "transmission - automatic or manual\n",
      "door_count\n",
      "seat_count\n",
      "fuel_type - gasoline, diesel, cng, lpg, electric\n",
      "date_created - when the ad was scraped\n",
      "date_last_seen - when the ad was last seen. our policy was to remove all ads older than 60 days\n",
      "price_eur - list price converted to eur\n",
      "inspiration\n",
      "which factors determine the price of a car?\n",
      "with what accuracy can the price be predicted?\n",
      "can a model trained on all cars be used to accurately predict prices of models with only a few samples?\n",
      "in my analysis, there is too much variance even within individual models to reliably predict the price, can you prove me wrong? i would love to understand what i did wrong if you can.\n",
      "context\n",
      "starting something in fintech is the most difficult thing. you have no open data. these days i'm trying to do some algo-trading. maybe not in true sense, because it's not high frequency scalping. but anyway that's that.\n",
      "what?\n",
      "the data gives almost-realtime data for half of the nifty 50 stocks for last week of may and first 2 weeks of july.\n",
      "now here is the obvious question. the dataset does not have timestamp. that's because it is collected via web-socket streaming as it happens. sometimes once in a couple of seconds, sometimes 10-15 times in the same span. so there is no point to timestamp imho. anyway it'll be client-side timestamp, so not a true timestamp.\n",
      "description\n",
      "tick_data.csv contains only the price-volume data.\n",
      "volume: total volumes traded for the day\n",
      "last_price: denotes the quote price for latest trade\n",
      "list item instrument_list.csv contains description of the underlying instrument.\n",
      "p.s:\n",
      "*all the data points are not tick-by-tick update. rather it is mostly an update after 600 ms, provided a trade happened *\n",
      "what is the world saying about donald trump? find out in this dataset of over 37,000 reddit comments about the new us president.\n",
      "photo credit: gage skidmore, cc by-sa 2.0\n",
      "context\n",
      "list of all ufc fights since 2013 with summed up entries of each fighter's round by round record preceding that fight. created in the attempt to create a ufc fight winner predictor. dataset may not be great, i'm still new to this thing so appreciate any tips on cleaning up the set.\n",
      "content\n",
      "each row represents a single fight - with each fighter's previous records summed up prior to the fight. blank stats mean its the fighter's first fight since 2013 which is where granular data for ufc fights beings\n",
      "acknowledgements\n",
      "https://github.com/valish/ufc-api for the ufc api beautifulsoup and it's creators and hitkul my partner in crime\n",
      "inspiration\n",
      "can we draw decent predictions from this dataset?\n",
      "context\n",
      "this is all of shakespeare's plays.\n",
      "content\n",
      "this is a dataset comprised of all of shakespeare's plays. it includes the following:\n",
      "the first column is the data-line, it just keeps track of all the rows there are.\n",
      "the second column is the play that the lines are from.\n",
      "the third column is the actual line being spoken at any given time.\n",
      "the fourth column is the act-scene-line from which any given line is from.\n",
      "the fifth column is the player who is saying any given line.\n",
      "the sixth column is the line being spoken.\n",
      "inspiration\n",
      "i've been doing shakespeare for a while and i wanted to make a shakespearean chatbot.\n",
      "us adult census data relating income to social factors such as age, education, race etc.\n",
      "the us adult income dataset was extracted by barry becker from the 1994 us census database. the data set consists of anonymous information such as occupation, age, native country, race, capital gain, capital loss, education, work class and more. each row is labelled as either having a salary greater than \">50k\" or \"<=50k\".\n",
      "this data set is split into two csv files, named adult-training.txt and adult-test.txt.\n",
      "the goal here is to train a binary classifier on the training dataset to predict the column income_bracket which has two possible values \">50k\" and \"<=50k\" and evaluate the accuracy of the classifier with the test dataset.\n",
      "note that the dataset is made up of categorical and continuous features. it also contains missing values the categorical columns are: workclass, education, marital_status, occupation, relationship, race, gender, native_country\n",
      "the continuous columns are: age, education_num, capital_gain, capital_loss, hours_per_week\n",
      "this dataset was obtained from the uci repository, it can be found on\n",
      "https://archive.ics.uci.edu/ml/datasets/census+income, http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/\n",
      "usage this dataset is well suited to developing and testing wide linear classifiers, deep neutral network classifiers and a combination of both. for more info on combined deep and wide model classifiers, refer to the research paper by google https://arxiv.org/abs/1606.07792\n",
      "refer to this kernel for sample usage : https://www.kaggle.com/johnolafenwa/wage-prediction\n",
      "complete tutorial is available from http://johnolafenwa.blogspot.com.ng/2017/07/machine-learning-tutorial-1-wage.html?m=1\n",
      "context\n",
      "this dataset consist of data from 1985 ward's automotive yearbook. here are the sources\n",
      "sources:\n",
      "1) 1985 model import car and truck specifications, 1985 ward's automotive yearbook. 2) personal auto manuals, insurance services office, 160 water street, new york, ny 10038 3) insurance collision report, insurance institute for highway safety, watergate 600, washington, dc 20037\n",
      "content\n",
      "this data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. the second rating corresponds to the degree to which the auto is more risky than its price indicates. cars are initially assigned a risk factor symbol associated with its price. then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. actuarians call this process \"symboling\". a value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
      "the third factor is the relative average loss payment per insured vehicle year. this value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.\n",
      "note: several of the attributes in the database could be used as a \"class\" attribute.\n",
      "inspiration\n",
      "please bring it on whatever inferences you can get it.\n",
      "context\n",
      "the previous new york city policies eliminated all housing resources for homeless families and single adults. i wanted to see the consequences.\n",
      "content\n",
      "\"these raw data sets contain point-in-time (pit) estimates and national pit estimates of homelessness as well as national estimates of homelessness by state and estimates of chronic homelessness from 2007 - 2016. estimates of homeless veterans are also included beginning in 2011. the accompanying housing inventory count (hic) data is available as well from 2007 - 2016.\" (department of housing and urban development\n",
      "acknowledgements\n",
      "i would like to thank matthew schnars for providing this dataset from: https://www.hudexchange.info/resource/3031/pit-and-hic-data-since-2007/\n",
      "inspiration\n",
      "many of our fellow human beings go through hardships that we would never know about. but it's our obligation as a society to take care of one another. that's why i was hoping this dataset shine light on some of the challenges our cities and states are still facing in this topic.\n",
      "context\n",
      "this dataset is a compilation of 2.7 million news headlines published by times of india from 2001 to 2017, 17 years.\n",
      "a majority of the data is focusing on indian local news including national, city level and entertainment.\n",
      "agency website: https://timesofindia.indiatimes.com\n",
      "prepared by rohit kulkarni\n",
      "content\n",
      "csv rows: 2,735,347\n",
      "publish_date: date of the article being published online in yyyymmdd format\n",
      "headline_category: category of the headline, ascii, dot delimited, lowercase values\n",
      "headline_text: text of the headline in english, very rare non-ascii characters\n",
      "start date: 2001-01-01 end date: 2017-12-31\n",
      "see this kernal for overview of trends and categories\n",
      "inspiration\n",
      "this news dataset is a persistent historical archive of noteable events in the indian subcontinent from start-2001 to end-2017, recorded in real time by the journalists of india.\n",
      "times group as a news agency, reaches out a very wide audience across asia and drawfs every other agency in the quantity of english articles published per day. due to the heavy daily volume (avg. 650 articles) over multiple years, this data offers a deep insight into indian society, its priorities, events, issues and talking points and how they have unfolded over time.\n",
      "it is possible to chop this dataset into a smaller piece for a more focused analysis, based on one or more facets.\n",
      "time range: records during 2014 election, 2006 mumbai bombings\n",
      "one or more categories: like mumbai, movie releases, icc updates, magazine, middle east\n",
      "one or more keywords: like crime or ecology related words; names of political parties, celebrities, corporations.\n",
      "acknowledgements\n",
      "the headlines are extracted from several gb of raw html files using jsoup, java and bash. the entire process takes 3.5 minutes.\n",
      "this logic also : chooses the best worded headline for each article (longest one is usually picked) ; clusters about 17k categories to 200 large groups ; removes records where the date is ambiguous (9k cases) ; finally cleans the selected headline via a string 'domestication' function (which i use for any wild text from the internet).\n",
      "the final categories are as per the latest sitemap. around 1.5k rare categories remain and these records (~20k) can be filtered out easily during analysis. the category is unknown for ~200k records.\n",
      "similar news datasets exploring other attributes, countries and topics can be seen on my profile.\n",
      "citation for usage:\n",
      "rohit kulkarni (2017), news headlines of india 2001-2017 [csv data file], doi:10.7910/dvn/j7byrx, retrieved from: [this url]\n",
      "context\n",
      "the data is technical spec of cars. the dataset is downloaded from uci machine learning repository\n",
      "content\n",
      "title: auto-mpg data\n",
      "sources: (a) origin: this dataset was taken from the statlib library which is maintained at carnegie mellon university. the dataset was used in the 1983 american statistical association exposition. (c) date: july 7, 1993\n",
      "past usage:\n",
      "see 2b (above)\n",
      "quinlan,r. (1993). combining instance-based and model-based learning. in proceedings on the tenth international conference of machine learning, 236-243, university of massachusetts, amherst. morgan kaufmann.\n",
      "relevant information:\n",
      "this dataset is a slightly modified version of the dataset provided in the statlib library. in line with the use by ross quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. the original dataset is available in the file \"auto-mpg.data-original\".\n",
      "\"the data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (quinlan, 1993)\n",
      "number of instances: 398\n",
      "number of attributes: 9 including the class attribute\n",
      "attribute information:\n",
      "mpg: continuous\n",
      "cylinders: multi-valued discrete\n",
      "displacement: continuous\n",
      "horsepower: continuous\n",
      "weight: continuous\n",
      "acceleration: continuous\n",
      "model year: multi-valued discrete\n",
      "origin: multi-valued discrete\n",
      "car name: string (unique for each instance)\n",
      "missing attribute values: horsepower has 6 missing values\n",
      "acknowledgements\n",
      "dataset: uci machine learning repository data link : https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
      "inspiration\n",
      "i have used this dataset for practicing my exploratory analysis skills.\n",
      "context\n",
      "this dataset contains the prevalence and trends of tobacco use for 1995-2010. percentages are weighted to population characteristics. data are not available if it did not meet behavioral risk factor surveillance system (brfss) stability requirements. for more information on these requirements, as well as risk factors and calculated variables, see the technical documents and survey data for a specific year - http://www.cdc.gov/brfss/annual_data/annual_data.htm.\n",
      "content\n",
      "this dataset has 7 variables:\n",
      "year\n",
      "state\n",
      "smoke everyday\n",
      "smoke some days\n",
      "former smoker\n",
      "never smoked\n",
      "location 1\n",
      "acknowledgements\n",
      "the original dataset can be found here.\n",
      "recommended citation: centers for disease control and prevention (cdc). behavioral risk factor surveillance system. atlanta, georgia: u.s. department of health and human services, centers for disease control and prevention, [appropriate year].\n",
      "inspiration\n",
      "how does tobacco use change over time?\n",
      "does tobacco use differ by state?\n",
      "context\n",
      "latest poverty and inequality indicators compiled from officially recognized international sources. poverty indicators include the poverty headcount ratio, poverty gap, and number of poor at both international and national poverty lines. inequality indicators include the gini index and income or consumption distributions. the database includes national, regional and global estimates.\n",
      "content\n",
      "this dataset contains country names and indicator variables from 1974 until 2015. additional materials and detailed descriptions of the datasets can be downloaded from here.\n",
      "acknowledgement\n",
      "the original datasets and data dictionaries can be found here.\n",
      "inspiration\n",
      "some ideas for exploring the dataset:\n",
      "how does the poverty headcount ratio differ across countries? can you visualize the temporal trend?\n",
      "which countries have the highest or lowest gini index as estimated by the world bank? is this indicator correlated with other indicators such as urban/rural poverty or income status?\n",
      "context\n",
      "the gun violence archive is an online archive of gun violence incidents collected from over 2,000 media, law enforcement, government and commercial sources daily in an effort to provide near-real time data about the results of gun violence. gva in an independent data collection and research group with no affiliation with any advocacy organization.\n",
      "content\n",
      "this dataset includes files that separate gun violence incidents by category, including deaths and injuries of children and teens, and a collection of mass shootings.\n",
      "inspiration\n",
      "what has been the trend of gun violence in the past few years?\n",
      "what states have the highest incidents per capita per year? how has this metric changed over time?\n",
      "are officer involved shootings on the rise? where are they most concentrated? do they correlate with the rates of accidental deaths and mass shootings?\n",
      "acknowledgements\n",
      "this dataset is owned by the gun violence archive, and can be accessed in its original form here.\n",
      "this dataset consists of the official statistics on the 11,538 athletes and 306 events at the 2016 olympic games in rio de janeiro. the athletes file includes the name, nationality (as a three letter ioc country code), gender, age (as date of birth), height in meters, weight in kilograms, sport, and quantity of gold, silver, and/or bronze medals won for every olympic athlete at rio. the events file lists the name, sport, discipline (if available), gender of competitors, and venue(s) for every olympic event at rio 2016.\n",
      "credits\n",
      "source data: rio 2016 website\n",
      "data files: github user flother\n",
      "context\n",
      "these datasets provides an opportunity to perform analyses on the fashion trend of innerwear and swimwear products.\n",
      "content\n",
      "they were created by extracting data from from the popular retail sites via promptcloud's data extraction solutions.\n",
      "sites covered:\n",
      "amazon\n",
      "victoria's secret\n",
      "btemptd\n",
      "calvin klein\n",
      "hanky panky\n",
      "american eagle\n",
      "macy's\n",
      "nordstrom\n",
      "topshop usa\n",
      "time period: june, 2017 to july, 2017\n",
      "inspiration\n",
      "some of the most common questions that can be answered are:\n",
      "how does the pricing differ depending on the brand?\n",
      "topic modelling on the product description\n",
      "what are the most common color used by different brands?\n",
      "analyses on the product ratings (wherever applicable)\n",
      "common style attributes (wherever applicable)\n",
      "context\n",
      "banksim is an agent-based simulator of bank payments based on a sample of aggregated transactional data provided by a bank in spain. the main purpose of banksim is the generation of synthetic data that can be used for fraud detection research. statistical and a social network analysis (sna) of relations between merchants and customers were used to develop and calibrate the model. our ultimate goal is for banksim to be usable to model relevant scenarios that combine normal payments and injected known fraud signatures. the data sets generated by banksim contain no personal information or disclosure of legal and private customer transactions. therefore, it can be shared by academia, and others, to develop and reason about fraud detection methods. synthetic data has the added benefit of being easier to acquire, faster and at less cost, for experimentation even for those that have access to their own data. we argue that banksim generates data that usefully approximates the relevant aspects of the real data.\n",
      "content\n",
      "we ran banksim for 180 steps (approx. six months), several times and calibrated the parameters in order to obtain a distribution that get close enough to be reliable for testing. we collected several log files and selected the most accurate. we injected thieves that aim to steal an average of three cards per step and perform about two fraudulent transactions per day. we produced 594643 records in total. where 587443 are normal payments and 7200 fraudulent transactions. since this is a randomised simulation the values are of course not identical to original data.\n",
      "acknowledgements\n",
      "this research was conducted during my phd studies in sweden at blekinge institute of technology (bth ww.bth.se). more about it: http://edgarlopez.net\n",
      "original paper\n",
      "please refer to this dataset using the following citations:\n",
      "lopez-rojas, edgar alonso ; axelsson, stefan banksim: a bank payments simulator for fraud detection research inproceedings 26th european modeling and simulation symposium, emss 2014, bordeaux, france, pp. 144–152, dime university of genoa, 2014, isbn: 9788897999324. https://www.researchgate.net/publication/265736405_banksim_a_bank_payment_simulation_for_fraud_detection_research\n",
      "context\n",
      "religious texts play a key role in isis ideology, propaganda, and recruitment. this dataset is a compilation of all of the religious and ideological texts (muslim, christian, jewish, and other) used in isis english-based magazines.\n",
      "content\n",
      "we scraped 15 issues of dabiq (6/2014 to 7/2016) and 9 issues of rumiyah (9/2016 to 5/2017) producing a total of 2,685 texts. we classified the data as follows:\n",
      "magazine: dabiq or rumiyah\n",
      "issue #\n",
      "date (month and year)\n",
      "type of text (qur'an, hadeeth, religious figure, etc)\n",
      "source: what the source of the text was\n",
      "the quote itself\n",
      "purpose: support isis, refute another group\n",
      "the article from which the quote is derived\n",
      "acknowledgements\n",
      "we would like to thank asma shah for helping to compile this dataset. asma is a junior at the university of maryland, college park, where she is studying criminal justice and computer science. she is currently an intern with the department of justice where she does data science work. she has previously done counter-terrorism research with fifth tribe and the national consortium for the studies of terrorism and responses to terrorism.\n",
      "we would also like to express our gratitude to abdul aziz suraqah (canada), saif ul hadi (india), and abdulbasit kassam (nigeria) for helping with the classification of some of the more obscure texts.\n",
      "inspiration\n",
      "we would like this data to be analyzed by religious clerics to develop rebuttals of isis propaganda, data scientists to generate insights from the texts, and policymakers to understand how faith can shape countering violent extremism efforts. we also need help classifying some of the data that could not be identified and has been marked 'unknown.'\n",
      "context\n",
      "the script used to acquire all of the following data can be found in this github repository. this repository also contains the modeling codes and will be updated continually, so welcome starring or watching!\n",
      "stock market data can be interesting to analyze and as a further incentive, strong predictive models can have large financial payoff. the amount of financial data on the web is seemingly endless. a large and well structured dataset on a wide array of companies can be hard to come by. here provided a dataset with historical stock prices (last 12 years) for 29 of 30 djia companies (excluding 'v' because it does not have the whole 12 years data).\n",
      "      ['mmm', 'axp', 'aapl', 'ba', 'cat', 'cvx', 'csco', 'ko', 'dis', 'xom', 'ge',\n",
      "\n",
      "      'gs', 'hd', 'ibm', 'intc', 'jnj', 'jpm', 'mcd', 'mrk', 'msft', 'nke', 'pfe',\n",
      "\n",
      "      'pg', 'trv', 'utx', 'unh', 'vz', 'wmt', 'googl', 'amzn', 'aaba']\n",
      "in the future if you wish for a more up to date dataset, this can be used to acquire new versions of the .csv files.\n",
      "content\n",
      "the data is presented in a couple of formats to suit different individual's needs or computational limitations. i have included files containing 13 years of stock data (in the all_stocks_2006-01-01_to_2018-01-01.csv and corresponding folder) and a smaller version of the dataset (all_stocks_2017-01-01_to_2018-01-01.csv) with only the past year's stock data for those wishing to use something more manageable in size.\n",
      "the folder individual_stocks_2006-01-01_to_2018-01-01 contains files of data for individual stocks, labelled by their stock ticker name. the all_stocks_2006-01-01_to_2018-01-01.csv and all_stocks_2017-01-01_to_2018-01-01.csv contain this same data, presented in merged .csv files. depending on the intended use (graphing, modelling etc.) the user may prefer one of these given formats.\n",
      "all the files have the following columns: date - in format: yy-mm-dd\n",
      "open - price of the stock at market open (this is nyse data so all in usd)\n",
      "high - highest price reached in the day\n",
      "low close - lowest price reached in the day\n",
      "volume - number of shares traded\n",
      "name - the stock's ticker name\n",
      "inspiration\n",
      "this dataset lends itself to a some very interesting visualizations. one can look at simple things like how prices change over time, graph an compare multiple stocks at once, or generate and graph new metrics from the data provided. from these data informative stock stats such as volatility and moving averages can be easily calculated. the million dollar question is: can you develop a model that can beat the market and allow you to make statistically informed trades!\n",
      "acknowledgement\n",
      "this data description is adapted from the dataset named 's&p 500 stock data'. this data is scrapped from google finance using the python library 'pandas_datareader'. special thanks to kaggle, github and the market.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this dataset contains 4242 images of flowers. the data collection is based on the data flicr, google images, yandex images. you can use this datastet to recognize plants from the photo.\n",
      "content\n",
      "the pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion. for each class there are about 800 photos. photos are not high resolution, about 320x240 pixels. photos are not reduced to a single size, they have different proportions!\n",
      "description:\n",
      "happydb is a corpus of more than 100,000 happy moments crowd-sourced via amazon’s mechanical turk.\n",
      "each worker is given the following task: what made you happy today? reflect on the past 24 hours, and recall three actual events that happened to you that made you happy. write down your happy moment in a complete sentence. (write three such moments.)\n",
      "the goal of the corpus is to advance the understanding of the causes of happiness through text-based reflection.\n",
      "more information is available on the happydb website (https://rit-public.github.io/happydb/).\n",
      "content:\n",
      "cleaned_hm.csv: the cleaned-up corpus of 100,000 crowd-sourced happy moments. for cleaning up, we have done spell checking over the whole corpus, and remove empty or one-word statements. the raw happy moments are retained for reference, and the author of each happy moment is represented by the his/her worker id.\n",
      "demographic.csv: the demographic information of the worker who provided the moment. the information includes worker id, age, country, gender, marital status, and status of parenthood.\n",
      "have fun with the data! feel free to contact us with any questions.\n",
      "sample questions:\n",
      "to provide some inspiration, here are a few sample interesting exploration questions.\n",
      "what are the popular sports/movies/books/purchased products/tourist destinations/... that make people happy?\n",
      "can we predict gender/marriage status/parenthood/age groups based on happy moment texts?\n",
      "how many indoor and outdoor activities are in the corpus respectively?\n",
      "can we find interesting ways of clustering happy moments?\n",
      "citation:\n",
      "please cite the following publication if you are using the dataset for your work:\n",
      "happydb: a corpus of 100,000 crowdsourced happy moments, lrec 2018 (to appear)\n",
      "akari asai, sara evensen, behzad golshan, alon halevy, vivian li, andrei lopatenko, daniela stepanov, yoshihiko suhara, wang-chiew tan and yinzhan xu\n",
      "context\n",
      "deskdrop is an internal communications platform developed by ci&t, focused in companies using google g suite. among other features, this platform allows companies employees to share relevant articles with their peers, and collaborate around them.\n",
      "content\n",
      "this rich and rare dataset contains a real sample of 12 months logs (mar. 2016 - feb. 2017) from ci&t's internal communication platform (deskdrop).\n",
      "i contains about 73k logged users interactions on more than 3k public articles shared in the platform.\n",
      "this dataset features some distinctive characteristics:\n",
      "item attributes: articles' original url, title, and content plain text are available in two languages (english and portuguese).\n",
      "contextual information: context of the users visits, like date/time, client (mobile native app / browser) and geolocation.\n",
      "logged users: all users are required to login in the platform, providing a long-term tracking of users preferences (not depending on cookies in devices).\n",
      "rich implicit feedback: different interaction types were logged, making it possible to infer the user's level of interest in the articles (eg. comments > likes > views).\n",
      "multi-platform: users interactions were tracked in different platforms (web browsers and mobile native apps)\n",
      "if you like it, please upvote!\n",
      "take a look in these featured python kernels:\n",
      "- deskdrop datasets eda: exploratory analysis of the articles and interactions in the dataset\n",
      "- deskdrop articles topic modeling: a statistical analysis of the main articles topics using lda\n",
      "- recommender systems in python 101: a practical introduction of the main recommender systems approaches: popularity model, collaborative filtering, content-based filtering and hybrid filtering.\n",
      "acknowledgements\n",
      "we thank ci&t for the support and permission to share a sample of real usage data from its internal communication platform: deskdrop.\n",
      "inspiration\n",
      "the two main approaches for recommender systems are collaborative filtering and content-based filtering.\n",
      "in the recsys community, there are some popular datasets available with users ratings on items (explicit feedback), like movielens and netflix prize, which are useful for collaborative filtering techniques.\n",
      "therefore, it is very difficult to find open datasets with additional item attributes, which would allow the application of content-based filtering techniques or hybrid approaches, specially in the domain of ephemeral textual items (eg. articles and news).\n",
      "news datasets are also reported in academic literature as very sparse, in the sense that, as users are usually not required to log in in news portals, ids are based on device cookies, making it hard to track the users page visits in different portals, browsing sessions and devices.\n",
      "this difficult scenario for research and experiments on content recommender systems was the main motivation for the sharing of this dataset.\n",
      "brazilian? you can read a portuguese version of this article here.\n",
      "context\n",
      "last year, while i was attending a data science course in germany, my country was impeaching its president. my colleagues asked me to explain what was happening in brazil and the possible political outcomes in south america. although i was able to give a general context and tell multiple arguments in favor and against the impeachment, deep inside, my answer was \"i really don't know\".\n",
      "understanding what happens in politics is something that takes a lot of effort and research. when i decided i had to use my tech skills to make myself a better citizen, i dived into government data and started operation serenata de amor.\n",
      "after reporting hundreds of politicians for small acts of corruption and learning how to encourage the population to engage in the democratic processes, my studies drove me to understand the legislative activity.\n",
      "brazilians elect 594 citizens to be their representatives in the national congress. how can we be sure that they are not defending their own interests or those who paid for their campaigns? my way, as a data scientist, is to ask the data.\n",
      "content\n",
      "the national congress of brazil is composed of a lower (chamber of deputies) and an upper house (federal senate). in the first version of this dataset, you are going to find data only from the chamber of deputies. with 513 representatives, 86% of the congresspeople, i hope you have enough data to explore for some time.\n",
      "would be impossible for me, a citizen without government ties, to collect this data without the help of public servants. i processed 9,717 fixed-width files and 73 xml's made officially available by the chamber of deputies and created 5 csv's containing the same information. multiple fields of the same file telling the same thing (e.g. body_id, body_name and body_abbreviation) were removed.\n",
      "data on session attendance, votes, and propositions since past century were collected and scripted in a reproducible manner. the data collection and pre-processing scripts are available in a github repository, under an open source license.\n",
      "everything was collected from the chamber of deputies website at december 27, 2017, containing the whole legislative activity of the year. presence and votes date from 1999, propositions go as far as 1946.\n",
      "when in question about the legislative process and how the sessions work in real world, the internal regulation of the chamber of deputies is the best portuguese documentation for research. it's free!\n",
      "acknowledgements\n",
      "since the data was collected from a government website and the brazilian law states that access to this information is free to any citizen, i am placing my own work published here in public domain.\n",
      "i'd like to thank the hundreds of people financially supporting the work of operation serenata de amor and those responsible for passing the information access bill in 2011.\n",
      "inspiration\n",
      "the legislative activity should tell the history while it's happening. how much has the congress changed over the past decades? do the congresspeople maintain the same political views or they vary on a weekly basis? do people vote together with their state or party peers? how often? can you model an algorithm to tell us the real parties inside brazilian congress?\n",
      "introduction\n",
      "here you find a very simple, beginner-friendly data set. no sparse matrices, no fancy tools needed to understand what's going on. just a couple of rows and columns. super simple stuff. as explained below, this data set is used for a competition. as it turns out, this competition tends to reveal a common truth in data science: kiss - keep it simple stupid\n",
      "what is so special about this data set is, given it's simplicity, it pays off to use \"simple\" classifiers as well. this year's competition was won by a c5.0 . can you do better?\n",
      "description\n",
      "we are looking at cold call results. turns out, same salespeople called existing insurance customers up and tried to sell car insurance. what you have are details about the called customers. their age, job, marital status, whether the have home insurance, a car loan, etc. as i said, super simple.\n",
      "what i would love to see is some of you applying some crazy xgboost classifiers, which we can square off against some logistic regressions. it would be curious to see what comes out on top. thank you for your time, i hope you enjoy using the data set.\n",
      "acknowledgements\n",
      "thanks goes to the decision science and systems chair of technical university of munich (tum) for getting the data set from a real world company and making it available to be shared publicly. also vladimir fux, who oversees the challenge associated with this data set.\n",
      "inspiration\n",
      "this is a data set used for teaching entry level data mining skills at the tum. every year there is a competition as part of the curriculum of a particular course. this data mining cup teaches some of the very fundamentals that are always worthy to be revisited, especially by pros abundant at kaggle. for some of my thoughts see the verbose comments in the kernel.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "ministry of human resource development (mhrd), govt of india has initiated an all india survey on higher education (aishe) in the year 2010-11 to build a robust database and to assess the correct picture of higher education in the country.\n",
      "the main objectives of the survey was to\n",
      "identify & capture all the institutions of higher learning in the country\n",
      "collect the data from all the higher education institutions on various aspects of higher education.\n",
      "data was collected on following broad items\n",
      "institution’s basic details\n",
      "teacher’s details\n",
      "details of non-teaching staff\n",
      "programme conducted under various faculties/schools & departments/centres\n",
      "students enrolled in these programme\n",
      "examination result of terminal year of each programme\n",
      "financial information such as receipt and expenditure under various heads\n",
      "availability of infrastructure\n",
      "scholarships, loans & accreditation\n",
      "source: aishe(pdf)\n",
      "content\n",
      "this dataset contains unit level data from aishe from 2011-12 to 2015-16.\n",
      "csv file list:\n",
      "accreditation.csv\n",
      "college.csv\n",
      "college_institution.csv\n",
      "college_institution_accreditation.csv\n",
      "college_institution_department.csv\n",
      "college_institution_faculty.csv\n",
      "college_institution_non_teaching_staff_count.csv\n",
      "college_institution_student_hostel.csv\n",
      "college_institution_teaching_staff.csv\n",
      "college_institution_teaching_staff_sanctioned_strength.csv\n",
      "course.csv\n",
      "course_enrolled_foreign_student_count.csv\n",
      "course_enrolled_student_count.csv\n",
      "course_examination_result.csv\n",
      "department.csv\n",
      "educational_institution_course.csv\n",
      "enrolled_distance_student_university.csv\n",
      "enrolled_distance_student_university_count.csv\n",
      "enrolled_foreign_student_count.csv\n",
      "enrolled_student_count.csv\n",
      "examination_result.csv\n",
      "faculty.csv\n",
      "faculty_department.csv\n",
      "infrastructure.csv\n",
      "loan.csv\n",
      "metadata.csv\n",
      "non_teaching_staff_count.csv\n",
      "other_minority_college_regular .csv\n",
      "other_minority_standalone_distance.csv\n",
      "other_minority_standalone_regular .csv\n",
      "other_minority_university_distance.csv\n",
      "other_minority_university_regular .csv\n",
      "persons_count_by_category.csv\n",
      "private_students_result.csv\n",
      "ref_broad_discipline_group.csv\n",
      "ref_broad_discipline_group_category.csv\n",
      "ref_college_institution_statutory_body.csv\n",
      "ref_count_by_category_remarks.csv\n",
      "ref_country.csv\n",
      "ref_course_level.csv\n",
      "ref_course_mode.csv\n",
      "ref_course_type.csv\n",
      "ref_diploma_course.csv\n",
      "ref_district.csv\n",
      "ref_examination_system.csv\n",
      "ref_institute_type.csv\n",
      "ref_institution_management.csv\n",
      "ref_non_teaching_staff_group.csv\n",
      "ref_non_teaching_staff_type.csv\n",
      "ref_programme.csv\n",
      "ref_programme_broad_discipline_group_and_category.csv\n",
      "ref_programme_statutory_body.csv\n",
      "ref_speciality.csv\n",
      "ref_standalone_institution.csv\n",
      "ref_state.csv\n",
      "ref_state_body.csv\n",
      "ref_student_hostel_type.csv\n",
      "ref_teaching_staff_designation.csv\n",
      "ref_teaching_staff_selection_mode.csv\n",
      "ref_university.csv\n",
      "ref_university_college_type.csv\n",
      "ref_university_type.csv\n",
      "regional_center.csv\n",
      "scholarship.csv\n",
      "staff_quarter.csv\n",
      "standalone_institution.csv\n",
      "standalone_institution_accreditation.csv\n",
      "standalone_institution_department.csv\n",
      "standalone_institution_faculty.csv\n",
      "standalone_institution_non_teaching_staff_count.csv\n",
      "standalone_institution_student_hostel.csv\n",
      "standalone_institution_teaching_staff.csv\n",
      "standalone_institution_teaching_staff_sanctioned_strength.csv\n",
      "student_hostel.csv\n",
      "teaching_staff.csv\n",
      "teaching_staff_count.csv\n",
      "teaching_staff_sanctioned_strength.csv\n",
      "university.csv\n",
      "university_accreditation.csv\n",
      "university_department.csv\n",
      "university_enrolled_distance_student.csv\n",
      "university_faculty.csv\n",
      "university_non_teaching_staff_count.csv\n",
      "university_private_students_result.csv\n",
      "university_student_hostel.csv\n",
      "university_teaching_staff.csv\n",
      "university_teaching_staff_sanctioned_strength.csv\n",
      "acknowledgements\n",
      "ministry of human resource development (mhrd), govt of india has published this dataset on open govt data india platform under govt. open data license - india.\n",
      "mhrd has also published some reports from this survey.\n",
      "inspiration\n",
      "this is an interesting dataset to get the holistic picture of higher education system in india. one of the main objective of dept. of higher education is to increase the gross enrolment ratio (grt) to 15% by 2011-12 and to 21% by 12th five year plan (2012-17). one can look at things like the objective like this has been achieved or can be achieved based on the progress of past data. there are several other things that can be analysed from this dataset.\n",
      "pupil-teacher ratio (ptr)\n",
      "out-turn\n",
      "gender parity index (gpi) etc.,\n",
      "context\n",
      "contains weekly purchased quantities of 800 over products over 52 weeks. these data were used in the paper \"time series clustering: a superior alternative for market basket analysis\" by tan, swee chuan and san lau, jess pei.\n",
      "content\n",
      "each row represents a different product\n",
      "each column represents a week of the year (52 total weeks). the last half of the columns are normalized for you.\n",
      "values represent quantity of the products sold during the week\n",
      "52 weeks: w0, w1, ..., w51\n",
      "normalised vlaues of weekly data: normalised 0, normalised 1, ..., normalised 51\n",
      "acknowledgements\n",
      "tan, swee chuan and san lau, jess pei, time series clustering: a superior alternative for market basket analysis.\n",
      "this dataset was downloaded from the uci machine learning repository. https://archive.ics.uci.edu/ml/datasets/sales_transactions_dataset_weekly\n",
      "context\n",
      "i was thinking about a dataset that i could provide and when i was reading through the livefromnewyork subreddit i got the idea: what about a saturday night live dataset? wouldn't it be fun to analyze the data about a tv show that airs since the 70s?\n",
      "content\n",
      "i aim to improve the dataset over time and update the files with more data. but i think that i have enough already so that people can work with it.\n",
      "there are files for the following objects:\n",
      "season\n",
      "episode\n",
      "title\n",
      "actor\n",
      "actor_title (mapping of actors and titles)\n",
      "rating (episode rating from imdb.com)\n",
      "acknowledgements\n",
      "a lot of the data comes from http://www.snlarchives.net where joel navaroli (@snlmedia) created a great snl archive. you can find everything about snl there. want to know about the 5th sketch in the 3rd episode in season 13? go there to find out!\n",
      "i got the ratings from imdb.com.\n",
      "i used scrapy to get the data from the websites.\n",
      "inspiration\n",
      "since snl is such a long running tv show i thought it would be interesting to see how it developed over time. there are also some prejudices around, like \"there was a big slump from season x to y\". do the user ratings reflect that? i provided an example analysis, so that everyone can get started easily with the data.\n",
      "source\n",
      "you can find everything about the dataset in the github repository: http://www.github.com/hhllcks/snldb\n",
      "planespotters.net has a full database on airlines around the world and the airplanes that each owns and operates. this dataset collects the top 100+ airlines in the world (by the size of their fleet). it is combined with information found on wikipedia on the respective airline's fleet and the average value/cost of the manufactured airplane.\n",
      "updated january 2017.\n",
      "dataset includes:\n",
      "parent airline: i.e. international airlines group (iag)\n",
      "airline: i.e. iberia, aer lingus, british airways...etc. which are owned by iag\n",
      "aircraft type: manufacturer & model\n",
      "current: quantity of airplanes in operation\n",
      "future: quantity of airplanes on order, from planespotter.net\n",
      "order: quantity airplanes on order, from wikipedia\n",
      "unit cost: average unit cost ($m) of aircraft type, as found by wikipedia and various google searches\n",
      "total cost: current quantity * unit cost ($m)\n",
      "average age: average age of \"current\" airplanes by \"aircraft type\"\n",
      "sources: planespotters.net wikipedia.org\n",
      "context\n",
      "i've always wanted to have a proper sample forex currency rates dataset for testing purposes, so i've created one.\n",
      "content\n",
      "the data contains forex eurusd currency rates in 15-minute slices (ohlc - open high low close, and volume). bid price only. spread is not provided, so be careful.\n",
      "(quick reminder: bid price + spread = ask price)\n",
      "the dates are in the yyyy-mm-dd hh:mm format, gmt. volume is in units.\n",
      "acknowledgements\n",
      "dukascopy bank sa https://www.dukascopy.com/swiss/english/marketwatch/historical/\n",
      "inspiration\n",
      "just would like to see if there is still an way to beat the current forex market conditions, with the prop traders' advanced automatic algorithms running in the wild.\n",
      "context:\n",
      "glottolog (http://glottolog.org) provides a comprehensive catalogue of the world's languages, language families and dialects. it assigns a unique and stable identifier (the glottocode) to (in principle) all languoids, i.e. all families, languages, and dialects.\n",
      "content:\n",
      "this dataset contains information on 1) the geographic location of languages and dialects, 2) their familial relationships and 3) a list of scholarly sources where information on languages was found.\n",
      "acknowledgements:\n",
      "this dataset was the current version of glottolog as of july 20, 2017. if you publish work using this dataset, please use the following citation:\n",
      "hammarström, harald & haspelmath, martin & forkel, robert. 2017. glottolog 3.0. jena: max planck institute for the science of human history. (available online at http://glottolog.org, accessed on 2017-03-23.)\n",
      "inspiration:\n",
      "can you plot the geographic location of each language family or langoid?\n",
      "where are most extinct languages found?\n",
      "can you find which language was documented in what decade? which areas of the focus of more or less documentation?\n",
      "you may also be interested in:\n",
      "atlas of pidgin and creole language structures\n",
      "the sign language analyses (slay) database\n",
      "world atlas of language structures: information on the linguistic structures in 2,679 languages\n",
      "context:\n",
      "crime in major metropolitan areas, such as london, occurs in distinct patterns. this data covers the number of criminal reports by month, lsoa borough, and major/minor category from jan 2008-dec 2016.\n",
      "content:\n",
      "13m rows containing counts of criminal reports by month, lsoa borough, and major/minor category.\n",
      "acknowledgements:\n",
      "txt file was pulled from google cloud platform and converted to csv. photo by james sutton.\n",
      "inspiration:\n",
      "are there seasonal or time-of-week/day changes in crime occurrences? any boroughs where particular crimes are increasing or decreasing? policy makers use this data to plan upcoming budgets and deployment--can you use previous year crime reports to reliably predict later trends? if you normalize by borough population, can you find any areas where crime is more or less likely?\n",
      "context\n",
      "pm2.5 readings are often included in air quality reports from environmental authorities and companies. pm2.5 refers to atmospheric particulate matter (pm) that have a diameter less than 2.5 micrometers. in other words, it's used as a measure of pollution.\n",
      "content\n",
      "the time period for this data is between jan 1st, 2010 to dec 31st, 2015. missing data are denoted as na.\n",
      "no: row number\n",
      "year: year of data in this row\n",
      "month: month of data in this row\n",
      "day: day of data in this row\n",
      "hour: hour of data in this row\n",
      "season: season of data in this row\n",
      "pm: pm2.5 concentration (ug/m^3)\n",
      "dewp: dew point (celsius degree)\n",
      "temp: temperature (celsius degree)\n",
      "humi: humidity (%)\n",
      "pres: pressure (hpa)\n",
      "cbwd: combined wind direction\n",
      "iws: cumulated wind speed (m/s)\n",
      "precipitation: hourly precipitation (mm)\n",
      "iprec: cumulated precipitation (mm)\n",
      "acknowledgements\n",
      "liang, x., s. li, s. zhang, h. huang, and s. x. chen (2016), pm2.5 data reliability, consistency, and air quality assessment in five chinese cities, j. geophys. res. atmos., 121, 10220â€“10236.\n",
      "the files were downloaded from the uci machine learning repository and have not been modified. https://archive.ics.uci.edu/ml/datasets/pm2.5+data+of+five+chinese+cities#\n",
      "context:\n",
      "hazardous air pollutants, also known as toxic air pollutants or air toxics, are those pollutants that are known or suspected to cause cancer or other serious health effects, such as reproductive effects or birth defects, or adverse environmental effects. the environmental protection agency (epa) tracks 187 air pollutants. see https://www.epa.gov/haps/ for more information.\n",
      "content:\n",
      "the daily summary file contains data for every monitor (sampled parameter) in the environmental protection agency (epa) database for each day. this file will contain a daily summary record that is: 1. the aggregate of all sub-daily measurements taken at the monitor. 2. the single sample value if the monitor takes a single, daily sample (e.g., there is only one sample with a 24-hour duration). in this case, the mean and max daily sample will have the same value.\n",
      "fields descriptions: 1. state code: the federal information processing standards (fips) code of the state in which the monitor resides.\n",
      "county code: the fips code of the county in which the monitor resides.\n",
      "site num: a unique number within the county identifying the site.\n",
      "parameter code: the aqs code corresponding to the parameter measured by the monitor.\n",
      "poc: this is the “parameter occurrence code” used to distinguish different instruments that measure the same parameter at the same site.\n",
      "latitude: the monitoring site’s angular distance north of the equator measured in decimal degrees.\n",
      "longitude: the monitoring site’s angular distance east of the prime meridian measured in decimal degrees.\n",
      "datum: the datum associated with the latitude and longitude measures.\n",
      "parameter name: the name or description assigned in aqs to the parameter measured by the monitor. parameters may be pollutants or non-pollutants.\n",
      "sample duration: the length of time that air passes through the monitoring device before it is analyzed (measured). so, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). for continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).\n",
      "pollutant standard: a description of the ambient air quality standard rules used to aggregate statistics. (see description at beginning of document.)\n",
      "date local: the calendar date for the summary. all daily summaries are for the local standard day (midnight to midnight) at the monitor.\n",
      "units of measure: the unit of measure for the parameter. qad always returns data in the standard units for the parameter. submitters are allowed to report data in any unit and epa converts to a standard unit so that we may use the data in calculations.\n",
      "event type: indicates whether data measured during exceptional events are included in the summary. a wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. no events means no events occurred. events included means events occurred and the data from them is included in the summary. events excluded means that events occurred but data form them is excluded from the summary. concurred events excluded means that events occurred but only epa concurred exclusions are removed from the summary. if an event occurred for the parameter in question, the data will have multiple records for each monitor.\n",
      "observation count: the number of observations (samples) taken during the day.\n",
      "observation percent: the percent representing the number of observations taken with respect to the number scheduled to be taken during the day. this is only calculated for monitors where measurements are required (e.g., only certain parameters).\n",
      "arithmetic mean: the average (arithmetic mean) value for the day.\n",
      "1st max value: the highest value for the day.\n",
      "1st max hour: the hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken.\n",
      "aqi: the air quality index for the day for the pollutant, if applicable.\n",
      "method code: an internal system code indicating the method (processes, equipment, and protocols) used in gathering and measuring the sample. the method name is in the next column.\n",
      "method name: a short description of the processes, equipment, and protocols used in gathering and measuring the sample.\n",
      "local site name: the name of the site (if any) given by the state, local, or tribal air pollution control agency that operates it.\n",
      "address: the approximate street address of the monitoring site.\n",
      "state name: the name of the state where the monitoring site is located.\n",
      "county name: the name of the county where the monitoring site is located.\n",
      "city name: the name of the city where the monitoring site is located. this represents the legal incorporated boundaries of cities and not urban areas.\n",
      "cbsa name: the name of the core bases statistical area (metropolitan area) where the monitoring site is located.\n",
      "date of last change: the date the last time any numeric values in this record were updated in the aqs data system.\n",
      "acknowledgements:\n",
      "these data came from the epa and are current up to may 01, 2017. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too: https://cloud.google.com/bigquery/public-data/epa.\n",
      "inspiration:\n",
      "people exposed to toxic air pollutants at sufficient concentrations and durations may have an increased chance of getting cancer or experiencing other serious health effects. these health effects can include damage to the immune system, as well as neurological, reproductive (e.g., reduced fertility), developmental, respiratory and other health problems. in addition to exposure from breathing air toxics, some toxic air pollutants such as mercury can deposit onto soils or surface waters, where they are taken up by plants and ingested by animals and are eventually magnified up through the food chain. like humans, animals may experience health problems if exposed to sufficient quantities of air toxics over time. use this dataset to find out where the highest concentrations of hazardous air pollutants are for each state. you could also use the gps locations to find out where the epa has the most monitoring stations and identify places that could use more.\n",
      "the snli corpus (version 1.0) is a collection of 570k human-written english sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (nli), also known as recognizing textual entailment (rte). we aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing nlp models of any kind.\n",
      "acknowledgements\n",
      "this dataset was kindly made available bye the stanford natural language processing group. please cite it as:\n",
      "samuel r. bowman, gabor angeli, christopher potts, and christopher d. manning. 2015. a large annotated corpus for learning natural language inference. in proceedings of the 2015 conference on empirical methods in natural language processing (emnlp)\n",
      "inspiration\n",
      "this dataset has been used to evaluate academic work on sentence encoding-based models for 3 way classification, with previous scores tabulated at https://nlp.stanford.edu/projects/snli/. most of the entries use deep learning. how close to those scores (peak of 88.8% test accuracy) can you get with less computationally intensive methods?\n",
      "context\n",
      "next time you take a bite, consider this: roughly one in six (or 48 million) people in the united states get sick from eating contaminated food per year. more than 250 pathogens and toxins have been known to cause foodborne illness and almost all of them can cause an outbreak.\n",
      "a foodborne disease outbreak occurs when two or more people get the same illness from the same contaminated food or drink. while most foodborne illnesses are not part of a recognized outbreak, outbreaks provide important information on how germs spread, which foods cause illness, and how to prevent infection.\n",
      "public health agencies in all 50 states, the district of columbia, u.s. territories, and freely associated states have primary responsibility for identifying and investigating outbreaks and use a standard form to report outbreaks voluntarily to cdc. during 1998–2008, reporting was made through the electronic foodborne outbreak reporting system (efors).\n",
      "content\n",
      "this dataset provides data on foodborne disease outbreaks reported to cdc from 1998 through 2015. data fields include year, state (outbreaks occurring in more than one state are listed as \"multistate\"), location where the food was prepared, reported food vehicle and contaminated ingredient, etiology (the pathogen, toxin, or chemical that caused the illnesses), status (whether the etiology was confirmed or suspected), total illnesses, hospitalizations, and fatalities. in many outbreak investigations, a specific food vehicle is not identified; for these outbreaks, the food vehicle variable is blank.\n",
      "inspiration\n",
      "are foodborne disease outbreaks increasing or decreasing? what contaminant has been responsible for the most illnesses, hospitalizations, and deaths? what location for food preparation poses the greatest risk of foodborne illness?\n",
      "fatality analysis reporting system (fars) was created in the united states by the national highway traffic safety administration (nhtsa) to provide an overall measure of highway safety, to help suggest solutions, and to help provide an objective basis to evaluate the effectiveness of motor vehicle safety standards and highway safety programs.\n",
      "fars contains data on a census of fatal traffic crashes within the 50 states, the district of columbia, and puerto rico. to be included in fars, a crash must involve a motor vehicle traveling on a trafficway customarily open to the public and result in the death of a person (occupant of a vehicle or a non-occupant) within 30 days of the crash. fars has been operational since 1975 and has collected information on over 989,451 motor vehicle fatalities and collects information on over 100 different coded data elements that characterizes the crash, the vehicle, and the people involved.\n",
      "fars is vital to the mission of nhtsa to reduce the number of motor vehicle crashes and deaths on our nation's highways, and subsequently, reduce the associated economic loss to society resulting from those motor vehicle crashes and fatalities. fars data is critical to understanding the characteristics of the environment, trafficway, vehicles, and persons involved in the crash.\n",
      "nhtsa has a cooperative agreement with an agency in each state government to provide information in a standard format on fatal crashes in the state. data is collected, coded and submitted into a micro-computer data system and transmitted to washington, d.c. quarterly files are produced for analytical purposes to study trends and evaluate the effectiveness highway safety programs.\n",
      "content\n",
      "there are 40 separate data tables. you can find the manual, which is too large to reprint in this space, here.\n",
      "querying bigquery tables\n",
      "you can use the bigquery python client library to query tables in this dataset in kernels. note that methods available in kernels are limited to querying data. tables are at bigquery-public-data.nhtsa_traffic_fatalities.[tablename]. fork this kernel to get started.\n",
      "acknowledgements\n",
      "this dataset was provided by the national highway traffic safety administration.\n",
      "context\n",
      "this dataset is a snapshot of all of the country profiles provided in the world factbook as of early 2017. the world factbook is a reference almanac published by the united states central intelligence agency on a continual basis. it is often used as a reference text in other academic works.\n",
      "content\n",
      "this dataset includes high-level textual information on the economy, politics, demography, culture, military, and society of every country in the world.\n",
      "acknowledgements\n",
      "this data was scraped here, then concatenated into a single entity before upload to kaggle.\n",
      "inspiration\n",
      "this dataset is an ideal basis of comparison for various world countries.\n",
      "analyzing international data? this dataset is a rich mix-in dataset for contextualizing such analyses.\n",
      "this database consist of over 400000 infos for android apps scraped with scrapy from google play. those fields are included:\n",
      "name\n",
      "datepublished\n",
      "numdownloadsmin\n",
      "filesize\n",
      "packagename\n",
      "price\n",
      "aggregaterating\n",
      "softwareversion\n",
      "ratingcount\n",
      "datecrawled\n",
      "url\n",
      "context\n",
      "competitions like luna (http://luna16.grand-challenge.org) and the kaggle data science bowl 2017 (https://www.kaggle.com/c/data-science-bowl-2017) involve processing and trying to find lesions in ct images of the lungs. in order to find disease in these images well, it is important to first find the lungs well. this dataset is a collection of 2d and 3d images with manually segmented lungs.\n",
      "challenge\n",
      "come up with an algorithm for accurately segmenting lungs and measuring important clinical parameters (lung volume, pd, etc)\n",
      "percentile density (pd)\n",
      "the pd is the density (in hounsfield units) the given percentile of pixels fall below in the image. the table includes 5 and 95% for reference. for smokers this value is often high indicating the build up of other things in the lungs.\n",
      "the numenta anomaly benchmark (nab) is a novel benchmark for evaluating algorithms for anomaly detection in streaming, online applications. it is comprised of over 50 labeled real-world and artificial timeseries data files plus a novel scoring mechanism designed for real-time applications. all of the data and code is fully open-source, with extensive documentation, and a scoreboard of anomaly detection algorithms: github.com/numenta/nab. the full dataset is included here, but please go to the repo for details on how to evaluate anomaly detection algorithms on nab.\n",
      "nab data corpus\n",
      "the nab corpus of 58 timeseries data files is designed to provide data for research in streaming anomaly detection. it is comprised of both real-world and artifical timeseries data containing labeled anomalous periods of behavior. data are ordered, timestamped, single-valued metrics. all data files contain anomalies, unless otherwise noted.\n",
      "the majority of the data is real-world from a variety of sources such as aws server metrics, twitter volume, advertisement clicking metrics, traffic data, and more. all data is included in the repository, with more details in the data readme. we are in the process of adding more data, and actively searching for more data. please contact us at nab@numenta.org if you have similar data (ideally with known anomalies) that you would like to see incorporated into nab.\n",
      "the nab version will be updated whenever new data (and corresponding labels) is added to the corpus; nab is currently in v1.0.\n",
      "real data\n",
      "realawscloudwatch/\n",
      "aws server metrics as collected by the amazoncloudwatch service. example metrics include cpu utilization, network bytes in, and disk read bytes.\n",
      "realadexchange/\n",
      "online advertisement clicking rates, where the metrics are cost-per-click (cpc) and cost per thousand impressions (cpm). one of the files is normal, without anomalies.\n",
      "realknowncause/\n",
      "this is data for which we know the anomaly causes; no hand labeling.\n",
      "ambient_temperature_system_failure.csv: the ambient temperature in an office setting.\n",
      "cpu_utilization_asg_misconfiguration.csv: from amazon web services (aws) monitoring cpu usage – i.e. average cpu usage across a given cluster. when usage is high, aws spins up a new machine, and uses fewer machines when usage is low.\n",
      "ec2_request_latency_system_failure.csv: cpu usage data from a server in amazon's east coast datacenter. the dataset ends with complete system failure resulting from a documented failure of aws api servers. there's an interesting story behind this data in the numenta blog.\n",
      "machine_temperature_system_failure.csv: temperature sensor data of an internal component of a large, industrial mahcine. the first anomaly is a planned shutdown of the machine. the second anomaly is difficult to detect and directly led to the third anomaly, a catastrophic failure of the machine.\n",
      "nyc_taxi.csv: number of nyc taxi passengers, where the five anomalies occur during the nyc marathon, thanksgiving, christmas, new years day, and a snow storm. the raw data is from the nyc taxi and limousine commission. the data file included here consists of aggregating the total number of taxi passengers into 30 minute buckets.\n",
      "rogue_agent_key_hold.csv: timing the key holds for several users of a computer, where the anomalies represent a change in the user.\n",
      "rogue_agent_key_updown.csv: timing the key strokes for several users of a computer, where the anomalies represent a change in the user.\n",
      "realtraffic/\n",
      "real time traffic data from the twin cities metro area in minnesota, collected by the minnesota department of transportation. included metrics include occupancy, speed, and travel time from specific sensors.\n",
      "realtweets/\n",
      "a collection of twitter mentions of large publicly-traded companies such as google and ibm. the metric value represents the number of mentions for a given ticker symbol every 5 minutes.\n",
      "artificial data\n",
      "artificialnoanomaly/\n",
      "artifically-generated data without any anomalies.\n",
      "artificialwithanomaly/\n",
      "artifically-generated data with varying types of anomalies.\n",
      "acknowledgments\n",
      "we encourage you to publish your results on running nab, and share them with us at nab@numenta.org. please cite the following publication when referring to nab:\n",
      "lavin, alexander and ahmad, subutai. \"evaluating real-time anomaly detection algorithms – the numenta anomaly benchmark\", fourteenth international conference on machine learning and applications, december 2015. [pdf]\n",
      "withdrawal of a particular form of currency (such a gold coins, currency notes) from circulation is known as demonetization .\n",
      "context:\n",
      "on november 8th, india’s prime minister announced that 86% of the country’s currency would be rendered null and void in 50 days and it will withdraw all 500 and 1,000 rupee notes — the country’s most popular currency denominations from circulation, while a new 2,000 rupee note added in. it was posited as a move to crackdown on corruption and the country’s booming under-regulated and virtually untaxed grassroots economy.\n",
      "content:\n",
      "the field names are following:\n",
      "id\n",
      "query\n",
      "tweet_id\n",
      "inserted date\n",
      "truncated\n",
      "language\n",
      "possibly_sensitive coordinates\n",
      "retweeted_status\n",
      "created_at_text\n",
      "created_at\n",
      "content\n",
      "from_user_screen_name\n",
      "from_user_id from_user_followers_count\n",
      "from_user_friends_count\n",
      "from_user_listed_count\n",
      "from_user_statuses_count\n",
      "from_user_description\n",
      "from_user_location\n",
      "from_user_created_at\n",
      "retweet_count\n",
      "entities_urls\n",
      "entities_urls_counts\n",
      "entities_hashtags\n",
      "entities_hashtags_counts\n",
      "entities_mentions\n",
      "entities_mentions_counts\n",
      "in_reply_to_screen_name\n",
      "in_reply_to_status_id\n",
      "source\n",
      "entities_expanded_urls\n",
      "json_output\n",
      "entities_media_count\n",
      "media_expanded_url\n",
      "media_url\n",
      "media_type\n",
      "video_link\n",
      "photo_link\n",
      "twitpic\n",
      "acknowledgements:\n",
      "dataset is created by pulling tweets by hashtag from twitter.\n",
      "inspiration:\n",
      "dataset can be used to understand trending tweets. dataset can be used for sentiment analysis and topic mining. dataset can be used for time series analysis of tweets.\n",
      "what questions would you like answered by the community ?\n",
      "what is the general sentiment of tweets ?\n",
      "conclusion regarding tweet sentiments varying over time.\n",
      "what feedback would be helpful on the data itself ?\n",
      "an in depth analysis of data.\n",
      "these datasets contain information from nba draft classes and subsequent advanced stats years. the idea is to evaluate which draft classes are better than others, and in which ways they are better (did they produce more stars in the top 10, or more solid role players at the middle or end. i posted an analysis of this data here, but there's a lot more to be done. for example, my initial analysis just looks at drafts as a whole, not breaking it down by top 10, top 30, or top 60 picks. i also just analyzed drafts since 2000, but the dataset i uploaded has info all the way back to 1978. this data was scraped from baskeball-reference.com\n",
      "there are 2 datasets: season78, which has the fields:\n",
      "season: the nba season data is drawn from. the later year is the season value (e.g. 2015-2016 season is 2016)\n",
      "player: name of the player\n",
      "ws: win shares produced that season.\n",
      "draft78 has the fields:\n",
      "pick: the draft pick the player was.\n",
      "player: name of the player\n",
      "yrs: number of years the player played in the nba\n",
      "draft: year of the draft.\n",
      "context\n",
      "this dataset contains ground state energies of 16,242 molecules calculated by quantum mechanical simulations.\n",
      "content\n",
      "the data contains 1277 columns. the first 1275 columns are entries in the coulomb matrix that act as molecular features. the 1276th column is the pubchem id where the molecular structures are obtained. the 1277th column is the atomization energy calculated by simulations using the quantum espresso package.\n",
      "in the csv file, the first column (x1) is the data index and unused.\n",
      "past research\n",
      "the data is used for a publication in journal of chemical physics. a blog post was also published explaining the data and the research behind it in less technical terms.\n",
      "a github repository is available that contains the source code used for generating the data, as well as some of the r scripts used for analysis.\n",
      "inspiration\n",
      "simulations of molecular properties are computationally expensive. the purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database of simulations. if this can be done with high accuracy, properties of new molecules can be calculated using the trained model. this could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.\n",
      "the purpose is to use the 1275 molecular features to predict the atomization energy. this is a regression problem so mean squared error is minimized during training.\n",
      "i am looking for kagglers to find the best model and reduce mean squared error as much as possible!\n",
      "these three extremists attacks happened in the last 24 hours (as of 18th of july, 2016):\n",
      "extremists send rockets into a residential neighborhood, taking out a child and two women. aleppo, syria.\n",
      "suicide bombers attack yemeni army checkpoints, killing 10. yemen, mukalla.\n",
      "5 killed, 9 injured in almaty terrorist attack on police station [graphic]. kazakhstan,almaty.\n",
      "can we come together to predict where the next terrorist attacks will likely occur?\n",
      "the best weapon to fight extremists might be information. in fact, machine learning is already being used to predict and prevent terrorist attacks. we can do the same with data gathered from the religion of peace website.\n",
      "this website gathers events that resulted in killings and which were committed out of religious duty since 2002. this dataset contains a table summary of their data, including:\n",
      "date\n",
      "country\n",
      "city\n",
      "(# of people) killed\n",
      "(# of people) injured\n",
      "description (including type of attack descriptors such as \"bomb\",\"car\",\"shooting\",\"rocket\")\n",
      "i webscraped the data using r rvest.\n",
      "# webscraping in terror data by ping_freud\n",
      "# you can use the following gadget to find out which nodes to select in a page \n",
      "# https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html\n",
      "library(dplyr) #for %>% pipe flow\n",
      "library(ggplot2)\n",
      "library(rvest) #webscraping\n",
      "\n",
      "# building strings and declaring variables.\n",
      "# link with data.\n",
      "data.link <-\"https://www.thereligionofpeace.com/attacks/attacks.aspx?\"\n",
      "years <- as.character(2002:2016)\n",
      "links <- paste(data.link,\"yr=\",years,sep = \"\")\n",
      "terror.nodes <- terror.data <- vector(\"list\",length(links))\n",
      "\n",
      "# for loop to extract data\n",
      "for (i in 1:length(links)){ \n",
      "  terror.nodes[[i]] <- read_html(links[i]) %>% html_nodes(xpath=\"//table\")\n",
      "  #11 is the node where the table with data is \n",
      "  terror.data[[i]] <- as.data.frame(html_table(terror.nodes[[i]][11])) \n",
      "  terror.data[[i]] <- terror.data[[i]][nrow(terror.data[[i]]):1,]\n",
      "}\n",
      "\n",
      "# combines data frames\n",
      "terror.alldata <- do.call(\"rbind\",terror.data)\n",
      "# convert strings with dates to date format\n",
      "terror.alldata$date <- as.date(terror.alldata$date,\"%y.%m.%d\")\n",
      "row.names(terror.alldata) <- as.character(1:nrow(terror.alldata))\n",
      "write.csv(terror.alldata,\"attacks_data.csv\")\n",
      "i have not worked on the analysis yet, but we have geospacial distribution, type (hidden in the description strings) and magnitude of the attacks. there's also the possibility of using socioeconomical data available for the places listed.\n",
      "context\n",
      "fantasy premier league is the online global competition for football enthusiasts to try their luck at picking up their \"dream team\" and collect points. it involves a deep understanding of the sport, clubs, players and the fixtures apart from many other things. all this makes for a compelling data science (read machine learning problem).\n",
      "english premier league (epl) - one of the famous leagues in the sport of football. most viewed and followed across the globe. fpl provides an opportunity for enthusiasts to try their hand at decision making. to predict the best set of players who perform every game. points are given based on various parameters.\n",
      "goal - to get the maximum team score every week\n",
      "content\n",
      "time period - year 2016-17 season\n",
      "dataset consists of\n",
      "fpl users data (basic information)\n",
      "fixtures (week-wise)\n",
      "points earned by every user (gameweek-wise)\n",
      "data extraction\n",
      "detailed information about the code github repo - https://github.com/chaibapchya/fantasypremierleague-datascience\n",
      "acknowledgements\n",
      "thanks to fantasy premier league, without which this data would not have been available.\n",
      "inspiration\n",
      "diego costa scores a goal every 15 minutes of the match he plays.\n",
      "harry kane is the youngest player to have scored 100 premier league goals.\n",
      "such statistics (if true) are a compelling read.\n",
      "but, to know -\n",
      "nathaniel chalobah has 70% probability of scoring against stoke city this weekend.\n",
      "alexis sanchez will score around 2 goals this month.\n",
      "cesc fabregas is going to assist this weekend.\n",
      "there's 70% david de gea is going to have a clean-sheet.\n",
      "such statistics and much more lend so much credibility to decision making. it would enable fantasy premier league team owners to decide :-\n",
      "a. when to choose which player?\n",
      "b. when is the right time to use the wildcard?\n",
      "c. if it is time to be patient with a signing?\n",
      "d. who to watch out for?\n",
      "in order to do this, one needs data to back your predictions. hence, i was keen on retrieving all this data.\n",
      "future scope -\n",
      "predict the best team for the upcoming week.\n",
      "predict the best player (highest value for money) (goalkeeper, defender, mid-fielder, attacker)\n",
      "suggest possible changes in formation if need be.\n",
      "context\n",
      "in the last decade, new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. in this new context, personality is a key determinant in the decision making of the consumer when shopping. a person's buying choices are influenced by psychological factors like impulsiveness; indeed some consumers may be more susceptible to making impulse purchases than others. since affective metadata are more closely related to the user's experience than generic parameters, accurate predictions reveal important aspects of user's attitudes, social life, including attitude of others and social identity. this work proposes a highly innovative research that uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. in fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of recent algorithms. we present the ads dataset, a publicly available benchmark consisting of 300 real advertisements (i.e., rich media ads, image ads, text ads) rated by 120 unacquainted individuals, enriched with big-five users' personality factors and 1,200 personal users' pictures.\n",
      "content\n",
      "the content of the zip files are folders. the directory tree of this disk is as follows:\n",
      "20 ads folder: ads belong to 20 product/service categories. all the ads are here. 120 users folders: each folder contains data for one of the involved subjects. 300 real advertisements have been scored, ratings according to the users’ interests (1 star to 5 stars), ~1,200 personal pictures (labelled as positive/negative), big-five personality scores (o-c-e-a-n).\n",
      "data can be easily analysed in matlab, or python\n",
      "acknowledgements\n",
      "if you use our dataset please cite:\n",
      "[1] roffo, g., & vinciarelli, a. (2016, august). personality in computational advertising: a benchmark. in 4 th workshop on emotions and personality in personalized systems (empire) 2016 (p. 18).\n",
      "inspiration\n",
      "we collected and introduced a representative benchmark for computational advertising enriched with affective-like metadata such as personality factors. the benchmark allows to (i) explore the relationship between consumer characteristics, attitude toward online shopping and advert recommendation, (ii) identify the underlying dimensions of consumer shopping motivations and attitudes toward online in-store conversions, and (iii) have a reference benchmark for comparison of state-of-the-art advertisement recommender systems (arss). to the best of our knowledge, the ads dataset is the first attempt at providing a set of advertisements scored by the users according to their interest into the content. we hope that this work motivates researchers to take into account the use of personality factors as an integral part of their future work, since there is a high potential that incorporating these kind of users' characteristics into ars could enhance recommendation quality and user experience.\n",
      "recent updates (11-27-2017)\n",
      "i've posted a clustered full dataset. this might give you a boost on the work you're doing with the data!\n",
      "i've posted a vectorized subset w/ 100,000 data points sampled after manual reduction of the dataset after eda.\n",
      "context\n",
      "cleaned data used in researching public comments for fcc proceeding 17-108 (net neutrality repeal).\n",
      "data collected from the beginning of submissions (april 2017) until oct 27th, 2017. the long-running comment scraping script suffered from a couple of disconnections and i estimate that i lost ~50,000 comments because of it. even though the net neutrality public comment period ended on august 30, 2017, the fcc ecfs system continued to take comments afterward, which were included in the analysis.\n",
      "i did a write-up on the results here: https://hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6\n",
      "content\n",
      "document id, text of the post, and number of duplicates found for that text was all that was necessary to generate the results. text is raw & unchanged from the original. i'm working hard to get online a fuller set of data w/ other important metadata fields.\n",
      "cleaned-up notebooks used are available on github. i am posting the notebook for exploratory data analysis first, and will include others as they are cleaned up. please share with the rest of us what interesting insights you glean from the data! tweet at me @jeffykao.\n",
      "content\n",
      "emojinet is the largest machine-readable emoji sense inventory that links unicode emoji representations to their english meanings extracted from the web. emojinet is a dataset consisting of:\n",
      "12,904 sense labels over 2,389 emoji, which were extracted from the web and linked to machine-readable sense definitions seen in babelnet\n",
      "context words associated with each emoji sense, which are inferred through word embedding models trained over google news corpus and a twitter message corpus for each emoji sense definition\n",
      "specification of the most likely platform-based emoji sense for a selected set of emoji (since emoji presentation is different on different platforms)\n",
      "acknowledgements:\n",
      "emojinet was developed by sanjaya wijeratne, lakshika balasuriya, amit sheth and derek doran. emojinet is licensed under acreative commons attribution-noncommercial-sharealike 3.0 unported (cc by-nc-sa 3.0) license.. please cite the following paper when using emojinet dataset(s):\n",
      "sanjaya wijeratne, lakshika balasuriya, amit sheth, derek doran. emojinet: an open service and api for emoji sense discovery. in 11th international aaai conference on web and social media (icwsm 2017). montreal, canada; 2017.\n",
      "you can also find more information about the dataset on the project website.\n",
      "the banner photo is by frank behrens and is licensed under a cc by-sa 2.0 license.\n",
      "inspiration:\n",
      "can you use these senses to create a sentiment lexicon for emoji?\n",
      "can you cluster emoji based on their sense?\n",
      "which emoji are the most different across platforms?\n",
      "context:\n",
      "everyone who speaks a language, speaks it with an accent. a particular accent essentially reflects a person's linguistic background. when people listen to someone speak with a different accent from their own, they notice the difference, and they may even make certain biased social judgments about the speaker.\n",
      "the speech accent archive is established to uniformly exhibit a large set of speech accents from a variety of language backgrounds. native and non-native speakers of english all read the same english paragraph and are carefully recorded. the archive is constructed as a teaching tool and as a research tool. it is meant to be used by linguists as well as other people who simply wish to listen to and compare the accents of different english speakers.\n",
      "this dataset allows you to compare the demographic and linguistic backgrounds of the speakers in order to determine which variables are key predictors of each accent. the speech accent archive demonstrates that accents are systematic rather than merely mistaken speech.\n",
      "all of the linguistic analyses of the accents are available for public scrutiny. we welcome comments on the accuracy of our transcriptions and analyses.\n",
      "content:\n",
      "this dataset contains 2140 speech samples, each from a different talker reading the same reading passage. talkers come from 177 countries and have 214 different native languages. each talker is speaking in english.\n",
      "this dataset contains the following files:\n",
      "reading-passage.txt: the text all speakers read\n",
      "speakers_all.csv: demographic information on every speaker\n",
      "recording: a zipped folder containing .mp3 files with speech\n",
      "acknowledgements:\n",
      "this dataset was collected by many individuals (full list here) under the supervision of steven h. weinberger. the most up-to-date version of the archive is hosted by george mason university. if you use this dataset in your work, please include the following citation:\n",
      "weinberger, s. (2013). speech accent archive. george mason university.\n",
      "this datasets is distributed under a cc by-nc-sa 2.0 license.\n",
      "inspiration:\n",
      "the following types of people may find this dataset interesting:\n",
      "esl teachers who instruct non-native speakers of english\n",
      "actors who need to learn an accent\n",
      "engineers who train speech recognition machines\n",
      "linguists who do research on foreign accent\n",
      "phoneticians who teach phonetic transcription\n",
      "speech pathologists\n",
      "anyone who finds foreign accent to be interesting\n",
      "context\n",
      "the kepler space observatory is a nasa-build satellite that was launched in 2009. the telescope is dedicated to searching for exoplanets in star systems besides our own, with the ultimate goal of possibly finding other habitable planets besides our own. the original mission ended in 2013 due to mechanical failures, but the telescope has nevertheless been functional since 2014 on a \"k2\" extended mission.\n",
      "kepler had verified 1284 new exoplanets as of may 2016. as of october 2017 there are over 3000 confirmed exoplanets total (using all detection methods, including ground-based ones). the telescope is still active and continues to collect new data on its extended mission.\n",
      "content\n",
      "this dataset is a cumulative record of all observed kepler \"objects of interest\" — basically, all of the approximately 10,000 exoplanet candidates kepler has taken observations on.\n",
      "this dataset has an extensive data dictionary, which can be accessed here. highlightable columns of note are:\n",
      "kepoi_name: a koi is a target identified by the kepler project that displays at least one transit-like sequence within kepler time-series photometry that appears to be of astrophysical origin and initially consistent with a planetary transit hypothesis\n",
      "kepler_name: [these names] are intended to clearly indicate a class of objects that have been confirmed or validated as planets—a step up from the planet candidate designation.\n",
      "koi_disposition: the disposition in the literature towards this exoplanet candidate. one of candidate, false positive, not dispositioned or confirmed.\n",
      "koi_pdisposition: the disposition kepler data analysis has towards this exoplanet candidate. one of false positive, not dispositioned, and candidate.\n",
      "koi_score: a value between 0 and 1 that indicates the confidence in the koi disposition. for candidates, a higher value indicates more confidence in its disposition, while for false positives, a higher value indicates less confidence in that disposition.\n",
      "acknowledgements\n",
      "this dataset was published as-is by nasa. you can access the original table here. more data from the kepler mission is available from the same source here.\n",
      "inspiration\n",
      "how often are exoplanets confirmed in the existing literature disconfirmed by measurements from kepler? how about the other way round?\n",
      "what general characteristics about exoplanets (that we can find) can you derive from this dataset?\n",
      "what exoplanets get assigned names in the literature? what is the distribution of confidence scores?\n",
      "see also: the kepler labeled time series and open exoplanets catalogue datasets.\n",
      "this dataset contains stats on players, coaches, and teams in men's professional basketball leagues from 1937 to 2012.\n",
      "acknowledgments\n",
      "this dataset was downloaded from the open source sports website. it did not come with an explicit license, but based on other datasets from open source sports, we treat it as follows:\n",
      "this database is copyright 1996-2015 by sean lahman.\n",
      "this work is licensed under a creative commons attribution-sharealike 3.0 unported license. for details see: http://creativecommons.org/licenses/by-sa/3.0/\n",
      "the data\n",
      "this dataset contains 11 files, each corresponding to a data table. there are five main tables:\n",
      "master: biographical information for all the players and coaches\n",
      "teams: stats on each team, per year\n",
      "players: stats for each player, per year\n",
      "coaches: stats for each coach, per year\n",
      "series_post: information on post-season winners, per year\n",
      "and there are six supplementary tables:\n",
      "abbrev: a key to the abbreviations used in other tables\n",
      "awards_coaches: coaching awards, per year\n",
      "awards_players: player awards, per year\n",
      "draft: draft information, per year\n",
      "hof: hall of fame information, per year\n",
      "player_allstar: individual player stats for the all-star game, per year\n",
      "the counted is a project by the guardian – and you – working to count the number of people killed by police and other law enforcement agencies in the united states throughout 2015 and 2016, to monitor their demographics and to tell the stories of how they died.\n",
      "the database will combine guardian reporting with verified crowdsourced information to build a more comprehensive record of such fatalities. the counted is the most thorough public accounting for deadly use of force in the us, but it will operate as an imperfect work in progress – and will be updated by guardian reporters and interactive journalists frequently.\n",
      "any deaths arising directly from encounters with law enforcement will be included in the database. this will inevitably include, but will likely not be limited to, people who were shot, tasered and struck by police vehicles as well those who died in police custody. self-inflicted deaths during encounters with law enforcement or in police custody or detention facilities will not be included.\n",
      "the us government has no comprehensive record of the number of people killed by law enforcement. this lack of basic data has been glaring amid the protests, riots and worldwide debate set in motion by the fatal police shooting of michael brown in august 2014. the guardian agrees with those analysts, campaign groups, activists and authorities who argue that such accounting is a prerequisite for an informed public discussion about the use of force by police.\n",
      "contributions of any information that may improve the quality of our data will be greatly welcomed as we work toward better accountability. please contact us at thecounted@theguardian.com.\n",
      "credits\n",
      "research and reporting: jon swaine, oliver laughland, jamiles lartey\n",
      "design and production: kenan davis, rich harris, nadja popovich, kenton powell\n",
      "can you predict the result?\n",
      "horse racing is one of the sport which involved many gambling activities. million of people in the world tried to find their 'winning formula' in order to gain profit from betting. since there are many factors which could affect the race result, data analysis on horse racing became much interesting.\n",
      "hong kong horse racing is especially interesting due to the follow reasons:\n",
      "- the handicap system made the race more competitive\n",
      "- horse pool is small compared to other countries so that horses will meet their rivalries very often in the races\n",
      "- limited number of jockey/trainer\n",
      "- data are well managed by the official :)\n",
      "the dataset\n",
      "the dataset contains the race result of 1561 local races throughout hong kong racing seasons 2014-16 and more information will be added into the dataset. the dataset is divided into two tables (which can be joined by race_id). most of the column description can be found below with one extra piece of information:\n",
      "finishing_position - for special incident, please refer to here\n",
      "so, can you find any pattern for the winner under some condition? did you spot out a winning strategy? (fyi, betting on all horse equally will bring a loss of ~17.5% on average) which jockey/trainer is worth to follow?\n",
      "don't wait and start the data analysis! you may find some of the kernels i created useful. enjoy! and please remember to share your finding with the community!\n",
      "acknowledgement\n",
      "the data are extracted from the website of the hong kong jockey club\n",
      "how to get started?\n",
      "in case you are not familiar with hong kong horse racing, please see this notebook as a get started tutorial.\n",
      "context\n",
      "a refugee is a person outside his or her country of nationality who is unable or unwilling to return to his or her country of nationality because of persecution or a well-founded fear of persecution on account of race, religion, nationality, membership in a particular social group, or political opinion. an asylee is a person who meets the definition of refugee and is already present in the united states or is seeking admission at a port of entry. refugees are required to apply for lawful permanent resident (“green card”) status one year after being admitted, and asylees may apply for green card status one year after being granted asylum.\n",
      "content\n",
      "the office of immigration statistics (ois) annual flow reports on refugees and asylees contain information obtained from the worldwide refugee admissions processing system (wraps) of the bureau of population, refugees, and migration of the us department of state on the numbers and demographic profiles of persons admitted to the united states as refugees and those granted asylum status during a given fiscal year.\n",
      "context\n",
      "invariance to translation and rotation is an important attribute we would like image classifiers to have in many applications. for many problems, even if there doesn't seem to be a lot of translation in the data, augmenting it with these transformations is often beneficial. there are not many datasets where these transformations are clearly relevant, though. the \"snake eyes\" dataset seeks to provide a problem where rotation and translation are clearly a fundamental aspect of the problem, and not just something intuitively believed to be involved.\n",
      "image classifiers are frequently utilized in a pipeline where a bounding box is first extracted from the complete image, and this process might provide centered data to the classifier. some translation might still be present in the data the classifier sees, though, making the phenomenon relevant to classification nevertheless. a snake eyes classifier can clearly benefit from such a pre-processing. but the point here is trying to learn how much a classifier can learn to do by itself. in special we would like to demonstrate the \"built-in\" invariance to translations from cnns.\n",
      "content\n",
      "snake eyes contains artificial images simulating the a roll of one or two dice. the face patterns were modified to contain at most 3 black spots, making it impossible to solve the problem by merely counting them. the data was synthesized using a python program, each image produced from a set of floating-point parameters modeling the position and angle of each dice.\n",
      "the data format is binary, with records of 401 bytes. the first byte contains the class (1 to 12, notice it does not start at 0), and the other 400 bytes are the image rows. we offer 1 million images, split in 10 files with 100k records each, and an extra test set with 10,000 images.\n",
      "inspiration\n",
      "we were inspired by the popular \"tiny image\" datasets often studied in ml research: mnist, cifar-10 and fashion-mnist. our dataset has smaller images, though, only 20x20, and 12 classes. the reduced proportions should help approximate the actual 3d and 6d manifolds of each class with the available number of data points (1 million images).\n",
      "the data is artificial, with limited and very well-defined patterns, noise-free and properly anti-aliased. this is not about improving from 95% to 97% accuracy and wondering if 99% is possible with a deeper network. we don't expect less than 100% precision to be achieved with any method eventually. what we are interested to see is how do different methods compare in efficiency, how hard is it to train different models, and how the translation and rotation invariance is enforced or achieved.\n",
      "we are also interested in studying the concept of manifold learning. the data has some intra-class variability due to different possible face combinations with two dice. but most of the variation comes from translation and rotation. we hope to have sampled enough data to really allow for the extraction of these manifolds in 400 dimensions, and to investigate topics such as the role of pre-training, and the relation between modeling the manifold of the whole data and of the separate classes.\n",
      "translations alone already create quite non-convex manifolds, but our classes also have the property that some linear combinations are actually a different class (e.g. two images from the \"2\" face make an image from the \"4\" class). we are curious to see how this property can make the problem more challenging to different techniques.\n",
      "we are also secretly hoping to have created the image-detection version of the infamous \"spiral\" problem for neural networks. we are offering the prize of one ham sandwich, collected at my local café, to the first person who manages to train a neural network to solve this problem, convolutional or not, and using just traditional techniques such as logistic or relu activation functions and sgd training. 99% accuracy is enough. the resulting network may be susceptible to adversarial instances, this is fine, but we'll be constantly complaining about it in your ear while you eat the sandwich.\n",
      "context\n",
      "where's waldo is a popular children's book series where the reader is presented with a sequence of scenes. each scene contains potentially hundreds of individuals doing different things. exactly one of these figures is waldo: a tall man in a striped red shirt, red beanie, and glasses, and the objective of the game is to find waldo is the least time possible. this dataset is raw data from the books for these challenges.\n",
      "content\n",
      "this dataset contains a number of cuts of where's waldo scenes, including scenes. see the complimentary kernel to learn more about the dataset contents!\n",
      "acknowledgements\n",
      "this dataset was collected and published as-is by valentino constantinou (vc1492a) on github (here).\n",
      "inspiration\n",
      "can you come up with a strategy better than randomly scanning the page for this task? can you identify waldos and not-waldos?\n",
      "context\n",
      "along with their core mission of counting the us population, the united states census bureau gathers a wide range of economic data. this dataset covers 16 of their economic reports and surveys:\n",
      "advance monthly sales for retail and food services\n",
      "construction spending\n",
      "housing vacancies and homeownership\n",
      "manufactured housing survey (1980-2013)\n",
      "manufactured housing survey (current)\n",
      "manufacturers' shipments, inventories, and orders\n",
      "manufacturing and trade inventories and sales\n",
      "monthly retail trade and food services\n",
      "monthly wholesale trade: sales and inventories\n",
      "new home sales\n",
      "new residential construction\n",
      "quarterly financial report\n",
      "quarterly services survey\n",
      "quarterly summary of state & local taxes\n",
      "quarterly survey of public pensions\n",
      "u.s. international trade in goods and services\n",
      "content\n",
      "the data csv is arranged in a long format, with the time_series_code column tying it back to the metadata csv. if you're trying to figure out what data is available, you'll want to start with the metadata.\n",
      "just over a third of the time series store error codes, usually confidence intervals, rather than actual values. the metadata for these time series will have values in the columns et_code, et_desc, and et_unit.\n",
      "all of the dates are stored as complete beginning of the period dates, but all of the time series are at either monthly, quarterly, or annual resolution. exact days and months are provided for convenience when aligning time series and so that you don't have to unpack period codes like 'q22009'.\n",
      "there may be many time series bundled under a given data category or description. for example, the largest category (taxes) contains dozens of types of tax categories, and each of those contains a separate time series for each state in the country.\n",
      "two of the error code time series have non-numeric values. to convert the values column into reasonable units you'll need to drop all entries equal to the string less than .05 percent.\n",
      "the data have been substantially reformatted from how they are provided by the census bureau. you can find the script i used to prepare the data here.\n",
      "acknowledgements\n",
      "this data was kindly made available by the united states census. you can find the original data here. if you enjoyed this dataset you might also like one of the other us census datasets available on kaggle.\n",
      "inspiration\n",
      "the national bureau of economic research's macroeconomic history of the united states covers many similar time series, but before the census data was reported. can you integrate it with this census data? this should allow you to generate many time series stretching from the present back to the 19th century.\n",
      "content\n",
      "the data is images and labels / annotations for mammography scans. more about the database can be found at mias. the 'preview' kernel shows how the info.txt and pgm files can be parsed correctly.\n",
      "labels\n",
      "1st column: mias database reference number.\n",
      "2nd column: character of background tissue: f fatty g fatty-glandular d dense-glandular\n",
      "3rd column: class of abnormality present: calc calcification circ well-defined/circumscribed masses spic spiculated masses misc other, ill-defined masses arch architectural distortion asym asymmetry norm normal\n",
      "4th column: severity of abnormality; b benign m malignant\n",
      "5th, 6th columns: x,y image-coordinates of centre of abnormality.\n",
      "7th column: approximate radius (in pixels) of a circle enclosing the abnormality. there are also several things you should note:\n",
      "the list is arranged in pairs of films, where each pair represents the left (even filename numbers) and right mammograms (odd filename numbers) of a single patient. the size of all the images is 1024 pixels x 1024 pixels. the images have been centered in the matrix. when calcifications are present, centre locations and radii apply to clusters rather than individual calcifications. coordinate system origin is the bottom-left corner. in some cases calcifications are widely distributed throughout the image rather than concentrated at a single site. in these cases centre locations and radii are inappropriate and have been omitted.\n",
      "acknowledgements/licence\n",
      "mammographic image analysis society minimammographic database\n",
      "                   licence agreement\n",
      "this is a legal agreement between you, the end user and the mammographic image analysis society (\"mias\"). upon installing the minimammographic database (the \"database\") on your system you are agreeing to be bound by the terms of this agreement.\n",
      "grant of licence mias grants you the right to use the database, for research purposes only. for this purpose, you may edit, format, or otherwise modify the database provided that the unmodified portions of the database included in a modified work shall remain subject to the terms of this agreement.\n",
      "copyright the database is owned by mias and is protected by united kingdom copyright laws, international treaty provisions and all other applicable national laws. therefore you must treat the database like any other copyrighted material. if the database is used in any publications then reference must be made to the database within that publication.\n",
      "other restrictions you may not rent, lease or sell the database.\n",
      "liability to the maximum extent permitted by applicable law, mias shall not be liable for damages, other than death or personal injury, whatsoever (including without limitation, damages for negligence, loss of business, profits, business interruption, loss of business information, or other pecuniary loss) arising out of the use of or inability to use this database, even if mias has been advised of the possibility of such damages. in any case, mias's entire liability under this agreement shall be limited to the amount actually paid by you or your assignor, as the case may be, for the database.\n",
      "inspiration\n",
      "automatically finding lesions would be a very helpful tool for physicians, also predicting malignancy based on a found/marked lesion\n",
      "monthly/annual carbon dioxide emissions from electricity generation from the energy information administration. data is broken down by fuel type.\n",
      "http://www.eia.gov/electricity/data.cfm#elecenv\n",
      "introduction\n",
      "with multispectral images, we can capture more data per pixel, and understand objects based on their chemical composition or the variation of composition that encompasses an object. examples of this might be, is the image you see an apple or an orange? further, is the apple or the orange real? if it is plastic, was it made in mexico or india? real life impacts of using spectral data as part of object detection in images could one day save a life, if a self driving car could not only detect faces, but also the difference between skin and plastic, a lone pedestrian could avoid being if it was a choice between them or a group of three manikins.\n",
      "this sample data contains a series of multispectral images of handwritten numbers between 0 and 9, from six different peoples, using two different pens. and here i am asking the great kagglers to explore and build models to tell each of the numbers from one another and with what ink each was written in.\n",
      "data\n",
      "each csv file contains pixels for 10 grayscale images (350 * 350) that represent 10 channels for the multispectral image, where x, y represent the location of the pixel, and channel0 - channel9 represent channels.\n",
      "and we also have a labels csv that contains labels for each pixel csv file.\n",
      "licence\n",
      "you can do whatever you want with the data.\n",
      "context: diversity of united states counties\n",
      "content: diversity index of every us county using the simpson diversity index: d = 1 - ∑(n/n)^2 (where n = number of people of a given race and n is the total number of people of all races, to get the probability of randomly selecting two people and getting two people of different races (ecological entropy))\n",
      "from the data, you will have:\n",
      "- results from 12k+ participants, with the fastest one of 2hr12mins from world class athlete - midway time at 10km, halfway and 30km\n",
      "- overall and gender ranking\n",
      "the original source\n",
      "the data are captured from its official site\n",
      "http://www.hkmarathon.com/results/search_2016_results.htm\n",
      "only marathon results are included (but not 10km nor half marathon) because only this results has midway time, which can serve better analysis purposes.\n",
      "the fields:\n",
      "race no: runner id\n",
      "category: gender and age group. (e.g. mms and mfs denote male and female while the age group are the same.)\n",
      "official time: the \"gun time\"\n",
      "net time: the time between one passes the starting line and final line. it is usually a few minutes less than official time.\n",
      "10km time, half way time, 30km time: they are the midway times as described\n",
      "the files\n",
      "marathon challenge and marathon run 1 uses the same running path for racing but with a different starting time. athletes in challenge group are generally run faster.\n",
      "improving the dataset:\n",
      "- comparing the results of different marathons all over the world to find which one is the toughest or having the best participants, etc. - please let me know if there is any centralized database collecting the results from different races.\n",
      "context\n",
      "the digit recognizer competition uses the popular mnist dataset to challenge kagglers to classify digits correctly. in this dataset, the images are represented as strings of pixel values in train.csv and test.csv. often, it is beneficial for image data to be in an image format rather than a string format. therefore, i have converted the aforementioned datasets from text in .csv files to organized .jpg files.\n",
      "content\n",
      "this dataset is composed of four files:\n",
      "trainingset.tar.gz (10.2 mb) - this file contains ten sub folders labeled 0 to 9. each of the sub folders contains .jpg images from the digit recognizer competition's train.csv dataset, corresponding to the folder name (ie. folder 2 contains images of 2's, etc.). in total, there are 42,000 images in the training set.\n",
      "testset.tar.gz (6.8 mb) - this file contains the .jpg images from the digit recognizer competition's test.csv dataset. in total, there are 28,000 images in the test set.\n",
      "trainingsample.zip (407 kb) - this file contains ten sub folders labeled 0 to 9. each sub folder contains 60 .jpg images from the training set, for a total of 600 images.\n",
      "testsample.zip (233 kb) - this file contains a 350 image sample from the test set.\n",
      "acknowledgements\n",
      "as previously mentioned, all data presented here is simply a cleaned version of the data presented in kaggle's digit recognizer competition. the division of the mnist dataset into training and test sets exactly mirrors that presented in the competition.\n",
      "inspiration\n",
      "i created this dataset when exploring tensorflow's inception model. inception is a massive cnn built by google to compete in the imagenet competition. by way of transfer learning, the final layer of inception can be retrained, rendering the model useful for general classification tasks. in retraining the model, .jpg images must be used, thereby necessitating to the creation of this dataset.\n",
      "my hope in experimenting with inception was to achieve an accuracy of around 98.5% or higher on the mnist dataset. unfortunately, the maximum accuracy i reached with inception was only 95.314%. if you are interested in my code for said attempt, it is available on my github repository kaggle mnist inception cnn.\n",
      "to learn more about retraining inception, check out tensorflow for poets.\n",
      "the statistics of income (soi) division bases its zip code data on administrative records of individual income tax returns (forms 1040) from the internal revenue service (irs) individual master file (imf) system. included in these data are returns filed during the 12-month period, january 1, 2015 to december 31, 2015. while the bulk of returns filed during the 12-month period are primarily for tax year 2014, the irs received a limited number of returns for tax years before 2014 and these have been included within the zip code data.\n",
      "there is data for more years here:\n",
      "https://www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi\n",
      "see documentation file attached. crucially:\n",
      "zipcode - 5-digit zip code  \n",
      "agi_stub - size of adjusted gross income\n",
      "1 = $1 under $25,000 2 = $25,000 under $50,000 3 = $50,000 under $75,000 4 = $75,000 under $100,000 5 = $100,000 under $200,000 6 = $200,000 or more\n",
      "this is a database of the first 151 pokemon; the ones you can find in the pokemongo game. the stats include pokemon number, name, first and second type, max cp, max hp and a url from the bulbagarden.net gallery.\n",
      "pokemon no: number or id of the pokemon.\n",
      "name: the original name of the pokemon.\n",
      "first type: what type of pokemon it is.\n",
      "second type: some pokemon can have two types, if they don't, this cell is empty.\n",
      "max cp: this is the maximum amount of damage a pokemon can infringe.\n",
      "max hp: the maximum amount of damage a pokemon can receive.\n",
      "url: this is a link to the pokemon's image on bulbagarden.\n",
      "this database presents a great way of helping new generations of pokemon players learn about data science and pokemon at the same time. this data was scrapped from http://handbooks.bulbagarden.net/pokemongo/pokemon-index\n",
      "tweets scraped by chris albon on the day of the 2016 united states elections.\n",
      "chris albon's site only posted tweet ids, rather than full tweets. we're in the process of scraping the full information, but due to api limiting this is taking a very long time. version 1 of this dataset contains just under 400k tweets, about 6% of the 6.5 million originally posted.\n",
      "this dataset will be updated as more tweets become available.\n",
      "acknowledgements\n",
      "the original data was scraped by chris albon, and tweet ids were posted to his github page.\n",
      "the data\n",
      "since i (ed king) used my own twitter api key to scrape these tweets, this dataset contains a couple of fields with information on whether i have personally interacted with particular users or tweets. since kaggle encouraged me to not remove any data from a dataset, i'm leaving it in; feel free to build a classifier of the types of users i follow.\n",
      "the dataset consists of the following fields:\n",
      "text: text of the tweet\n",
      "created_at: date and time of the tweet\n",
      "geo: a json object containing coordinates [latitude, longitude] and a `type'\n",
      "lang: twitter's guess as to the language of the tweet\n",
      "place: a place object from the twitter api\n",
      "coordinates: a json object containing coordinates [longitude, latitude] and a `type'; note that coordinates are reversed from the geo field\n",
      "user.favourites_count: number of tweets the user has favorited\n",
      "user.statuses_count: number of statuses the user has posted\n",
      "user.description: the text of the user's profile description\n",
      "user.location: text of the user's profile location\n",
      "user.id: unique id for the user\n",
      "user.created_at: when the user created their account\n",
      "user.verified: bool; is user verified?\n",
      "user.following: bool; am i (ed king) following this user?\n",
      "user.url: the url that the user listed in their profile (not necessarily a link to their twitter profile)\n",
      "user.listed_count: number of lists this user is on (?)\n",
      "user.followers_count: number of accounts that follow this user\n",
      "user.default_profile_image: bool; does the user use the default profile pic?\n",
      "user.utc_offset: positive or negative distance from utc, in seconds\n",
      "user.friends_count: number of accounts this user follows\n",
      "user.default_profile: bool; does the user use the default profile?\n",
      "user.name: user's profile name\n",
      "user.lang: user's default language\n",
      "user.screen_name: user's account name\n",
      "user.geo_enabled: bool; does user have geo enabled?\n",
      "user.profile_background_color: user's profile background color, as hex in format \"rrggbb\" (no '#')\n",
      "user.profile_image_url: a link to the user's profile pic\n",
      "user.time_zone: full name of the user's time zone\n",
      "id: unique tweet id\n",
      "favorite_count: number of times the tweet has been favorited\n",
      "retweeted: is this a retweet?\n",
      "source: if a link, where is it from (e.g., \"instagram\")\n",
      "favorited: have i (ed king) favorited this tweet?\n",
      "retweet_count: number of times this tweet has been retweeted\n",
      "i've also included a file called bad_tweets.csv , which includes all of the tweet ids that could not be scraped, along with the error message i received while trying to scrape them. this typically happens because the tweet has been deleted, the user has deleted their account (or been banned), or the user has made their tweets private. the fields in this file are id and exception.response.\n",
      "context\n",
      "specific language impairment is a condition that effects roughly 7% of 5-year old children. it is characterised by a lack of language ability in comparison to your peers but with no obvious mental or physical disability. diagnosis can tend to be laborious, thus automating this process using nlp and ml techniques might be of interest to paediatricians and speech pathologists.\n",
      "content\n",
      "this study evaluated three datasets obtained via the childes project. all the datasets consist of narratives from a child attempting to complete a wordless picture task. the choice to use only narrative corpora was based on previous research which indicated it has the best ability to distinguish a language impairment in children. the first dataset consists of samples from british adolescents, the second from canadian children aged 4 to 9, and the third from u.s. children aged 4 to 12.\n",
      "unfortunately finding transcript data of this kind is rare, i have tried to find more data to no avail, so 1163 samples will have to do.\n",
      "conti-ramsden 4:\n",
      "the conti-ramsden 4 dataset was collected for a study to assess the effectiveness of narrative tests on adolescents. it consists of 99 td and 19 sli samples of children between the ages of 13.10 and 15.90. ideally all the corpora would only be from children, as sli is most prominent in children aged five years old, and is best detected early. however, it was included to enable a direct comparison between classifiers created by gabani and this study.\n",
      "the corpus contains transcripts of a story telling task based on mayer’s wordless picture book “frog, where are you”. the children first viewed the picture book in their own time before being prompted to retell the story in the past tense. if the children started telling the story in the present tense the interviewer would prompt them with the phrase “what happened next?” in order to attempt to revert them back to the past tense. if they failed to start to retell the story in the past tense after two prompts no further prompts were made.\n",
      "enni\n",
      "the enni dataset was collected during the course of a study aimed at identifying sli children using an index of storytelling ability based on the story grammar model. the corpus consists of 300 td and 77 sli samples of children aged between 4 and 9 years old. each child was presented with two wordless picture stories with one more complicated than the other. unlike conti-ramsden 4 the examiner held the book and turned the page after the child appeared to be finished telling the story for a particular picture. the children were also given the opportunity to practice on a training story, where the examiner gave more explicit prompts to the child about what to do.\n",
      "gillam\n",
      "the gillam dataset is based on another tool for narrative assessment known as “the test of narrative language (tnl). it consists of 250 language impaired children, and 520 controls aged between 5 and 12. a detailed description of each of the participants does not exist. the tnl consists of four storytelling tasks, the first is a recall of a script based story, the rest being wordless picture books. the first picture set depicts a story with a main protagonist having repeated attempts at the goal, and the rest are single picture stories. the single picture stories require more input from the child, and thus is better suited to older children. the tnl appears to be intermediary in difficulty compared to enni.\n",
      "features\n",
      "attribute | name | description\n",
      "y | the label | 0 for typically developing children 1 for language impaired\n",
      "child_tnw | total number of words | the total number of words in the transcript\n",
      "child_tns | total number of sentences | children with sli are more likely to speak in short sentences\n",
      "group | same as y | beware: is the same as y but easier to graph in python and r\n",
      "examiner_tnw | total number of words spoken by the examiner | children with sli are more likely to need support\n",
      "freq_ttr | frequency of word types to word token ratio | divides word types by word tokens and provides a rough measure of lexical diversity.\n",
      "r_2_i_verbs| ratio of raw to inflected verbs | children with sli often have difficulty with the morphemes -ed, -s, be, and do. this results in the use of raw verbs instead of their inflected forms.\n",
      "mor_words | number of words in the %mor tier |\n",
      "num_pos_tags | number of different part-of-speech tags |\n",
      "n_dos | number of do's | the number of time the word 'do' is used\n",
      "repetition | number of repetitions | counts the number of repetitions as tagged in the chat format inside square brackets e.g., milk milk milk milk = milk [x 4]\n",
      "retracing | number of retracings | a retracing is defined as when a speaker abandons an utterance but then continues again.\n",
      "fillers | number of fillers | counts the number of fillers used in total. a list of fillers was created by searching through the entire corpus (all 1038 samples) for all common variants of fillers such as um, umm, uh, uhh, etc.\n",
      "s_1g_ppl | perplexity of 1-gram sli | the perplexity of this sample in comparison to a language model trained on all the sli group for this corpora except the sample\n",
      "s_2g_ppl | perplexity of 2-gram sli | same as above but with a 2-gram lm\n",
      "s_3g_ppl | perplexity of 3-gram sli | same as above but with a 3-gram lm\n",
      "d_1g_ppl | perplexity of 1-gram td | the perplexity of this sample in comparison to a language model trained on all the td group for this corpora except the sample\n",
      "d_2g_ppl | perplexity of 2-gram td | same as above but with a 2-gram lm\n",
      "d_3g_ppl | perplexity of 3-gram td | same as above but with a 3-gram lm\n",
      "z_mlu_sli | sample z-score using sli group's mean length of utterance |\n",
      "z_mlu_td | sample z-score using td group's mean length of utterance |\n",
      "z_ndw_sli | sample z-score using sli group's raw:inflected verbs ratio |\n",
      "z_ndw_td | sample z-score using td group's raw:inflected verbs ratio |\n",
      "z_ipsyn_sli | sample z-score using sli group's developmental sentence score |\n",
      "z_ipsyn_td | sample z-score using td group's developmental sentence score |\n",
      "z_utts_sli |sample z-score using sli group's number of verb utterances |\n",
      "z_utts_td |sample z-score using td group's number of verb utterances |\n",
      "total_syl | total number of syllables | using a technique from\n",
      "average_syl | average number of syllables per word |\n",
      "mlu_words | mean length of utterance of words |\n",
      "mlu_morphemes | mean length of utterance of morphemes |\n",
      "mlu100_utts | mean length of utterance of 1st 100 words |\n",
      "verb_utt | number of verb utterances |\n",
      "dss | developmental sentence score |\n",
      "ipsyn_total | index of productive syntax score |\n",
      "the following fields are counts of instances of brown's stages of morphological development (see https://www.speech-language-therapy.com/index.php?option=com_content&view=article&id=33:brown&catid=2:uncategorised&itemid=117)\n",
      "present_progressive\n",
      "propositions_in\n",
      "propositions_on\n",
      "plural_s\n",
      "irregular_past_tense\n",
      "possessive_s\n",
      "uncontractible_copula\n",
      "articles\n",
      "regular_past_ed\n",
      "regular_3rd_person_s\n",
      "irregular_3rd_person\n",
      "uncontractible_aux\n",
      "contractible_copula\n",
      "contractible_aux\n",
      "back to normal\n",
      "word_errors | number of word errors | as marked in the transcripts\n",
      "f_k | flesch-kincaid score | see https://en.wikipedia.org/wiki/flesch%e2%80%93kincaid_readability_tests\n",
      "n_v | number of nouns followed immediately by a verb\n",
      "n_aux | number of nouns followed immediately by an auxillary verb\n",
      "n_3s_v | number of third singular nouns followed immediately by a verb\n",
      "det_n_pl | * number of determinant nouns followed by a personal pronoun*\n",
      "det_pl_n | * number of determinant pronouns followed by a noun\n",
      "pro_aux | * pronouns followed by auxillary verb*\n",
      "pro_3s_v | * 3rd. singular nominative pronoun followed by verb*\n",
      "total_error | total number of morphosyntactic errors | sum of the columns from nouns verbs down\n",
      "this table will take some time to finish, will get to it within a few days\n",
      "past research\n",
      "i have spent the last few months playing around with this data, i have uploaded it here mainly to speed up the computation of the analysis i have already done. but i'm excited to see what other people can do with this. there are some nice graphs to be made; especially using the mlu attributes.\n",
      "thus far using the combined corpora the best i have managed to get in terms of creating a predictive classifier is using neural networks with feature extraction and smote to get a mean roc of 0.8709 under 10-repeated-10-k-folds cv. you'll find that random forest and svm with an rbf kernel do comparably well with smote.\n",
      "acknowledgements\n",
      "all the data here was derived by me using the open source transcripts provided on the childes talkbank (http://childes.talkbank.org/). the methods i used are very close to those in:\n",
      "k. gabani, t. solorio, y. liu, k.-n. hassanali, and c. a. dollaghan, “exploring a corpus-based approach for detecting language impairment in monolingual english-speaking children,” artificial intelligence in medicine, vol. 53, no. 3, pp. 161–170, 2011.\n",
      "conti-4: d. wetherell, n. botting, and g. conti-ramsden, “narrative skills in adolescents with a history of sli in relation to non-verbal iq scores,” child language teaching and therapy, vol. 23, no. 1, pp. 95–113, 2007.\n",
      "enni: p. schneider, d. hayward, and r. v. dub, “storytelling from pictures using the edmonton narrative norms instrument,” 2006.\n",
      "gillam: r. gillam and n. pearson, test of narrative language. austin, tx: pro-ed inc., 2004.\n",
      "inspiration\n",
      "i'm hoping somebody can beat my score. i'm keen to learn more and see if this can become anything that might be of use to some child/family someday.\n",
      "context\n",
      "an insurance group consists of 10 property and casualty insurance, life insurance and insurance brokerage companies. the property and casualty companies in the group operate in a 17-state region. the group is a major regional property and casualty insurer, represented by more than 4,000 independent agents who live and work in local communities through a six-state region. define the metrics to analyse agent performance based on several attributes like demography, products sold, new business, etc. the goal is to improve their existing knowledge used for agent segmentation in a supervised predictive framework.\n",
      "inspiration\n",
      "vizualization:\n",
      "a. summary stats by agency\n",
      "b. product line: commercial line/personal line wise analysis\n",
      "c. agency wise - state wise - distribution of retention ratio (top 10)\n",
      "d. quote system wise hit ratio distribution (also by pl/cl)\n",
      "e. add few more based on your understanding of the data\n",
      "growth rates from 2006 to 2013 have to be computed and converted to independent attributes and include in the data for modeling.\n",
      "compute hit ratio based on bounds and quotes for each quotes system\n",
      "compute required aggregations at agency id and state and year\n",
      "decide if binning the data works for this situation\n",
      "some suggested approaches:\n",
      "a. model building - either regression or classification\n",
      "b. pattern extraction - classification model\n",
      "c. patterns from the data using decision trees\n",
      "context\n",
      "unlike this dataset, (which proved to be unusable). and this one which was filled with unnecessary columns; this donald trump dataset has the cleanest usability and consists of over 7,000 tweets, no nonsense\n",
      "you may need to use a decoder other than utf-8 if you want to see the emojis\n",
      "content\n",
      "data consists of:\n",
      "-date\n",
      "-time\n",
      "-tweet_text\n",
      "-type\n",
      "-media_type\n",
      "-hashtags\n",
      "-tweet_id\n",
      "-tweet_url\n",
      "-twt_favourites_is_this_like_question_mark\n",
      "-retweets\n",
      "i scrapped this from someone on reddit\n",
      "context\n",
      "aws spot instances allow users to bid on spare server capacity. you set a bid threshold for an instance that is usually upwards of 30% cheaper than standard on-demand aws instances. you can save a lot of money with aws spot instances.\n",
      "data content\n",
      "i pulled this data from the aws cli with the describe-spot-price-history command. i took a lot of time to acquire and transform, which is why i decided to provide it here.\n",
      "there are various time periods per region (i acquired all that i could). the columns are all fairly self-evident. please comment if you have any questions about the data or columns.\n",
      "the data includes the following column fields:\n",
      "price: the current spot price\n",
      "datetime: the date and time\n",
      "instance_type: the spot instance type\n",
      "os: the spot instance operating system\n",
      "region: the region and availability zone (az) for the spot instance\n",
      "inspiration\n",
      "while aws spot instances are significantly cheaper than on-demand instances, there is only one problem with spot instances: once the spot market price of an instance exceeds the bid threshold you purchased an instance for, the instance is terminated and given to others with higher bids. so while hourly server costs are cheaper, your server is liable to terminate without notice. but, there is a difference between regions and spot pricing. sometimes there is an arbitrage between regions, and some regions have more stable prices than others (fewer price spikes). if you can find which region/az is most stable, you can worry less about your instance terminating without notice.\n",
      "i started collecting this data because i wanted answers to two questions:\n",
      "which region/az is historically cheapest for instance x\n",
      "which region/az is historically most stable for instance x\n",
      "we could also use this data to predict which regions are likely to stay under a certain $ spot price, which would allow you to say with some amount of certainty whether a spot instance lasts the next [6,12,18]+ hours.\n",
      "this dataset contains the solution and the top 20 team's submissions for facebook's 5th recruiting competition on predicting checkin places.\n",
      "here's how an ensemble model leveraging the top results would have performed in the competition:\n",
      "context\n",
      "this dataset contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial pacific. this data was collected with the tropical atmosphere ocean (tao) array, which consists of nearly 70 moored buoys spanning the equatorial pacific, measuring oceanographic and surface meteorological variables critical for improved detection, understanding and prediction of seasonal-to-interannual climate variations originating in the tropics.\n",
      "content\n",
      "the data consists of the following variables: date, latitude, longitude, zonal winds (west<0, east>0), meridional winds (south<0, north>0), relative humidity, air temperature, sea surface temperature and subsurface temperatures down to a depth of 500 meters. data taken from the buoys from as early as 1980 for some locations. other data that was taken in various locations are rainfall, solar radiation, current levels, and subsurface temperatures.\n",
      "the latitude and longitude in the data showed that the bouys moved around to different locations. the latitude values stayed within a degree from the approximate location. yet the longitude values were sometimes as far as five degrees off of the approximate location.\n",
      "there are missing values in the data. not all buoys are able to measure currents, rainfall, and solar radiation, so these values are missing dependent on the individual buoy. the amount of data available is also dependent on the buoy, as certain buoys were commissioned earlier than others.\n",
      "all readings were taken at the same time of day.\n",
      "acknowledgement\n",
      "this dataset is part of the uci machine learning repository, and the original source can be found here. the original owner is the noaa pacific marine environmental laboratory.\n",
      "inspiration\n",
      "how can the data be used to predict weather conditions throughout the world?\n",
      "how do the variables relate to each other?\n",
      "which variables have a greater effect on the climate variations?\n",
      "does the amount of movement of the buoy effect the reliability of the data?\n",
      "context\n",
      "this dataset contains data behind the story, the dallas shooting was among the deadliest for police in u.s. history. the data are scraped from odmp and capture information on all tracked on-duty police officer deaths in the u.s. broken down by cause from 1971 until 2016.\n",
      "content\n",
      "this dataset tags every entry as human or canine. there are 10 variables:\n",
      "person\n",
      "dept: department\n",
      "eow: end of watch\n",
      "cause: cause of death\n",
      "cause_short: shortened cause of death\n",
      "date: cleaned eow\n",
      "year: year from eow\n",
      "canine\n",
      "dept_name\n",
      "state\n",
      "inspiration\n",
      "using the data, can you determine the temporal trend of police officer deaths by cause? by state? by department?\n",
      "acknowledgements\n",
      "the primary source of data is the officer down memorial page (odmp), started in 1996 by a college student who is now a police officer and who continues to maintain the database. the original data and code can be found on the fivethirtyeight github.\n",
      "this dataset contains geolocation information for thousands of twitter users during natural disasters in their area.\n",
      "abstract\n",
      "(from original paper)\n",
      "natural disasters pose serious threats to large urban areas, therefore understanding and predicting human movements is critical for evaluating a population’s vulnerability and resilience and developing plans for disaster evacuation, response and relief. however, only limited research has been conducted into the effect of natural disasters on human mobility. this study examines how natural disasters influence human mobility patterns in urban populations using individuals’ movement data collected from twitter. we selected fifteen destructive cases across five types of natural disaster and analyzed the human movement data before, during, and after each event, comparing the perturbed and steady state movement data. the results suggest that the power-law can describe human mobility in most cases and that human mobility patterns observed in steady states are often correlated with those in perturbed states, highlighting their inherent resilience. however, the quantitative analysis shows that this resilience has its limits and can fail in more powerful natural disasters. the findings from this study will deepen our understanding of the interaction between urban dwellers and civil infrastructure, improve our ability to predict human movement patterns during natural disasters, and facilitate contingency planning by policymakers.\n",
      "acknowledgments\n",
      "the original journal article for which this dataset was collected:\n",
      "wang q, taylor je (2016) patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. plos one 11(1): e0147299. http://dx.doi.org/10.1371/journal.pone.0147299\n",
      "the dryad page that this dataset was downloaded from:\n",
      "wang q, taylor je (2016) data from: patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. dryad digital repository. http://dx.doi.org/10.5061/dryad.88354\n",
      "the data\n",
      "this dataset contains the following fields:\n",
      "disaster.event: the natural disaster during which the observation was collected. one of:\n",
      "-- one of:\n",
      "--- *01_wipha*, *02_halong*, *03_kalmaegi*, *04_rammasun_manila* (typhoons)\n",
      "--- *11_bohol*, *12_iquique*, *13_napa* (earthquakes)\n",
      "--- *21_norfolk*, *22_hamburg*, *23_atlanta* (winter storms)\n",
      "--- *31_phoenix*, *32_detroit*, *33_baltimore* (thunderstorms)\n",
      "--- *41_aufire1*, *42_aufire2* (wildfires)\n",
      "user.anon: an anonymous user id; unique within each disaster event\n",
      "latitude: latitude of user's tweet\n",
      "longitude.anon: longitude of user's tweet; shifted to preserve anonymity\n",
      "time: the date and time of the tweet\n",
      "about this dataset\n",
      "dalia research conducted the first representative poll on basic income across europe in the spring of 2016. the results, first presented together with neopolis at the future of work conference in zurich, showed that two thirds of europeans would vote for basic income. dalia's basic income poll is now an annual survey, and the first wave of results from 2016 are now being made public. although dalia's latest research on basic income is not yet public, you can visit here to see the results from the most recent spring 2017 survey.\n",
      "the study was conducted by dalia research in april 2016 on public opinion across 28 eu member states. the sample of n=9.649 was drawn across all 28 eu member states, taking into account current population distributions with regard to age (14-65 years), gender and region/country.\n",
      "enjoy perusing the dataset and exploring interesting connections between demographics and support for basic income.\n",
      "context\n",
      "conditions that could be caused by smoking resulted in 1.7 million admissions to hospitals in england, for adults aged 35 and over, in 2014-2015 -- an average of 4,700 admissions per day! these figures refer to admissions with a primary diagnosis of a disease that can be caused by smoking, but for which smoking may or may not have actually been the cause.\n",
      "content\n",
      "the statistics on smoking in england report aims to present a broad picture of health issues relating to smoking in england and covers topics such as smoking prevalence, habits, behaviours, and attitudes, smoking-related health issues and mortality, and associated costs.\n",
      "acknowledgements\n",
      "this report contains data and information previously published by the health and social care information centre (hscic), department of health, the office for national statistics, and her majesty’s revenue and customs.\n",
      "this is the data behind the rhythm of food visualisation by moritz stefaner. it shows seasonal food searches in different food types around the world since 2004. the data is indexed, with 0 being the least and 100 being the highest search interest.\n",
      "find out more here: http://rhythm-of-food.net/\n",
      "history\n",
      "i made the database from the fragments of my own photos of flowers. the images are selected to reflect the flowering features of these plant species.\n",
      "content\n",
      "the content is very simple: 210 images (128x128x3) with 10 species of flowering plants and the file with labels flower-labels.csv. photo files are in the .png format and the labels are the integers.\n",
      "label => name\n",
      "0 => phlox; 1 => rose; 2 => calendula; 3 => iris; 4 => leucanthemum maximum; 5 => bellflower; 6 => viola; 7 => rudbeckia laciniata (goldquelle); 8 => peony; 9 => aquilegia.\n",
      "acknowledgements\n",
      "as an owner of this database, i have published it for absolutely free using by any site visitor.\n",
      "usage\n",
      "accurate classification of plant species with a small number of images isn't a trivial task. i hope this set can be interesting for training skills in this field. a wide spectrum of algorithms can be used for classification.\n",
      "context\n",
      "data set with the football matches of the spanish league of the 1st and 2nd division from the 1970-71 to 2016-17 season, has been created with the aim of opening a line of research in the machine learning, for the prediction of results (1x2) of football matches.\n",
      "content\n",
      "this file contains information about a football matches with the follow features:\n",
      "4808,1977-78,1,8,rayo vallecano,real madrid,3,2,30/10/1977,247014000\n",
      "id (4808): unique identifier of football match\n",
      "season (1977-78): season in which the match was played\n",
      "division (1): división in which the match was played (1st '1', 2nd '2')\n",
      "round (8): round in which the match was played\n",
      "localteam (rayo vallecano): local team name\n",
      "visitorteam (real madrid): visitor team name\n",
      "localgoals (3): goals scored by the local team\n",
      "visitorgoals (2): goals scored by the visitor team\n",
      "fecha (30/10/1977): date in which the match was played\n",
      "date (247014000): timestamp in which the match was played\n",
      "acknowledgements\n",
      "scraping made from:\n",
      "http://www.bdfutbol.com\n",
      "http://www.resultados-futbol.com\n",
      "context\n",
      "a small subset of dataset of product reviews from amazon kindle store category.\n",
      "content\n",
      "5-core dataset of product reviews from amazon kindle store category from may 1996 - july 2014. contains total of 982619 entries. each reviewer has at least 5 reviews and each product has at least 5 reviews in this dataset.\n",
      "columns\n",
      "asin - id of the product, like b000fa64pk\n",
      "helpful - helpfulness rating of the review - example: 2/3.\n",
      "overall - rating of the product.\n",
      "reviewtext - text of the review (heading).\n",
      "reviewtime - time of the review (raw).\n",
      "reviewerid - id of the reviewer, like a3sptokdg7wbln\n",
      "reviewername - name of the reviewer.\n",
      "summary - summary of the review (description).\n",
      "unixreviewtime - unix timestamp.\n",
      "acknowledgements\n",
      "this dataset is taken from amazon product data, julian mcauley, ucsd website. http://jmcauley.ucsd.edu/data/amazon/\n",
      "license to the data files belong to them.\n",
      "inspiration\n",
      "sentiment analysis on reviews.\n",
      "understanding how people rate usefulness of a review/ what factors influence helpfulness of a review.\n",
      "fake reviews/ outliers.\n",
      "best rated product ids, or similarity between products based on reviews alone (not the best idea ikr).\n",
      "any other interesting analysis.\n",
      "context:\n",
      "word embeddings define the similarity between two words by the normalised inner product of their vectors. the matrices in this repository place languages in a single space, without changing any of these monolingual similarity relationships. when you use the resulting multilingual vectors for monolingual tasks, they will perform exactly the same as the original vectors.\n",
      "facebook recently open-sourced word vectors in 89 languages. however these vectors are monolingual; meaning that while similar words within a language share similar vectors, translation words from different languages do not have similar vectors. in this dataset are 78 matrices, which can be used to align the majority of the fasttext languages in a single space.\n",
      "contents:\n",
      "this repository contains 78 matrices, which can be used to align the majority of the fasttext languages in a single space.\n",
      "this dataset was obtained by first getting the 10,000 most common words in the english fasttext vocabulary, and then using the google translate api to translate these words into the 78 languages available. this vocabulary was then split in two, assigning the first 5000 words to the training dictionary, and the second 5000 to the test dictionary. the alignment procedure is discribed in this blog. it takes two sets of word vectors and a small bilingual dictionary of translation pairs in two languages; and generates a matrix which aligns the source language with the target. sometimes google translates an english word to a non-english phrase, in these cases we average the word vectors contained in the phrase. to place all 78 languages in a single space, every matrix is aligned to the english vectors (the english matrix is the identity). you can find more information on this dataset in the authors’ github repository, here.\n",
      "acknowledgements:\n",
      "this dataset was produced by samuel smith, david turban, steven hamblin and nils hammerly. if you use this repository, please cite: offline bilingual word vectors, orthogonal transformations and the inverted softmax. samuel l. smith, david h. p. turban, steven hamblin and nils y. hammerla. iclr 2017 (conference track)\n",
      "inspiration:\n",
      "can you use the word embeddings in this dataset to cluster languages into their families?\n",
      "can you create a visualization of the relationship between words for similar concepts across languages?\n",
      "context:\n",
      "linked data: “linked open data is linked data that is open content. in computing, linked data (often capitalized as linked data) is a method of publishing structured data so that it can be interlinked and become more useful through semantic queries. it builds upon standard web technologies such as http, rdf and uris, but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. this enables data from different sources to be connected and queried.” -- “linked open data” on wikipedia\n",
      "anime: “anime is a japanese term for hand-drawn or computer animation. the word is the abbreviated pronunciation of \"animation\" in japanese, where this term references all animation. outside japan, anime is used to refer specifically to animation from japan or as a japanese-disseminated animation style often characterized by colorful graphics, vibrant characters and fantastical themes.” -- “anime” on wikipedia\n",
      "this dataset is a linked open dataset that contains information on 391706 anime titles.\n",
      "content:\n",
      "this dataset contains two files. the first is the native n-triples format, which is suitable for tasks. the second is a .csv containing three columns:\n",
      "anime: the title of the anime\n",
      "concept: the concept\n",
      "value: the value of the concept for that anime\n",
      "the .csv is not a true linked data dataset, since it has removed many of the relevant url’s. however, it should prove easier for data analysis.\n",
      "acknowledgements:\n",
      "this dataset has been collected and maintained by pieter heyvaert. it is © between our worlds and reproduced here under an mit license. you can find more information on this dataset and the most recent version here.\n",
      "inspiration:\n",
      "many anime have summaries, under the “description” concept. can you use these to identify common themes in anime? what about training an anime description generator?\n",
      "can you plot the number of titles released over time? has the rate of anime production increased or decreased over time?\n",
      "context:\n",
      "in the united states, animal bites are often reported to law enforcement (such as animal control). the main concern with an animal bite is that the animal may be rabid. this dataset includes information on over 9,000 animal bites which occurred near louisville, kentucky from 1985 to 2017 and includes information on whether the animal was quarantined after the bite occurred and whether that animal was rabid.\n",
      "content:\n",
      "attributes of animal bite incidents reported to and investigated by louisville metro department of public health and wellness. personal/identifying data has been removed. this dataset is a single .csv with the following fields.\n",
      "bite_date: the date the bite occurred\n",
      "speciesiddesc: the species of animal that did the biting\n",
      "breediddesc: breed (if known)\n",
      "genderiddesc: gender (of the animal)\n",
      "color: color of the animal\n",
      "vaccination_yrs: how many years had passed since the last vaccination\n",
      "vaccination_date: the date of the last vaccination\n",
      "victim_zip: the zipcode of the victim\n",
      "advissuedyndesc: whether advice was issued\n",
      "wherebitteniddesc: where on the body the victim was bitten\n",
      "quarantine_date: whether the animal was quarantined\n",
      "dispositioniddesc: whether the animal was released from quarantine\n",
      "head_sent_date: the date the animal’s head was sent to the lab\n",
      "release_date: the date the animal was released\n",
      "resultsiddesc: results from lab tests (for rabies)\n",
      "acknowledgements:\n",
      "attributes of animal bite incidents reported to and investigated by louisville metro department of public health and wellness. this data is in the public domain.\n",
      "inspiration:\n",
      "which animals are most likely to bite humans?\n",
      "are some dog breeds more likely to bite?\n",
      "what factors are most strongly associated with a positive rabies id?\n",
      "content\n",
      "this is the better life index for 2017 gathered from the oecd stats page. grouping labels have been removed and the row for units of measurment for each column has been removed with the units added to the end of each column label as such: (percentage: 'as pct'; ratio: 'as rat'; us dollar: 'in usd'; average score: 'as avg score'; years: 'in years'; micrograms per cubic metre: 'in ugm3'; hours: 'in hrs'). also, although included in the report, brazil, russia, and south africa are non-oecd economies at the time of reporting\n",
      "acknowledgements\n",
      "oecd stats page for full index and others please visit: http://stats.oecd.org/index.aspx?datasetcode=bli\n",
      "context:\n",
      "the bureau of labor statistics defines the consumer price index (cpi) as “a statistical measure of change, over time, of the prices of goods and services in major expenditure groups--such as food, housing, apparel, transportation, and medical care--typically purchased by urban consumers. essentially, it compares the cost of a sample of goods and services in a specific month relative to the cost of the same \"market basket\" in an earlier reference period.\n",
      "make sure to read the cu.txt for more descriptive summaries on each data file and how to use the unique identifiers.\n",
      "content:\n",
      "this dataset was collected june 27th, 2017 and may not be up-to-date.\n",
      "the revised cpi introduced by the bls in 1998 includes indexes for two populations; urban wage earners and clerical workers (cw), and all urban consumers (cu). this dataset covers all urban consumers (cu).\n",
      "the consumer price index (cpi) is a statistical measure of change, over time, of the prices of goods and services in major expenditure groups--such as food, housing, apparel, transportation, and medical care--typically purchased by urban consumers. essentially, it compares the cost of a sample \"market basket\" of goods and services in a specific month relative to the cost of the same \"market basket\" in an earlier reference period. this reference period is designated as the base period.\n",
      "as a result of the 1998 revision, both the cw and the cu utilize updated expenditure weights based upon data tabulated from three years (1982, 1983, and 1984) of the consumer expenditure survey and incorporate a number of technical improvements, including an updated and revised item structure.\n",
      "to construct the two indexes, prices for about 100,000 items and data on about 8,300 housing units are collected in a sample of 91 urban places. comparison of indexes for individual cmsa's or cities show only the relative change over time in prices between locations. these indexes cannot be used to measure interarea differences in price levels or living costs.\n",
      "summary data available: u.s. average indexes for both populations are available for about 305 consumer items and groups of items. in addition, over 100 of the indexes have been adjusted for seasonality. the indexes are monthly with some beginning in 1913. semi-annual indexes have been calculated for about 100 items for comparison with semi-annual areas mentioned below. semi-annual indexes are available from 1984 forward.\n",
      "area indexes for both populations are available for 26 urban places. for each area, indexes are published for about 42 items and groups. the indexes are published monthly for three areas, bimonthly for eleven areas, and semi-annually for 12 urban areas.\n",
      "regional indexes for both populations are available for four regions with about 55 items and groups per region. beginning with january 1987, indexes are monthly, with some beginning as early as 1966. semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned above. semi-annual indexes have been calculated for about 42 items in the 27 urban places for comparison with semi-annual areas.\n",
      "city-size indexes for both populations are available for three size classes with about 55 items and groups per class. beginning with january 1987, indexes are monthly and most begin in 1977. semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned below.\n",
      "region/city-size indexes for both populations are available cross classified by region and city-size class. for each of 13 cross calculations, about 42 items and groups are available. beginning with january 1987, indexes are monthly and most begin in 1977. semi-annual indexes have been calculated for about 42 items in the 26 urban places for comparison with semi-annual areas.\n",
      "frequency of observations: u.s. city average indexes, some area indexes, and regional indexes, city-size indexes, and region/city-size indexes for both populations are monthly. other area indexes for both populations are bimonthly or semi-annual.\n",
      "annual averages: annual averages are available for all unadjusted series in the cw and cu.\n",
      "base periods: most indexes have a base period of 1982-1984 = 100. other indexes, mainly those which have been added to the cpi program with the 1998 revision, are based more recently. the base period value is 100.0, except for the \"purchasing power\" values (aaor and saor) where the base period value is 1.000.\n",
      "data characteristics: indexes are stored to one decimal place, except for the \"purchasing power\" values which are stored to three decimal places.\n",
      "references: bls handbook of methods, chapter 17, \"consumer price index\", bls bulletin 2285, april 1988.\n",
      "acknowledgements:\n",
      "this dataset was taken directly from the u.s. bureau of labor statistics website at http://www.bls.gov/data/ and converted to csv format.\n",
      "inspiration:\n",
      "the bureau of labor statistics has done a great job of providing this source of information for the public to explore. you can use this information to compare the cost of living in urban areas around the united states. what are the top 10 most expensive places to live? which cities have the most expensive snacks or college textbooks? coffee? beer?\n",
      "the objective of the brfss is to collect uniform, state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases, injuries, and preventable infectious diseases in the adult population. factors assessed by the brfss include tobacco use, health care coverage, hiv/aids knowledge or prevention, physical activity, and fruit and vegetable consumption. data are collected from a random sample of adults (one per household) through a telephone survey.\n",
      "the behavioral risk factor surveillance system (brfss) is the nation's premier system of health-related telephone surveys that collect state data about u.s. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. established in 1984 with 15 states, brfss now collects data in all 50 states as well as the district of columbia and three u.s. territories. brfss completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\n",
      "content\n",
      "each year contains a few hundred columns. please see one of the annual code books for complete details.\n",
      "these csv files were converted from a sas data format using pandas; there may be some data artifacts as a result.\n",
      "if you like this dataset, you might also like the data for 2001-2010.\n",
      "acknowledgements\n",
      "this dataset was released by the cdc. you can find the original dataset and additional years of data here.\n",
      "context\n",
      "the project tycho database was named after the danish nobleman tycho brahe, who is known for his detailed astronomical and planetary observations. tycho was not able to use all of his data for breakthrough discoveries, but his assistant johannes kepler used tycho's data to derive the laws of planetary motion. similarly, this project aims to advance the availablity of large scale public health data to the worldwide community to accelerate advancements in scientific discovery and technological progress.\n",
      "content\n",
      "the project tycho database (level one) includes standardized counts at the state level for smallpox, polio, measles, mumps, rubella, hepatitis a, and whooping cough from weekly national notifiable disease surveillance system (nndss) reports for the united states. the time period of data varies per disease somewhere between 1916 and 2010. the records include cases and incidence rates per 100,000 people based on historical population estimates. these data have been used by investigators at the university of pittsburgh to estimate the impact of vaccination programs in the united states, recently published in the new england journal of medicine.\n",
      "acknowledgements\n",
      "the project tycho database was digitized and standardized by a team at the university of pittsburgh, including professor wilbert van panhuis, md, phd, professor john grefenstette, phd, and dean donald burke, md.\n",
      "context\n",
      "the geonames geographical database contains over 10 million geographical names and consists of over 9 million unique features with 2.8 million populated places and 5.5 million alternate names. all features are categorized into one out of nine feature classes and further subcategorized into one out of 645 feature codes.\n",
      "content\n",
      "the main 'geoname' table has the following fields :\n",
      "geonameid : integer id of record in geonames database\n",
      "name : name of geographical point (utf8) varchar(200)\n",
      "asciiname : name of geographical point in plain ascii characters, varchar(200)\n",
      "alternatenames : alternatenames, comma separated, ascii names automatically transliterated, convenience attribute from alternatename table, varchar(10000)\n",
      "latitude : latitude in decimal degrees (wgs84)\n",
      "longitude : longitude in decimal degrees (wgs84)\n",
      "feature class : see http://www.geonames.org/export/codes.html, char(1)\n",
      "feature code : see http://www.geonames.org/export/codes.html, varchar(10)\n",
      "country code : iso-3166 2-letter country code, 2 characters\n",
      "cc2 : alternate country codes, comma separated, iso-3166 2-letter country code, 200 characters\n",
      "admin1 code : fipscode (subject to change to iso code), see exceptions below, see file admin1codes.txt for display names of this code; varchar(20)\n",
      "admin2 code : code for the second administrative division, a county in the us, see file admin2codes.txt; varchar(80)\n",
      "admin3 code : code for third level administrative division, varchar(20)\n",
      "admin4 code : code for fourth level administrative division, varchar(20)\n",
      "population : bigint (8 byte int)\n",
      "elevation : in meters, integer\n",
      "dem : digital elevation model, srtm3 or gtopo30, average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters, integer. srtm processed by cgiar/ciat.\n",
      "timezone : the iana timezone id (see file timezone.txt) varchar(40)\n",
      "modification date : date of last modification in yyyy-mm-dd format\n",
      "admincodes:\n",
      "most adm1 are fips codes. iso codes are used for us, ch, be and me. uk and greece are using an additional level between country and fips code. the code '00' stands for general features where no specific adm1 code is defined. the corresponding admin feature is found with the same countrycode and adminx codes and the respective feature code admx.\n",
      "feature classes:\n",
      "a: country, state, region,...\n",
      "h: stream, lake, ...\n",
      "l: parks,area, ...\n",
      "p: city, village,...\n",
      "r: road, railroad\n",
      "s: spot, building, farm\n",
      "t: mountain,hill,rock,...\n",
      "u: undersea\n",
      "v: forest,heath,...\n",
      "acknowledgements\n",
      "data sources: http://www.geonames.org/data-sources.html\n",
      "context\n",
      "health care in the united states is provided by many distinct organizations. health care facilities are largely owned and operated by private sector businesses. 58% of us community hospitals are non-profit, 21% are government owned, and 21% are for-profit. according to the world health organization (who), the united states spent more on healthcare per capita ($9,403), and more on health care as percentage of its gdp (17.1%), than any other nation in 2014. many different datasets are needed to portray different aspects of healthcare in us like disease prevalences, pharmaceuticals and drugs, nutritional data of different food products available in us. such data is collected by surveys (or otherwise) conducted by centre of disease control and prevention (cdc), foods and drugs administration, center of medicare and medicaid services and agency for healthcare research and quality (ahrq). these datasets can be used to properly review demographics and diseases, determining start ratings of healthcare providers, different drugs and their compositions as well as package informations for different diseases and for food quality. we often want such information and finding and scraping such data can be a huge hurdle. so, here an attempt is made to make available all us healthcare data at one place to download from in csv files.\n",
      "content\n",
      "nhanes survey (national health and nutrition examination survey) - the national health and nutrition examination survey (nhanes) is a program of studies designed to assess the health and nutritional status of adults and children in the united states. the survey is unique in that it combines interviews and physical examinations. nhanes is a major program of the national center for health statistics (nchs). nchs is part of the centers for disease control and prevention (cdc) and has the responsibility for producing vital and health statistics for the nation. the nhanes interview includes demographic, socioeconomic, dietary, and health-related questions. the examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel. the diseases, medical conditions, and health indicators to be studied include: anemia, cardiovascular disease, diabetes, environmental exposures, eye diseases, hearing loss, infectious diseases, kidney disease, nutrition, obesity, oral health, osteoporosis, physical fitness and physical functioning, reproductive history and sexual behavior, respiratory disease (asthma, chronic bronchitis, emphysema), sexually transmitted diseases, vision. 10000 individuals are surveyed to represent us statistics. five files in this datasets represent current recent nhanes data -\n",
      "*nhanes_2005_2006.csv*\n",
      "*nhanes_2007_2008.csv*\n",
      "*nhanes_2009_2010.csv*\n",
      "*nhanes_2011_2012.csv*\n",
      "*nhanes_2013_2014.csv*\n",
      "data fields' description -\n",
      "nhanes_2005_2006.csv - demographic, dietary, examinations, laboratory\n",
      "nhanes_2007_2008.csv - demographic, dietary, examinations, laboratory\n",
      "nhanes_2009_2010.csv - demographic, dietary, examinations, laboratory\n",
      "nhanes_2011_2012.csv - demographic, dietary, examinations, laboratory\n",
      "nhanes_2013_2014.csv - demographic, dietary, examinations, laboratory\n",
      "us drugs datasets - fda provides a database for searching all the published drugs and all the unpublished drugs on their website, this database provides all the information about package of drugs and compositions of drugs their ndc codes. description of variables for this datasets are as follows -\n",
      "*drugs_product (current and unfinished)*\n",
      "productid - id of the product\n",
      "productndc - national drug code of the product\n",
      "producttypename - type of the product\n",
      "proprietaryname - proprietary name of the product\n",
      "proprietarynamesuffix - proprietary name suffix\n",
      "nonproprietaryname - non- proprietary (common name) of the product\n",
      "dosageformname - dosage information\n",
      "routename - route of taking drugs (oral / injections)\n",
      "startmarketingdate - date on which marketing for the drug has started\n",
      "endmarketingdate - date on which the marketing for the drug has stopped\n",
      "marketingcategoryname - marketing category name\n",
      "applicationnumber - application number for registering drug\n",
      "labelername - labeler name\n",
      "substancename - names of the substances in drug\n",
      "active_numerator_strength - strength of the drug\n",
      "active_ingred_unit - unit of strength\n",
      "pharm_classes - pharmaceutical class of the drugs\n",
      "deaschedule - dea schedule\n",
      "drugs package (current and unfinished)\n",
      "productid - id of the product\n",
      "productndc - national drug code of the product\n",
      "ndcpackagecode national drug code of the package\n",
      "packagedescription - description of the p[ackage\n",
      "nutritions data from usda - whenever we buy a packaged food product, we find the nutritional fact written on it. united states department of agriculture agricultural research service’s food composition database. this database contains all kinds food products available in us and provides description of their nutritions. this dataset is web scrapped and converted into a csv file. variables are self-explanatory names yet the descriptions can be found at this link - variables descriptions -( all values are per 100 grams) -\n",
      "data fields' description -\n",
      "ndb_no - nutrition database number\n",
      "shrt_desc - short description\n",
      "water_(g) - water in grams per 100 grams\n",
      "energ_kcal - energy in kcal\n",
      "protein_(g) - protein\n",
      "lipid_tot_(g) - total lipid\n",
      "ash_(g) - ash\n",
      "carbohydrt_(g) - carbohydrate, by difference\n",
      "fiber_td_(g) - fiber, total dietary\n",
      "sugar_tot_(g) - total sugars\n",
      "calcium_(mg) - calcium\n",
      "iron_(mg) - iron\n",
      "magnesium_(mg) - magnesium\n",
      "phosphorus_(mg) - phosphorus\n",
      "potassium_(mg) - potassium\n",
      "zinc_(mg) - zinc\n",
      "copper_(mg) - copper\n",
      "manganese_(mg) - manganese\n",
      "selenium_(æg) - selenium\n",
      "vit_c_(mg) - vitamin c, total ascorbic acid\n",
      "thiamin_(mg) - thiamin\n",
      "riboflavin_(mg) - riboflavin\n",
      "niacin_(mg) - niacin\n",
      "panto_acid_(mg) - pantothenic acid\n",
      "vit_b6_(mg) - vitamin b6\n",
      "folate_tot_(æg) - folate, total\n",
      "folic_acid_(æg) - folic acid\n",
      "food_folate_(æg) - folate, food\n",
      "folate_dfe_(æg) - folate, dfe\n",
      "choline_tot_ (mg) - choline, total\n",
      "vit_b12_(æg) - vitamin b-12\n",
      "vit_a_iu - vitamin a, iu\n",
      "vit_a_rae - vitamin a, rae\n",
      "retinol_(æg) - retinol\n",
      "alpha_carot_(æg) - carotene, alpha\n",
      "beta_carot_(æg) - carotene, beta\n",
      "beta_crypt_(æg) - cryptoxanthin, beta\n",
      "lycopene_(æg) - lycopene\n",
      "lut+zea_ (æg) - lutein + zeaxanthin\n",
      "vit_e_(mg) - vitamin e (alpha-tocopherol)\n",
      "vit_d_æg - vitamin d (d2 + d3)\n",
      "vit_d_iu - vitamin d\n",
      "vit_k_(æg) - vitamin k (phylloquinone)\n",
      "fa_sat_(g) - fatty acids, total saturated\n",
      "fa_mono_(g) - fatty acids, total monounsaturated\n",
      "fa_poly_(g) - fatty acids, total polyunsaturated\n",
      "cholestrl_(mg) - cholesterol\n",
      "gmwt_1 - gram weight 1\n",
      "gmwt_desc1 gram weight 1 descriptions\n",
      "gmwt_2 - gram weight 2\n",
      "gmwt_desc2 - gram weight 2 description\n",
      "star rating of health care plans with hos-cahps measures - hos cahps survey measures are the base of determining star rating of healthcare plan. files related to star rating have two types of measures which are used to determine star rating of the healthcare plans - part c and part d. part c is has three type of information 1. chronic conditions (disease) 2. tests and vaccines 3. member experience with healthcare plans. all variables starting with c01 to c32 are related to part c of the surveys. similarly part d of the survey is related to drugs plans customer services. in data variables starting with d01 to d15 is related to part d. surveys such as hos cahps etc contains questions whose final standing results into c01 to c32, and d01 to d15 measures. dataset has two star rating and measurements data released in fall 2015 and spring 2016. files description -\n",
      "star_rating_fall/spring_2015_c_cutoff.csv - contains information about different cut off used in determining star rating of part c measures.\n",
      "star_rating_fall/spring_2016_d_cutoff - contains information about different cut off used in determining star rating of part d measures.\n",
      "star_rating_fall/spring_domain.csv - contains information about domain rating of plans\n",
      "star_rating_fall/spring_high_performing_plans.csv - list of high performing plans\n",
      "star_rating_fal/spring_low_performing_plans.csv - list of low performing plans\n",
      "star_rating_fall/spring_master_data.csv - contains information on all the measures of all plans\n",
      "star_rating_fall/spring_plans_final_star_rating.csv - having information of star rating of healthcare plans\n",
      "description -\n",
      "contract_id - healthcare plan id\n",
      "organization type - type of the organizer - employer/demo/local cpp etc\n",
      "contract name - name of the contract\n",
      "organization marketing name - self explanatory\n",
      "parent organization - healthcare provider\n",
      "hd1: staying healthy: screenings, tests and vaccines (domain)\n",
      "c01: breast cancer screening\n",
      "c02: colorectal cancer screening\n",
      "c03: annual flu vaccine\n",
      "c04: improving or maintaining physical health\n",
      "c05: improving or maintaining mental health\n",
      "c06: monitoring physical activity\n",
      "c07: adult bmi assessment\n",
      "hd2: managing chronic (long term) conditions (domain)\n",
      "c08: special needs plan (snp) care management\n",
      "c09: care for older adults – medication review\n",
      "c10: care for older adults – functional status assessment\n",
      "c11: care for older adults – pain assessment\n",
      "c12: osteoporosis management in women who had a fracture\n",
      "c13: diabetes care – eye exam\n",
      "c14: diabetes care – kidney disease monitoring\n",
      "c15: diabetes care – blood sugar controlled\n",
      "c16: controlling blood pressure\n",
      "c17: rheumatoid arthritis management\n",
      "c18: reducing the risk of falling\n",
      "c19: plan all-cause readmissions\n",
      "hd3: member experience with health plan (domain)\n",
      "c20: getting needed care\n",
      "c21: getting appointments and care quickly\n",
      "c22: customer service\n",
      "c23: rating of health care quality\n",
      "c24: rating of health plan\n",
      "c25: care coordination\n",
      "hd4: member complaints and changes in the health plan's performance (domain)\n",
      "c26: complaints about the health plan\n",
      "c27: members choosing to leave the plan\n",
      "c28: beneficiary access and performance problems\n",
      "c29: health plan quality improvement\n",
      "hd5: health plan customer service (domain)\n",
      "c30: plan makes timely decisions about appeals\n",
      "c31: reviewing appeals decisions\n",
      "c32: call center – foreign language interpreter and tty availability\n",
      "dd1: drug plan customer service\n",
      "d01: call center – foreign language interpreter and tty availability\n",
      "d02: appeals auto–forward\n",
      "d03: appeals upheld\n",
      "dd2: member complaints and changes in the drug plan’s performance\n",
      "d04: complaints about the drug plan\n",
      "d05: members choosing to leave the plan\n",
      "d06: beneficiary access and performance problems\n",
      "d07: drug plan quality improvement\n",
      "dd3: member experience with the drug plan\n",
      "d08: rating of drug plan\n",
      "d09: getting needed prescription drugs\n",
      "dd4: drug safety and accuracy of drug pricing\n",
      "d10: mpf price accuracy\n",
      "d11: high risk medication\n",
      "d12: medication adherence for diabetes medications\n",
      "d13: medication adherence for hypertension (ras antagonists)\n",
      "d14: medication adherence for cholesterol (statins)\n",
      "d15: mtm program completion rate for cmr\n",
      "snp - are they offering special plans\n",
      "sanction deduction - if sanction is deducted from last survey to this survey\n",
      "2016 part c summary - 2016 part c rating\n",
      "2016 part d summary - 2016 part d rating\n",
      "2016 overall - 2016 overall star rating of the plan\n",
      "rated-as - category name\n",
      "highest rating - category -c/d/overall for which rating is high\n",
      "rating - star rating of the plan\n",
      "acknowledgements\n",
      "i have collected these files from various data websites and data sources listed below -\n",
      "nhanes - from cds's national health and nutrition examination survey. link\n",
      "drugs' dataset - from fda drug database. link\n",
      "nutritions' dataset - usda food composition databsase. link\n",
      "star rating dataset - cms website. link\n",
      "inspiration\n",
      "these datasets are used for hundreds of publications per year worldwide. link\n",
      "content\n",
      "the dataset consists of two files, training and validation. each folder contains 10 subforders labeled as n0~n9, each corresponding a species form wikipedia's monkey cladogram. images are 400x300 px or larger and jpeg format (almost 1400 images). images were downloaded with help of the googliser open source code.\n",
      "label mapping:\n",
      "> label, latin nama\n",
      "> n0, alouatta_palliata\n",
      "> n1, erythrocebus_patas\n",
      "> n2, cacajao_calvus\n",
      "> n3, macaca_fuscata\n",
      "> n4, cebuella_pygmea\n",
      "> n5, cebus_capucinus\n",
      "> n6, mico_argentatus\n",
      "> n7, saimiri_sciureus\n",
      "> n8, aotus_nigriceps\n",
      "> n9, trachypithecus_johnii\n",
      "for more information on the monkey species and number of images per class make sure to check monkey_labels.txt file.\n",
      "aim\n",
      "this dataset is intended as a test case for fine-grain classification tasks, perhaps best used in combination with transfer learning. hopefully someone can help us expand the number of classes or number of images.\n",
      "acknowledgements\n",
      "thanks to romain renard for his help with the code implementation. also, thanks to gustavo montoya, jacky zhang and sofia loaiciga for their help with the dataset curation.\n",
      "notes\n",
      "some demo code for usage of the dataset in combination with keras can be found in this repo.\n",
      "charlottesville, virgina\n",
      "charlottesville is home to a statue of robert e. lee which is slated to be removed. (for those unfamiliar with american history, robert e. lee was a us army general who defected to the confederacy during the american civil war and was considered to be one of their best military leaders.) while many americans support the move, believing the main purpose of the confederacy was to defend the institution of slavery, many others do not share this view. furthermore, believing confederate symbols to be merely an expression of southern pride, many have not taken its planned removal lightly.\n",
      "as a result, many people--including white nationalists and neo-nazis--have descended to charlottesville to protest its removal. this in turn attracted many counter-protestors. tragically, one of the counter-protestors--heather heyer--was killed and many others injured after a man intentionally rammed his car into them. in response, president trump blamed \"both sides\" for the chaos in charlottesville, leading many americans to denounce him for what they see as a soft-handed approach to what some have called an act of \"domestic terrorism.\"\n",
      "this dataset below captures the discussion--and copious amounts of anger--revolving around this past week's events.\n",
      "the data\n",
      "description\n",
      "this data set consists of a random sample of 50,000 tweets per day (in accordance with the twitter developer agreement) of tweets mentioning charlottesville or containing \"#charlottesville\" extracted via the twitter streaming api, starting on august 15. the files were copied from a large postgres database containing--currently--over 2 million tweets. finally, a table of tweet counts per timestamp was created using the whole database (not just the kaggle sample). the data description pdf provides a full summary of the attributes found in the csv files.\n",
      "note: while the tweet timestamps are in utc, the cutoffs were based on eastern standard time, so the august 16 file will have timestamps ranging from 2017-08-16 4:00:00 utc to 2017-08-17 4:00:00 utc.\n",
      "format\n",
      "the dataset is available as either separate csv files or a single sqlite database.\n",
      "license\n",
      "i'm releasing the dataset under the cc by-sa 4.0 license. furthermore, because this data was extracted via the twitter streaming api, its use must abide by the twitter developer agreement. most notably, the display of individual tweets should satisfy these requirements. more information can be found in the data description file, or on twitter's website.\n",
      "acknowledgements\n",
      "obviously, i would like to thank twitter for providing a fast and reliable streaming service. i'd also like to thank the developers of the python programming language, psycopg2, and postgres for creating amazing software with which this data set would not exist.\n",
      "image credit\n",
      "the banner above is a personal modification of these images:\n",
      "evan nesterak: image source image license\n",
      "wikipedia user cville dog image source\n",
      "the associated press image source\n",
      "inspiration\n",
      "i almost removed the header \"inspiration\" from this section, because this is a rather sad and dark data set. however, this is preciously why this is an important data set to analyze. good history books have never shied away from unpleasant events, and never should we.\n",
      "this data set provides a rich opportunity for many types of research, including:\n",
      "natural language processing\n",
      "sentiment analysis\n",
      "data visualization\n",
      "furthermore, given the political nature of this dataset, there are a lot of social science questions that can potentially be answered, or at least piqued, by this data.\n",
      "context:\n",
      "there are over 7,000 human languages in the world. the world atlas of language structures (wals) contains information on the structure of 2,679 of them. it also includes information about where languages are used. wals is widely-cited and used in the linguistics research community.\n",
      "content:\n",
      "the world atlas of language structures (wals) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors. the atlas provides information on the location, linguistic affiliation and basic typological features of a great number of the world's languages\n",
      "wals online is a publication of the (max planck institute for evolutionary anthropology)[http://www.eva.mpg.de/]. it is a separate publication, edited by dryer, matthew s. & haspelmath, martin (leipzig: max planck institute for evolutionary anthropology, 2013) the main programmer is robert forkel.\n",
      "this dataset includes three files:\n",
      "source.bib: a bibtex file with all of the sources cited in the dataset in it\n",
      "language.csv: a file with a list of all the languages included in wals\n",
      "wals-data.csv: a file containing information on the features associated with each individual language\n",
      "acknowledgements:\n",
      "this dataset is licensed under a creative commons attribution 4.0 international license .\n",
      "the world atlas of language structures was edited by matthew dryer and martin haspelmath. if you use this data in your work, please include the following citation:\n",
      "dryer, matthew s. & haspelmath, martin (eds.) 2013. the world atlas of language structures online. leipzig: max planck institute for evolutionary anthropology. (available online at http://wals.info, accessed on september 7, 2017.)\n",
      "inspiration:\n",
      "this dataset was designed to make interactive maps of language features. can you make an interactive map that shows different linguistic features? you might find it helpful to use leaflet (for r) or plotly (for python). this blog post is a great resource to help you get started.\n",
      "there’s a lot of discussion of “linguistic universals” in linguistics. these are specific features that every language (should) have. can you identify any features that you think may be universals from this dataset?\n",
      "you may also like:\n",
      "atlas of pidgin and creole language structures: information on 76 creole and pidgin languages\n",
      "world language family map\n",
      "the sign language analyses (slay) database\n",
      "general info\n",
      "this is a set of just over 20,000 games collected from a selection of users on the site lichess.org, and how to collect more. i will also upload more games in the future as i collect them. this set contains the:\n",
      "game id;\n",
      "rated (t/f);\n",
      "start time;\n",
      "end time;\n",
      "number of turns;\n",
      "game status;\n",
      "winner;\n",
      "time increment;\n",
      "white player id;\n",
      "white player rating;\n",
      "black player id;\n",
      "black player rating;\n",
      "all moves in standard chess notation;\n",
      "opening eco (standardised code for any given opening, list here);\n",
      "opening name;\n",
      "opening ply (number of moves in the opening phase)\n",
      "for each of these separate games from lichess. i collected this data using the lichess api, which enables collection of any given users game history. the difficult part was collecting usernames to use, however the api also enables dumping of all users in a lichess team. there are several teams on lichess with over 1,500 players, so this proved an effective way to get users to collect games from.\n",
      "possible uses\n",
      "lots of information is contained within a single chess game, let alone a full dataset of multiple games. it is primarily a game of patterns, and data science is all about detecting patterns in data, which is why chess has been one of the most invested in areas of ai in the past. this dataset collects all of the information available from 20,000 games and presents it in a format that is easy to process for analysis of, for example, what allows a player to win as black or white, how much meta (out-of-game) factors affect a game, the relationship between openings and victory for black and white and more.\n",
      "context\n",
      "the task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\").\n",
      "content\n",
      "there are 1559 columns in the data.each row in the data represent one image which is tagged as ad or nonad in the last column.column 0 to 1557 represent the actual numerical attributes of the images\n",
      "acknowledgements\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "here is a bibtex citation as well:\n",
      "@misc{lichman:2013 , author = \"m. lichman\", year = \"2013\", title = \"{uci} machine learning repository\", url = \"http://archive.ics.uci.edu/ml\", institution = \"university of california, irvine, school of information and computer sciences\" } https://archive.ics.uci.edu/ml/citation_policy.html\n",
      "context\n",
      "this corpus contains 5001 female names and 2943 male names, sorted alphabetically, one per line created by mark kantrowitz and redistributed in nltk.\n",
      "the names.zip file includes\n",
      "readme: the readme file.\n",
      "female.txt: a line-delimited list of words.\n",
      "male.txt: a line-delimited list of words.\n",
      "license/usage\n",
      "names corpus, version 1.3 (1994-03-29)\n",
      "copyright (c) 1991 mark kantrowitz\n",
      "additions by bill ross\n",
      "\n",
      "this corpus contains 5001 female names and 2943 male names, sorted\n",
      "alphabetically, one per line.\n",
      "\n",
      "you may use the lists of names for any purpose, so long as credit is\n",
      "given in any published work. you may also redistribute the list if you\n",
      "provide the recipients with a copy of this readme file. the lists are\n",
      "not in the public domain (i retain the copyright on the lists) but are\n",
      "freely redistributable.  if you have any additions to the lists of\n",
      "names, i would appreciate receiving them.\n",
      "\n",
      "mark kantrowitz <mkant+@cs.cmu.edu>\n",
      "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n",
      "inspiration\n",
      "this corpus is used for the text classification chapter in the nltk book.\n",
      "context:\n",
      "identification of adverse drug reactions (adrs) during the post-marketing phase is one of the most important goals of drug safety surveillance. spontaneous reporting systems (srs) data, which are the mainstay of traditional drug safety surveillance, are used for hypothesis generation and to validate the newer approaches. the publicly available us food and drug administration (fda) adverse event reporting system (faers) data requires substantial curation before they can be used appropriately, and applying different strategies for data cleaning and normalization can have material impact on analysis results.\n",
      "content:\n",
      "we provide a curated and standardized version of faers removing duplicate case records, applying standardized vocabularies with drug names mapped to rxnorm concepts and outcomes mapped to snomed-ct concepts, and pre-computed summary statistics about drug-outcome relationships for general consumption. this publicly available resource, along with the source code, will accelerate drug safety research by reducing the amount of time spent performing data management on the source faers reports, improving the quality of the underlying data, and enabling standardized analyses using common vocabularies.\n",
      "acknowledgements:\n",
      "data available from this source.\n",
      "when using this data, please cite the original publication:\n",
      "banda jm, evans l, vanguri rs, tatonetti np, ryan pb, shah nh (2016) a curated and standardized adverse drug event resource to accelerate drug safety research. scientific data 3: 160026. http://dx.doi.org/10.1038/sdata.2016.26\n",
      "additionally, please cite the dryad data package:\n",
      "banda jm, evans l, vanguri rs, tatonetti np, ryan pb, shah nh (2016) data from: a curated and standardized adverse drug event resource to accelerate drug safety research. dryad digital repository. http://dx.doi.org/10.5061/dryad.8q0s4\n",
      "inspiration:\n",
      "this is a large-ish dataset (~4.5 gb uncompressed), so try out your batch processing skills in a kernel\n",
      "what groups of drugs are most risky?\n",
      "what medical conditions are most at risk to drug-associated risks?\n",
      "context\n",
      "this dataset is a snapshot of the openpowerlifting database as of february 2018. openpowerlifting is an organization which tracks meets and competitor results in the sport of powerlifting, in which competitors complete to lift the most weight for their class in three separate weightlifting categories.\n",
      "content\n",
      "this dataset includes two files. meets.csv is a record of all meets (competitions) included in the openpowerlifting database. competitors.csv is a record of all competitors who attended those meets, and the stats and lifts that they recorded at them.\n",
      "for more on how this dataset was collected, see the openpowerlifting faq.\n",
      "acknowledgements\n",
      "this dataset is republished as-is from the openpowerlifting source.\n",
      "inspiration\n",
      "how much influence does overall weight have on lifting capacity?\n",
      "how big of a difference does gender make? what is demographic of lifters more generally?\n",
      "context\n",
      "the imdb movies dataset contains information about 14,762 movies. information about these movies was downloaded with wget for the purpose of creating a movie recommendation app. the data was preprocessed and cleaned to be ready for machine learning applications.\n",
      "content\n",
      "title,\n",
      "wordsintitle,\n",
      "url,\n",
      "imdbrating,\n",
      "ratingcount,\n",
      "duration,\n",
      "year,\n",
      "type,\n",
      "nrofwins,\n",
      "nrofnominations,\n",
      "nrofphotos,\n",
      "nrofnewsarticles,\n",
      "nrofuserreviews,\n",
      "nrofgenre,\n",
      "the rest of the fields are dummy (0/1) variables indicating if the movie has the given genre:\n",
      "action,\n",
      "adult,\n",
      "adventure,\n",
      "animation,\n",
      "biography,\n",
      "comedy,\n",
      "crime,\n",
      "documentary,\n",
      "drama,\n",
      "family,\n",
      "fantasy,\n",
      "filmnoir,\n",
      "gameshow,\n",
      "history,\n",
      "horror,\n",
      "music,\n",
      "musical,\n",
      "mystery,\n",
      "news,\n",
      "realitytv,\n",
      "romance,\n",
      "scifi,\n",
      "short,\n",
      "sport,\n",
      "talkshow,\n",
      "thriller,\n",
      "war,\n",
      "western\n",
      "past research\n",
      "movie recommendation app (learning to rank)\n",
      "https://github.com/orgesleka/filmempfehlung,\n",
      "inspiration:\n",
      "movie recommendation app: https://github.com/orgesleka/filmempfehlung\n",
      "learning to rank: https://play.google.com/store/apps/details?id=de.leka.orges.filmempfehlung&hl=de\n",
      "context\n",
      "the dataset has been downloaded from a buzzfeed news article that was posted on jan 15, 2017. the link to the original source can be checked in the acknowledgements section.\n",
      "the authors have created a database of more than 1500 people/organization who have a connection with the trump family, his top advisors or his cabinet picks.\n",
      "the dataset can help us to capture how policy decisions may be impacted by these varied connections.\n",
      "content\n",
      "you have three datasets to play with.\n",
      "person_person.csv: each row represents a connection between a person and another person (eg. charles kushner and alan hammer)\n",
      "person_org.csv: each row represents a connection between a person and an organization (eg. 401 north wabash venture llc. and donald j. trump)\n",
      "org_org.csv: each row represents a connection between an organization and another organization (eg. trump commercial chicago llc and 401 north wabash venture llc. )\n",
      "all the three files are in the following format:\n",
      "column1: person or organization (a)\n",
      "column2: person or organization (b)\n",
      "column3: connection between (a) and (b)\n",
      "column4: source url from which the connection is derived\n",
      "acknowledgements\n",
      "source: https://www.buzzfeed.com/johntemplon/help-us-map-trumpworld\n",
      "inspiration\n",
      "https://github.com/buzzfeednews/trumpworld\n",
      "this is an incomplete database, and you are free to add more connections.\n",
      "context\n",
      "abstract: surveys for more than 9,500 households were conducted in the growing seasons 2002/2003 or 2003/2004 in eleven african countries: burkina faso, cameroon, ghana, niger and senegal in western africa; egypt in northern africa; ethiopia and kenya in eastern africa; south africa, zambia and zimbabwe in southern africa. households were chosen randomly in districts that are representative for key agro-climatic zones and farming systems. the data set specifies farming systems characteristics that can help inform about the importance of each system for a country’s agricultural production and its ability to cope with short- and long-term climate changes or extreme weather events. further it informs about the location of smallholders and vulnerable systems and permits benchmarking agricultural systems characteristics.\n",
      "content\n",
      "the data file contains survey data collected from different families and has 9597 rows that represent the households and 1753 columns with details about the households. the questionnaire was organized into seven sections and respondents were asked to relate the information provided to the previous 12 months’ farming season. there are too many columns to describe here, however they are described in detail in this paper: https://www.nature.com/articles/sdata201620?wt.ec_id=sdata-201605\n",
      "questionnaire.pdf: this file contains the questionnaire used, a description for each variable name and the question id.\n",
      "surveymanual.pdf: this file gives further information on the household questionnaire, the research design and surveying. it was produced for the team leaders and interviewers in the world bank/gef project.\n",
      "adaptationcoding.pdf: this file describes codes for variables ‘ad711’ to ‘ad7625’ from section vii of the questionnaire on adaptation options.\n",
      "there is also some description in how the data was collected in survey.pdf.\n",
      "acknowledgements\n",
      "waha, katharina; zipf, birgit; kurukulasuriya, pradeep; hassan, rashid (2016): an agricultural survey for more than 9,500 african households. figshare. https://doi.org/10.6084/m9.figshare.c.1574094\n",
      "https://www.nature.com/articles/sdata201620?wt.ec_id=sdata-201605\n",
      "the original dta file was converted to csv\n",
      "inspiration\n",
      "this dataset contains a huge amount of information related to farming households in africa. data like these are important for studying the impact of global warming on african agriculture and farming families.\n",
      "context\n",
      "i wanted to find a better way to provide live traffic updates. we dont all have access to the data from traffic monitoring sensors or whatever gets uploaded from people's smart phones to apple, google etc plus i question how accurate the traffic congestion is on google maps or other apps. so i figured that since buses are also in the same traffic and many buses stream their gps location and other data live, that would be an ideal source for traffic data. i investigated the data streams available from many bus companies around the world and found mta in nyc to be very reliable.\n",
      "content\n",
      "this dataset is from the nyc mta buses data stream service. in roughly 10 minute increments the bus location, route, bus stop and more is included in each row. the scheduled arrival time from the bus schedule is also included, to give an indication of where the bus should be (how much behind schedule, or on time, or even ahead of schedule).\n",
      "data for the entire month of june 2017 is included.\n",
      "due to space limitations on kaggle for datasets, only selected bus routes have been included.\n",
      "acknowledgements\n",
      "data is recorded from the mta siri real time data feed and the mta gtfs schedule data.\n",
      "inspiration\n",
      "i want to see what exploratory & discovery people come up with from this data. feel free to download this dataset for your own use however i would appreciate as many kernals included on kaggle as we can get.\n",
      "based on the interest this generates i plan to collect more data for subsequent months down the track.\n",
      "reddit is a social network which divide topics into so called 'subreddits'.\n",
      "in subreddit 'worldnews', news of the whole world are published. the dataset contains following columns: time_created - a unix timestamp of the submission creation date date_created - creation time in %y-%m-%d up_votes - how often the submission was upvoted down_votes - how often the submission was downvoted title - the title of the submission over_18 - if the submission is for mature persons author - the reddit username of the author subreddit - this is always 'worldnews'\n",
      "with the dataset, you can estimate several things in contrast to world politics and special events.\n",
      "about the missing migrants data\n",
      "this data is sourced from the international organization for migration. the data is part of a specific project called the missing migrants project which tracks deaths of migrants, including refugees , who have gone missing along mixed migration routes worldwide. the research behind this project began with the october 2013 tragedies, when at least 368 individuals died in two shipwrecks near the italian island of lampedusa. since then, missing migrants project has developed into an important hub and advocacy source of information that media, researchers, and the general public access for the latest information.\n",
      "where is the data from?\n",
      "missing migrants project data are compiled from a variety of sources. sources vary depending on the region and broadly include data from national authorities, such as coast guards and medical examiners; media reports; ngos; and interviews with survivors of shipwrecks. in the mediterranean region, data are relayed from relevant national authorities to iom field missions, who then share it with the missing migrants project team. data are also obtained by iom and other organizations that receive survivors at landing points in italy and greece. in other cases, media reports are used. iom and unhcr also regularly coordinate on such data to ensure consistency. data on the u.s./mexico border are compiled based on data from u.s. county medical examiners and sheriff’s offices, as well as media reports for deaths occurring on the mexico side of the border. estimates within mexico and central america are based primarily on media and year-end government reports. data on the bay of bengal are drawn from reports by unhcr and ngos. in the horn of africa, data are obtained from media and ngos. data for other regions is drawn from a combination of sources, including media and grassroots organizations. in all regions, missing migrants projectdata represents minimum estimates and are potentially lower than in actuality.\n",
      "updated data and visuals can be found here: https://missingmigrants.iom.int/\n",
      "who is included in missing migrants project data?\n",
      "iom defines a migrant as any person who is moving or has moved across an international border or within a state away from his/her habitual place of residence, regardless of\n",
      "    (1) the person’s legal status; \n",
      "    (2) whether the movement is voluntary or involuntary; \n",
      "    (3) what the causes for the movement are; or \n",
      "    (4) what the length of the stay is.[1]\n",
      "missing migrants project counts migrants who have died or gone missing at the external borders of states, or in the process of migration towards an international destination. the count excludes deaths that occur in immigration detention facilities, during deportation, or after forced return to a migrant’s homeland, as well as deaths more loosely connected with migrants’ irregular status, such as those resulting from labour exploitation. migrants who die or go missing after they are established in a new home are also not included in the data, so deaths in refugee camps or housing are excluded. this approach is chosen because deaths that occur at physical borders and while en route represent a more clearly definable category, and inform what migration routes are most dangerous. data and knowledge of the risks and vulnerabilities faced by migrants in destination countries, including death, should not be neglected, rather tracked as a distinct category.\n",
      "how complete is the data on dead and missing migrants?\n",
      "data on fatalities during the migration process are challenging to collect for a number of reasons, most stemming from the irregular nature of migratory journeys on which deaths tend to occur. for one, deaths often occur in remote areas on routes chosen with the explicit aim of evading detection. countless bodies are never found, and rarely do these deaths come to the attention of authorities or the media. furthermore, when deaths occur at sea, frequently not all bodies are recovered - sometimes with hundreds missing from one shipwreck - and the precise number of missing is often unknown. in 2015, over 50 per cent of deaths recorded by the missing migrants project refer to migrants who are presumed dead and whose bodies have not been found, mainly at sea.\n",
      "data are also challenging to collect as reporting on deaths is poor, and the data that does exist are highly scattered. few official sources are collecting data systematically. many counts of death rely on media as a source. coverage can be spotty and incomplete. in addition, the involvement of criminal actors in incidents means there may be fear among survivors to report deaths and some deaths may be actively covered-up. the irregular immigration status of many migrants, and at times their families as well, also impedes reporting of missing persons or deaths.\n",
      "the varying quality and comprehensiveness of data by region in attempting to estimate deaths globally may exaggerate the share of deaths that occur in some regions, while under-representing the share occurring in others.\n",
      "what can be understood through this data?\n",
      "the available data can give an indication of changing conditions and trends related to migration routes and the people travelling on them, which can be relevant for policy making and protection plans. data can be useful to determine the relative risks of irregular migration routes. for example, missing migrants project data show that despite the increase in migrant flows through the eastern mediterranean in 2015, the central mediterranean remained the more deadly route. in 2015, nearly two people died out of every 100 travellers (1.85%) crossing the central route, as opposed to one out of every 1,000 that crossed from turkey to greece (0.095%). from the data, we can also get a sense of whether groups like women and children face additional vulnerabilities on migration routes.\n",
      "however, it is important to note that because of the challenges in data collection for the missing and dead, basic demographic information on the deceased is rarely known. often migrants in mixed migration flows do not carry appropriate identification. when bodies are found it may not be possible to identify them or to determine basic demographic information. in the data compiled by missing migrants project, sex of the deceased is unknown in over 80% of cases. region of origin has been determined for the majority of the deceased. even this information is at times extrapolated based on available information – for instance if all survivors of a shipwreck are of one origin it was assumed those missing also came from the same region.\n",
      "the missing migrants project dataset includes coordinates for where incidents of death took place, which indicates where the risks to migrants may be highest. however, it should be noted that all coordinates are estimates.\n",
      "why collect data on missing and dead migrants?\n",
      "by counting lives lost during migration, even if the result is only an informed estimate, we at least acknowledge the fact of these deaths. what before was vague and ill-defined is now a quantified tragedy that must be addressed. politically, the availability of official data is important. the lack of political commitment at national and international levels to record and account for migrant deaths reflects and contributes to a lack of concern more broadly for the safety and well-being of migrants, including asylum-seekers. further, it drives public apathy, ignorance, and the dehumanization of these groups.\n",
      "data are crucial to better understand the profiles of those who are most at risk and to tailor policies to better assist migrants and prevent loss of life. ultimately, improved data should contribute to efforts to better understand the causes, both direct and indirect, of fatalities and their potential links to broader migration control policies and practices.\n",
      "counting and recording the dead can also be an initial step to encourage improved systems of identification of those who die. identifying the dead is a moral imperative that respects and acknowledges those who have died. this process can also provide a some sense of closure for families who may otherwise be left without ever knowing the fate of missing loved ones.\n",
      "identification and tracing of the dead and missing\n",
      "as mentioned above, the challenge remains to count the numbers of dead and also identify those counted. globally, the majority of those who die during migration remain unidentified. even in cases in which a body is found identification rates are low. families may search for years or a lifetime to find conclusive news of their loved one. in the meantime, they may face psychological, practical, financial, and legal problems.\n",
      "ultimately missing migrants project would like to see that every unidentified body, for which it is possible to recover, is adequately “managed”, analysed and tracked to ensure proper documentation, traceability and dignity. common forensic protocols and standards should be agreed upon, and used within and between states. furthermore, data relating to the dead and missing should be held in searchable and open databases at local, national and international levels to facilitate identification.\n",
      "for more in-depth analysis and discussion of the numbers of missing and dead migrants around the world, and the challenges involved in identification and tracing, read our two reports on the issue, fatal journeys: tracking lives lost during migration (2014) and fatal journeys volume 2, identification and tracing of dead and missing migrants\n",
      "content\n",
      "the data set records incidents of missing persons and deaths of migrants\n",
      "columns in the data:\n",
      "id - unique key documenting incident\n",
      "cause of death - reason for death\n",
      "region of origin\n",
      "nationality\n",
      "missing persons - counts\n",
      "dead - counts of deaths\n",
      "incident region - region where incident was recorded\n",
      "date - the date when the incident was recorded. note the data set includes records from 2014 to june 2017\n",
      "latitude - spatial coordinates\n",
      "longitude - spatial coordinates\n",
      "acknowledgements\n",
      "this data set was created by the international organization for migration.\n",
      "https://www.iom.int/about-iom\n",
      "established in 1951, iom is the leading inter-governmental organization in the field of migration and works closely with governmental, intergovernmental and non-governmental partners.\n",
      "with 166 member states, a further 8 states holding observer status and offices in over 100 countries, iom is dedicated to promoting humane and orderly migration for the benefit of all. it does so by providing services and advice to governments and migrants.\n",
      "iom works to help ensure the orderly and humane management of migration, to promote international cooperation on migration issues, to assist in the search for practical solutions to migration problems and to provide humanitarian assistance to migrants in need, including refugees and internally displaced people.\n",
      "the iom constitution recognizes the link between migration and economic, social and cultural development, as well as to the right of freedom of movement.\n",
      "iom works in the four broad areas of migration management:\n",
      "migration and development\n",
      "facilitating migration\n",
      "regulating migration\n",
      "forced migration.\n",
      "iom activities that cut across these areas include the promotion of international migration law, policy debate and guidance, protection of migrants' rights, migration health and the gender dimension of migration.\n",
      "start a new kernel\n",
      "context\n",
      "this datasheet is an extension of the job of \"murder accountability project\". in this datasheet is included a vectorial file of states to make easier the labour of geographical plotting.\n",
      "content\n",
      "the murder accountability project is the most complete database of homicides in the united states currently available. this dataset includes murders from the fbi's supplementary homicide report from 1976 to the present and freedom of information act data on more than 22,000 homicides that were not reported to the justice department. this dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used.\n",
      "acknowledgements\n",
      "the data was compiled and made available by the murder accountability project, founded by thomas hargrove.\n",
      "inspiration\n",
      "can you develop an algorithm to detect serial killer activity?\n",
      "content\n",
      "age-adjusted death rates for selected major causes of death: united states, 1900-2013\n",
      "age adjusting rates\n",
      "is a way to make fairer comparisons between groups with different age distributions. for example, a county having a higher percentage of elderly people may have a higher rate of death or hospitalization than a county with a younger population, merely because the elderly are more likely to die or be hospitalized. (the same distortion can happen when comparing races, genders, or time periods.) age adjustment can make the different groups more comparable. a \"standard\" population distribution is used to adjust death and hospitalization rates. the age-adjusted rates are rates that would have existed if the population under study had the same age distribution as the \"standard\" population. therefore, they are summary measures adjusted for differences in age distributions.\n",
      "acknowledgements\n",
      "scrap data from data.gov\n",
      "context\n",
      "this dataset contains check-ins in nyc and tokyo collected for about 10 month (from 12 april 2012 to 16 february 2013). it contains 227,428 check-ins in new york city and 573,703 check-ins in tokyo. each check-in is associated with its time stamp, its gps coordinates and its semantic meaning (represented by fine-grained venue-categories). this dataset is originally used for studying the spatial-temporal regularity of user activity in lbsns.\n",
      "content\n",
      "this dataset includes long-term (about 10 months) check-in data in new york city and tokyo collected from foursquare from 12 april 2012 to 16 february 2013. it contains two files in tsv format. each file contains 8 columns, which are:\n",
      "user id (anonymized)\n",
      "venue id (foursquare)\n",
      "venue category id (foursquare)\n",
      "venue category name (fousquare)\n",
      "latitude\n",
      "longitude\n",
      "timezone offset in minutes (the offset in minutes between when this check-in occurred and the same time in utc)\n",
      "utc time\n",
      "the file dataset_tsmc2014_nyc.txt contains 227428 check-ins in new york city. the file dataset_tsmc2014_tky.txt contains 537703 check-ins in tokyo.\n",
      "acknowledgements\n",
      "this dataset is acquired from here\n",
      "following is the citation of the dataset author's paper:\n",
      "dingqi yang, daqing zhang, vincent w. zheng, zhiyong yu. modeling user activity preference by leveraging user spatial temporal characteristics in lbsns. ieee trans. on systems, man, and cybernetics: systems, (tsmc), 45(1), 129-142, 2015. pdf\n",
      "inspiration\n",
      "one of the questions that i am trying to answer is if there is a pattern in users' checkin behaviour. for example, if it's a friday evening, what all places they might be interested to visit.\n",
      "nan\n",
      "context\n",
      "if you use this data, please be sure to give credit to witten, et. al., since it is their data set.\n",
      "cervical cancer tumor vs matched control data. data set is gene expression profiling data from tumor and matched normal samples (29 each). the data are the raw read counts (not normalized) from sequencing of microrna. this is not my data, but was published by:\n",
      "witten, d., et al. (2010) ultra-high throughput sequencing-based small rna discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. bmc biology, 8:58\n",
      "content\n",
      "the rows are each micro rna name and the columns are the sample names (n=normal, t=tumor). the values are raw read counts.\n",
      "acknowledgements\n",
      "witten, d., et al. (2010) ultra-high throughput sequencing-based small rna discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. bmc biology, 8:58\n",
      "inspiration\n",
      "use this data to practice making predictive models from machine learning/deep learning algorithms on gene expression profiling data.\n",
      "context\n",
      "predict whether or not a horse can survive based upon past medical conditions.\n",
      "noted by the \"outcome\" variable in the data.\n",
      "content\n",
      "all of the binary representation have been converted into the words they actually represent. however, a fuller description is provided by the data dictionary (datadict.txt).\n",
      "there are a lot of na's in the data. this is the real struggle here. try to find a way around it through imputation or other means.\n",
      "acknowledgements\n",
      "this dataset was originally published by the uci machine learning database: http://archive.ics.uci.edu/ml/datasets/horse+colic\n",
      "context\n",
      "this dataset contains information on pesticide residues in food. the u.s. department of agriculture (usda) agricultural marketing service (ams) conducts the pesticide data program (pdp) every year to help assure consumers that the food they feed themselves and their families is safe. ultimately, if epa determines a pesticide is not safe for human consumption, it is removed from the market.\n",
      "the pdp tests a wide variety of domestic and imported foods, with a strong focus on foods that are consumed by infants and children. epa relies on pdp data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. usda uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance usda’s integrated pest management objectives. usda also works with u.s. growers to improve agricultural practices.\n",
      "content\n",
      "while the original 2015 ms access database can be found [here (https://www.ams.usda.gov/datasets/pdp/pdpdata), the data has been transferred to a sqlite database for easier, more open use. the database contains two tables, sample data and results data. each sampling includes attributes such as extraction method, the laboratory responsible for the test, and epa tolerances among others. these attributes are labeled with codes, which can be referenced in pdf format here, or integrated into the database using the included csv files.\n",
      "inspiration\n",
      "what are the most common types of pesticides tested in this study?\n",
      "do certain states tend to use one particular pesticide type over another?\n",
      "does pesticide type correspond more with crop type or location (state)?\n",
      "are any produce types found to have higher pesticide levels than assumed safe by epa standards?\n",
      "by combining databases from several years of pdp tests, can you see any trends in pesticide use?\n",
      "acknowledgement\n",
      "this dataset is part of the usda pdp yearly database, and the original source can be found here.\n",
      "about this data\n",
      "this is a list of over 18,000 restaurants in the us that serve vegetarian or vegan food provided by datafiniti's business database. the dataset includes address, city, state, business name, business categories, menu data, phone numbers, and more.\n",
      "what you can do with this data\n",
      "you can use this data to determine the most vegetarian and vegan-friendly cities in the us. e.g.:\n",
      "how many restaurants in each metro area offers vegetarian options?\n",
      "which metros among the 25 most popular metro areas have the most and least vegetarian restaurants per 100,000 residents?\n",
      "which metros with at least 10 vegetarian restaurants have the most vegetarian restaurants per 100,000 residents?\n",
      "how many restaurants in each metro area offers vegan options?\n",
      "which metros among the 25 most popular metro areas have the most and least vegan restaurants per 100,000 residents?\n",
      "which metros with at least 10 vegan restaurants have the most vegan restaurants per 100,000 residents?\n",
      "which cuisines are served the most at vegetarian restaurants?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "this dataset contains the prevalence and trends of health care access/coverage for 1995-2010. percentages are weighted to population characteristics. data are not available if it did not meet behavioral risk factor surveillance system (brfss) stability requirements. for more information on these requirements, as well as risk factors and calculated variables, see the technical documents and survey data for a specific year - http://www.cdc.gov/brfss/annual_data/annual_data.htm.\n",
      "content\n",
      "this dataset has 7 variables:\n",
      "year\n",
      "state\n",
      "yes\n",
      "no\n",
      "category\n",
      "condition\n",
      "location 1\n",
      "acknowledgements\n",
      "the original dataset can be found here.\n",
      "recommended citation: centers for disease control and prevention (cdc). behavioral risk factor surveillance system. atlanta, georgia: u.s. department of health and human services, centers for disease control and prevention, [appropriate year].\n",
      "inspiration\n",
      "how does health care coverage change over time?\n",
      "does health care access differ by state?\n",
      "the gender statistics database is a comprehensive source for the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.\n",
      "the data\n",
      "the data is split into several files, with the main one being data.csv. the data.csv contains all the variables of interest in this dataset, while the others are lists of references and general nation-by-nation information.\n",
      "data.csv contains the following fields:\n",
      "data.csv\n",
      "country.name: the name of the country\n",
      "country.code: the country's code\n",
      "indicator.name: the name of the variable that this row represents\n",
      "indicator.code: a unique id for the variable\n",
      "1960 - 2016: one column each for the value of the variable in each year it was available\n",
      "the other files\n",
      "i couldn't find any metadata for these, and i'm not qualified to guess at what each of the variables mean. i'll list the variables for each file, and if anyone has any suggestions (or, even better, actual knowledge/citations) as to what they mean, please leave a note in the comments and i'll add your info to the data description.\n",
      "country-series.csv\n",
      "countrycode\n",
      "seriescode\n",
      "description\n",
      "country.csv\n",
      "country.code\n",
      "short.name\n",
      "table.name\n",
      "long.name\n",
      "2-alpha.code\n",
      "currency.unit\n",
      "special.notes\n",
      "region\n",
      "income.group\n",
      "wb-2.code\n",
      "national.accounts.base.year\n",
      "national.accounts.reference.year\n",
      "sna.price.valuation\n",
      "lending.category\n",
      "other.groups\n",
      "system.of.national.accounts\n",
      "alternative.conversion.factor\n",
      "ppp.survey.year\n",
      "balance.of.payments.manual.in.use\n",
      "external.debt.reporting.status\n",
      "system.of.trade\n",
      "government.accounting.concept\n",
      "imf.data.dissemination.standard\n",
      "latest.population.census\n",
      "latest.household.survey\n",
      "source.of.most.recent.income.and.expenditure.data\n",
      "vital.registration.complete\n",
      "latest.agricultural.census\n",
      "latest.industrial.data\n",
      "latest.trade.data\n",
      "latest.water.withdrawal.data\n",
      "footnote.csv\n",
      "countrycode\n",
      "seriescode\n",
      "year\n",
      "description\n",
      "series-time.csv\n",
      "seriescode\n",
      "year\n",
      "description\n",
      "series.csv\n",
      "series.code\n",
      "topic\n",
      "indicator.name\n",
      "short.definition\n",
      "long.definition\n",
      "unit.of.measure\n",
      "periodicity\n",
      "base.period\n",
      "other.notes\n",
      "aggregation.method\n",
      "limitations.and.exceptions\n",
      "notes.from.original.source\n",
      "general.comments\n",
      "source\n",
      "statistical.concept.and.methodology\n",
      "development.relevance\n",
      "related.source.links\n",
      "other.web.links\n",
      "related.indicators\n",
      "license.type\n",
      "acknowledgements\n",
      "this dataset was downloaded from the world bank's open data project. the summary of the terms of use of this data is as follows:\n",
      "you are free to copy, distribute, adapt, display or include the data in other products for commercial and noncommercial purposes at no cost subject to certain limitations summarized below.\n",
      "you must include attribution for the data you use in the manner indicated in the metadata included with the data.\n",
      "you must not claim or imply that the world bank endorses your use of the data by or use the world bank’s logo(s) or trademark(s) in conjunction with such use.\n",
      "other parties may have ownership interests in some of the materials contained on the world bank web site. for example, we maintain a list of some specific data within the datasets that you may not redistribute or reuse without first contacting the original content provider, as well as information regarding how to contact the original content provider. before incorporating any data in other products, please check the list: terms of use: restricted data.\n",
      "-- [ed. note: this last is not applicable to the gender statistics database]\n",
      "the world bank makes no warranties with respect to the data and you agree the world bank shall not be liable to you in connection with your use of the data.\n",
      "this is only a summary of the terms of use for datasets listed in the world bank data catalogue. please read the actual agreement that controls your use of the datasets, which is available here: terms of use for datasets. also see world bank terms and conditions.\n",
      "context\n",
      "the emergence in the united states of large-scale “megaregions” centered on major metropolitan areas is a phenomenon often taken for granted in both scholarly studies and popular accounts of contemporary economic geography.\n",
      "this dataset comes from a paper (nelson & rae, 2016. an economic geography of the united states: from commutes to megaregions) that uses a data set of more than 4,000,000 commuter flows as the basis for an empirical approach to the identification of such megaregions.\n",
      "content\n",
      "this dataset consists of two files: one contains the commuting data, and one is a gazetteer describing the population and locations of the census tracts referred to by the commuting data. the fields ofips and dfips (fips codes for the originating and destination census tracts, respectively) in commute_data.csv refer to the geoid field in census_tracts_2010.csv.\n",
      "commute_data\n",
      "this file contains information on over 4 million commute flows. it has the following fields:\n",
      "ofips: the full fips code for the origin census tract of an individual flow line\n",
      "*dfips *: the full fips code for the destination census tract of an individual flow line\n",
      "ostfips: the fips code for the origin state of an individual flow line\n",
      "octfips: the fips code for the origin county of an individual flow line\n",
      "otrfips: the fips code for the destination census tract of an individual flow line\n",
      "dstfips: the fips code for the destination state of an individual flow line\n",
      "dctfips: the fips code for the destination county of an individual flow line\n",
      "dtrfips: the fips code for the destination census tract of an individual flow line\n",
      "flow: the total number of commuters associated with this individual point to point flow line (i.e. the total number of journeys to work)\n",
      "moe: margin of error of the flow value above\n",
      "lenkm: length of each flow line, in kilometers\n",
      "estdivmoe: the flow value divided by the margin of error of the estimate\n",
      "census_tracts_2010\n",
      "this file contains the following fields, which represent information about different u.s. census tracts:\n",
      "usps: united states postal service state abbreviation\n",
      "geoid: geographic identifier - fully concatenated geographic code (state fips and county fips)\n",
      "ansicode: american national standards institute code\n",
      "name: name\n",
      "pop10: 2010 census population count.\n",
      "hu10: 2010 census housing unit count.\n",
      "aland: land area (square meters) - created for statistical purposes only.\n",
      "awater: water area (square meters) - created for statistical purposes only.\n",
      "aland_sqmi: land area (square miles) - created for statistical purposes only.\n",
      "awater_sqmi: water area (square miles) - created for statistical purposes only.\n",
      "intptlat: latitude (decimal degrees) first character is blank or \"-\" denoting north or south latitude respectively.\n",
      "intptlong: longitude (decimal degrees) first character is blank or \"-\" denoting east or west longitude respectively.\n",
      "acknowledgements\n",
      "this dataset comes from the following article:\n",
      "nelson & rae, 2016. an economic geography of the united states: from commutes to megaregions\n",
      "the full dataset (in gis shapefile format) can be found on figshare here\n",
      "content\n",
      "the smithsonian institution's global volcanism program (gvp) documents earth's volcanoes and their eruptive history over the past 10,000 years. the gvp reports on current eruptions from around the world and maintains a database repository on active volcanoes and their eruptions. the gvp is housed in the department of mineral sciences, part of the national museum of natural history, on the national mall in washington, d.c.\n",
      "the gvp database includes the names, locations, types, and features of more than 1,500 volcanoes with eruptions during the holocene period (approximately the last 10,000 years) or exhibiting current unrest.\n",
      "content\n",
      "this dataset consists of digitized paper mission reports from wwii. each record includes the date, conflict, geographic location, and other data elements to form a live-action sequence of air warfare from 1939 to 1945. the records include u.s. and royal air force data, in addition to some australian, new zealand and south african air force missions.\n",
      "acknowledgements\n",
      "lt col jenns robertson of the us air force developed the theater history of operations reports (thor) and posted them online after receiving department of defense approval.\n",
      "context\n",
      "time's person of the year hasn't always secured his or her place in the history books, but many honorees remain unforgettable: gandhi, khomeini, kennedy, elizabeth ii, the apollo 8 astronauts anders, borman and lovell. each has left an indelible mark on the world.\n",
      "time's choices for person of the year are often controversial. editors are asked to choose the person or thing that had the greatest impact on the news, for good or ill — guidelines that leave them no choice but to select a newsworthy, not necessarily praiseworthy, cover subject. controversial choices have included adolf hitler (1938), joseph stalin (1939, 1942), and ayatullah khomeini (1979).\n",
      "time's choices for person of the year are often politicians and statesmen. eleven american presidents, from fdr to george w. bush, have graced the person of the year cover, many of them more than once. as commander in chief of one of the world's greatest nations, it's hard not to be a newsmaker.\n",
      "content\n",
      "this dataset includes a record for every time magazine cover which has honored an individual or group as \"men of the year\", \"women of the year\", or (as of 1999) \"person of the year\".\n",
      "acknowledgements\n",
      "the data was scraped from time magazine's website.\n",
      "inspiration\n",
      "who has been featured on the magazine cover the most times? did any american presidents not receive the honor for their election victory? how has the selection of person of the year changed over time? have the magazine's choices become more or less controversial?\n",
      "abstract\n",
      "this dataset provides a collection of vital signals and reference blood pressure values acquired from 26 subjects that can be used for the purpose of non-invasive cuff-less blood pressure estimation.\n",
      "source\n",
      "creators: amirhossein esmaili, mohammad kachuee, mahdi shabany department of electrical engineering, sharif university of technology, tehran, iran date: october 2017\n",
      "relevant information\n",
      "this dataset is to be used for research methods trying to estimate blood pressure in a cuff-less non-invasive manner. for each subject in this dataset, phonocardiogram (pcg), electrocardiogram (ecg), and photoplethysmogram (ppg) signals are acquired. alongside the acquisition of the signals per subject, a number of reference bps are measured. here, a signal from a force-sensing resistor (fsr), placed under the cuff of the bp reference device, is used to distinguish exact moments of reference bp measurements, which are corresponding to the inflation and deflation of the cuff. the signal from fsr is also included in our dataset. for each subject, age, weight, and height are also recorded.\n",
      "attribute information\n",
      "in the dataset, corresponding to each subject there is a “.json” file. in each file, we have the following attributes:\n",
      "“uid”: subject number\n",
      "“age”: age of the subject\n",
      "“weight”: weight of the subject (kg)\n",
      "“height”: height of the subject (cm)\n",
      "“data_pcg”: acquired pcg signal from chest (fs = 1 khz)\n",
      "“data_ecg”: acquired ecg signal (fs = 1 khz)\n",
      "“data_ppg”: acquired ppg signal from fingertip (fs = 1 khz)\n",
      "“data_fsr”: acquired fsr signal (fs = 1 khz)\n",
      "“data_bp”: reference systolic blood pressure (sbp) and diastolic blood pressure (dbp) values acquired from the subjects. to distinguish the time instances in the signals in which sbp and dbp values are measured, fsr signal can be used. for more details, please refer to our paper.\n",
      "please note that the fsr signal should be inverted prior to any analysis to reflect the instantaneous cuff pressure. also, the moment of each reference device measurement is approximately the time at which the cuff pressure drops with a considerable negative slope.\n",
      "relevant papers\n",
      "a. esmaili, m. kachuee, m. shabany, nonlinear cuffless blood pressure estimation of healthy subjects using pulse transit time and arrival time, ieee transactions on instrumentation and measurement, 2017.\n",
      "a. esmaili, m. kachuee, m. shabany, non-invasive blood pressure estimation using phonocardiogram, ieee international symposium on circuits and systems (iscas'17), 2017.\n",
      "citation request\n",
      "a. esmaili, m. kachuee, m. shabany, nonlinear cuffless blood pressure estimation of healthy subjects using pulse transit time and arrival time, ieee transactions on instrumentation and measurement, 2017.\n",
      "can you beat the market?\n",
      "horse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. inspired by the pioneers of computerised horse betting, i'm sharing this dataset in the hope of finding more data scientists willing to take up the challenge and find new ways of exploiting it!\n",
      "as always, the goal for most of us is to find information in the data that can be used to generate profit, usually by finding information that has not already been considered by the other players in the game. but i'm always interested in finding new uses for the data, whatever they may be.\n",
      "horse racing is a huge business in hong kong, which has two race tracks in a city that is only 1,104 square km. the betting pools are bigger than all us racetracks combined, which means that the opportunity is unlimited for those who are successful.\n",
      "so are you up for it?\n",
      "content\n",
      "the data was obtained from various free sources and is presented in csv format. personally-identifiable information, such as horse and jockey names, has not been included. however these should have no relevance to the purpose of this dataset, which is purely for experimental use.\n",
      "there are two files:\n",
      "races.csv\n",
      "each line describes the condition of an individual race.\n",
      "race_id - unique identifier for the race\n",
      "date - date of the race, in yyyy-mm-dd format (note that the dates given have been obscured and are not the real ones, although the durations between each race should be correct)\n",
      "venue - a 2-character string, representing which of the 2 race courses this race took place at: st = shatin, hv = happy valley\n",
      "race_no - race number of the race in the day's meeting\n",
      "config - race track configuration, mostly related to the position of the inside rail. for more details, see the hkjc website.\n",
      "surface - a number representing the type of race track surface: 1 = dirt, 0 = turf\n",
      "distance - distance of the race, in metres\n",
      "going - track condition. for more details, see the hkjc website.\n",
      "horse_ratings - the range of horse ratings that may participate in this race\n",
      "prize - the winning prize, in hk dollars\n",
      "race_class - a number representing the class of the race\n",
      "sec_time1 - time taken by the leader of the race to reach the end of the end of the 1st sectional point (sec)\n",
      "sec_time2 - time taken by the leader of the race to reach the end of the 2nd sectional point (sec)\n",
      "sec_time3 - time taken by the leader of the race to reach the end of the 3rd sectional point (sec)\n",
      "sec_time4 - time taken by the leader of the race to reach the end of the 4th sectional point, if any (sec)\n",
      "sec_time5 - time taken by the leader of the race to reach the end of the 5th sectional point, if any (sec)\n",
      "sec_time6 - time taken by the leader of the race to reach the end of the fourth sectional point, if any (sec)\n",
      "sec_time7 - time taken by the leader of the race to reach the end of the fourth sectional point, if any (sec)\n",
      "time1 - time taken by the leader of the race in the 1st section only (sec)\n",
      "time2 - time taken by the leader of the race in the 2nd section only (sec)\n",
      "time3 - time taken by the leader of the race in the 3rd section only (sec)\n",
      "time4 - time taken by the leader of the race in the 4th section only, if any (sec)\n",
      "time5 - time taken by the leader of the race in the 5th section only, if any (sec)\n",
      "time6 - time taken by the leader of the race in the 6th section only, if any (sec)\n",
      "time7 - time taken by the leader of the race in the 7th section only, if any (sec)\n",
      "place_combination1 - placing horse no (1st)\n",
      "place_combination2 - placing horse no (2nd)\n",
      "place_combination3 - placing horse no (3rd)\n",
      "place_combination4 - placing horse no (4th)\n",
      "place_dividend1 - placing dividend paid (for place_combination1)\n",
      "place_dividend2 - placing dividend paid (for place_combination2)\n",
      "place_dividend3 - placing dividend paid (for place_combination2)\n",
      "place_dividend4 - placing dividend paid (for place_combination2)\n",
      "win_combination1 - winning horse no\n",
      "win_dividend1 - winning dividend paid (for win_combination1)\n",
      "win_combination2 - joint winning horse no, if any\n",
      "win_dividend2 - winning dividend paid (for win_combination2, if any)\n",
      "runs.csv\n",
      "each line describes the characteristics of one horse run, in one of the races given in races.csv.\n",
      "race_id - unique identifier for the race\n",
      "horse_no - the number assigned to this horse, in the race\n",
      "horse_id - unique identifier for this horse\n",
      "result - finishing position of this horse in the race\n",
      "won - whether horse won (1) or otherwise (0)\n",
      "lengths_behind - finishing position, as the number of horse lengths behind the winner\n",
      "horse_age - current age of this horse at the time of the race\n",
      "horse_country - country of origin of this horse\n",
      "horse_type - sex of the horse, e.g. 'gelding', 'mare', 'horse', 'rig', 'colt', 'filly'\n",
      "horse_rating - rating number assigned by hkjc to this horse at the time of the race\n",
      "horse_gear - string representing the gear carried by the horse in the race. an explanation of the codes used may be found on the hkjc website.\n",
      "declared_weight - declared weight of the horse and jockey, in lbs\n",
      "actual_weight - actual weight carried by the horse, in lbs\n",
      "draw - post position number of the horse in this race\n",
      "position_sec1 - position of this horse (ranking) in section 1 of the race\n",
      "position_sec2 - position of this horse (ranking) in section 2 of the race\n",
      "position_sec3 - position of this horse (ranking) in section 3 of the race\n",
      "position_sec4 - position of this horse (ranking) in section 4 of the race, if any\n",
      "position_sec5 - position of this horse (ranking) in section 5 of the race, if any\n",
      "position_sec6 - position of this horse (ranking) in section 6 of the race, if any\n",
      "behind_sec1 - position of this horse (lengths behind leader) in section 1 of the race\n",
      "behind_sec2 - position of this horse (lengths behind leader) in section 2 of the race\n",
      "behind_sec3 - position of this horse (lengths behind leader) in section 3 of the race\n",
      "behind_sec4 - position of this horse (lengths behind leader) in section 4 of the race, if any\n",
      "behind_sec5 - position of this horse (lengths behind leader) in section 5 of the race, if any\n",
      "behind_sec6 - position of this horse (lengths behind leader) in section 6 of the race, if any\n",
      "time1 - time taken by the horse to pass through the 1st section of the race (sec)\n",
      "time2 - time taken by the horse to pass through the 2nd section of the race (sec)\n",
      "time3 - time taken by the horse to pass through the 3rd section of the race (sec)\n",
      "time4 - time taken by the horse to pass through the 4th section of the race, if any (sec)\n",
      "time5 - time taken by the horse to pass through the 5th section of the race, if any (sec)\n",
      "time6 - time taken by the horse to pass through the 6th section of the race, if any (sec)\n",
      "finish_time - finishing time of the horse in this race (sec)\n",
      "win_odds - win odds for this horse at start of race\n",
      "place_odds - place (finishing in 1st, 2nd or 3rd position) odds for this horse at start of race\n",
      "trainer_id - unique identifier of the horse's trainer at the time of the race\n",
      "jockey_id - unique identifier of the jockey riding the horse in this race\n",
      "acknowledgements\n",
      "none of this research would have even started without me standing on the shoulders of giants such as william benter, ruth bolton and randall chapman and many others who have published the results of their research.\n",
      "inspiration\n",
      "it is probably not going to be enough to just take this dataset and feed it into google cloud machine learning, azure mi, etc... but let me know if you find otherwise!\n",
      "questions that need to be answered include:\n",
      "feature engineering - what features are needed and how best to estimate them from the data given?\n",
      "modelling - what kind of model works best? maybe more than one model?\n",
      "other data - is there any other data needed, apart from that given in this data set?\n",
      "as a cytogeneticist studying some features of the chromosomes (their telomeres, structural anomalies ...) you need to take pictures from chromosomal preparations fixed on glass slides . unfortunately, like the sticks in the mikado game, sometime a chromosome or more can fall on an other one yielding overlapping chromosomes in the image, the plague of cytogeneticists. before computers and images processing with photography, chromosomes were cut from a paper picture and then classified (at least two paper pictures were required), with computers and quantitative analysis, automatic methods were developped to overcome this problem (lunsteen & piper, minaee et al.)\n",
      "this dataset modelizes overlapping chromosomes. couples of non overlapping chromosomes were combined by relative translations and rotations to generate two kind of images:\n",
      "a grey scaled image of the two overlapping chromosomes combining a dapi stained chromosome and the labelling of the telomeres (showing the chromosomes extremities)\n",
      "a label image (the ground truth) were the value equal to 3 labels the pixels (displayed in red) belonging to the overlapping domain.\n",
      "the images were saved in a numpy array as an hdf5 file. the following minimalist python 2 code can load the data (assuming that the unzipped data are in the same folder than the code) :\n",
      "import h5py\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "#h5f = h5py.file('overlapping_chromosomes_examples.h5','r')\n",
      "h5f = h5py.file('lowres_13434_overlapping_pairs.h5','r')\n",
      "pairs = h5f['dataset_1'][:]\n",
      "h5f.close()\n",
      "\n",
      "grey = pairs[220,:,:,0]\n",
      "mask = pairs[220,:,:,1]\n",
      "#%matplotlib inline\n",
      "plt.subplot(121)\n",
      "plt.imshow(grey)\n",
      "plt.title('max='+str(grey.max()))\n",
      "plt.subplot(122)\n",
      "plt.imshow(mask)\n",
      "i hope this dataset to be suitable to apply supervised learning methods, possibly similar to segnet or its implementation with keras.\n",
      "thinking of making a move to the lovely twin cities? first check out this dataset (curtesy of open data minneapolis) before you pack your bags for the \"little apple.\" the datasets included contain information about 311 calls and crimes committed between 2010 to 2016 which will help you convince your friends, family, and loved ones that minneapolis is the place to be (or not). snow plow noise complaints be darned!\n",
      "patterns in the brazilian congress voting behavior\n",
      "the brazilian government house of representatives maintains a public database, that contains legislative information since 1970. one type of information that is available are the records of bills. for each bill, the database gives a list of votes choices, state and party of each deputy, and a list of details about the bill itself like type, year, text of proposal, benches orientations and situation (a bill can be voted more than one time, in this work we will treat each votation as a single one). we retrieved more than 100000 bills (proplist), where less than 1% was voted (propvotlist) until november 2016.\n",
      "our objective is detect regularity patterns of legislative behavior, institutional arrangements, and legislative outcome.\n",
      "raw data from: http://www2.camara.leg.br/transparencia/dados-abertos/dados-abertos-legislativo/webservices/proposicoes-1/proposicoes\n",
      "context\n",
      "the iowa department of commerce requires that every store that sells alcohol in bottled form for off-the-premises consumption must hold a class \"e\" liquor license (an arrangement typical of most of the state alcohol regulatory bodies). all alcoholic sales made by stores registered thusly with the iowa department of commerce are logged in the commerce department system, which is in turn published as open data by the state of iowa.\n",
      "content\n",
      "this dataset contains information on the name, kind, price, quantity, and location of sale of sales of individual containers or packages of containers of alcoholic beverages.\n",
      "this dataset is relatively straightforward, but one source of further information on the contents of the data is this gist.\n",
      "acknowledgements\n",
      "this data was originally published by the state of iowa here and has been republished as-is on kaggle.\n",
      "inspiration\n",
      "this data is probably a representative sample of sale activity for alcohol in the united states, and can be used to answer many questions thereof, like: how much alcohol is sold and consumed in the united states? what kind? what are the most popular brands and labels? what are the most popular mixers? what is the distribution of prices paid in-store? etcetera.\n",
      "context\n",
      "this dataset contains 2.6 million words from urban dictionary, including their definitions and votes in csv format.\n",
      "source: https://www.urbandictionary.com\n",
      "to lookup a word in the dataset via urban dictionary api: http://api.urbandictionary.com/v0/define?defid={word_id}\n",
      "warning that this dataset contains a lot of profanity and racial slurs.\n",
      "content\n",
      "rows: 2,606,522\n",
      "column 1: word_id - for usage in urban dictionary api\n",
      "column 2: word - the text being defined\n",
      "column 3: up_votes - thumbs up count as of may 2016\n",
      "column 4: down_votes - thumbs down count as of may 2016\n",
      "column 5: author - hash of username of submitter\n",
      "column 6: definition - text with possible utf8 chars, double semi-colon denotes a newline\n",
      "acknowledgements\n",
      "remixed from data posted anonymously on reddit.\n",
      "https://archive.org/details/urbandictionary1999-may2016definitionscorpus\n",
      "inspiration\n",
      "modified and cleaned the original source which is in a very un-friendly format.\n",
      "grant red book (grb) full text\n",
      "context\n",
      "every tuesday, the us patent and trademark office (uspto) issues approximately 6,000 patent grants (patents) and posts the full text of the patents online. these patent grant documents contain much of the supporting details for a given patent. from this data, we can track trends in innovation across industries.\n",
      "frequency: weekly (tuesdays)\n",
      "period: 10/11/2016\n",
      "content\n",
      "the fields include patent number, series code and application number, type of patent, filing date, title, issue date, applicant information, inventor information, assignee(s) at time of issue, foreign priority information, related us patent documents, classification information (ipcr, cpc, us), us and foreign references, attorney, agent or firm/legal representative, examiner, citations, patent cooperation treaty (pct) information, abstract, specification, and claims.\n",
      "inspiration\n",
      "how many times will you find “some assembly required”? what inventions are at the cutting edge of machine learning? to answer these questions and any others you may have about this catalog of knowledge, fork the kernel library of inventions and innovations which demonstrates how to work with xml files in python.\n",
      "acknowledgements\n",
      "the uspto owns the dataset. these files are a subset and concatenation of the patent grant data/xml version 4.5 ice (grant red book). because of the concatenation of the individual xml documents, these files will not parse successfully or open/display by default in internet explorer. they also will not import into ms excel. each xml document within the file should have one start tag and one end tag. concatenation creates a file that contains 6,000 plus start/end tag combinations. if you take one document out of the patent grant full text file and place it in a directory with the correct dtd and then double click that individual document, internet explorer will parse/open the document successfully. note: you may receive a warning about active x controls. note: all patent grant full text files will open successfully in ms word; notepad; wordpad; and textpad.\n",
      "license\n",
      "creative commons - public domain mark 1.0\n",
      "what is the sherlock dataset?\n",
      "a long-term smartphone sensor dataset with a high temporal resolution. the dataset also offers explicit labels capturing the to activity of malwares running on the devices. the dataset currently contains 10 billion data records from 30 users collected over a period of 2 years and an additional 20 users for 10 months (totaling 50 active users currently participating in the experiment).\n",
      "the primary purpose of the dataset is to help security professionals and academic researchers in developing innovative methods of implicitly detecting malicious behavior in smartphones. specifically, from data obtainable without superuser (root) privileges. however, this dataset can be used for research in domains that are not strictly security related. for example, context aware recommender systems, event prediction, user personalization and awareness, location prediction, and more. the dataset also offers opportunities that aren't available in other datasets. for example, the dataset contains the ssid and signal strength of the connected wifi access point (ap) which is sampled once every second, over the course of many months.\n",
      "to gain full free access to the sherlock dataset, follow these two steps:\n",
      "1) read, complete and sign the license agreement. the general restrictions are:\n",
      "-the license lasts for 3 years, afterwhich the data must be deleted.\n",
      "-do not share the data with those who are not bound by the license agreement.\n",
      "-do not attempt to de-anonymize the individuals (volunteers) who have contributed the data.\n",
      "-any of your publication that benefit from the sherlock project must cite the following article: mirsky, yisroel, et al. \"sherlock vs moriarty: a smartphone dataset for cybersecurity research.\" proceedings of the 2016 acm workshop on artificial intelligence and security. acm, 2016.\n",
      "2)send the scanned document as a pdf to bgu.sherlock@gmail.com and provide a gmail account to share a google drive folder with.\n",
      "more information can be found here, or in this publication (download link).\n",
      "a 2 week data sample from a single user is provided on this kaggle page. to access the full dataset for free, please visit our site. note: the format of the sample dataset may differ from the full dataset.\n",
      "this is the google search interest data that powers the visualisation searching for health. google trends data allows us to see what people are searching for at a very local level. this visualization tracks the top searches for common health issues in the united states, from cancer to diabetes, and compares them with the actual location of occurrences for those same health conditions to understand how search data reflects life for millions of americans.\n",
      "how does search interest for top health issues change over time? from 2004–2017, the data shows that search interest gradually increased over the past few years. certain regions show a more significant increase in search interest than others. the increase in search activity is greatest in the midwest and northeast, while the changes are noticeably less dramatic in california, texas, and idaho. are people generally becoming more aware of health conditions and health risks?\n",
      "the search interest data was collected using the google trends api. the visualisation also brings in incidences of each condition so they can be compared. the health conditions were hand-selected from the community health status indicators (chsi) which provides key indicators for local communities in the united states. the chsi dataset includes more than 200 measures for each of the 3,141 united states counties. more information about the chsi can be found on healthdata.gov.\n",
      "many striking similarities exist between searches and actual conditions—but the relationship between the obesity and diabetes maps stands out the most. “there are many risk factors for type 2 diabetes such as age, race, pregnancy, stress, certain medications, genetics or family history, high cholesterol and obesity. however, the single best predictor of type 2 diabetes is overweight or obesity. almost 90% of people living with type 2 diabetes are overweight or have obesity. people who are overweight or have obesity have added pressure on their body's ability to use insulin to properly control blood sugar levels, and are therefore more likely to develop diabetes.” —obesity society via obesity.org\n",
      "this dataset contains 16,000 images of four shapes; square, star, circle, and triangle. each image is 200x200 pixels.\n",
      "the data was collected using a garmin virb 1080p action camera. the shapes were cut from poster board, and then painted green. i held each shape in view of the camera for two minutes. while the camera was recording the shape, i moved the shape around and rotated it.\n",
      "the four videos were then processed using opencv in python. using colorspaces, the green shape is cropped out of the image and resized to 200x200 pixels. the data is arranged into four folders; square, circle, triangle, and star. the images are labeled 0.png, 1.png, etc...\n",
      "a fifth video was taken with all of the shapes in the frame. this fifth video is for testing purposes. the goal is to classify the shapes in the test video using a model created with the training data. these classifications were made using a model made in keras.\n",
      "how is this different than the minst handwritten digits dataset? there are 10 classes in the minst dataset and 4 in this shapes dataset. the images in this data set are rotated, and the digits in the minst data set are not.\n",
      "the datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the northeast atlantic region, which are officially submitted by 20 international council for the exploration of the sea (ices) member countries between 2006 and 2014.\n",
      "context\n",
      "this dataset contains data about the earthquakes that hit the center of italy between august and november 2016. for some simple visualizations of this dataset you can checkout this post.\n",
      "content\n",
      "the dataset contains events from 2016-08-24 to 2016-11-30. it's a single .csv file with the following header:\n",
      "time,latitude,longitude,depth/km,magnitude\n",
      "the dataset contains 8087 rows (8086 of data + 1 of header)\n",
      "acknowledgements\n",
      "the dataset was collected from this real-time updated list from the italian earthquakes national center.\n",
      "inspiration\n",
      "i hope that someone in the kaggle community will find this dataset interesting to analyze and/or visualize.\n",
      "context\n",
      "the data set includes information about different leagues in different sports (basketball and soccer) all around the world, as well as some basic facts about each country, regarding the home advantage phenomenon in sports.\n",
      "content\n",
      "the data is comprised of 3 data sets:\n",
      "the home and away performance of 8365 soccer teams from a few dozens of countries during the years 2010-2016.\n",
      "the home and away performance of 1216 nba teams during the years 1968-2010\n",
      "general facts about 88 countries including soccer data such as their fifa rank, the average attendance of soccer matches and the home advatnage factor of the leagure\n",
      "acknowledgement\n",
      "the soccer data was scraped from here: http://footballdatabase.com/competitions-index\n",
      "the nba data was scraped from nba.com.\n",
      "the world facts were copied from wikipedia.\n",
      "context\n",
      "there are many kinds of birds: pigeons, ducks, ostriches, penguins... some are good at flying, others can't fly but run fast. some swim under water, others wading in shallow pool.\n",
      "according to their living environments and living habits, birds are classified into different ecological groups. there are 8 ecological groups of birds:\n",
      "swimming birds\n",
      "wading birds\n",
      "terrestrial birds\n",
      "raptors\n",
      "scansorial birds\n",
      "singing birds\n",
      "cursorial birds (not included in dataset)\n",
      "marine birds (not included in dataset)\n",
      "first 6 groups are main and are covered by this dataset.\n",
      "apparently, birds belong to different ecological groups have different appearances: flying birds have strong wings and wading birds have long legs. their living habits are somewhat reflected in their bones' shapes. as data scientists we may think of examining the underlying relationship between sizes of bones and ecological groups , and recognising birds' ecological groups by their bones' shapes.\n",
      "content\n",
      "there are 420 birds contained in this dataset. each bird is represented by 10 measurements (features):\n",
      "length and diameter of humerus\n",
      "length and diameter of ulna\n",
      "length and diameter of femur\n",
      "length and diameter of tibiotarsus\n",
      "length and diameter of tarsometatarsus\n",
      "all measurements are continuous float numbers (mm) with missing values represented by empty strings. the skeletons of this dataset are collections of natural history museum of los angeles county. they belong to 21 orders, 153 genera, 245 species.\n",
      "each bird has a label for its ecological group:\n",
      "sw: swimming birds\n",
      "w: wading birds\n",
      "t: terrestrial birds\n",
      "r: raptors\n",
      "p: scansorial birds\n",
      "so: singing birds\n",
      "acknowledgements\n",
      "this dataset is provided by dr. d. liu of beijing museum of natural history.\n",
      "inspiration\n",
      "this dataset is a 420x10 size continuous values unbalanced multi-class dataset. what can be done include:\n",
      "data visualisation\n",
      "statical analysis\n",
      "supervised classification\n",
      "unsupervised clustering\n",
      "license\n",
      "please do not publish or cite this dataset in research papers or other public publications.\n",
      "from wikipedia:\n",
      "\"the 500 greatest albums of all time\" is a 2003 special issue of american magazine rolling stone, and a related book published in 2005. the lists presented were compiled based on votes from selected rock musicians, critics, and industry figures, and predominantly feature american and british music from the 1960s and the 1970s.\n",
      "in 2012, rolling stone published a revised edition of the list drawing on the original and a later survey of albums in the 2000s. it was made available in \"bookazine\" format on newsstands in the us from april 27 to july 25. the new list contained 38 albums not present in the previous one, 16 of them released after 2003.\n",
      "i took the albums from musicbrainz but the genres weren't listed. i wrote a python script to get the genres and subgenres of each album from the discogs api.\n",
      "the data collected are:\n",
      "position on the list\n",
      "year of release\n",
      "album name\n",
      "artist name\n",
      "genre name\n",
      "subgenre name\n",
      "some of the genres/subgenres may not be entirely correct - discogs seems to not consider some of the smaller genres. let me know if there are any glaring issues and i'll try to fix them.\n",
      "context\n",
      "the wta (women's tennis association) is the principal organizing body of women's professional tennis, it governs its own tour worldwide. on its website, it provides a lot of data about the players as individuals as well the tour matches with results and the current rank during it.\n",
      "luckily for us, jeff sackmann scraped the website and collected everything from there and put in a nice way into easily consumable datasets.\n",
      "on jeff's github account you can find a lot more data about tennis!\n",
      "content\n",
      "the dataset present here is directly downloaded from the source, no alteration on the data was made, the files were only placed in subdirectories so one can easily locate them.\n",
      "it covers statistics of players registered on the wta, the matches that happened on each tour by year, with results, as well some qualifying matches for the tours.\n",
      "as a reminder, you may not find all data of the matches prior to 2006, so be warned when working with those sets.\n",
      "acknowledgements\n",
      "thanks to jeff sackmann for maintaining such collection and making it public!\n",
      "also, a thank you for wta for collecting those stats and making them accessible to anyone on their site.\n",
      "inspiration\n",
      "here are some things to start:\n",
      "which player did the most rapidly climb the ranks through the years?\n",
      "does the rank correlates with the money earn by the player?\n",
      "what can we find about the age?\n",
      "there is some deterministic factor to own the match?\n",
      "context\n",
      "this dataset contains results from the 2015 and 2013 elections in israel. results are given by voting booths (of comparable sizes of 0-800) and not by settlements (which are very varied - think tel aviv compared to a small kibbutz).\n",
      "content\n",
      "the first seven columns are information about each settlement and voting booth, and from the eighth to the end is the number of votes each party has received in each booth.\n",
      "acknowledgements\n",
      "this data is freely available at http://votes20.gov.il/ and http://www.votes-19.gov.il/nationalresults, i just translated the column headers into english. settlement names are translated according to this key from the central bureau of statistics, which uses the same settlement_code as the election results.\n",
      "inspiration\n",
      "personally, i've viewed this dataset in order to map out the relationships between different parties (i.e which are 'closer', which are more 'central'). this question is significant in israel, where the composition of the parliament is determined almost directly by the popular vote (e.g a party with 25% of the total proper votes will recieve 25% of seats in parliament), but the government is formed by a coalition of parties (so the head of the largest party in parliament will not necessarily be the prime minister).\n",
      "https://www.youtube.com/watch?v=a8syqeftbkc\n",
      "context\n",
      "the stanford mass shootings in america (msa) is a dataset released under creative commons attribution 4.0 international license by the stanford geospatial center. while not an exhaustive collection of mass shootings, it is a high-quality dataset ranging from 1966 to 2016 with well-defined methodology, definitions and source urls for user validation.\n",
      "this dataset can be used to validate other datasets, such as us-mass-shootings-last-50-years, which contains more recent data, or conduct other analysis, as more information is provided.\n",
      "content\n",
      "this dataset contains data by the msa project both from it's website and from it's github account. the difference between the two sources is only on the data format (i.e. .csv versus .geojson for the data, or .csv versus .pdf for the dictionary).\n",
      "mass_shooting_events_stanford_msa_release_06142016\n",
      "contains a nonexaustive list of us mass shootings from 1966 to 2016 in both .csv and .geojson formats.\n",
      "dictionary_stanford_msa_release_06142016\n",
      "contains the data dictionary in .csv and .pdf formats. note the .pdf format provides an easier way to visualize sub-fields.\n",
      "note the data was reproduced here without any modifications other than file renaming for clarity, the content is the same as in the source.\n",
      "the following sections are reproduced from the dataset creators website. for more details, please see the source.\n",
      "project background\n",
      "the stanford mass shootings of america (msa) data project began in 2012, in reaction to the mass shooting in sandy hook, ct. in our initial attempts to map this phenomena it was determined that no comprehensive collection of these incidents existed online. the stanford geospatial center set out to create, as best we could, a single point repository for as many mass shooting events as could be collected via online media. the result was the stanford msa.\n",
      "what the stanford msa is\n",
      "the stanford msa is a data aggregation effort. it is a curated set of spatial and temporal data about mass shootings in america, taken from online media sources. it is an attempt to facilitate research on gun violence in the us by making raw data more accessible.\n",
      "what the stanford msa is not\n",
      "the stanford msa is not a comprehensive, longitudinal research project. the data collected in the msa are not investigated past the assessment for inclusion in the database. the msa is not an attempt to answer specific questions about gun violence or gun laws.\n",
      "the stanford geospatial center does not provide analysis or commentary on the contents of this database or any derivatives produced with it.\n",
      "data collection methodology\n",
      "the information collected for the stanford msa is limited to online resources. an initial intensive investigation was completed looking back over existing online reports to fill in the historic record going back to 1966. contemporary records come in as new events occur and are cross referenced against a number of online reporting sources. in general a minimum of three corroborating sources are required to add the full record into the msa (as many as 6 or 7 sources may have been consulted in many cases). all sources for each event are listed in the database.\n",
      "due to the time involved in vetting the details of any new incident, there is often a 2 to 4 week lag between a mass shooting event and its inclusion in the public release database.\n",
      "it is important to note the records in the stanford msa span a time from well before the advent of online media reporting, through its infancy, to the modern era of web based news and information resources. researchers using this database need to be aware of the reporting bias these changes in technology present. a spike in incidents for recent years is likely due to increased online reporting and not necessarily indicative of the rate of mass shootings alone. researchers should look at this database as a curated collection of quality checked data regarding mass shootings, and not an exhaustive research data set itself. independent verification and analysis will be required to use this data in examining trends in mass shootings over time.\n",
      "definition of mass shooting\n",
      "the definition of mass shooting used for the stanford database is 3 or more shooting victims (not necessarily fatalities), not including the shooter. the shooting must not be identifiably gang, drug, or organized crime related.\n",
      "acknowledgements\n",
      "the stanford mass shootings in america (msa) is a dataset released under creative commons attribution 4.0 international license by the stanford geospatial center.\n",
      "how to cite the msa\n",
      "the stanford msa is released under a creative commons attribution 4.0 international license. please cite the msa as “stanford mass shootings in america, courtesy of the stanford geospatial center and stanford libraries”.\n",
      "inspiration\n",
      "there is already a great number of interesting datasets in kaggle surrounding the subject of mass shootings, however, little has been done leveraging information from multiple sources. can you see a story among them? can we learn anything, for example, comparing the different sources by city or state?\n",
      "from a bigger picture\n",
      "leading causes of death in us: https://www.kaggle.com/cdc/mortality\n",
      "gun violence database: https://www.kaggle.com/gunviolencearchive/gun-violence-database\n",
      "gun deaths in us: https://www.kaggle.com/hakabuk/gun-deaths-in-the-us\n",
      "homicide reports: https://www.kaggle.com/murderaccountability/homicide-reports\n",
      "global terrorism database: https://www.kaggle.com/start-umd/gtd\n",
      "crime rates in america: https://www.kaggle.com/marshallproject/crime-rates\n",
      "\"prevention?\n",
      "firearms provisions in us states: https://www.kaggle.com/jboysen/state-firearms\n",
      "trial and terror: https://www.kaggle.com/jboysen/trial-and-terror\n",
      "connecticut inmates waiting trial: https://www.kaggle.com/connecticut-open-data/connecticut-inmates-awaiting-trial\n",
      "are there warning signs?\n",
      "mental health in tech survey: https://www.kaggle.com/osmi/mental-health-in-tech-2016/data (not directly related but can be used to make a parenthesis about mental health being an issue in our surroundings).\n",
      "context\n",
      "this dataset includes the median list price divided by the square footage of a 1-bedroom home for a select number of neighborhoods around the united states.\n",
      "content\n",
      "when available, data includes median price per square foot on a monthly basis between january 2010 and september 2016.\n",
      "selected neighborhoods include:\n",
      "upper east side, new york, ny\n",
      "spring valley, las vegas, nv\n",
      "hollywood, los angeles, ca\n",
      "williamsburg, new york, ny\n",
      "harlem, new york, ny\n",
      "enterprise, las vegas,nv\n",
      "downtown, san jose, ca\n",
      "sheepshead bay, new york, ny\n",
      "forest hills, new york, ny\n",
      "jackson heights, new york, ny\n",
      "gramercy, new york, ny\n",
      "flagami, miami, fl\n",
      "downtown, memphis, tn\n",
      "chelsea, new york, ny\n",
      "oak lawn, dallas, tx\n",
      "greater uptown, houston, tx\n",
      "south loop, chicago, il\n",
      "makiki-lower punchbowl-tantalus, honolulu, hi\n",
      "downtown, los angeles, ca\n",
      "capitol hill, seattle, wa\n",
      "clinton, new york, ny\n",
      "alexandria west, alexandria, va\n",
      "financial district, new york, ny\n",
      "flatiron district, new york, ny\n",
      "landmark-van dom, alexandria, va\n",
      "flamingo lummus, miami beach, fl\n",
      "winchester, las vegas, nv\n",
      "brickell, miami, fl\n",
      "waikiki, honolulu, hi\n",
      "back bay, boston, ma\n",
      "sutton place, new york, ny\n",
      "and several others\n",
      "inspiration\n",
      "what neighborhoods have the most expensive real estate per square foot? least expensive?\n",
      "which neighborhoods and/or cities have the fastest growth rates in price?\n",
      "are there any neighborhoods that remain relatively steady in price?\n",
      "given that this metric is listing price per square foot, is there a similar dataset that could help you compare median square footage in a 1-bedroom home across neighborhoods?\n",
      "acknowledgement\n",
      "this dataset is part of zillow data, and the original source can be found here, under the neighborhoods link.\n",
      "context\n",
      "as part of the house intelligence committee investigation into how russia may have influenced the 2016 us election, twitter released the screen names of almost 3000 twitter accounts believed to be connected to russia’s internet research agency, a company known for operating social media troll accounts. twitter immediately suspended these accounts, deleting their data from twitter.com and the twitter api. a team at nbc news including ben popken and ej fox was able to reconstruct a dataset consisting of a subset of the deleted data for their investigation and were able to show how these troll accounts went on attack during key election moments. this dataset is the body of this open-sourced reconstruction.\n",
      "for more background, read the nbc news article publicizing the release: \"twitter deleted 200,000 russian troll tweets. read them here.\"\n",
      "content\n",
      "this dataset contains two csv files. tweets.csv includes details on individual tweets, while users.csv includes details on individual accounts.\n",
      "to recreate a link to an individual tweet found in the dataset, replace user_key in https://twitter.com/user_key/status/tweet_id with the screen-name from the user_key field and tweet_id with the number in the tweet_id field.\n",
      "following the links will lead to a suspended page on twitter. but some copies of the tweets as they originally appeared, including images, can be found by entering the links on web caches like archive.org and archive.is.\n",
      "acknowledgements\n",
      "if you publish using the data, please credit nbc news and include a link to this page. send questions to ben.popken@nbcuni.com.\n",
      "inspiration\n",
      "what are the characteristics of the fake tweets? are they distinguishable from real ones?\n",
      "context\n",
      "this is kaggle's facial keypoint detection dataset that is uploaded in order to allow kernels to work on it, as was also requested by a fellow kaggler in this discussion thread.\n",
      "content\n",
      "the dataset contains 7049 facial images and up to 15 keypoints marked on them.\n",
      "the keypoints are in the facial_keypoints.csv file.\n",
      "the image are in the face_images.npz file.\n",
      "look at the exploration script for code that reads and presents the dataset.\n",
      "acknowledgements\n",
      "this is a kaggle dataset, so all acknowledgements are to kaggle.\n",
      "from the original dataset acknowledgements : \"the data set for this competition was graciously provided by dr. yoshua bengio of the university of montreal\"\n",
      "inspiration\n",
      "i hope the dataset can serve as a decent deep learning repository of code.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "this dataset contains the complete detail about the prison and various characteristics of inmates. this will help to understand better about prison system in india.\n",
      "content\n",
      "details of jail wise population of prison inmates\n",
      "details about the list of jails in india at the end of year 2015.\n",
      "jail category wise population of inmates.\n",
      "capacity of jails by inmate population.\n",
      "age group, nationality and gender wise population of inmates.\n",
      "religion and gender wise population of inmates.\n",
      "caste and gender wise population of inmates.\n",
      "education standards of inmates.\n",
      "domicile of inmates.\n",
      "incidence of recidivism.\n",
      "rehabilitation of prisoners.\n",
      "distribution of sentence periods of convicts in various jails by sex and age-groups.\n",
      "details of under trial prisoners by the type of ipc (indian penal code) offences.\n",
      "details of convicts by the type of ipc (indian penal code) offences.\n",
      "details of sll (special & local law) crime headwise distribution of inmates who convicted\n",
      "details of sll (special & local law) crime head wise distribution of inmates under trial\n",
      "details of educational facilities provided to prisoners.\n",
      "details of jail breaks, group clashes and firing in jail (tranquility).\n",
      "details of wages per day to convicts.\n",
      "details of prison inmates trained under different vocational training.\n",
      "details of capital punishment (death sentence) and life imprisonment.\n",
      "details of prison inmates escaped.\n",
      "details of prison inmates released.\n",
      "details of strength of officials\n",
      "details of total budget and actual expenditure during the year 2015-16.\n",
      "details of budget\n",
      "details of expenditure\n",
      "details of expenditure on inmates\n",
      "details of inmates suffering from mental ilness\n",
      "details of period of detention of undertrials\n",
      "details of number of women prisoners with children\n",
      "details of details of inmates parole during the year\n",
      "details of value of goods produced by inmates\n",
      "details of number of vehicles available\n",
      "details of training of jail officers\n",
      "details of movements outside jail premises\n",
      "details of details of electronic equipment used in prison\n",
      "inspiration\n",
      "there are many questions about indian prison with this dataset. some of the interesting questions are\n",
      "percentage of jails over crowded. is there any change in percentage over time?\n",
      "how many percentage of inmates re-arrested?\n",
      "which state/u.t pay more wages to the inmates?\n",
      "which state/u.t has more capital punishment/life imprisonment inmates?\n",
      "inmates gender ratio per state\n",
      "acknowledgements\n",
      "national crime records bureau (ncrb), govt of india has shared this dataset under govt. open data license - india. ncrb has also shared prison data on their website.\n",
      "context\n",
      "the data is of national stock exchange of india. the data is compiled to felicitate machine learning, without bothering much about stock apis.\n",
      "content\n",
      "the data is of national stock exchange of india's stock listings for each trading day of 2016 and 2017. a brief description of columns. symbol: symbol of the listed company. series: series of the equity. values are [eq, be, bl, bt, gc and il] open: the opening market price of the equity symbol on the date. high: the highest market price of the equity symbol on the date. low: the lowest recorded market price of the equity symbol on the date. close: the closing recorded price of the equity symbol on the date. last: the last traded price of the equity symbol on the date. prevclose: the previous day closing price of the equity symbol on the date. tottrdqty: total traded quantity of the equity symbol on the date. tottrdval: total traded volume of the equity symbol on the date. timestamp: date of record. totaltrades: total trades executed on the day. isin: international securities identification number.\n",
      "acknowledgements\n",
      "all data is fetched from nse official site. https://www.nseindia.com/\n",
      "inspiration\n",
      "this dataset is compiled to felicitate machine learning on stocks.\n",
      "about the data\n",
      "the data used in this project is real and is based on the collection of over 20 years. the total number of record in this dataset is roughly around 120 million rows and the size of the data is approximately 12gb. the data consists of flight arrival and departure details for all commercial flights within the usa, from october 1987 to april 2008. this is a large dataset. there are around 29 attributes.\n",
      "how to get the data? the data originally comes from http://stat-computing.org/dataexpo/2009/the-data.html\n",
      "you can download the data for each year by clicking the appropriate link in the above website (remember the size is going to be more than 12gb).\n",
      "(i) problem statement (a) check the skewness of distance travelled by airlines. (b) calculate the mean, median and quantiles of the distance travelled by us airlines (us). (c) check the standard deviation of distance travelled by american airlines (aa). (d) draw a boxplot of uniquecarrier with distance. (e) draw the direction of relationship between arrdelay and depdelay by drawing a scatterplot.\n",
      "(ii) problem statement\n",
      "(a) what is the probability that a flight which is landing/taking off is “wn” airlines (marginal probability) (b) what is the probability that a flight which is landing/taking off is either “wn” or “aa” airlines (disjoint events) (c) what is the joint probability that a flight is both “wn” and travels less than 600 miles (joint probability) (d) what is the conditional probability that the flight travels less than 2500 miles given that the flight is “aa” airlines (conditional probability) (e) what is the joint probability of a flight getting cancelled and is supposed to travel less than 2500 miles given that the flight is “aa” airlines (joint + conditional probability)\n",
      "(iii) problem statement\n",
      "(a) suppose arrival delays of flights belonging to “aa” are normally distributed with mean 15 minutes and standard deviation 3 minutes. if the “aa” plans to announce a scheme where it will give 50% cash back if their flights are delayed by 20 minutes, how much percentage of the trips “aa” is supposed to loose this money. (hint: pnorm) (b) assume that 65% of flights are diverted due to bad weather through the weather system. what is the probability that in a random sample of 10 flights, 6 are diverted through the weather system. (hint: dbinorm) (c) do linear regression between the arrival delay and departure delay of the flights. (d) find out the confidence interval of the fitted linear regression line. (e) perform a multiple linear regression between the arrival delay along with the departure delay and distance travelled by flights.\n",
      "context\n",
      "github is the leader in hosting open source projects. for those who are not familiar with open source projects, a group of developers share and contribute to common code to develop software. example open source projects include, chromium (which makes google chrome), wordpress, and hadoop. open source projects are said to have disrupted the software industry (2008 kansas keynote).\n",
      "content\n",
      "for this study, i crawled the leader in hosting open source projects, github.com and extracted a list of the top starred open source projects. on github, a user may choose the star a repository representing that they “like” the project. for each project, i gathered the repository username or organization the project resided in, the repository name, a description, the last updated date, the language of the project, the number of stars, any tags, and finally the url of the project.\n",
      "acknowledgements\n",
      "this data wouldn't be available if it weren't for github. an example micro-study can be found at the concept center\n",
      "context\n",
      "tweets containing hurricane harvey from the morning of 8/25/2017. i hope to keep this updated if computer problems do not persist.\n",
      "*8/30 update this update includes the most recent tweets tagged \"tropical storm harvey\", which spans from 8/20 to 8/30 as well as the properly merged version of dataset including tweets from when harvey before it was downgraded back to a tropical storm.\n",
      "inspiration\n",
      "what are the popular tweets?\n",
      "can we find popular news stories from this?\n",
      "can we identify people likely staying or leaving, and is there a difference in sentiment between the two groups?\n",
      "is it possible to predict popularity with respect to retweets, likes, and shares?\n",
      "context & content\n",
      "this dataset features the salaries of 874 nhl players for the 2016/2017 season. i have randomly split the players into a training (612 players) and test (262 players) populations. there are 151 predictor columns (described in column legend section, if you're not familiar with hockey the meaning of some of these may be a bit cryptic!) as well as a leading column with the players 2016/2017 annual salary. for the test population the actual salaries have been broken off into a separate .csv file.\n",
      "acknowledgements\n",
      "raw excel sheet was acquired http://www.hockeyabstract.com/\n",
      "inspiration\n",
      "can you build a model to predict nhl player's salaries? what are the best predictors of how much a player will make?\n",
      "column legend\n",
      "acronym - meaning\n",
      "%fot - percentage of all on-ice faceoffs taken by this player.\n",
      "+/- - plus/minus\n",
      "1g - first goals of a game\n",
      "a/60 - events against per 60 minutes, defaults to corsi, but can be set to another stat\n",
      "a1 - first assists, primary assists\n",
      "a2 - second assists, secondary assists\n",
      "blk% - percentage of all opposing shot attempts blocked by this player\n",
      "born - birth date\n",
      "c.close - a player shot attempt (corsi) differential when the game was close\n",
      "c.down - a player shot attempt (corsi) differential when the team was trailing\n",
      "c.tied - a player shot attempt (corsi) differential when the team was tied\n",
      "c.up - a player shot attempt (corsi) differential when the team was in the lead\n",
      "ca - shot attempts allowed (corsi, sat) while this player was on the ice\n",
      "cap hit - the player's cap hit\n",
      "cbar - crossbars hit\n",
      "cf - the team's shot attempts (corsi, sat) while this player was on the ice\n",
      "cf.qoc - a weighted average of the corsi percentage of a player's opponents\n",
      "cf.qot - a weighted average of the corsi percentage of a player's linemates\n",
      "chip - cap hit of injured player is games lost to injury multiplied by cap hit per game\n",
      "city - city of birth\n",
      "cntry - country of birth\n",
      "dap - disciplined aggression proxy, which is hits and takeaways divided by minor penalties\n",
      "dfa - dangerous fenwick against, which is on-ice unblocked shot attempts weighted by shot quality\n",
      "dff - dangerous fenwick for, which is on-ice unblocked shot attempts weighted by shot quality\n",
      "dff.qoc - quality of competition metric based on dangerous fenwick, which is unblocked shot attempts weighted for shot quality\n",
      "dftrd - round in which the player was drafted\n",
      "dftyr - year drafted\n",
      "diff - events for minus event against, defaults to corsi, but can be set to another stat\n",
      "diff/60 - events for minus event against, per 60 minutes, defaults to corsi, but can be set to another stat\n",
      "dps - defensive point shares, a catch-all stats that measures a player's defensive contributions in points in the standings\n",
      "dsa - dangerous shots allowed while this player was on the ice, which is rebounds plus rush shots\n",
      "dsf - the team's dangerous shots while this player was on the ice, which is rebounds plus rush shots\n",
      "dzf - shifts this player has ended with an defensive zone faceoff\n",
      "dzfol - faceoffs lost in the defensive zone\n",
      "dzfow - faceoffs win in the defensive zone\n",
      "dzgapf - team goals allowed after faceoffs taken in the defensive zone\n",
      "dzgfpf - team goals scored after faceoffs taken in the defensive zone\n",
      "dzs - shifts this player has started with an defensive zone faceoff\n",
      "dzsapf - team shot attempts allowed after faceoffs taken in the defensive zone\n",
      "dzsfpf - team shot attempts taken after faceoffs taken in the defensive zone\n",
      "e+/- - a player's expected +/-, based on his team and minutes played\n",
      "eng - empty-net goals\n",
      "exp dzngpf - expected goal differential after faceoffs taken in the defensive zone, based on the number of them\n",
      "exp dznspf - expected shot differential after faceoffs taken in the defensive zone, based on the number of them\n",
      "exp ozngpf - expected goal differential after faceoffs taken in the offensive zone, based on the number of them\n",
      "exp oznspf - expected shot differential after faceoffs taken in the offensive zone, based on the number of them\n",
      "f.close - a player unblocked shot attempt (fenwick) differential when the game was close\n",
      "f.down - a player unblocked shot attempt (fenwick) differential when the team was trailing\n",
      "f.tied - a player unblocked shot attempt (fenwick) differential when the team was tied\n",
      "f.up - a player unblocked shot attempt (fenwick) differential when the team was in the lead. not the best acronym.\n",
      "f/60 - events for per 60 minutes, defaults to corsi, but can be set to another stat\n",
      "fa - unblocked shot attempts allowed (fenwick, usat) while this player was on the ice\n",
      "ff - the team's unblocked shot attempts (fenwick, usat) while this player was on the ice\n",
      "first name -\n",
      "fo% - faceoff winning percentage\n",
      "fo%vsl - faceoff winning percentage against lefthanded opponents\n",
      "fo%vsr - faceoff winning percentage against righthanded opponents\n",
      "fol - the team's faceoff losses while this player was on the ice\n",
      "fol.close - faceoffs lost when the score was close\n",
      "fol.down - faceoffs lost when the team was trailing\n",
      "fol.up - faceoffs lost when the team was in the lead\n",
      "fovsl - faceoffs taken against lefthanded opponents\n",
      "fovsr - faceoffs taken against righthanded opponents\n",
      "fow - the team's faceoff wins while this player was on the ice\n",
      "fow.close - faceoffs won when the score was close\n",
      "fow.down - faceoffs won when the team was trailing\n",
      "fow.up - faceoffs won when the team was in the lead\n",
      "g - goals\n",
      "g.bkhd - goals scored on the backhand\n",
      "g.dflct - goals scored with deflections\n",
      "g.slap - goals scored with slap shots\n",
      "g.snap - goals scored with snap shots\n",
      "g.tip - goals scored with tip shots\n",
      "g.wrap - goals scored with a wraparound\n",
      "g.wrst - goals scored with a wrist shot\n",
      "ga - goals allowed while this player was on the ice\n",
      "game - game misconduct penalties\n",
      "gf - the team's goals while this player was on the ice\n",
      "gp - games played\n",
      "grit - defined as hits, blocked shots, penalty minutes, and majors\n",
      "gs - the player's combined game score\n",
      "gs/g - the player's average game score\n",
      "gva - the team's giveaways while this player was on the ice\n",
      "gwg - game-winning goals\n",
      "gwg - game-winning goals\n",
      "ha - the team's hits taken while this player was on the ice\n",
      "hand - handedness\n",
      "hf - the team's hits thrown while this player was on the ice\n",
      "hopfo - opening faceoffs taken at home\n",
      "hopfow - opening faceoffs won at home\n",
      "ht - height\n",
      "iblk - shots blocked by this individual\n",
      "icf - shot attempts (corsi, sat) taken by this individual\n",
      "ids - dangerous shots taken by this player, the sum of rebounds and shots off the rush\n",
      "iff - unblocked shot attempts (fenwick, usat) taken by this individual\n",
      "ifol - faceoff losses by this individual\n",
      "ifow - faceoff wins by this individual\n",
      "igva - giveaways by this individual\n",
      "iha - hits taken by this individual\n",
      "ihdf - the difference in hits thrown by this individual minus those taken\n",
      "ihf - hits thrown by this individual\n",
      "imiss - individual shots taken that missed the net.\n",
      "injuries - list of types of injuries incurred, if any\n",
      "ipend - penalties drawn by this individual\n",
      "ipendf - the difference in penalties drawn minus those taken\n",
      "ipent - penalties taken by this individual\n",
      "ipp% - individual points percentage, which is on-ice goals for which this player had the goal or an assist\n",
      "irb - rebound shots taken by this individual\n",
      "irs - shots off the rush taken by this individual\n",
      "iscf - all scoring chances taken by this individual\n",
      "isf - shots on goal taken by this individual\n",
      "itka - takeaways by this individual\n",
      "ixg - expected goals (weighted shots) for this individual, which is shot attempts weighted by shot location\n",
      "last name -\n",
      "maj - major penalties taken\n",
      "match - match penalties\n",
      "mgl - games lost due to injury\n",
      "min - minor penalties taken\n",
      "misc - misconduct penalties\n",
      "nat - nationality\n",
      "ngpf - net goals post faceoff. a differential of all goals within 10 seconds of a faceoff, relative to expectations set by the zone in which they took place\n",
      "nhlid - nhl player id useful when looking at the raw data in game files\n",
      "nmc - what kind of no-movement clause this player's contract has, if any\n",
      "npd - net penalty differential is the player's penalty differential relative to a player of the same position with the same ice time per manpower situation\n",
      "nspf - net shots post faceoff. a differential of all shot attempts within 10 seconds of a faceoff, relative to expectations set by the zone in which they took place\n",
      "nzf - shifts this player has ended with a neutral zone faceoff\n",
      "nzfol - faceoffs lost in the neutral zone\n",
      "nzfow - faceoffs won in the neutral zone\n",
      "nzgapf - team goals allowed after faceoffs taken in the neutral zone\n",
      "nzgfpf - team goals scored after faceoffs taken in the neutral zone\n",
      "nzs - shifts this player has started with a neutral zone faceoff\n",
      "nzsapf - team shot attempts allowed after faceoffs taken in the neutral zone\n",
      "nzsfpf - team shot attempts taken after faceoffs taken in the neutral zone\n",
      "oca - shot attempts allowed (corsi, sat) while this player was not on the ice\n",
      "ocf - the team's shot attempts (corsi, sat) while this player was not on the ice\n",
      "odzs - defensive zone faceoffs that occurred without this player on the ice\n",
      "ofa - unblocked shot attempts allowed (fenwick, usat) while this player was not on the ice\n",
      "off - the team's unblocked shot attempts (fenwick, usat) while this player was not on the ice\n",
      "oga - goals allowed while this player was not on the ice\n",
      "ogf - the team's goals while this player was not on the ice\n",
      "onzs - neutral zone faceoffs that occurred without this player on the ice\n",
      "oozs - offensive zone faceoffs that occurred without this player on the ice\n",
      "opfo - opening faceoffs taken\n",
      "opfow - opening faceoffs won\n",
      "oppca60 - a weighted average of the shot attempts (corsi, sat) the team allowed per 60 minutes of a player's opponents\n",
      "oppcf60 - a weighted average of the shot attempts (corsi, sat) the team generated per 60 minutes of a player's opponents\n",
      "oppfa60 - a weighted average of the unblocked shot attempts (fenwick, usat) the team allowed per 60 minutes of a player's opponents\n",
      "oppff60 - a weighted average of the unblocked shot attempts (fenwick, usat) the team generated per 60 minutes of a player's opponents\n",
      "oppga60 - a weighted average of the goals the team allowed per 60 minutes of a player's opponents\n",
      "oppgf60 - a weighted average of the goals the team scored per 60 minutes of a player's opponents\n",
      "oppsa60 - a weighted average of the shots on goal the team allowed per 60 minutes of a player's opponents\n",
      "oppsf60 - a weighted average of the shots on goal the team generated per 60 minutes of a player's opponents\n",
      "ops - offensive point shares, a catch-all stats that measures a player's offensive contributions in points in the standings\n",
      "osa - shots on goal allowed while this player was not on the ice\n",
      "osca - scoring chances allowed while this player was not on the ice\n",
      "oscf - the team's scoring chances while this player was not on the ice\n",
      "osf - the team's shots on goal while this player was not on the ice\n",
      "otf - shifts this player started with an on-the-fly change\n",
      "otg - overtime goals\n",
      "otoi - the amount of time this player was not on the ice.\n",
      "over - shots that went over the net\n",
      "ovrl - where the player was drafted overall\n",
      "oxga - expected goals allowed (weighted shots) while this player was not on the ice, which is shot attempts weighted by location\n",
      "oxgf - the team's expected goals (weighted shots) while this player was not on the ice, which is shot attempts weighted by location\n",
      "ozf - shifts this player has ended with an offensive zone faceoff\n",
      "ozfo - faceoffs taken in the offensive zone\n",
      "ozfol - faceoffs lost in the offensive zone\n",
      "ozfow - faceoffs won in the offensive zone\n",
      "ozgapf - team goals allowed after faceoffs taken in the offensive zone\n",
      "ozgfpf - team goals scored after faceoffs taken in the offensive zone\n",
      "ozs - shifts this player has started with an offensive zone faceoff\n",
      "ozsapf - team shot attempts allowed after faceoffs taken in the offensive zone\n",
      "ozsfpf - team shot attempts taken after faceoffs taken in the offensive zone\n",
      "pace - the average game pace, as estimated by all shot attempts per 60 minutes\n",
      "pass - an estimate of the player's setup passes (passes that result in a shot attempt)\n",
      "pct% - percentage of all events produced by this team, defaults to corsi, but can be set to another stat\n",
      "pdo - the team's shooting and save percentages added together, times a thousand\n",
      "pend - the team's penalties drawn while this player was on the ice\n",
      "pent - the team's penalties taken while this player was on the ice\n",
      "pim - penalties in minutes\n",
      "position - positions played. nhl source listed first, followed by those listed by any other source.\n",
      "post - times hit the post\n",
      "pr/st - province or state of birth\n",
      "ps - point shares, a catch-all stats that measures a player's contributions in points in the standings\n",
      "psa - penalty shot attempts\n",
      "psg - penalty shot goals\n",
      "pts - points. goals plus all assists\n",
      "pts/60 - points per 60 minutes\n",
      "qrelca60 - shot attempts allowed per 60 minutes relative to how others did against the same competition\n",
      "qrelcf60 - shot attempts per 60 minutes relative to how others did against the same competition\n",
      "qreldfa60 - weighted unblocked shot attempts (dangeorus fenwick) allowed per 60 minutes relative to how others did against the same competition\n",
      "qreldff60 - weighted unblocked shot attempts (dangeorus fenwick) per 60 minutes relative to how others did against the same competition\n",
      "rba - rebounds allowed while this player was on the ice. two very different sources.\n",
      "rbf - the team's rebounds while this player was on the ice. two very different sources.\n",
      "rela/60 - the player's a/60 relative to the team when he's not on the ice\n",
      "relc/60 - corsi differential per 60 minutes relative to his team\n",
      "relc% - corsi percentage relative to his team\n",
      "reldf/60 - the player's diff/60 relative to the team when he's not on the ice\n",
      "relf/60 - the player's f/60 relative to the team when he's not on the ice\n",
      "relf/60 - fenwick differential per 60 minutes relative to his team\n",
      "relf% - fenwick percentage relative to his team\n",
      "relpct% - the players pct% relative to the team when he's not on the ice\n",
      "relzs% - the player's zone start percentage when he's on the ice relative to when he's not.\n",
      "ropfo - opening faceoffs taken at home\n",
      "ropfow - opening faceoffs won at home\n",
      "rsa - shots off the rush allowed while this player was on the ice\n",
      "rsf - the team's shots off the rush while this player was on the ice\n",
      "s.bkhd - backhand shots\n",
      "s.dflct - deflections\n",
      "s.slap - slap shots\n",
      "s.snap - snap shots\n",
      "s.tip - tipped shots\n",
      "s.wrap - wraparound shots\n",
      "s.wrst - wrist shots\n",
      "sa - shots on goal allowed while this player was on the ice\n",
      "salary - the player's salary\n",
      "sca - scoring chances allowed while this player was on the ice\n",
      "scf - the team's scoring chances while this player was on the ice\n",
      "sdist - the average shot distance of shots taken by this player\n",
      "sf - the team's shots on goal while this player was on the ice\n",
      "sh% - the team's (not individual's) shooting percentage when the player was on the ice\n",
      "sog - shootout goals\n",
      "sogdg - game-deciding shootout goals\n",
      "sos - shootout shots\n",
      "status - this player's free agency status\n",
      "sv% - the team's save percentage when the player was on the ice\n",
      "team -\n",
      "tka - the team's takeaways while this player was on the ice\n",
      "tmca60 - a weighted average of the shot attempts (corsi, sat) the team allowed per 60 minutes of a player's linemates\n",
      "tmcf60 - a weighted average of the shot attempts (corsi, sat) the team generated per 60 minutes of a player's linemates\n",
      "tmfa60 - a weighted average of the unblocked shot attempts (fenwick, usat) the team allowed per 60 minutes of a player's linemates\n",
      "tmff60 - a weighted average of the unblocked shot attempts (fenwick, usat) the team generated per 60 minutes of a player's linemates\n",
      "tmga60 - a weighted average of the goals the team allowed per 60 minutes of a player's linemates\n",
      "tmgf60 - a weighted average of the goals the team scored per 60 minutes of a player's linemates\n",
      "tmsa60 - a weighted average of the shots on goal the team allowed per 60 minutes of a player's linemates\n",
      "tmsf60 - a weighted average of the shots on goal the team generated per 60 minutes of a player's linemates\n",
      "tmxgf - a weighted average of a player's linemates of the expected goals the team scored\n",
      "tmxga - a weighted average of a player's linemates of the expected goals the team allowed\n",
      "tmga - a weighted average of a player's linemates of the goals the team scored\n",
      "tmgf - a weighted average of a player's linemates of the goals the team allowed\n",
      "toi - time on ice, in minutes, or in seconds (nhl)\n",
      "toi.qoc - a weighted average of the toi% of a player's opponents.\n",
      "toi.qot - a weighted average of the toi% of a player's linemates.\n",
      "toi/gp - time on ice divided by games played\n",
      "toi% - percentage of all available ice time assigned to this player.\n",
      "wide - shots that went wide of the net\n",
      "wt - weight\n",
      "xga - expected goals allowed (weighted shots) while this player was on the ice, which is shot attempts weighted by location\n",
      "xgf - the team's expected goals (weighted shots) while this player was on the ice, which is shot attempts weighted by location\n",
      "xgf.qoc - a weighted average of the expected goal percentage of a player's opponents\n",
      "xgf.qot - a weighted average of the expected goal percentage of a player's linemates\n",
      "zs% - zone start percentage, the percentage of shifts started in the offensive zone, not counting neutral zone or on-the-fly changes\n",
      "context\n",
      "movehub city ranking as published on http://www.movehub.com/city-rankings\n",
      "content\n",
      "movehubqualityoflife.csv\n",
      "cities ranked by\n",
      "movehub rating: a combination of all scores for an overall rating for a city or country.\n",
      "purchase power: this compares the average cost of living with the average local wage.\n",
      "health care: compiled from how citizens feel about their access to healthcare, and its quality.\n",
      "pollution: low is good. a score of how polluted people find a city, includes air, water and noise pollution.\n",
      "quality of life: a balance of healthcare, pollution, purchase power, crime rate to give an overall quality of life score.\n",
      "crime rating: low is good. the lower the score the safer people feel in this city.\n",
      "movehubcostofliving.csv\n",
      "unit: gbp\n",
      "city\n",
      "cappuccino\n",
      "cinema\n",
      "wine\n",
      "gasoline\n",
      "avg rent\n",
      "avg disposable income\n",
      "cities.csv\n",
      "cities to countries as parsed from wikipedia https://en.wikipedia.org/wiki/list_of_towns_and_cities_with_100,000_or_more_inhabitants/cityname:_a (a-z)\n",
      "acknowledgements\n",
      "movehub\n",
      "http://www.movehub.com/city-rankings\n",
      "wikipedia\n",
      "https://en.wikipedia.org/wiki/list_of_towns_and_cities_with_100,000_or_more_inhabitants/cityname:_a\n",
      "this dataset contains 38,269 jokes of the question-answer form, obtained from the r/jokes subreddit. the dataset contains a csv file, where a row contains a question (\"why did the chicken cross the road\"), the corresponding answer (\"to get to the other side\") and a unique id.\n",
      "the data comes from the end of 2016 all the way to 2008. the entries with a higher id correspond to the ones submitted earlier.\n",
      "an example of what one might do with the data is build a sequence-to-sequence model where the input is a question and the output is an answer. then, given a question, the model should generate a funny answer. this is what i did as the final project for my fall 2016 machine learning class. the project page can be viewed here.\n",
      "disclaimer: the dataset contains jokes that some may find inappropriate.\n",
      "license\n",
      "released under reddit's api terms\n",
      "context\n",
      "i am working on building a classifier that will examine today's 'top 40' and determine whether or not they are worthy of appearing on the next 'now that's what i call music' album.\n",
      "content\n",
      "the dataset includes all 61 us released now that's what i call music tracklistings.\n",
      "columns are: volume_number - the album number corresponding with the volume. (ex: a value of 60 would represent the album 'now that's what i call music vol. 60)\n",
      "artist - the name of the artist singing the track\n",
      "title - the song name\n",
      "number - the song's track number on it's album\n",
      "duration - the song's length in seconds\n",
      "acknowledgements\n",
      "thanks to wikipedia contributors for maintaining this data!\n",
      "improvements\n",
      "i am currently working on adding another csv file that contains this same data joined with each song's audio features from the spotify web api.\n",
      "in the future i would also be interested in scraping now releases in other countries as well as the 'special' releases (ex: now that's what i call christmas music, etc).\n",
      "context\n",
      "agricuture production in india from 2001-2014\n",
      "content\n",
      "this dataset describes the agricuture crops cultivation/production in india. this is from https://data.gov.in/ fully licensed\n",
      "acknowledgements\n",
      "this dataset can solves the problems of various crops cultivation/production in india.\n",
      "columns\n",
      "crop:string, crop name variety:string,crop subsidary name state: string,crops cultivation/production place quantity:integer,no of quintals/hectars production:integer,no of years production season:datetime,medium(no of days),long(no of days) unit:string , tons cost:integer, cost of cutivation and production recommended zone:string ,place(state,mandal,village)\n",
      "inspiration\n",
      "across the globe india is the second largest country having people more than 1.3 billion. many people are dependent on the agricuture and it is the main resource. in agricuturce cultivation/production having more problems. i want to solve the big problem in india and usefull to many more people\n",
      "context\n",
      "these are all the flights tracked by the national civil aviation agency, in brazil, from january 2015 to august 2017. i plan on keeping this dataset updated and on gradually adding data from previous years as well.\n",
      "content\n",
      "the dataset is in portuguese so i had to remove some characters that were not supported on kaggle. you can see the translation for the columns in the main dataset description.\n",
      "\"nao identificado\" means \"unidentified\".\n",
      "uf means the state where the airport is located. for every airport not located in brazil, the value is n/i.\n",
      "feel free to ask me if you need translation of any other word.\n",
      "context\n",
      "1k dataset of speckled pharmaceutical pills. using a cnn to extract features and create binary hash code, these pills can be retrieved from a mobile device for remote identification. every pill can be tracked using a mobile phone app.\n",
      "content\n",
      "1 k pharmaceutical pills jpeg images that have been convoluted by: rotations, grey scale, noise, non-pill\n",
      "acknowledgements\n",
      "special thanks for funding and support of microsoft - paul debaun and nwcadence- steve borg\n",
      "inspiration\n",
      "the pill crisis in america 1) fake fentanyl - killing young people  2) opioid abuse - killing all ages of people  3) fake online drugs - killing unknown numbers 4) non-compliance - killing older people\n",
      "non-compliance  up to 90% of diabetics don't take their meds enough to benefit \n",
      "up to 75% of hypertensive patients do not adhere to their medicine \n",
      "less than 27% depressed patients adhere to their medication\n",
      "41-59% of mentally ill take their meds infrequently or not at all\n",
      "33% of patients with schizophrenia don’t take their medicine at all\n",
      "context\n",
      "(u) my purpose is to analyze amazon web services (aws) honeypot data for any trends and/or correlations that could possibly be used in predictive cyber threat vectors. i spent a lot of time looking for data sets and most of the ones i found had no documentation and the data was hard to interpret just from the file. this data is well formatted and straight forward.\n",
      "content\n",
      "(u) the aws honeypot database is an open-source database including information on cyber attacks/attempts.\n",
      "(u) data has 451,581 data points collected from 9:53pm on 3 march 2013 to 5:55am on 8 september 2013.\n",
      "acknowledgements\n",
      "http://datadrivensecurity.info/blog/pages/dds-dataset-collection.html jay jacobs & bob rudis\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "this data set is focused on wlan fingerprint positioning technologies and methodologies (also know as wifi fingerprinting). it was the official database used in the ipin2015 competition.\n",
      "many real world applications need to know the localization of a user in the world to provide their services. therefore, automatic user localization has been a hot research topic in the last years. automatic user localization consists of estimating the position of the user (latitude, longitude and altitude) by using an electronic device, usually a mobile phone. outdoor localization problem can be solved very accurately thanks to the inclusion of gps sensors into the mobile devices. however, indoor localization is still an open problem mainly due to the loss of gps signal in indoor environments. although, there are some indoor positioning technologies and methodologies, this database is focused on wlan fingerprint-based ones (also know as wifi fingerprinting).\n",
      "although there are many papers in the literature trying to solve the indoor localization problem using a wlan fingerprint-based method, there still exists one important drawback in this field which is the lack of a common database for comparison purposes. so, ujiindoorloc database is presented to overcome this gap.\n",
      "the ujiindoorloc database covers three buildings of universitat jaume i (http://www.uji.es) with 4 or more floors and almost 110.000m2. it can be used for classification, e.g. actual building and floor identification, or regression, e.g. actual longitude and latitude estimation. it was created in 2013 by means of more than 20 different users and 25 android devices. the database consists of 19937 training/reference records (trainingdata.csv file) and 1111 validation/test records (validationdata.csv file)\n",
      "the 529 attributes contain the wifi fingerprint, the coordinates where it was taken, and other useful information.\n",
      "each wifi fingerprint can be characterized by the detected wireless access points (waps) and the corresponding received signal strength intensity (rssi). the intensity values are represented as negative integer values ranging -104dbm (extremely poor signal) to 0dbm. the positive value 100 is used to denote when a wap was not detected. during the database creation, 520 different waps were detected. thus, the wifi fingerprint is composed by 520 intensity values.\n",
      "then the coordinates (latitude, longitude, floor) and building id are provided as the attributes to be predicted.\n",
      "the particular space (offices, labs, etc.) and the relative position (inside/outside the space) where the capture was taken have been recorded. outside means that the capture was taken in front of the door of the space.\n",
      "information about who (user), how (android device & version) and when (timestamp) wifi capture was taken is also recorded.\n",
      "content\n",
      "attributes 001 to 520 (wap001-wap520): intensity value for wap001. negative integer values from -104 to 0 and +100. positive value 100 used if wap001 was not detected.\n",
      "attribute 521 (longitude): longitude. negative real values from -7695.9387549299299000 to -7299.786516730871000\n",
      "attribute 522 (latitude): latitude. positive real values from 4864745.7450159714 to 4865017.3646842018.\n",
      "attribute 523 (floor): altitude in floors inside the building. integer values from 0 to 4.\n",
      "attribute 524 (buildingid): id to identify the building. measures were taken in three different buildings. categorical integer values from 0 to 2.\n",
      "attribute 525 (spaceid): internal id number to identify the space (office, corridor, classroom) where the capture was taken. categorical integer values.\n",
      "attribute 526 (relativeposition): relative position with respect to the space (1 - inside, 2 - outside in front of the door). categorical integer values.\n",
      "attribute 527 (userid): user identifier (see below). categorical integer values.\n",
      "attribute 528 (phoneid): android device identifier (see below). categorical integer values.\n",
      "attribute 529 (timestamp): unix time when the capture was taken. integer value.\n",
      "relevent paper\n",
      "more information can be found in this paper:\n",
      "joaquín torres-sospedra, raúl montoliu, adolfo martínez-usó, tomar j. arnau, joan p. avariento, mauri benedito-bordonau, joaquín huerta. ujiindoorloc: a new multi-building and multi-floor database for wlan fingerprint-based indoor localization problems. in proceedings of the fifth international conference on indoor positioning and indoor navigation, 2014.\n",
      "available at: http://www.ipin2014.org/wp/pdf/4a-3.pdf\n",
      "if your are going to use this dataset in your research, please cite this paper\n",
      "acknowledgements\n",
      "the dataset was created by:\n",
      "joaquín torres-sospedra, raul montoliu, adolfo martínez-usó, tomar j. arnau, joan p. avariento, mauri benedito-bordonau, joaquín huerta, yasmina andreu, óscar belmonte, vicent castelló, irene garcia-martí, diego gargallo, carlos gonzalez, nadal francisco, josep lópez, ruben martínez, roberto mediero, javier ortells, nacho piqueras, ianisse quizán, david rambla, luis e. rodríguez, eva salvador balaguer, ana sanchís, carlos serra, and sergi trilles.\n",
      "inspiration\n",
      "the objective is to estimate the building, floor and coordinates (latitude and longitude) of the 1111 samples included in the validation set. since the real values of the building, floor and coordinates are also included, it is posible to determine the localization error.\n",
      "the formula used in the ipin2015 competition was the mean of the localization error of each sample. the localization error of each sample can be estimated as follows:\n",
      "error = building_penality * building_error + floor_penality * floor_error + coordinates_error\n",
      "where:\n",
      "building_error is 1 if the estimated building is not equal to the real one. 0 otherwise\n",
      "floor_error is 1 if the estimated floor is not equal to the real one. 0 otherwise\n",
      "coordinates_error is sqrt( (estimated_latitude - real_latitude)^2 + (estimated_longitude-real_longitude)^2)\n",
      "in the ipin2015 competition building_penalty and floor_penalty where set to 50 and 4 meters, respectively.\n",
      "content\n",
      "executive orders are official documents, numbered consecutively, through which the president of the united states manages the operations of the federal government. the text of executive orders appears in the daily federal register as each executive order is signed by the president and received by the office of the federal register. the total number of executive orders issued by each administration includes number-and-letter designated orders, such as 9577-a, 9616-a, etc.\n",
      "acknowledgements\n",
      "the data was compiled and published by the national archives as executive order disposition tables, available for franklin d. roosevelt and later presidents.\n",
      "context\n",
      "we should definitely stop hate crimes. let's use data science to stop them. this is mainly classification but any other approach is welcome. example; prediction for next possible hate crime.\n",
      "content\n",
      "3700 rows of csv from google trend. headline, date, location, url.\n",
      "acknowledgements\n",
      "special thanks to ; http://googletrends.github.io/data/\n",
      "inspiration\n",
      "media data is mainly nlp csv. we have to come up with other ways to add the value from it.\n",
      "context:\n",
      "restaurant inspections for permitted food establishments in nyc. restaurants are graded on a-f scale with regular visits by city health department.\n",
      "content:\n",
      "dataset includes address, cuisine description, inspection date, type, action, violation code and description(s). data covers all of nyc and starts jan 1, 2010-aug 29, 2017.\n",
      "acknowledgements:\n",
      "data was collected by the nyc department of health and is available here.\n",
      "inspiration:\n",
      "can you predict restaurant closings?\n",
      "are certain violations more prominent in certain neighborhoods? by cuisine?\n",
      "who gets worse grades--chain restaurants or independent establishments?\n",
      "context\n",
      "in the past, the ancient city of krakow was known as the capital of poland. in 2000, it became known as the official european capital of culture. now, it is known for having some of the most polluted air in europe... in a world health organiation (who) study krakow has been rated amongst the most polluted in the world. in the report, city was ranked 8th among 575 cities for levels of pm2.5 and 145th among 1100 cities for levels of pm10. hazardous air quality is a common problem particularly during the colder months when many residents use solid fuels (mostly coal) for household heating. air pollution in krakow poses a significant danger to human health and life. krakow's poisoned air includes amongst other things: particulate matter, benzo(a)pyrene and nitrogen dioxide. the state-run network of monitoring stations consists of 8 monitoring stations in krakow. we decided to go step further - to build network of low-cost air quality sensors that can be deployed across entire city. the first step in the fight against smog is to identify areas of problem and to raise awareness among residents and the authorities. it is very important to create a network of sensors – only then you can check the actual conditions in various areas of the city. the technology enables real-time monitoring of air quality via map.airly.eu, so the information about the air in a specific location is easily accessible and always up to date.\n",
      "content\n",
      "this dataset consists air quality data (the concentrations of particulate matter pm1, pm2.5 and pm10, temperature, air pressure and humidity) from 2017 generated by network of 56 low-cost sensors located in krakow, poland. each had its own location (6 of them where replaced during this time period and have almost the same latitude and longitude). measurements are grouped in 12 files, one for each month. resolution of data is 1 hour.\n",
      "known issues: - pm1 is not calibrated and therefore can be bigger than pm2.5 - pm2.5 can be bigger than pm10 within the limits of measurement error - for the first two months humidity and temperature were not calibrated and therefore can show inaccurate values\n",
      "acknowledgements\n",
      "the data was generated by airly network - the project is still in its beginning stage, but over 1000 sensors have already been implemented in poland. airly is a startup definitely worth watching, especially for citizens of the most polluted cities. after all, it’s clean air we all want to breathe.\n",
      "inspiration\n",
      "i think that this dataset offers some great opportunities for predictive models and data visualization. airly's goal is to develop an effective forecast and monitoring of air quality, employ artificial intelligence and utilise data from extensive sensor network. if anyone has any ideas, breakthroughs or other interesting models please post them.\n",
      "some questions worth exploring: - what are the best prediction models based on extensive sensor network - statistical or numerical forecast? - how weather affects air quality? - how much pollution comes from cars, factories and coal-fired power plants?\n",
      "context\n",
      "experimental data used to create regression models of appliances energy use in a low energy building.\n",
      "content\n",
      "the data set is at 10 min for about 4.5 months. the house temperature and humidity conditions were monitored with a zigbee wireless sensor network. each wireless node transmitted the temperature and humidity conditions around 3.3 min. then, the wireless data was averaged for 10 minutes periods. the energy data was logged every 10 minutes with m-bus energy meters. weather from the nearest airport weather station (chievres airport, belgium) was downloaded from a public data set from reliable prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters).\n",
      "acknowledgements\n",
      "luis candanedo, luismiguel.candanedoibarra '@' umons.ac.be, university of mons (umons)\n",
      "luis m. candanedo, veronique feldheim, dominique deramaix, data driven prediction models of energy use of appliances in a low-energy house, energy and buildings, volume 140, 1 april 2017, pages 81-97, issn 0378-7788, [web link].\n",
      "inspiration\n",
      "data used include measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station and recorded energy use of lighting fixtures. data filtering to remove non-predictive parameters and feature ranking plays an important role with this data. different statistical models could be developed over this dataset. highlights: the appliances energy consumption prediction in a low energy house is the dataset content weather data from a nearby station was found to improve the prediction.\n",
      "pressure, air temperature and wind speed are important parameters in the prediction.\n",
      "data from a wsn that measures temperature and humidity increase the pred. accuracy.\n",
      "from the wsn, the kitchen, laundry and living room data ranked high in importance.\n",
      "data set information:\n",
      "the main goal of this data set is providing clean and valid signals for designing cuff-less blood pressure estimation algorithms. the raw electrocardiogram (ecg), photoplethysmograph (ppg), and arterial blood pressure (abp) signals are originally collected from the physionet.org and then some preprocessing and validation performed on them. (for more information about the process please refer to our paper)\n",
      "attribute information:\n",
      "this database consists of a cell array of matrices, each cell is one record part. in each matrix each row corresponds to one signal channel:\n",
      "1: ppg signal, fs=125hz; photoplethysmograph from fingertip\n",
      "2: abp signal, fs=125hz; invasive arterial blood pressure (mmhg)\n",
      "3: ecg signal, fs=125hz; electrocardiogram from channel ii\n",
      "relevant papers:\n",
      "m. kachuee, m. m. kiani, h. mohammadzade, m. shabany, cuff-less high-accuracy calibration-free blood pressure estimation using pulse transit time, ieee international symposium on circuits and systems (iscas'15), 2015.\n",
      "a. goldberger, l. amaral, l. glass, j. hausdorff, p. ivanov, r. mark, j.mietus, g. moody, c. peng and h. stanley, â€œphysiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals,â€ circulation, vol. 101, no. 23, pp. 215â€“220, 2000.\n",
      "citation request:\n",
      "if you found this data set useful please cite the following:\n",
      "m. kachuee, m. m. kiani, h. mohammadzade, m. shabany, cuff-less high-accuracy calibration-free blood pressure estimation using pulse transit time, ieee international symposium on circuits and systems (iscas'15), 2015.\n",
      "m. kachuee, m. m. kiani, h. mohammadzadeh, m. shabany, cuff-less blood pressure estimation algorithms for continuous health-care monitoring, ieee transactions on biomedical engineering, 2016.\n",
      "content\n",
      "this dataset contains the username of any reddit account that has left at least one comment, and their number of comments.\n",
      "this data was grabbed in december 2017 from the reddit comments dataset hosted on google bigquery. it should be current up to november 2017.\n",
      "quick stats\n",
      "26 million users\n",
      "8 million have left only a single comment\n",
      "13 million (50%) have left no more than 5 comments\n",
      "42,000 usernames demand something via pm (e.g. pm_me_pix_of_ur_cat, pm_me_your_successes, pmmeyourrgb, and lots of less wholesome ones)\n",
      "acknowledgements\n",
      "thanks to /u/stuck_in_the_matrix, who collected and maintains the original comments dataset.\n",
      "inspiration\n",
      "what words commonly appear in reddit usernames?\n",
      "can you identify frequently occurring username 'recipes' using clustering techniques?\n",
      "what numbers most commonly appear as suffixes in reddit usernames?\n",
      "can you train a generative language model to output new usernames based on this dataset?\n",
      "arabic handwritten digits dataset\n",
      "abstract\n",
      "in recent years, handwritten digits recognition has been an important area due to its applications in several fields. this work is focusing on the recognition part of handwritten arabic digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases. the paper provided a deep learning technique that can be effectively apply to recognizing arabic handwritten digits. lenet-5, a convolutional neural network (cnn) trained and tested madbase database (arabic handwritten digits images) that contain 60000 training and 10000 testing images. a comparison is held amongst the results, and it is shown by the end that the use of cnn was leaded to significant improvements across different machine-learning classification algorithms.\n",
      "the convolutional neural network was trained and tested madbase database (arabic handwritten digits images) that contain 60000 training and 10000 testing images. moreover, the cnn is giving an average recognition accuracy of 99.15%.\n",
      "context\n",
      "the motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of arabic handwritten digits recognition. in recent years, arabic handwritten digits recognition with different handwriting styles as well, making it important to find and work on a new and advanced solution for handwriting recognition. a deep learning systems needs a huge number of data (images) to be able to make a good decisions.\n",
      "content\n",
      "the madbase is modified arabic handwritten digits database contains 60,000 training images, and 10,000 test images. madbase were written by 700 writers. each writer wrote each digit (from 0 -9) ten times. to ensure including different writing styles, the database was gathered from different institutions: colleges of engineering and law, school of medicine, the open university (whose students span a wide range of ages), a high school, and a governmental institution. madbase is available for free and can be downloaded from (http://datacenter.aucegypt.edu/shazeem/) .\n",
      "acknowledgements\n",
      "cnn for handwritten arabic digits recognition based on lenet-5 http://link.springer.com/chapter/10.1007/978-3-319-48308-5_54 ahmed el-sawy, hazem el-bakry, mohamed loey proceedings of the international conference on advanced intelligent systems and informatics 2016 volume 533 of the series advances in intelligent systems and computing pp 566-575\n",
      "inspiration\n",
      "creating the proposed database presents more challenges because it deals with many issues such as style of writing, thickness, dots number and position. some characters have different shapes while written in the same position. for example the teh character has different shapes in isolated position.\n",
      "arabic handwritten characters dataset\n",
      "https://www.kaggle.com/mloey1/ahcd1\n",
      "benha university\n",
      "http://bu.edu.eg/staff/mloey\n",
      "https://mloey.github.io/\n",
      "free code camp is an open source community where you learn to code and build projects for nonprofits.\n",
      "we surveyed more than 20,000 people who started coding within the past 5 years. we reached them through the twitter accounts and email lists of various organizations that help people learn to code.\n",
      "our goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background.\n",
      "we've written in depth about this dataset here: https://medium.freecodecamp.com/we-asked-20-000-people-who-they-are-and-how-theyre-learning-to-code-fff5d668969\n",
      "context\n",
      "we all know that there is relationship between tides and moon. what if we try to find the match between the position of objects in our solar system and earthquakes? so i prepared the dataset to answer the question.\n",
      "content\n",
      "the dataset has information about earthquakes (magnitude 6.1+) occur between yyyy.mm.dd: 1986.05.04 to 2016.05.04 and position (and other params) of solar system planets + sun and moon at the time when the specific earthquake occured each row has: 1) info about the specific earthquake: date and time, where it occur: latitude and longitude, magnitude, place (it is useless field since we have latitude and longitude but i left the filed just to have humanreading meaning e.g. 7km sw of ueki, japan) 2) planets, moon and sun information (position and etc) relatively latitude and longitude of place and time of the earthquake.\n",
      "acknowledgements\n",
      "tha dataset has two sources of information. both ones are free and available for public. 1) earthquakes info: the usgs earthquake hazards program https://earthquake.usgs.gov/ 2) solar system objects info: nginov astropositions: http://api.nginov note: nginov uses a bit specific calculation of azimuth (as far as i know more often the one calculates from geographicall north). the note from nginov: \"an astronomical azimuth is expressed in degree or schedules arcs. taking as a reference point, geographically south of the place of observation and a dextrorotatory angular progression.\" note: i am not quite strong in astronomic stuff. i expressed the idea about possible relationship and wrote a simple app to match and merge the info from two data sources into the dataset which possibly help to answer the question.\n",
      "inspiration\n",
      "may be there is a relationship between the position and other params of objects in our solar system and earthquakes? hope someone will find the match! :) good luck!!\n",
      "data dictionary:\n",
      "title: credit data\n",
      "source: credit one bank\n",
      "number of instances: 5000\n",
      "name of dataset: analysis_of_default\n",
      "number of attributes: 20 (7 numerical, 13 categorical)\n",
      "attribute description\n",
      "attribute 1: (qualitative / categorical) status of existing checking account a11: ... < 0 usd a12: 0 <= ... < 10000 usd a13: ... >= 10000 usd a14: no checking account\n",
      "attribute 2: (numerical) duration in month\n",
      "attribute 3: (qualitative / categorical) credit history a30: no credits taken/all credits paid back duly a31: all credits at this bank paid back duly a32: existing credits paid back duly till now a33: delay in paying off in the past a34:critical account/other credits existing(not at this bank)\n",
      "attribute 4: (qualitative / categorical) purpose a40: car (new) a41: car (used) a42: furniture/equipment a43: radio/television a44: domestic appliances a45: repairs a46: education a47: (vacation - does not exist?) a48: retraining a49: business a410: others\n",
      "attribute 5: (numerical) credit amount\n",
      "attribute 6: (qualitative / categorical) savings account/bonds a61: ... < 1000 usd a62: 1000 <= ... < 5000 usd a63: 5000 <= ... < 10000 usd a64: .. >= 10000 usd a65: unknown/ no savings account\n",
      "attribute 7: (qualitative / categorical) present employment since a71: unemployed a72: ... < 1 year a73: 1 <= ... < 4 years\n",
      "a74: 4 <= ... < 7 years a75: .. >= 7 years\n",
      "attribute 8: (numerical) installment rate in percentage of disposable income\n",
      "attribute 9: (qualitative / categorical) personal status and sex a91: male : divorced/separated a92: female: divorced/separated/married a93: male : single a94: male : married/widowed a95: female: single\n",
      "attribute 10: (qualitative / categorical) other debtors / guarantors a101: none a102: co-applicant a103: guarantor\n",
      "attribute 11: (numerical) present residence since\n",
      "attribute 12: (qualitative / categorical) property a121: real estate a122: if not a121: building society savings agreement/ life insurance a123: if not a121/a122: car or other, not in attribute 6 a124: unknown / no property\n",
      "attribute 13: (numerical) age in years\n",
      "attribute 14: (qualitative / categorical) other installment plans a141: bank a142: stores a143: none\n",
      "attribute 15: (qualitative / categorical) housing a151: rent a152: own a153: for free\n",
      "attribute 16: (numerical) number of existing credits at this bank\n",
      "attribute 17: (qualitative / categorical) job a171: unemployed/ unskilled - non-resident a172: unskilled - resident a173: skilled employee / official a174: management/ self-employed/ highly qualified employee/ officer\n",
      "attribute 18: (numerical) number of people being liable to provide maintenance for\n",
      "attribute 19: (qualitative / categorical) telephone a191: none a192: yes, registered under the customer’s name\n",
      "attribute 20: (qualitative / categorical) foreign worker a201: yes a202: no\n",
      "default on payment due\n",
      "1 (defaulted) 0 (no default)\n",
      "why?\n",
      "last year a redditor created a survey to collect demographic data on the subreddit /r/foreveralone. since then they have deleted their account but they left behind their data set.\n",
      "columns are below:\n",
      "content\n",
      "timestamp\n",
      "datetime\n",
      "what is your gender?\n",
      "string\n",
      "what is your sexual orientation?\n",
      "string\n",
      "how old are you?\n",
      "datetime\n",
      "what is your level of income?\n",
      "datetime\n",
      "what is your race?\n",
      "string\n",
      "how would you describe your body/weight?\n",
      "string\n",
      "are you a virgin?\n",
      "string\n",
      "is prostitution legal where you live?\n",
      "string\n",
      "would you pay for sex?\n",
      "string\n",
      "how many friends do you have irl?\n",
      "datetime\n",
      "do you have social anxiety/phobia?\n",
      "string\n",
      "are you depressed?\n",
      "string\n",
      "what kind of help do you want from others? (choose all that apply)\n",
      "string\n",
      "have you attempted suicide?\n",
      "string\n",
      "employment status: are you currently…?\n",
      "string\n",
      "what is your job title?\n",
      "string\n",
      "what is your level of education?\n",
      "string\n",
      "what have you done to try and improve yourself? (check all that apply)\n",
      "'champsdata.csv' and runnerupsdata.csv'\n",
      "'champs.csv' contains game-by-game team totals for the championship team from every finals game between 1980 and 2017. 'runnerups.csv' contains game-by-game team totals for the runner-up team from every finals game between 1980 and 2017. the 1980 nba finals was the first finals series since the nba added the three point line.\n",
      "content\n",
      "the data was scrapped from basketball-reference.com.\n",
      "variables in 'champs.csv' and 'runnerups.csv'\n",
      "year: the year the series was played\n",
      "team: the name of the team.\n",
      "win: 1 = win. 0 = loss\n",
      "home: 1 = home team. 0 = away team.\n",
      "game: game #\n",
      "mp - total minutes played. equals 240 (48x5=240) if game did not go to overtime. mp>240 if game went to overtime.\n",
      "fg - field goals made\n",
      "fga - field goal attempts\n",
      "fgp - field goal percentage\n",
      "tp - 3 point field goals made\n",
      "tpa - three point attempts\n",
      "tpp - three point percentage\n",
      "ft - free throws made\n",
      "fta - free throws attempted\n",
      "ftp - free throw percentage\n",
      "orb - offensive rebounds\n",
      "drb - defensive rebounds\n",
      "trb - total rebounds\n",
      "ast - assists\n",
      "stl - steals\n",
      "blk - blocks\n",
      "tov - turnovers\n",
      "pf - personal fouls\n",
      "pts - points scored\n",
      "datasets created from 'champsionsdata.csv' and 'runnerupsdata.csv'\n",
      "the r code that i used to make the three files listed below can be found here\n",
      "'champs_and_runner_ups_series_averages.csv'\n",
      "this data frame contains series averages for the champion and runnerup each year.\n",
      "'champs_series_averages.csv'\n",
      "this data frame contains series averages for just the champion each year.\n",
      "'runner_ups_series_averages.csv'\n",
      "this data frame contains decade-by-decade averages for champions and runners up.\n",
      "context:\n",
      "short message service (sms) messages are short messages sent from one person to another from their mobile phones. they represent a means of personal communication that is an important communicative artifact in our current digital era. this dataset contains sms messages that were collected from users who knew they were participating in a research project and that their messages would be shared publicly. this dataset contains two sms messages in two languages: singapore english and mandarin chinese.\n",
      "content:\n",
      "this is a corpus of sms (short message service) messages collected for research at the department of computer science at the national university of singapore. this dataset consists of 67,093 sms messages taken from the corpus on mar 9, 2015. the messages largely originate from singaporeans and mostly from students attending the university. these messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. the data collectors opportunistically collected as much metadata about the messages and their senders as possible, so as to enable different types of analyses.\n",
      "acknowledgements:\n",
      "this corpus was collected by tao chen and min-yen kan. if you use this data, please cite the following paper:\n",
      "tao chen and min-yen kan (2013). creating a live, public short message service corpus: the nus sms corpus. language resources and evaluation, 47(2)(2013), pages 299-355. url: https://link.springer.com/article/10.1007%2fs10579-012-9197-9\n",
      "inspiration:\n",
      "this dataset contains a lot of short, informal texts and is ideal for trying your hand at various natural language processing tasks. there’s also a lot of information about the messages which might reveal interesting insights. here are some ideas to get you started:\n",
      "this dataset contains singapore english. how well do tools trained on other varieties of english, like stemmers or part of speech taggers, work on it?\n",
      "what time of day are most sms messages sent? is this different for the english and mandarin datasets?\n",
      "unlike english, mandarin does not have spaces between words, which can be made up of several characters. can you build or implement a system for word identification?\n",
      "data sources\n",
      "this dataset compiles data from the following massachusetts department of education reports:\n",
      "enrollment by grade\n",
      "enrollment by selected population\n",
      "enrollment by race/gender\n",
      "class size by gender and selected populations\n",
      "teacher salaries\n",
      "per pupil expenditure\n",
      "graduation rates\n",
      "graduates attending higher ed\n",
      "advanced placement participation\n",
      "advanced placement performance\n",
      "sat performance\n",
      "mcas achievement results\n",
      "accountability report\n",
      "in each case, the data is the latest available data as of august 2017.\n",
      "data dictionary\n",
      "the data dictionary lists the report from which each field is sourced. it also includes the original field names - minor changes have been made to make the field names easier to understand. data definitions can be found on the about the data section of the ma doe website.\n",
      "questions\n",
      "what contributes to differences in schools outcomes?\n",
      "are there meaningful regional differences within ma?\n",
      "which schools do well despite limited resources?\n",
      "emotion expression is an essential part of human interaction. the same text can hold different meanings when expressed with different emotions. thus understanding the text alone is not enough for getting the meaning of an utterance. acted and natural corpora have been used to detect emotions from speech. many speech databases for different languages including english, german, chinese, japanese, russian, italian, swedish and spanish exist for modeling emotion recognition. since there is no reported reference of an available arabic corpus, we decided to collect the first arabic natural audio dataset (anad) to recognize discrete emotions.\n",
      "embedding an effective emotion detection feature in speech recognition system seems a promising solution for decreasing the obstacles faced by the deaf when communicating with the outside world. there exist several applications that allow the deaf to make and receive phone calls normally, as the hearing-impaired individual can type a message and the person on the other side hears the words spoken, and as they speak, the words are received as text by the deaf individual. however, missing the emotion part still makes these systems not hundred percent reliable. having an effective speech to text and text to speech system installed in their everyday life starting from a very young age will hopefully replace the human ear. such systems will aid deaf people to enroll in normal schools at very young age and will help them to adapt better in classrooms and with their classmates. it will help them experience a normal childhood and hence grow up to be able to integrate within the society without external help.\n",
      "eight videos of live calls between an anchor and a human outside the studio were downloaded from online arabic talk shows. each video was then divided into turns: callers and receivers. to label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. silence, laughs and noisy chunks were removed. every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records.\n",
      "twenty five acoustic features, also known as low-level descriptors, were extracted. these features are: intensity, zero crossing rates, mfcc 1-12 (mel-frequency cepstral coefficients), f0 (fundamental frequency) and f0 envelope, probability of voicing and, lsp frequency 0-7. on every feature nineteen statistical functions were applied. the functions are: maximum, minimum, range, absolute position of maximum, absolute position of minimum, arithmetic of mean, linear regression1, linear regression2, linear regressiona, linear regressionq, standard deviation, kurtosis, skewness, quartiles 1, 2, 3 and, inter-quartile ranges 1-2, 2-3, 1-3. the delta coefficient for every lld is also computed as an estimate of the first derivative hence leading to a total of 950 features.\n",
      "i would have never reached that far without the help of my supervisors. i warmly thank and appreciate dr. rached zantout, dr. lama hamandi, and dr. ziad osman for their guidance, support and constant supervision.\n",
      "context\n",
      "in daily fantasy sports (dfs) contests, contestants construct a virtual lineup of players that score points based on their real-world performances. unlike in season-long fantasy sports contests,in dfs contestants submit a new lineup for each set of games. dfs contests are held for several professional sports leagues, including the national football league (nfl), national basketball league (nba), and national hockey league (nhl). the leading dfs sites today are draftkings and fanduel, which control approximately 90% of the $3b dfs market.\n",
      "there are three primary types of dfs games: head-to-heads (h2hs), double-ups, and guaranteed prize pools (gpps). in h2h games, two contestants play for a single cash prize. in double-up games, a pool of contestants compete to place in the top 50% of lineups, which are awarded twice the entry fee. in gpps, a pool of contestants compete for a fixed prize structure that tends to be very top heavy; some contests payout hundreds of thousands of dollars to the top finisher.\n",
      "over the last year, i have developed a winning system for daily fantasy football and baseball contests. building this system from scratch was a fantastic compliment to the things i learned as a student, from machine learning and optimization to optimal learning and game theory. i hope others can join me in researching daily fantasy basketball and perhaps get involved with the burgeoning world of daily fantasy sports.\n",
      "content\n",
      "this dataset contains 20 days of draftkings nba contest data scraped between 2017-11-27 and 2017-12-28. for draftkings nba daily fantasy basketball contest rules, see https://www.draftkings.com/help/rules/nba.\n",
      "format:\n",
      "one folder per day\n",
      "one folder per contest for a given day\n",
      "salary file (“dksalaries.csv”), payout structure file (“payout_structure.csv”), and contest results file (“contest-standings.csv”) for a given contest. column headers in each files are pretty self-explanatory.\n",
      "some additional files (e.g. “players.csv”, “covariance_mat_unfiltered.csv”, “hist_fpts_mat.csv”) for a given contest. these files were for my personal research, feel free to use or ignore.\n",
      "“projections” folder contains projections data for each player from rotogrinders and daily fantasy nerd, labeled by date.\n",
      "“contests.csv” contains information about each contest, e.g. entry fee, slate, and contest size.\n",
      "acknowledgements\n",
      "thank you to my friend from college, michael chiang, for contributions to this project.\n",
      "inspiration\n",
      "a few ideas to get started:\n",
      "what kind of position \"stacks\" tend to maximize correlation within a lineup?\n",
      "how can you minimize correlation between lineups, such that you maximize your chances of winning a gpp?\n",
      "what are the tendencies of some of the top dfs pros?\n",
      "can you improve rotogrinders and daily fantasy nerd player projections?\n",
      "can you predict which players are undervalued (i.e. high fantasy points / salary ratio)?\n",
      "can you predict the ownership percentage for each player in a given contest?\n",
      "context\n",
      "the million song dataset (msd) is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. this is a subset of the msd and contains audio features of songs with the year of the song. the purpose being to predict the release year of a song from audio features.\n",
      "content\n",
      "the owners recommend that you split the data like this to avoid the 'producer effect' by making sure no song from a given artist ends up in both the train and test set.\n",
      "train: first 463,715 examples\n",
      "test: last 51,630 examples\n",
      "field descriptions:\n",
      "the first value is the year (target), ranging from 1922 to 2011.\n",
      "then there are 90 attributes\n",
      "timbreaverage[1-12]\n",
      "timbrecovariance[1-78]\n",
      "these features were extracted from the 'timbre' features from the echo nest api. the authors took the average and covariance over all 'segments' and each segment was described by a 12-dimensional timbre vector.\n",
      "acknowledgements\n",
      "original dataset: thierry bertin-mahieux, daniel p.w. ellis, brian whitman, and paul lamere. the million song dataset. in proceedings of the 12th international society for music information retrieval conference (ismir 2011), 2\n",
      "subset downloaded from: https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
      "inspiration\n",
      "use this dataset to predict the years that each song was released based on it's audio features\n",
      "context:\n",
      "carbon monoxide (co) is a colorless, odorless gas that can be harmful when inhaled in large amounts. co is released when something is burned. the greatest sources of co to outdoor air are cars, trucks and other vehicles or machinery that burn fossil fuels. a variety of items in your home such as unvented kerosene and gas space heaters, leaking chimneys and furnaces, and gas stoves also release co and can affect air quality indoors.\n",
      "content:\n",
      "the daily summary file contains data for every monitor (sampled parameter) in the environmental protection agency (epa) database for each day. this file will contain a daily summary record that is:\n",
      "the aggregate of all sub-daily measurements taken at the monitor.\n",
      "the single sample value if the monitor takes a single, daily sample (e.g., there is only one sample with a 24-hour duration). in this case, the mean and max daily sample will have the same value.\n",
      "within the data file you will find these fields: 1. state code: the federal information processing standards (fips) code of the state in which the monitor resides.\n",
      "county code: the fips code of the county in which the monitor resides.\n",
      "site num: a unique number within the county identifying the site.\n",
      "parameter code: the aqs code corresponding to the parameter measured by the monitor.\n",
      "poc: this is the “parameter occurrence code” used to distinguish different instruments that measure the same parameter at the same site.\n",
      "latitude: the monitoring site’s angular distance north of the equator measured in decimal degrees.\n",
      "longitude: the monitoring site’s angular distance east of the prime meridian measured in decimal degrees.\n",
      "datum: the datum associated with the latitude and longitude measures.\n",
      "parameter name: the name or description assigned in aqs to the parameter measured by the monitor. parameters may be pollutants or non-pollutants.\n",
      "sample duration: the length of time that air passes through the monitoring device before it is analyzed (measured). so, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). for continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).\n",
      "pollutant standard: a description of the ambient air quality standard rules used to aggregate statistics. (see description at beginning of document.)\n",
      "date local: the calendar date for the summary. all daily summaries are for the local standard day (midnight to midnight) at the monitor.\n",
      "units of measure: the unit of measure for the parameter. qad always returns data in the standard units for the parameter. submitters are allowed to report data in any unit and epa converts to a standard unit so that we may use the data in calculations.\n",
      "event type: indicates whether data measured during exceptional events are included in the summary. a wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. no events means no events occurred. events included means events occurred and the data from them is included in the summary. events excluded means that events occurred but data form them is excluded from the summary. concurred events excluded means that events occurred but only epa concurred exclusions are removed from the summary. if an event occurred for the parameter in question, the data will have multiple records for each monitor.\n",
      "observation count: the number of observations (samples) taken during the day.\n",
      "observation percent: the percent representing the number of observations taken with respect to the number scheduled to be taken during the day. this is only calculated for monitors where measurements are required (e.g., only certain parameters).\n",
      "arithmetic mean: the average (arithmetic mean) value for the day.\n",
      "1st max value: the highest value for the day.\n",
      "1st max hour: the hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken.\n",
      "aqi: the air quality index for the day for the pollutant, if applicable.\n",
      "method code: an internal system code indicating the method (processes, equipment, and protocols) used in gathering and measuring the sample. the method name is in the next column.\n",
      "method name: a short description of the processes, equipment, and protocols used in gathering and measuring the sample.\n",
      "local site name: the name of the site (if any) given by the state, local, or tribal air pollution control agency that operates it.\n",
      "address: the approximate street address of the monitoring site.\n",
      "state name: the name of the state where the monitoring site is located.\n",
      "county name: the name of the county where the monitoring site is located.\n",
      "city name: the name of the city where the monitoring site is located. this represents the legal incorporated boundaries of cities and not urban areas.\n",
      "cbsa name: the name of the core bases statistical area (metropolitan area) where the monitoring site is located.\n",
      "date of last change: the date the last time any numeric values in this record were updated in the aqs data system.\n",
      "acknowledgements:\n",
      "these data come from the epa and are current up to may 1, 2017. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too: https://cloud.google.com/bigquery/public-data/epa.\n",
      "inspiration:\n",
      "breathing air with a high concentration of co reduces the amount of oxygen that can be transported in the bloodstream to critical organs like the heart and brain. at very high levels, which are possible indoors or in other enclosed environments, co can cause dizziness, confusion, unconsciousness and death. very high levels of co are not likely to occur outdoors. however, when co levels are elevated outdoors, they can be of particular concern for people with some types of heart disease. these people already have a reduced ability for getting oxygenated blood to their hearts in situations where the heart needs more oxygen than usual. they are especially vulnerable to the effects of co when exercising or under increased stress. in these situations, short-term exposure to elevated co may result in reduced oxygen to the heart accompanied by chest pain also known as angina.\n",
      "about this data\n",
      "this is a list of over 7,000 breweries and brewpubs in the usa provided by datafiniti's business database. the dataset includes the category, name, address, city, state, and more for each listing.\n",
      "what you can do with this data\n",
      "you can use this geographical and categorical information for business locations to determine which cities and states have the most breweries. e.g.:\n",
      "what is the number of breweries in each state?\n",
      "what are the cities with the most breweries per person?\n",
      "what are the states with the most breweries per person?\n",
      "what are the top cities for breweries?\n",
      "what are the top states for breweries?\n",
      "what industry categories are typically grouped with breweries?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "sharing economy and vacation rentals are among the hottest topics that has touched millions of lives across the globe. airbnb has been instrumental in this space and currently operating in more than 191 countries. hence, it'd be good idea to analyze this data and uncover insights.\n",
      "content\n",
      "dataset contains more than 18,000 property listings from texas, united staes. given below are the data fields:\n",
      "rate per night number of bedrooms city joining month and year longitude latitude property description property title property url\n",
      "the airbnb data was extracted by promptcloud’s data-as-a-service solution.\n",
      "initial analysis\n",
      "the following article covers spatial data visualization and topic modelling of the description text: http://www.kdnuggets.com/2017/08/insights-data-mining-airbnb.html\n",
      "inspiration\n",
      "some of the interesting analysis are related to spatial mapping and text mining of the description text apart from the exploratory analysis.\n",
      "context\n",
      "the federal reserve sets interest rates to promote conditions that achieve the mandate set by the congress — high employment, low and stable inflation, sustainable economic growth, and moderate long-term interest rates. interest rates set by the fed directly influence the cost of borrowing money. lower interest rates encourage more people to obtain a mortgage for a new home or to borrow money for an automobile or for home improvement. lower rates encourage businesses to borrow funds to invest in expansion such as purchasing new equipment, updating plants, or hiring more workers. higher interest rates restrain such borrowing by consumers and businesses.\n",
      "content\n",
      "this dataset includes data on the economic conditions in the united states on a monthly basis since 1954. the federal funds rate is the interest rate at which depository institutions trade federal funds (balances held at federal reserve banks) with each other overnight. the rate that the borrowing institution pays to the lending institution is determined between the two banks; the weighted average rate for all of these types of negotiations is called the effective federal funds rate. the effective federal funds rate is determined by the market but is influenced by the federal reserve through open market operations to reach the federal funds rate target. the federal open market committee (fomc) meets eight times a year to determine the federal funds target rate; the target rate transitioned to a target range with an upper and lower limit in december 2008. the real gross domestic product is calculated as the seasonally adjusted quarterly rate of change in the gross domestic product based on chained 2009 dollars. the unemployment rate represents the number of unemployed as a seasonally adjusted percentage of the labor force. the inflation rate reflects the monthly change in the consumer price index of products excluding food and energy.\n",
      "acknowledgements\n",
      "the interest rate data was published by the federal reserve bank of st. louis' economic data portal. the gross domestic product data was provided by the us bureau of economic analysis; the unemployment and consumer price index data was provided by the us bureau of labor statistics.\n",
      "inspiration\n",
      "how does economic growth, unemployment, and inflation impact the federal reserve's interest rates decisions? how has the interest rate policy changed over time? can you predict the federal reserve's next decision? will the target range set in march 2017 be increased, decreased, or remain the same?\n",
      "context\n",
      "one of the main challenges in any marketplace business is achieving the balance between demand and supply. at wuzzuf we optimize for demand, relevance and quality while connecting employers with the matching applicants, and recommending relevant jobs to the job seekers.\n",
      "content\n",
      "the dataset includes:\n",
      "wuzzuf_job_posts_sample : a sample of jobs posted on wuzzuf during 2014-2016.\n",
      "wuzzuf_applications_sample : the corresponding applications (excluding some entries).\n",
      "note: the jobs are mainly in egypt but other locations are included.\n",
      "exploration ideas\n",
      "there are several areas to explore, including, but not limited to:\n",
      "correlations between different features\n",
      "salaries trends\n",
      "insights about supply/demand\n",
      "growth opportunities\n",
      "data quality\n",
      "this data and analysis originally provided information for the february 5, 2017, los angeles times story \"californians are paying billions for power they don't need\" by documenting california's glut of power and the increasing cost to consumers. it also underpins a complementary interactive graphic. the data are drawn from the energy information administration, a branch of the united states government.\n",
      "acknowledgements\n",
      "data and analysis originally published by ryan menezes and ben welsh on the la times data desk github.\n",
      "this data set will contain the results from all the 2016 kitefoil races. this allows analysis to be done including the calculation of world rankings.\n",
      "pokemon go type, latitude, longitude, and despawn times - july 26 to july 28.\n",
      "this was an early mined dataset from pokemon go. representing a fairly large spatial area within a thin time frame. mostly useful in identifying potential spatial patterns in pokemon spawns.\n",
      "context\n",
      "people have been making music for tens of thousands of years [1]. today, making music is easier and more accessible than ever before. the technological developments of the last few decades allow people to simulate playing every imaginable instrument on their computers. audio sequencers enable users to arrange their songs on a time line, sample by sample. digital audio workstations (daws) ship with virtual instruments and synthesizers which allow users to virtually play a whole band or orchestra in their bedrooms.\n",
      "one challenge in working with daws is organizing samples and recordings in a structured way; so, users can easily access them. in addition to their own recordings, many users download samples. browsing through sample collections to find the perfect sound is time consuming and may impede the user's creative flow [2]. on top of this, manually naming and tagging recordings is a time-consuming and tedious task, so not many users do [3]. the consequence is that finding the right sound at the right moment becomes a challenging problem [4].\n",
      "modeling the relationship between the acoustic content and semantic descriptions of sounds could allow users to retrieve sounds using text queries. this dataset was collected to support research on content-based audio retrieval systems, focused on sounds used in creative context.\n",
      "content\n",
      "this dataset was collected from freesound [5] in june 2016. it contains the frame-based mfccs of about 230,000 sounds and the associated tags.\n",
      "sounds.json: sound metadata originally downloaded from the freesound api. this file includes the id, associated tags, links to previews, and links to an analysis_frames file, which contains frame-based low-level features, for each sound.\n",
      "preprocessed_tags.csv: preprocessed tags. contains only tags which are associated to at least 0.01% of sounds. moreover, tags were split on hyphens and stemmed. tags containing numbers and short tags with less than three characters were removed.\n",
      "queries.csv: an aggregated query-log of real user-queries against the freesound database, collected between may 11 and november 24 in 2016.\n",
      "preprocessed_queries.csv queries were preprocessed in the same way tags were preprocessed.\n",
      "*_mfccs.csv.bz2: the original mfccs for each sound, extracted from the url provided in the analysis_frames field of sounds.json, split across ten files.\n",
      "cb_{512|1024|2048|4096}_sparse.pkl: codebook representation of sounds saved as sparse pd.dataframe. the first-order and second-order derivatives of the 13 mfccs were appended to the mfcc feature vectors of each sound. all frames were clustered using k-means (mini-batch k-means) to find {512|1024|2048|4096} cluster centers. each frame was, then, assigned to its closest cluster center and the counts used to represent a sound as a single {512|1024|2048|4096}-dimensional vector.\n",
      "acknowledgements\n",
      "thanks to the music technology group of the universitat pompeu fabra in barcelona for creating and maintaining the freesound [5] database and for providing the aggregated query-logs.\n",
      "inspiration\n",
      "who can create the best content-based audio retrieval system measured by precision-at-k for values of k in {1, ..., 20} and mean average precision.\n",
      "getting started\n",
      "here's the accompanying github repository: https://github.com/dschwertfeger/cbar\n",
      "references\n",
      "[1] n. l. wallin and b. merker, the origins of music. mit press, 2001.\n",
      "[2] m. csikszentmihalyi, flow: the psychology of optimal experience. new york: harper perennial modern classics, 2008.\n",
      "[3] e. pampalk, a. rauber, and d. merkl, \"content-based organization and visualization of music archives\", in proceedings of the tenth acm international conference on multimedia, 2002, pp. 570–579.\n",
      "[4] t. bertin-mahieux, d. eck, and m. mandel, \"automatic tagging of audio: the state-of-the-art\", machine audition: principles, algorithms and systems, pp. 334–352, 2010.\n",
      "[5] f. font, g. roma, and x. serra, \"freesound technical demo\", 2013, pp. 411–412.\n",
      "in the pursuit of any goal, the first step is invariably data collection. as put up on the openai blog, writing a program which can write other programs is an incredibly important problem.\n",
      "this dataset collects publicly available information from the codechef site's practice section to provide about 1000 problem statements and a little over 1 million solutions in total to these problems in various languages.\n",
      "the ultimate aim is to allow a program to learn program generation in any language to satisfy a given problem statement.\n",
      "context\n",
      "i've uploaded a dataset previously that contains paradise papers, panama papers, bahamas and offshore leaks. i've been getting a lot of requests to upload paradise papers alone to make it less confusing for my fellow data scientists. here it is, the complete cache of paradise papers released so far. i will keep updating it.\n",
      "the paradise papers is a cache of some 13gb of data that contains 13.4 million confidential records of offshore investment by 120,000 people and companies in 19 tax jurisdictions (tax heavens - an awesome video to understand this); that was published by the international consortium of investigative journalists (icij) on november 5, 2017. subsequent data was released on november 20, 2017. here is a brief video about the leak. the people include queen elizabeth ii, the president of columbia (juan manuel santos), former prime minister of pakistan (shaukat aziz), u.s secretary of commerce (wilbur ross) and many more. according to an estimate by the boston consulting group, the amount of money involved is around $10 trillion. the leak contains many famous companies, including facebook, apple, uber, nike, walmart, allianz, siemens, mcdonald’s and yahoo.\n",
      "it also contains a lot of u. s president donald trump allies including rax tillerson, wilbur ross, koch brothers, paul singer, sheldon adelson, stephen schwarzman, thomas barrack and steve wynn etc. the complete list of politicians involve is available here.\n",
      "i am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye.\n",
      "content\n",
      "the data is the effort of more than 100 journalists from 60+ countries\n",
      "the original data is available under creative common license and can be downloaded from this link.\n",
      "i will keep updating the datasets with more leaks and data as it’s available\n",
      "acknowledgements\n",
      "international consortium of investigative journalists (icij)\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "how many companies and individuals are there in all of the leaks data\n",
      "\n",
      "how many countries involved\n",
      "\n",
      "total money involved\n",
      "\n",
      "what is the biggest best tax heaven\n",
      "\n",
      "can we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country\n",
      "\n",
      "who are the biggest cheaters and where they live\n",
      "\n",
      "what role fortune 500 companies play in this game\n",
      "i need your help to make this world corruption free in the age of nlp and big data\n",
      "the gss gathers data on contemporary american society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. hundreds of trends have been tracked since 1972. in addition, since the gss adopted questions from earlier surveys, trends can be followed for up to 70 years.\n",
      "the gss contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.\n",
      "altogether the gss is the single best source for sociological and attitudinal trend data covering the united states. it allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the united states to other nations. (source)\n",
      "this dataset is a csv version of the cumulative data file, a cross-sectional sample of the gss from 1972-current.\n",
      "history\n",
      "i made the database from my own photos of russian lowercase letters written by hand.\n",
      "content\n",
      "the github repository with examples\n",
      "github\n",
      "the main dataset (letters.zip)\n",
      "1650 (50x33) color images (32x32x3) with 33 letters and the file with labels letters.txt.\n",
      "photo files are in the .png format and the labels are integers and values.\n",
      "additional letters.csv file.\n",
      "the file lettercolorimages.h5 consists of preprocessing images of this set: image tensors and targets (labels)\n",
      "the additional dataset (letters2.zip)\n",
      "5940 (180x33) color images (32x32x3) with 33 letters and the file with labels letters2.txt.\n",
      "photo files are in the .png format and the labels are integers and values.\n",
      "additional letters2.csv file.\n",
      "the file lettercolorimages2.h5 consists of preprocessing images of this set: image tensors and targets (labels)\n",
      "letter symbols => letter labels\n",
      "а=>1, б=>2, в=>3, г=>4, д=>5, е=>6, ё=>7, ж=>8, з=>9, и=>10, й=>11, к=>12, л=>13, м=>14, н=>15, о=>16, п=>17, р=>18, с=>19, т=>20, у=>21, ф=>22, х=>23, ц=>24, ч=>25, ш=>26, щ=>27, ъ=>28, ы=>29, ь=>30, э=>31, ю=>32, я=>33\n",
      "background images => background labels\n",
      "striped=>0, gridded=>1, no background=>2\n",
      "acknowledgements\n",
      "as an owner of this database, i have published it for absolutely free using by any site visitor.\n",
      "usage\n",
      "classification, image generation, etc. in a case of handwritten letters with a small number of images are useful exercises.\n",
      "improvement\n",
      "there are lots of ways for increasing this set and the machine learning algorithms applying to it. for example: add the same images but written by other person or add capital letters.\n",
      "context:\n",
      "this dataset contains information obtained from an impact sensor within a taekwondo chest protector. participants were asked to perform various taekwondo techniques on this chest protector for analysis of sensor readings.\n",
      "content:\n",
      "data was obtained from 6 participants performing 4 different taekwondo techniques – roundhouse/round kick, back kick, cut kick & punch. participant details are summarized in table 1. the table is organized in ascending order according to participant weight/experience level.\n",
      "in the file 'taekwondo_technique_classification_stats.csv’, the data is organized as follows. the rows display:\n",
      "technique – roundhouse/round kick (r), back kick (b), cut kick (c), punch (p)\n",
      "participant id – p1, p2, p3, p4, p5, p6\n",
      "trial # – for each technique, each participant performed a total of 5 trials\n",
      "sensor readings – data shows the adc readings obtained from a 12-bit adc connected to the sensor (not listed as voltage but can be converted to it)\n",
      "the columns identify type of technique, participant, trial # and showcase the sensor readings. there are a total of 115 columns of sensor readings. each participant performed 5 trials for each type of technique with hard intensity. the only exception is that participant 6 (p6) did not perform back kicks.\n",
      "acknowledgements:\n",
      "the dataset was collected at a local taekwondo school. we would like to thank the instructor and students for taking their time to participate in our data collection!\n",
      "past research:\n",
      "previous work to classify taekwondo techniques included using a butter-worth low pass filter on matlab and performing integration around the maximum signal peak. the goal was to observe a pattern in the resulting integration values to determine impact intensity for each type of technique.\n",
      "inspiration:\n",
      "main analysis questions:\n",
      "1) determine impact intensity that is proportional to the participant’s weight/experience level\n",
      "ideally, impact intensity should increase with the increasing participant weight/experience level or id (participant id in table 1 corresponds to ascending weight/experience level)\n",
      "2) classify or distinguish between types of impact (round kick, back kick, cut kick or punch)\n",
      "each taekwondo technique usually has a unique waveform to be identified\n",
      "context\n",
      "i'm going straight to the point: i'm obsessed with steven wilson. i can't help it, i love his music. and i need more music with similar (almost identical) style. so, what i'm trying to solve here is, how to find songs that match sw's style with almost zero error?\n",
      "i'm aware that spotify gives you recommendations, like similar artists and such. but that's not enough -- spotify always gives you varied music. progressive rock is a very broad genre, and i just want those songs that sound very, very similar to steven wilson or porcupine tree.\n",
      "btw, porcupine tree was steven wilson's band, and they both sound practically the same. i made an analysis where i checked their musical similarities.\n",
      "content\n",
      "i'm using the spotify web api to get the data. they have an amazingly rich amount of information, especially the audio features.\n",
      "this repository has 5 datasets:\n",
      "stevenwilson.csv: contains steven wilson discography (65 songs)\n",
      "porcupinetree.csv: 65 porcupine tree songs\n",
      "complete steven wilson.csv: a merge between the past two datasets (steven wilson + porcupine tree)\n",
      "train.csv: 200 songs used to train knn. 100 are steven wilson songs and the rest are totally different songs\n",
      "test.csv: 100 songs that may or may not be like steven wilson's. i picked this songs from various prog rock playlists and my discover weekly from spotify.\n",
      "also, so far i've made two kernels:\n",
      "comparing steven wilson and porcupine tree\n",
      "finding songs that match sw's style using k-nearest neighbors\n",
      "data\n",
      "there are 21 columns in the datasets.\n",
      "numerical: this columns were scraped using get_audio_features from the spotify api.\n",
      "acousticness: a confidence measure from 0.0 to 1.0 of whether the track is acoustic; 1.0 represents high confidence the track is acoustic\n",
      "danceability: it describes how suitable a track is for dancing; a value of 0.0 is least danceable and 1.0 is most danceable\n",
      "duration_ms: the duration of the track in milliseconds\n",
      "energy: a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity\n",
      "instrumentalness: predicts whether a track contains no vocals; values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0\n",
      "liveness: detects the presence of an audience in the recording; 1.0 represents high confidence that the track was performed live\n",
      "loudness: the overall loudness of a track in decibels (db)\n",
      "speechiness: detects the presence of spoken words in a track; the more exclusively speech-like the recording (e.g. talk show), the closer to 1.0 the attribute value\n",
      "tempo: the overall estimated tempo of a track in beats per minute (bpm)\n",
      "valence: a measure from 0.0 to 1.0 describing the musical positiveness; tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
      "categorical: these features are categories represented as numbers.\n",
      "key: the musical key the track is in. e.g. 0 = c, 1 = c♯/d♭, 2 = d, and so on\n",
      "mode: mode indicates the modality (major or minor); major is represented by 1 and minor is 0\n",
      "time_signature: an estimated overall time signature of a track; it is a notational convention to specify how many beats are in each bar (or measure). e.g. 4/4, 4/3, 3/4, 8/4 etc.\n",
      "strings: these fields are mostly useless (except for name, album, artist and lyrics)\n",
      "id: the spotify id of the song\n",
      "name: name of the song\n",
      "album: album of the song\n",
      "artist: artist of the song\n",
      "uri: the spotify uri of the song\n",
      "type: the type of the spotify object\n",
      "track_href: the spotify api link of the song\n",
      "analysis_url: the url used for getting the audio features\n",
      "lyrics: lyrics of the song in lower case\n",
      "future\n",
      "ever been obsessed with a song? an album? an artist? i'm planning on building a web app that solves this. it will help you find music extremely similar to other.\n",
      "context\n",
      "analyze the world's largest rock climbing logbook!\n",
      "who's the biggest downgrader? is it better to be short, tall, or average height? how many years does it take the average climber to send her first 5.12? after countless crag debates over these and similar topics, i set out to find the answers. now you can prove statistically why that dude who tagged your multi-year project \"soft\" is wrong. (sorry, he might actually be right.)\n",
      "content\n",
      "i used python3 to build a web-scraper to collect all of the user and ascent information from the world's largest rock climbing logbook, https://www.8a.nu/. i actually ended up scraping their beta site, https://beta.8a.nu/, as it provided well-formed json objects. the scraper dumps all of the data into an sqlite database. check out https://github.com/dcohen21/8a.nu-scraper for more information about the project. this dataset was collected on 9/13/2017.\n",
      "the database consists of four tables: user, ascent, method, and grade.\n",
      "acknowledgements\n",
      "thanks to andrew cassidy (https://github.com/andrewcassidy) for the idea and mentorship. thanks to jens larssen and 8a.nu for creating the logbook and maintaining a thriving community.\n",
      "inspiration\n",
      "the sky's the limit!\n",
      "the dataset is approximately 1,400 cleaned and standardized product listings from dream market's \"cocaine\" category. it was collected with web-scraping and text extraction techniques in july 2017.\n",
      "extracted features for each listing include:\n",
      "product_title\n",
      "ships_from_to\n",
      "quantity in grams\n",
      "quality\n",
      "btc_price\n",
      "vendor details\n",
      "shipping dummy variables (true/false columns)\n",
      "for further details on the creation of this dataset and what it contains, see the blog post here: https://medium.com/thought-skipper/dark-market-regression-calculating-the-price-distribution-of-cocaine-from-market-listings-10aeff1e89e0\n",
      "context\n",
      "nowadays everybody is talking about cryptocurrencies and chances are this phenomenon will keep growing in the future. not only bitcoin and ethereum, but also other cryptos are slowly attracting more and more investments. here i have included all the available historical data of the top 50 cryptocurrencies listed on coinmarketcap.com (which at the time of writing is listing 1296 cryptocurrencies in total). data is available at 1-day interval from when the crypto was listed on coinmarketcap.com up to 17 november 2017.\n",
      "good luck!\n",
      "content\n",
      "as specified above, all the data was retrieved from coinmarketcap.com for the top 50 (at the time of writing) cryptocurrencies. everytime marketcap was not available (listed as '-', usually during the first days after the launch) has been replaced with a \"0\"\n",
      "files are organized as follows: date (dd/mm/yyyy) | open ($) | high ($) | low ($) | close ($) | 24-hrs volume ($) | marketcap ($)\n",
      "this dataset includes (with starting date):\n",
      "ardor.csv (23/07/2016) ark.csv (22/03/2017) attention-token-of-media.csv (04/10/2017) augur.csv (27/10/2015) basic-attention-token.csv (01/06/2017) binance-coin.csv (25/07/2017) bitcoin-cash.csv (23/07/2017) bitcoin.csv (28/04/2013) bitcoindark.csv (16/07/2014) bitconnect.csv (20/10/2017) bitshares.csv (21/07/2014) byteball.csv (27/12/2017) bytecoin-bcn.csv (17/06/2014) cardano.csv (01/10/2017) dash.csv (14/02/2014) decred.csv (10/02/2016) digixdao.csv (18/04/2016) dogecoin.csv (15/12/2013) eos.csv (01/07/2017) ethereum-classic.csv (24/07/2016) ethereum.csv (07/08/2015) factom.csv (06/10/2015) gas.csv (06/07/2017) golem.csv (19/10/2017) hshare.csv (20/08/2017) iota.csv (13/06/2017) komodo.csv (05/02/2017) kyber-network.csv (24/09/2017) lisk.csv (06/04/2016) litecoin.csv (28/04/2013) maidsafecoin.csv (28/04/2014) monacoin.csv (20/03/2014) monero.csv (21/05/2014) nem.csv (01/04/2015) neo.csv (09/09/2016) omisego.csv (14/07/2017) pivx.csv (13/02/2016) populous.csv (11/07/2017) qtum.csv (24/05/2017) ripple.csv (04/08/2013) salt.csv (29/09/2017) steem.csv (18/04/2016) stellar.csv (05/08/2014) stratis.csv (12/08/2016) tenx.csv (27/06/2017) tether.csv (25/02/2015) veritaseum.csv (08/06/2017) vertcoin.csv (20/01/2014) waves.csv (02/06/2016) zcash.csv (29/10/2016)\n",
      "i will keep this collection up to date as much as i can. please let me know if you are interested in additional cryptos.\n",
      "acknowledgements and inspiration\n",
      "if these files exist is thanks to coinmarketcap (coinmarketcap.com) which is an awesome database of cryptocurrencies (and makes an awesome homepage).\n",
      "many have tried, few have succeeded. can you predict tomorrow's price? what data patterns do you recognise? can't wait to see what code/results you have to share! comment below and please upvote this if you like it.\n",
      "context\n",
      "what’s the best (or at least the most popular) halloween candy? that was the question this dataset was collected to answer. data was collected by creating a website where participants were shown presenting two fun-sized candies and asked to click on the one they would prefer to receive. in total, more than 269 thousand votes were collected from 8,371 different ip addresses.\n",
      "content\n",
      "candy-data.csv includes attributes for each candy along with its ranking. for binary variables, 1 means yes, 0 means no. the data contains the following fields:\n",
      "chocolate: does it contain chocolate?\n",
      "fruity: is it fruit flavored?\n",
      "caramel: is there caramel in the candy?\n",
      "peanutalmondy: does it contain peanuts, peanut butter or almonds?\n",
      "nougat: does it contain nougat?\n",
      "crispedricewafer: does it contain crisped rice, wafers, or a cookie component?\n",
      "hard: is it a hard candy?\n",
      "bar: is it a candy bar?\n",
      "pluribus: is it one of many candies in a bag or box?\n",
      "sugarpercent: the percentile of sugar it falls under within the data set.\n",
      "pricepercent: the unit price percentile compared to the rest of the set.\n",
      "winpercent: the overall win percentage according to 269,000 matchups.\n",
      "acknowledgements:\n",
      "this dataset is copyright (c) 2014 espn internet ventures and distributed under an mit license. check out the analysis and write-up here: the ultimate halloween candy power ranking. thanks to walt hickey for making the data available.\n",
      "inspiration:\n",
      "which qualities are associated with higher rankings?\n",
      "what’s the most popular candy? least popular?\n",
      "can you recreate the 538 analysis of this dataset?\n",
      "the hockey database is a collection of historical statistics from men's professional hockey teams in north america.\n",
      "note that as of v1, this dataset is missing a few files, due to kaggle restrictions on the number of individual files that can be uploaded. the missing files will be noted in the description below.\n",
      "the data\n",
      "the dataset contains the following tables (all are csv):\n",
      "master: names and biographical information\n",
      "scoring: scoring statistics\n",
      "scoringsup: supplemental scoring statistics. missing in v1\n",
      "scoringsc: scoring for stanley cup finals, 1917-18 through 1925-26\n",
      "scoringshootout: scoring statistics for shootouts\n",
      "goalies: goaltending statistics\n",
      "goaliessc: goaltending for stanley cup finals, 1917-18 through 1925-26\n",
      "goaliesshootout: goaltending statistics for shootouts\n",
      "awardsplayers: player awards, trophies, postseason all-star teams\n",
      "awardscoaches: coaches awards, trophies, postseason all-star teams\n",
      "awardsmisc: miscellaneous awards. missing in v1\n",
      "coaches: coaching statistics\n",
      "teams: team regular season statistics\n",
      "teamspost: team postseason statistics\n",
      "teamssc: team stanley cup finals statistics, 1917-18 through 1925-26\n",
      "teamshalf: first half / second half standings, 1917-18 through 1920-21\n",
      "teamsplits: team home/road and monthly splits\n",
      "teamvsteam: team vs. team results\n",
      "seriespost: postseason series\n",
      "combinedshutouts: list of combined shutouts.\n",
      "abbrev: abbreviations used in teams and seriespost tables\n",
      "hof: hall of fame information\n",
      "descriptions of the individual fields in each file can be found in the file's description.\n",
      "copyright notice\n",
      "the hockey databank project allows for free usage of its data, including the production of a commercial product based upon the data, subject to the terms outlined below.\n",
      "1) in exchange for any usage of data, in whole or in part, you agree to display the following statement prominently and in its entirety on your end product:\n",
      "\"the information used herein was obtained free of charge from and is copyrighted by the hockey databank project. for more information about the hockey databank project please visit http://sports.groups.yahoo.com/group/hockey-databank\"\n",
      "2) your usage of the data constitutes your acknowledgment, acceptance, and agreement that the hockey databank project makes no guarantees regarding the accuracy of the data supplied, and will not be held responsible for any consequences arising from the use of the information presented.\n",
      "acknowledgments\n",
      "this dataset was downloaded from the hockey database at open source sports. the original acknowledgments are as follows:\n",
      "a variety of sources were consulted while constructing this database. these are listed below in no particular order.\n",
      "books:\n",
      "national hockey league guide (various years)\n",
      "national hockey league official record book (1982-83 and 1983-84)\n",
      "national hockey league official guide & record book (1984-85 to present)\n",
      "the stanley cup records and statistics (various years)\n",
      "world hockey association media guide (various years)\n",
      "wha schedule & statistics (1974-75)\n",
      "the sporting news hockey guide (various years)\n",
      "official nhl record book 1917-64\n",
      "the complete historical and statistical reference to the world hockey association 1972-1979, by scott surgent; xaler press (7th edition, 2004; 8th edition, 2008)\n",
      "total hockey; total sports publishing (1st edition, 1998; 2nd edition, 2000)\n",
      "the encyclopedia of hockey, by robert a. styer; a.s. barnes (2nd edition, 1973)\n",
      "the hockey encyclopedia, by stan fischler and shirley walton fischler; macmillan (1983)\n",
      "the trail of the stanley cup (vol. 1, 2, and 3), by charles l. coleman\n",
      "periodicals:\n",
      "the sporting news\n",
      "on-line sources:\n",
      "espn.com: http://www.espn.com/nhl/statistics\n",
      "find a grave: http://www.findagrave.com\n",
      "the goaltender home page (doug norris): http://hockeygoalies.org\n",
      "history of nhl trades: http://nhltradeshistory.blogspot.com\n",
      "hockey research association: http://www.hockeyresearch.com/stats\n",
      "hockey-reference.com (justin kubatko): http://www.hockey-reference.com\n",
      "hockey summary project: http://sports.groups.yahoo.com/group/hockey_summary_project/, http://hsp.flyershistory.com (previously at http://www.shrpsports.com/hsp)\n",
      "internet hockey database (ralph slate): http://www.hockeydb.com\n",
      "legends of hockey.net (hockey hall of fame): http://www.legendsofhockey.net/html/search.htm\n",
      "losthockey.com: http://www.losthockey.com\n",
      "national hockey league: http://www.nhl.com\n",
      "nhl hockey shootout statistics: http://jeays.net/shootout/index.htm\n",
      "nhl shootouts: http://www.nhlshootouts.com\n",
      "north american pro hockey: http://www.ottawavalleyonline.com/sites/tomking_01/index.html\n",
      "puckerings: http://www.puckerings.com\n",
      "society for international hockey research: http://www.sihrhockey.org\n",
      "the sports network: http://www.sportsnetwork.com\n",
      "usa today hockey stats archive: http://www.usatoday.com/sports/hockey, http://www.usatoday.com/sports/hockey/archive.htm\n",
      "yahoo sports: http://sports.yahoo.com/nhl\n",
      "thanks to the following individuals:\n",
      "ralph dinger (nhl publishing / dan diamond and associates) has confirmed a number of corrections to errors found in the nhl's official statistics. thanks also to justin kubatko of hockey-reference.com for a number of discussions in this area.\n",
      "morey holzman provided information on lloyd cook's 1921-22 goaltending appearance.\n",
      "stu mcmurray provided correct 1917-18 scoring statistics, including gwg.\n",
      "doug norris provided corrected 1984-85 statistics for rick st. croix.\n",
      "paul reeths created the hall of fame table, and provided updates for the coaches table\n",
      "other contributors include roger brewer, mike burton, eric hornick, and claude paradis.\n",
      "an acknowledgement is also given to the team led by sean forman and sean lahman that has developed and maintained the lahman baseball database. this database follows the same general design.\n",
      "context\n",
      "the los angeles city controller office releases payroll information for all city employees on a quarterly basis since 2013.\n",
      "content\n",
      "data includes department titles, job titles, projected annual salaries (with breakdowns of quarterly pay), bonuses, and benefits information.\n",
      "inspiration\n",
      "how do benefits and salaries differ for employees across departments and titles? are there any unusually large differences between lowest and highest employee salaries?\n",
      "how have salaries changed over the past three years?\n",
      "have the costs of benefits changed dramatically since the passing of the affordable care act?\n",
      "what is the most common government role in los angeles?\n",
      "context\n",
      "the uniform crime reporting (ucr) program has been the starting place for law enforcement executives, students of criminal justice, researchers, members of the media, and the public at large seeking information on crime in the nation. the program was conceived in 1929 by the international association of chiefs of police to meet the need for reliable uniform crime statistics for the nation. in 1930, the fbi was tasked with collecting, publishing, and archiving those statistics.\n",
      "today, four annual publications, crime in the united states, national incident-based reporting system, law enforcement officers killed and assaulted, and hate crime statistics are produced from data received from over 18,000 city, university/college, county, state, tribal, and federal law enforcement agencies voluntarily participating in the program. the crime data are submitted either through a state ucr program or directly to the fbi’s ucr program.\n",
      "this dataset focuses on the crime rates and law enforcement employment data in the state of california.\n",
      "content\n",
      "crime and law enforcement employment rates are separated into individual files, focusing on offenses by enforcement agency, college/university campus, county, and city. categories of crimes reported include violent crime, murder and nonnegligent manslaughter, rape, robbery, aggravated assault, property crime, burglary, larceny-theft, motor vehicle damage, and arson. in the case of rape, data is collected for both revised and legacy definitions. in some cases, a small number of enforcement agencies switched definition collection sometime within the same year.\n",
      "acknowledgements\n",
      "this dataset originates from the fbi ucr project, and the complete dataset for all 2015 crime reports can be found here.\n",
      "inspiration\n",
      "what are the most common types of crimes in california? are there certain crimes that are more common in a particular place category, such as a college/university campus, compared to the rest of the state?\n",
      "how does the number of law enforcement officers compare to the crime rates of a particular area? is the ratio similar throughout the state, or do certain campuses, counties, or cities have a differing rate?\n",
      "how does the legacy vs. refined definition of rape differ, and how do the rape counts compare? if you pulled the same data from fbi datasets for previous years, can you see a difference in rape rates over time?\n",
      "context\n",
      "we all know of the roman empire, but what about its emperors specifically?\n",
      "content\n",
      "here, you will find information on each of the emperors of the roman empire, which lasted between 26 bc and 395 ad. specifically, you can use data on their:\n",
      "names\n",
      "date of birth\n",
      "city and province of birth\n",
      "date of death\n",
      "method of accession to power\n",
      "date of accession to power\n",
      "date of end of reign\n",
      "cause of death\n",
      "identity of killer\n",
      "dynasty\n",
      "era\n",
      "photo\n",
      "acknowledgements\n",
      "this dataset was provided by zonination, who made it available on wikipedia. see his repository on github\n",
      "inspiration\n",
      "what kind of trend can you find in these emperors' lives and reigns? what aspects of them allowed them to live longer?\n",
      "path of exile league statistic\n",
      "data contains stats of 59000 players, from 4th august of 2017 and before now.\n",
      "content\n",
      "one file with 12 data sections. one league - harbinger, but 4 different types of divisions:\n",
      "harbinger\n",
      "hardcore harbinger\n",
      "ssf harbinger\n",
      "ssf harbinger hc\n",
      "each division has own ladder with leaders\n",
      "acknowledgements\n",
      "i found this data at the pathofstats.com as a json format and exported at cvs. data have been collecting by this api, and free to use for interested people. if ggg or pathofstats.com don't want to share it here, please contact me and it will be removed.\n",
      "as i could understand, it is simple data for people, who are new at data science and want to have practice.\n",
      "questions for participants\n",
      "a total number of players in each division, usage of each class in descending order.\n",
      "some of the players streaming their game (twitch colum). do they play better than people, who does not?\n",
      "predict chance to be at top 30 in each division, if we are necromancer. with and without stream.\n",
      "average number of finished challanges for each division, show division with highest and lowest average challanges.\n",
      "show dependency between level and class of died characters. only for hc divisions.\n",
      "context:\n",
      "over 1.5 billions pounds of pumpkin are grown annually in the united states. where are they sold, and for how much?\n",
      "this dataset contains prices for which pumpkins were sold at selected u.s. cities’ terminal markets. prices are differentiated by the commodities’ growing origin, variety, size, package and grade.\n",
      "content:\n",
      "this dataset contains terminal market prices for different pumpkin crops in 13 cities in the united states from september 24, 2016 to september 30, 2017. in keeping with the structure of the original source data, information on each city has been uploaded as a separate file.\n",
      "atlanta, ga\n",
      "baltimore, md\n",
      "boston, ma\n",
      "chicago, il\n",
      "columbia, sc\n",
      "dallas, tx\n",
      "detroit, mi\n",
      "los angeles, ca\n",
      "miami, fl\n",
      "new york, ny\n",
      "philadelphia, pa\n",
      "san francisco, ca\n",
      "saint louis, mo\n",
      "data for each city includes the following columns (although not all information is available for every city)\n",
      "commodity name: always pumpkin, since this is a pumpkin-only dataset\n",
      "city name: city where the pumpkin was sold\n",
      "type\n",
      "package\n",
      "variety\n",
      "sub variety\n",
      "grade: in the us, usually only canned pumpkin is graded\n",
      "date: date of sale (rounded up to the nearest saturday)\n",
      "low price\n",
      "high price\n",
      "mostly low\n",
      "mostly high\n",
      "origin: where the pumpkins were grown\n",
      "origin district\n",
      "item size\n",
      "color\n",
      "environment\n",
      "unit of sale\n",
      "quality\n",
      "condition\n",
      "appearance\n",
      "storage\n",
      "crop\n",
      "repack: whether the pumpkin has been repackaged before sale\n",
      "trans mode\n",
      "acknowledgements:\n",
      "this dataset is based on specialty crops terminal markets standard reports distributed by the united states department of agriculture. up-to-date reports can be generated here. this data is in the public domain.\n",
      "inspiration:\n",
      "which states produce the most pumpkin?\n",
      "where are pumpkin prices highest?\n",
      "how does pumpkin size relate to price?\n",
      "which pumpkin variety is the most expensive? least expensive?\n",
      "context\n",
      "this dataset is an aggregated count of all crimes committed in france, broken down by month and category.\n",
      "content\n",
      "this data was aggregated by the french national government and published online on the french open data portal. it is a combination of records kept by both local and national police forces. it's important to note that the name of the categories of crime are in french!\n",
      "acknowledgements\n",
      "this data is a part of a larger group of excel files published by the french goverment on the french open data portal. it has been converted to a single csv file before uploading here.\n",
      "inspiration\n",
      "this is a simple time series dataset that can be probed for trends in the underlying types of crimes committed. is petty theft more or less popular today than it was ten years ago? how much variation is there in the amount of robberies year-to-year? can you normalize the growth in the number of crimes against the growth in the number of people? how do crimes committed here differ from those committed in, say, los angeles?\n",
      "sample sales data, order info, sales, customer, shipping, etc., used for segmentation, customer analytics, clustering and more. inspired for retail analytics. this was originally used for pentaho di kettle, but i found the set could be useful for sales simulation training.\n",
      "originally written by maría carina roldán, pentaho community member, bi consultant (assert solutions), argentina. this work is licensed under the creative commons attribution-noncommercial-share alike 3.0 unported license. modified by gus segura june 2014.\n",
      "build the proposed optimized pricing model: determine the largest or key value driver from the data build price segments using product characteristics, distribution channel, behavior and demographic customer characteristics. proposed new pricing strategy bases of customer segmentation. identify segments where we can lower the rates and as well as highlight segments where it is underpriced without impacting the profitability\n",
      "context\n",
      "this is a dataset that i built by scraping the united states department of labor's bureau of labor statistics. i was looking for county-level unemployment data and realized that there was a data source for this, but the data set itself hadn't existed yet, so i decided to write a scraper and build it out myself.\n",
      "content\n",
      "this data represents the local area unemployment statistics from 1990-2016, broken down by state and month. the data itself is pulled from this mapping site:\n",
      "https://data.bls.gov/map/maptoolservlet?survey=la&map=county&seasonal=u\n",
      "further, the ever-evolving and ever-improving codebase that pulled this data is available here:\n",
      "https://github.com/jayrav13/bls_local_area_unemployment\n",
      "acknowledgements\n",
      "of course, a huge shoutout to bls.gov and their open and transparent data. i've certainly been inspired to dive into us-related data recently and having this data open further enables my curiosities.\n",
      "inspiration\n",
      "i was excited about building this data set out because i was pretty sure something similar didn't exist - curious to see what folks can do with it once they run with it! a curious question i had was surrounding unemployment vs 2016 presidential election outcome down to the county level. a comparison can probably lead to interesting questions and discoveries such as trends in local elections that led to their most recent election outcome, etc.\n",
      "next steps\n",
      "version 1 of this is as a massive json blob, normalized by year / month / state. i intend to transform this into a csv in the future as well.\n",
      "context\n",
      "the data set includes information when the free throw was taken during the game, who took the shot and if it went in or not.\n",
      "content\n",
      "the data was scraped from espn.com. one example site is: http://www.espn.com/nba/playbyplay?gameid=261229030\n",
      "weather data barajas airport, madrid, between 1997 and 2015. gathered web https://www.wunderground.com/ the weather company, llc\n",
      "fields:\n",
      "max temperaturec\n",
      "mean temperaturec\n",
      "min temperaturec\n",
      "dew pointc\n",
      "meandew pointc\n",
      "min dewpointc\n",
      "max humidity\n",
      "mean humidity\n",
      "min humidity\n",
      "max sea level pressurehpa\n",
      "mean sea level pressurehpa\n",
      "min sea level pressurehpa\n",
      "max visibilitykm\n",
      "mean visibilitykm\n",
      "min visibilitykm\n",
      "max wind speedkm/h\n",
      "mean wind speedkm/h\n",
      "max gust speedkm/h\n",
      "precipitationmm\n",
      "cloudcover\n",
      "events\n",
      "winddirdegrees\n",
      "script for download the data:\n",
      "#!/bin/bash\n",
      "\n",
      "local='weather_madrid'\n",
      "station='lemd'\n",
      "ini=1997\n",
      "end=2015\n",
      "file=${local}_${station}_${ini}_${end}.csv\n",
      "site='airport'\n",
      "\n",
      "echo \"cet,max temperaturec,mean temperaturec,min temperaturec,dew pointc,meandew pointc,min dewpointc,max humidity, mean humidity, min humidity, max sea level pressurehpa, mean sea level pressurehpa, min sea level pressurehpa, max visibilitykm, mean visibilitykm, min visibilitykm, max wind speedkm/h, mean wind speedkm/h, max gust speedkm/h,precipitationmm, cloudcover, events,winddirdegrees\" > ${file}\n",
      "\n",
      "for year in $(seq ${ini} ${end})\n",
      "do\n",
      "   echo \"year $year\"\n",
      "   wget \"https://www.wunderground.com/history/${site}/${station}/${year}/1/1/customhistory.html?dayend=31&monthend=12&yearend=${year}&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1\" -o \"${local}_${year}.csv\"\n",
      "   tail -n +3 ${local}_${year}.csv > ${local}_${year}_1.csv\n",
      "   sed 's/<br\\ \\/>//g' ${local}_${year}_1.csv >> ${file}\n",
      "   rm ${local}_${year}.csv ${local}_${year}_1.csv\n",
      "done\n",
      "nan\n",
      "why?\n",
      "the nfl, espn, and many others have their own quarterback rating system. can you create your own? how many points does a qb contribute to a given game? and, with mvp trophy season coming up, who really stands out as an mvp and who is carried by their team?\n",
      "qb stats\n",
      "this is scraped from footballdb.com using pandas' read_html function. this dataset contains every regular season nfl game and every nfl passer (including non-quarterbacks) from 1996 to 2016. individual years are available for the past 10 years, and all 21 years are in qbstats_all. in addition to the traditional stats, the total points from the game have been appended to the stats. win/loss is up and coming, but is not a priority at the moment since a qb cannot control how well the defense stops the opposing offense.\n",
      "content\n",
      "inside you'll find:\n",
      "quarterback name (qb)\n",
      "attempts (att)\n",
      "completions (cmp)\n",
      "yards (yds)\n",
      "yards per attempt (ypa)\n",
      "touchdowns (td)\n",
      "interceptions (int)\n",
      "longest throw (lg)\n",
      "sacks (sack)\n",
      "loss of yards (loss)\n",
      "the nfl's quarterback rating for the game (rate)\n",
      "total points scored in the game (game_points)\n",
      "home or away game (home_away)\n",
      "year (year)\n",
      "important note:\n",
      "because of the way that these were scraped, the the game week is not supplied. however, the games are all in oldest to most recent which would allow for some time-series analysis.\n",
      "additionally:\n",
      "feel free to make any requests for additional information. but due to the time that it takes to scrape 21 years of nfl stats, it will likely take a while before i finish updating the dataset.\n",
      "acknowledgements\n",
      "i would very much like to thank footballdb.com for not blacklisting me after numerous scrapes and potential future scrapes for information on other positions.\n",
      "context\n",
      "reuven ramaty high energy solar spectroscopic imager (rhessi, originally high energy solar spectroscopic imager or hessi) is a nasa solar flare observatory. it is the sixth mission in the small explorer program, selected in october 1997 and launched on 5 february 2002. its primary mission is to explore the physics of particle acceleration and energy release in solar flares. hessi was renamed to rhessi on 29 march 2002 in honor of reuven ramaty, a pioneer in the area of high energy solar physics. rhessi is the first space mission named after a nasa scientist. rhessi was built by spectrum astro for goddard space flight center and is operated by the space sciences laboratory in berkeley, california. the principal investigator from 2002 to 2012 was robert lin, who was succeeded by säm krucker.\n",
      "useful links: https://en.wikipedia.org/wiki/reuven_ramaty_high_energy_solar_spectroscopic_imager https://hesperia.gsfc.nasa.gov/hessi/objectives.htm\n",
      "content\n",
      "ramaty high energy solar spectroscopic imager (rhessi)\n",
      "notes: note that only events with non-zero position and energy range not equal to 3-6 kev are confirmed as solar sources. events which have no position and show up mostly in the front detectors, but were not able to be imaged are flagged as \"ps\".\n",
      "events which do not have valid position are only confirmed to be non-solar if the ns flag is set.\n",
      "peak rate: peak counts/second in energy range 6-12 kev, averaged over active collimators, including background.\n",
      "total counts: counts in energy range 6-12 kev integrated over duration of flare summed over all subcollimators, including background.\n",
      "energy: the highest energy band in which the flare was observed. electron kev (kilo electron volt) https://en.wikipedia.org/wiki/electronvolt\n",
      "radial distance: distance from sun center\n",
      "quality codes: qn, where n is the total number of data gap, saa, particle, eclipse or decimation flags set for event. n ranges from 0 to 11. use care when analyzing the data when the quality is not zero.\n",
      "active_region: a number for the closest active region, if available\n",
      "radial_offset: the offset of the flare position from the spin axis of the spacecraft in arcsec. this is used i spectroscopy.\n",
      "peak_c/s: peak count rate in corrected counts.\n",
      "flare flag codes: a0 - in attenuator state 0 (none) sometime during flare a1 - in attenuator state 1 (thin) sometime during flare a2 - in attenuator state 2 (thick) sometime during flare a3 - in attenuator state 3 (both) sometime during flare an - attenuator state (0=none, 1=thin, 2=thick, 3=both) at peak of flare df - front segment counts were decimated sometime during flare dr - rear segment counts were decimated sometime during flare ed - spacecraft eclipse (night) sometime during flare ee - flare ended in spacecraft eclipse (night) es - flare started in spacecraft eclipse (night) fe - flare ongoing at end of file fr - in fast rate mode fs - flare ongoing at start of file gd - data gap during flare ge - flare ended in data gap gs - flare started in data gap mr - spacecraft in high-latitude zone during flare ns - non-solar event pe - particle event: particles are present ps - possible solar flare; in front detectors, but no position pn - position quality: p0 = position is not valid, p1 = position is valid qn - data quality: q0 = highest quality, q11 = lowest quality sd - spacecraft was in saa sometime during flare se - flare ended when spacecraft was in saa ss - flare started when spacecraft was in saa\n",
      "acknowledgements\n",
      "what is a solar flare?\n",
      "a solar flare is the rapid release of a large amount of energy stored in the solar atmosphere. during a flare, gas is heated to 10 to 20 million degrees kelvin (k) and radiates soft x rays and longer-wavelength emission. unable to penetrate the earth's atmosphere, the x rays can only be detected from space. instruments on skylab, smm, the japanese/us yohkoh mission and other spacecraft have recorded many flares in x rays over the last twenty years or so. ground-based observatories have recorded the visible and radio outputs. these data form the basis of our current understanding of a solar flare. but there are many possible mechanisms for heating the gas, and observations to date have not been able to differentiate between them.\n",
      "hessi's new approach\n",
      "researchers believe that much of the energy released during a flare is used to accelerate, to very high energies, electrons (emitting primarily x-rays) and protons and other ions (emitting primarily gamma rays). the new approach of the hessi mission is to combine, for the first time, high-resolution imaging in hard x-rays and gamma rays with high-resolution spectroscopy, so that a detailed energy spectrum can be obtained at each point of the image.\n",
      "this new approach will enable researchers to find out where these particles are accelerated and to what energies. such information will advance understanding of the fundamental high-energy processes at the core of the solar flare problem. https://hesperia.gsfc.nasa.gov/hessi/objectives.htm\n",
      "inspiration\n",
      "explore,\n",
      "know something new,\n",
      "predict the solar flare,\n",
      "respect the sun and value it and\n",
      "take care of the environments.\n",
      "thanks\n",
      "this isn't a dataset, it is a collection of kernels written on kaggle that use no data at all.\n",
      "the 2015 american community survey public use microdata sample\n",
      "context\n",
      "the american community survey (acs) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.\n",
      "frequency: annual\n",
      "period: 2015\n",
      "pwgtp (weights)\n",
      "please note. each record is weighted with pwgtp. for accurate analysis, these weights need to be applied. reference getting started 'python' for a simple kernel on how this field gets used. or, click on the image below to see how this can be done in r (see code in this kernel).\n",
      "the data dictionary can be found here, but you'll need to scroll down to the person record section.\n",
      "content\n",
      "through the acs, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. public officials, planners, and entrepreneurs use this information to assess the past and plan the future. when you respond to the acs, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. the data dictionary can be found here.\n",
      "inspiration\n",
      "kernels created using the 2014 acs and 2013 acs can serve as excellent starting points for working with the 2015 acs. for example, the following analyses were created using acs data:\n",
      "work arrival times and earnings in the usa\n",
      "inequality in stem careers\n",
      "acknowledgements\n",
      "the american community survey (acs) is administered, processed, researched and disseminated by the u.s. census bureau within the u.s. department of commerce.\n",
      "context\n",
      "project aristo at the allen institute for artificial intelligence (ai2) is focused on building a system that acquires and stores a vast amount of knowledge in computable form, then applies this knowledge to answer a variety of science questions from standardized exams for students in multiple grade levels. we are inviting the wider ai research community to work on this grand challenge with us by providing this dataset of student science assessment questions.\n",
      "content\n",
      "these are english language questions that span several grade levels as indicated in the files. each question is a 4-way multiple choice structure. some of these questions include a diagram, either as part of the question text, as an answer option, or both. the diagrams are represented in the text with filenames that correspond to the diagram file itself in the companion folder. these questions come pre-split into train, development, and test sets.\n",
      "the data set includes the following fields:\n",
      "questionid: a unique identifier for the question\n",
      "originalquestionid: the question number on the test\n",
      "totalpossiblepoints: how many points the question is worth\n",
      "answerkey: the correct answer option\n",
      "ismultiplechoicequestion: 1 = multiple choice, 0 = other\n",
      "includesdiagram: 1 = includes diagram, 0 = other\n",
      "examname: the source of the exam\n",
      "schoolgrade: grade level\n",
      "year: year the source exam was published\n",
      "question: the question itself\n",
      "subject: science\n",
      "category: test, train, or dev (data comes pre-split into these categories)\n",
      "evaluation\n",
      "ai2 has made available aristo mini, a light-weight question answering system that can quickly evaluate science questions with an evaluation web server and provided baseline solvers. you can extend the provided solvers with your own implementation to try out new approaches and compare results.\n",
      "acknowledgements\n",
      "the aristo project team at ai2 compiled this dataset and we use it actively in our research. for a description of the motivations and intention for this data, please see:\n",
      "clark, peter. “elementary school science and math tests as a driver for ai: take the aristo challenge!” aaai (2015).\n",
      "context\n",
      "the main reason for making this dataset is the publication of the paper: learning from simulated and unsupervised images through adversarial training and the idea of the simgan. the dataset and kernels should make it easier to get started making simgan networks and testing them out and comparing them to other approaches like knn, gan, infogan and the like.\n",
      "source\n",
      "the synthetic images were generated with the windows version of unityeyes http://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/tutorial.html\n",
      "the real images were taken from https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/, which can be cited like this: appearance-based gaze estimation in the wild, x. zhang, y. sugano, m. fritz and a. bulling, proc. of the ieee international conference on computer vision and pattern recognition (cvpr), june, p.4511-4520, (2015).\n",
      "challenges\n",
      "enhancement\n",
      "one of the challenges (as covered in the paper) is enhancing the simulated images by using the real images. one possible approach is using the simgan which is implemented for reference in one of the notebooks. there are a number of other approaches (pix2pix, cyclegan) which could have interesting results.\n",
      "gaze detection\n",
      "the synthetic dataset has the gaze information since it was generated by unityeyes with a predefined look-vector. the overview notebook covers what this vector means and how each component can be interpreted. it would be very useful to have a simple, quick network for automatically generating this look vector from an image\n",
      "context\n",
      "this is fivethirtyeight's congress trump score. as the website itself puts it, it's \"an updating tally of how often every member of the house and the senate votes with or against the president\".\n",
      "content\n",
      "there are two tables: cts and votes. the first one has summary information for every congressperson: their name, their state, their trump score, trump's share of the votes in the election, etc. the second one has information about every vote each congressperson has cast: their vote, trump's position on the issue, etc.\n",
      "the data was extracted using r. the code is available as a package on github.\n",
      "acknowledgements\n",
      "the data is 100% collected and maintained by fivethirtyeight. they are awesome.\n",
      "the united states drought monitor collects weekly data on drought conditions around the u.s.\n",
      "acknowledgements\n",
      "all data was downloaded from the united states drought monitor webpage.\n",
      "the u.s. drought monitor is jointly produced by the national drought mitigation center at the university of nebraska-lincoln, the united states department of agriculture, and the national oceanic and atmospheric administration. map courtesy of ndmc-unl.\n",
      "the data\n",
      "the data contains weekly observations about the extent and severity of drought in each county of the united states. the dataset contains the following fields:\n",
      "releasedate: when this data was released on the usdm website\n",
      "fips: the fips code for this county\n",
      "county: the county name\n",
      "state: the state the county is in\n",
      "none: percentage of the county that is not in drought\n",
      "d0: percentage of the county that is in abnormally dry conditions\n",
      "d1: percentage of the county that is in moderate drought\n",
      "d2: percentage of the county that is in severe drought\n",
      "d3: percentage of the county that is in extreme drought\n",
      "d4: percentage of the county that is in exceptional drought\n",
      "validstart: the starting date of the week that these observations represent\n",
      "validend: the ending date of the week that these observations represent\n",
      "domstatisticformatid: seems to always be 1\n",
      "note: the drought categories are cumulative: if an area is in d3, then it is also in d2, d1, and d0. this means that, for every observation, d4 <= d3 <= d2 <= d1 <= d0.\n",
      "county info\n",
      "to make some analyses slightly easier, i've also included *county_info_2016.csv*, which contains physical size information about each county. this file contains the following fields:\n",
      "usps: united states postal service state abbreviation\n",
      "geoid: fips code\n",
      "ansicode: american national standards institute code\n",
      "name: name\n",
      "aland: land area (square meters) - created for statistical purposes only\n",
      "awater: water area (square meters) - created for statistical purposes only\n",
      "aland_sqmi: land area (square miles) - created for statistical purposes only\n",
      "awater_sqmi: water area (square miles) - created for statistical purposes only\n",
      "intptlat: latitude (decimal degrees) first character is blank or \"-\" denoting north or south latitude respectively\n",
      "intptlong: longitude (decimal degrees) first character is blank or \"-\" denoting east or west longitude respectively\n",
      "baseball databank is a compilation of historical baseball data in a convenient, tidy format, distributed under open data terms.\n",
      "this version of the baseball databank was downloaded from sean lahman's website.\n",
      "note that as of v1, this dataset is missing a few tables because of a restriction on the number of individual files that can be added. this is in the process of being fixed. the missing tables are parks, homegames, collegeplaying, schools, appearances, and fieldingpost.\n",
      "the data\n",
      "the design follows these general principles. each player is assigned a unique number (playerid). all of the information relating to that player is tagged with his playerid. the playerids are linked to names and birthdates in the master table.\n",
      "the database is comprised of the following main tables:\n",
      "master - player names, dob, and biographical info\n",
      "batting - batting statistics\n",
      "pitching - pitching statistics\n",
      "fielding - fielding statistics\n",
      "it is supplemented by these tables:\n",
      "allstarfull - all-star appearances\n",
      "halloffame - hall of fame voting data\n",
      "managers - managerial statistics\n",
      "teams - yearly stats and standings\n",
      "battingpost - post-season batting statistics\n",
      "pitchingpost - post-season pitching statistics\n",
      "teamfranchises - franchise information\n",
      "fieldingof - outfield position data\n",
      "fieldingpost- post-season fielding data\n",
      "managershalf - split season data for managers\n",
      "teamshalf - split season data for teams\n",
      "salaries - player salary data\n",
      "seriespost - post-season series information\n",
      "awardsmanagers - awards won by managers\n",
      "awardsplayers - awards won by players\n",
      "awardssharemanagers - award voting for manager awards\n",
      "awardsshareplayers - award voting for player awards\n",
      "appearances - details on the positions a player appeared at\n",
      "schools - list of colleges that players attended\n",
      "collegeplaying - list of players and the colleges they attended\n",
      "descriptions of each of these tables can be found attached to their associated files, below.\n",
      "acknowledgments\n",
      "this work is licensed under a creative commons attribution-sharealike 3.0 unported license. for details see: http://creativecommons.org/licenses/by-sa/3.0/\n",
      "person identification and demographics data are provided by chadwick baseball bureau (http://www.chadwick-bureau.com), from its register of baseball personnel.\n",
      "player performance data for 1871 through 2014 is based on the lahman baseball database, version 2015-01-24, which is copyright (c) 1996-2015 by sean lahman.\n",
      "the tables parks.csv and homegames.csv are based on the game logs and park code table published by retrosheet. this information is available free of charge from and is copyrighted by retrosheet. interested parties may contact retrosheet at http://www.retrosheet.org.\n",
      "context\n",
      "the datasets are from a companion website for the book modeling online auctions, by wolfgang jank and galit shmueli (wiley and sons, isbn: 978-0-470-47565-2, july 2010).\n",
      "content\n",
      "the datasets contain ebay auction information on cartier wristwatches, palm pilot m515 pdas, xbox game consoles, and swarowski beads.\n",
      "auction.csv includes 9 variables:\n",
      "auctionid: unique identifier of an auction\n",
      "bid: the proxy bid placed by a bidder\n",
      "bidtime: the time in days that the bid was placed, from the start of the auction\n",
      "bidder: ebay username of the bidder\n",
      "bidderrate: ebay feedback rating of the bidder\n",
      "openbid: the opening bid set by the seller\n",
      "price: the closing price that the item sold for (equivalent to the second highest bid + an increment)\n",
      "item: auction item\n",
      "auction_type\n",
      "swarovski.csv includes 5 variables:\n",
      "seller\n",
      "bidder\n",
      "weight\n",
      "bidder.volume\n",
      "seller.volume\n",
      "acknowledgements\n",
      "the original dataset can be found here.\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "for each item, what is the relationship between bids, bid time, and the closing price? does this differ by length of the auction, opening bid, or by bidder rating?\n",
      "context\n",
      "the election of donald trump has taken the world by surprise and is fuelling populist movements in europe, e.g. in italy, austria and france. understanding populism and assessing the impact of the “trump effect” on europe is a tremendous challenge, and dalia wants to help pool brainpower to find answers.\n",
      "the goal is to find out where the next wave of populism could hit in europe by comparing and contrasting us and eu voter profiles, opinions of trump vs clinton voters, brexit vs. bremain voters, and future expectations.\n",
      "content\n",
      "expanding dalia’s quarterly \"europulse\" omnibus survey to the usa, dalia has conducted a representative survey with n=11.283 respondents across all 28 eu member countries and n=1.052 respondents from the united states of america. to find out where the next wave of populism could hit europe, dalia’s survey traces commonalities in social and political mindsets (like authoritarianism, prejudice, open-mindedness, xenophobia, etc.), voting behaviour and socio-demographic profiles on both sides of the atlantic.\n",
      "inspiration\n",
      "the sources of our inspirations are many, but to name a few who influenced the way we asked questions: we were very inspired by the 'angry voter' profile laid out by douglas rivers, the influence of political and moral attitudes pointed out by jonathan haidt and the profile of \"america's forgotten working class\" by j. d. vance.\n",
      "researchers should apply the necessary logic, caution and diligence when analysing and interpreting the results.\n",
      "context\n",
      "a referendum was held on the 23 june 2016 to decide whether the united kingdom should remain a member of the european union or leave. approximately 52%, or more than 17 million people, voted to leave the eu. the referendum turnout was 72.2%, with more than 33.5 million votes cast.\n",
      "content\n",
      "the electoral commission published the results of the eu referendum by district and region after the vote. the office of national statistics provided the population demographics by district from the 2011 united kingdom census.\n",
      "content\n",
      "this dataset includes annual county-level pesticide use estimates for 423 pesticides (active ingredients) applied to agricultural crops grown in the contiguous united states. two different methods were used to estimate a range of pesticide use for all states except california. both low and high estimate methods incorporated proprietary surveyed rates for united states department of agriculture crop reporting districts, but the estimates differed in how they treated situations when a district was surveyed and pesticide use was not reported. low estimates assumed zero use in the district for that pesticide; however, high estimates treated the unreported use of pesticides as missing data and estimated the pesticide usage from neighboring locations within the same region.\n",
      "acknowledgements\n",
      "data for the state of california was provided by the 2014 department of pesticide regulation pesticide use report. the 2015 report is not yet available.\n",
      "content\n",
      "the u. s. fire administration tracks and collects information on the causes of on-duty firefighter fatalities that occur in the united states. we conduct an annual analysis to identify specific problems so that we may direct efforts toward finding solutions that will reduce firefighter fatalities in the future.\n",
      "acknowledgements\n",
      "this study of firefighter fatalities would not have been possible without members of individual fire departments, chief fire officers, fire service organizations, the national fire protection association, and the national fallen firefighters foundation.\n",
      "nfl football stats\n",
      "my family has always been serious about fantasy football. i've managed my own team since elementary school. it's a fun reason to talk with each other on a weekly basis for almost half the year.\n",
      "ever since i was in 8th grade i've dreamed of building an ai that could draft players and choose lineups for me. i started off in excel and have since worked my way up to more sophisticated machine learning. the one thing that i've been lacking is really good data, which is why i decided to scrape pro-football-reference.com for all recorded nfl player data.\n",
      "from what i've been able to determine researching, this is the most complete public source of nfl player stats available online. i scraped every nfl player in their database going back to the 1940s. that's over 25,000 players who have played over 1,000,000 football games.\n",
      "the scraper code can be found here. feel free to user, alter, or contribute to the repository.\n",
      "the data was scraped 12/1/17-12/4/17\n",
      "i finished in last place this year in fantasy football, so hopefully this data will help me improve my performance next year. only 8 months until draft day!\n",
      "my ultimate goal is to create an ai that ranks players every week, which could be used to set lineups and draft players. i'm also interested in predicting the winners of games. if you have any ideas or would like to collaborate, please contact me!\n",
      "the data is broken into two parts. there is a players table where each player has been asigned an id and a game stats table that has one entry per game played. these tables can be linked together using the player id.\n",
      "player profile fields\n",
      "player id: the assigned id for the player.\n",
      "name: the player's full name.\n",
      "position: the position the player played abbreviated to two characters. if the player played more than one position, the position field will be a comma-separated list of positions (i.e. \"hb,qb\").\n",
      "height: the height of the player in feet and inches. the data format is -. so 6-5 would be six feet and five inches tall.\n",
      "weight: the weight of the player in pounds.\n",
      "current team: the three-letter code of the team the player plays for. this is null if they are not currently active.\n",
      "birth date: the day, month, and year the player was born. this is null if unknown.\n",
      "birth place: the city, state or city, country the player was born in. this is null if unknown.\n",
      "death date: the day, month, and year the player died. this is null if they are still alive.\n",
      "college: the name of the college they played football at. this is null if they did not play football in college.\n",
      "high school: the city, state or city, country the player went to high school. this is null if the player didn't go to high school or if the school is unknown.\n",
      "draft team: the three letter code of the team that drafted the player. this is null if the player was not drafted.\n",
      "draft position: the draft position number the player was taken. again, null if the player was not drafted.\n",
      "draft round: the round of the draft the player was drafted in. null if the player was not drafted.\n",
      "draft position: the position the player was drafted at as a two-letter code. null if the player was not drafted.\n",
      "draft year: the year the player was drafted. null if the player was not drafted.\n",
      "current salary cap hit: the player's current salary hit for their current team. null if the player is not currently active on a team.\n",
      "hall of fame induction year: the year the player was inducted into the nfl hall of fame. null if the player has not been inducted into the hof yet.\n",
      "game stats fields\n",
      "note that if there are games missing in the season for a player (i.e. the player has logs for games 1, 2, 3, 5, 6,...), then they didn't play in game 4 because of injury, suspension, etc.\n",
      "game info:\n",
      "player id: the assigned id for the player.\n",
      "year: the year the game took place.\n",
      "date: the date the game took place.\n",
      "game number: the number of the game when all games in a season are numbered sequentially.\n",
      "age: the age of the player when the game was played. this is in the format -. so 22-344 would be 22 years and 344 days old.\n",
      "team: the three-letter code of the team the player played for.\n",
      "game location: one of h, a, or n. h=home, a=away, and n=neutral.\n",
      "opponent: the three-letter code of the team the game was played against.\n",
      "player team score: the score of the team the player played for.\n",
      "opponent score: the score of the team the player played against. you can use this field and the last field to determine if the player's team won.\n",
      "passing stats:\n",
      "passing attempts: the number of passes thrown by the player.\n",
      "passing completions: the number of completions thrown by the player.\n",
      "passing yards: the number of passing yards thrown by the player.\n",
      "passing rating: the nfl passer rating for the player in that game.\n",
      "passing touchdowns: the number of passing touchdowns the player threw.\n",
      "passing interceptions: the number of interceptions the player threw.\n",
      "passing sacks: the number of times the player was sacked.\n",
      "passing sacks yards lost: the cumulative yards lost from the player being sacked.\n",
      "rushing stats:\n",
      "rushing attempts: the number of times the the player attempted a rush.\n",
      "rushing yards: the number of yards the player rushed for.\n",
      "rushing touchdowns: the number of touchdowns the player rushed for.\n",
      "receiving stats:\n",
      "receiving targets: the number of times the player was thrown to.\n",
      "receiving receptions: the number of times the player caught a pass thrown to them.\n",
      "receiving yards: the number of yards the player gained through receiving.\n",
      "receiving touchdowns: the number of touchdowns scored through receiving.\n",
      "kick/punt return stats\n",
      "kick return attempts: the number of times the player attempted to return a kick.\n",
      "kick return yards: the cumulative number of yards the player returned kicks for.\n",
      "kick return touchdowns: the number of touchdowns the player scored through kick returns.\n",
      "punt return attempts: the number of times the player attempted to return a punt.\n",
      "punt return yards: the cumulative number of yards the player returned punts for.\n",
      "punt return touchdowns: the number of touchdowns the player scored through punt returns.\n",
      "kick/punt stats\n",
      "point after attempts: the number of pas the player attempted kicking.\n",
      "point after makes: the number of pas the player made.\n",
      "field goal attempts: the number of field goals the player attempted.\n",
      "field goal makes: the number of field goals the player made.\n",
      "defense stats\n",
      "sacks: the number of sacks the player got.\n",
      "tackles: the number of tackles the player got.\n",
      "tackle assists: the number of tackles the player assisted on.\n",
      "interceptions: the number of times the player intercepted the ball.\n",
      "interception yards: the number of yards the player gained after interceptions.\n",
      "interception touchdowns: the number of touchdowns the player scored after interceptions.\n",
      "safeties: the number of safeties the player caused.\n",
      "futute improvements\n",
      "format data in an sqlite database\n",
      "extract team ids to make relating players across teams and games easier\n",
      "resolve game ids to make relating players in a given game easier\n",
      "scrape college data (there are links on the website that shouldn't be too difficult to scrape)\n",
      "figure out another method of scraping some additional data that isn't available on pro-football-reference.com, such as fumbles, passes defended, etc.\n",
      "resolve blocking stats back to lineman based on the team they played for and the qb's sack stats for that game.\n",
      "contributing\n",
      "if you would like to contribute, please feel free to put up a pr or reach out to me with ideas. i would love to collaborate with some fellow football fans on this project.\n",
      "if you're interested in collaborating, the repository can be found here.\n",
      "connect with me\n",
      "if you'd like to collaborate on a project, learn more about me, or just say hi, feel free to contact me using any of the social channels listed below.\n",
      "personal website\n",
      "email\n",
      "linkedin\n",
      "twitter\n",
      "medium\n",
      "quora\n",
      "hackernews\n",
      "reddit\n",
      "kaggle\n",
      "instagram\n",
      "500px\n",
      "context\n",
      "wikipedia, the world's largest encyclopedia, is a crowdsourced open knowledge project and website with millions of individual web pages. this dataset is a grab of the title of every article on wikipedia as of september 20, 2017.\n",
      "content\n",
      "this dataset is a simple newline (\\n) delimited list of article titles. no distinction is made between redirects (like schwarzenegger) and actual article pages (like arnold schwarzenegger).\n",
      "acknowledgements\n",
      "this dataset was created by scraping special:allpages on wikipedia. it was originally shared here.\n",
      "inspiration\n",
      "what are common article title tokens? how do they compare against frequent words in the english language?\n",
      "what is the longest article title? the shortest?\n",
      "what countries are most popular within article titles?\n",
      "context\n",
      "what distinguishes the great from the good, the remembered from the accomplished, and the genius from the merely brilliant? scrapping english wikipedia, joseph philleo has cleaned and compiled a database of more than 8,500 famous mathematicians for the kaggle data science community to analyze and better understand.\n",
      "inspiration\n",
      "what are the common characteristics of famous mathematicians?\n",
      "how old do they live, which fields do they work in, where are they born, and where do they live?\n",
      "can you predict which mathematicians will win a fields medal, join the royal society, or secure tenure at harvard?\n",
      "all bpd data on open baltimore is preliminary data and subject to change. the information presented through open baltimore represents part i victim based crime data. the data do not represent statistics submitted to the fbi's uniform crime report (ucr); therefore any comparisons are strictly prohibited. for further clarification of ucr data, please visit http://www.fbi.gov/about-us/cjis/ucr/ucr. please note that this data is preliminary and subject to change. prior month data is likely to show changes when it is refreshed on a monthly basis. all data is geocoded to the approximate latitude/longitude location of the incident and excludes those records for which an address could not be geocoded. any attempt to match the approximate location of the incident to an exact address is strictly prohibited.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the city of baltimore. you can find the original dataset, which is updated regularly, here.\n",
      "this dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output.\n",
      "the overall scope of emhires is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in europe and not to mime the actual evolution of wind power production in the latest decades. for this reason, the hourly wind power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the wind installed capacity. thus, the installed capacity considered is fixed as the one installed at the end of 2015. for this reason, data from emhires should not be compared with actual power generation data other than referring to the reference year 2015.\n",
      "content\n",
      "the data is available at both the national level and the nuts 2 level. the nuts 2 system divides the eu into 276 statistical units.\n",
      "please see the manual for the technical details of how these estimates were generated.\n",
      "this product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. please don't use it commercially.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the european commission's stetis program. you can find the original dataset here.\n",
      "inspiration\n",
      "how clean is the dataset?\n",
      "what does a typical year look like? one common approach is to stitch together 12 months of raw data, using the 12 most typical months per this iso standard.\n",
      "can you identify more useful geographical areas for this sort of analysis, such as valleys that would share similar wind patterns?\n",
      "if you like\n",
      "if you like this dataset, you might also enjoy: - 30 years of european solar - google's project sunroof data\n",
      "context\n",
      "this corpus consists of truthful and deceptive hotel reviews of 20 chicago hotels. the data is described in two papers according to the sentiment of the review. in particular, we discuss positive sentiment reviews in [1] and negative sentiment reviews in [2]. while we have tried to maintain consistent data preprocessing procedures across the data, there are differences which are explained in more detail in the associated papers. please see those papers for specific details.\n",
      "content\n",
      "this corpus contains:\n",
      "400 truthful positive reviews from tripadvisor (described in [1])\n",
      "400 deceptive positive reviews from mechanical turk (described in [1])\n",
      "400 truthful negative reviews from expedia, hotels.com, orbitz, priceline, tripadvisor and yelp (described in [2])\n",
      "400 deceptive negative reviews from mechanical turk (described in [2])\n",
      "each of the above datasets consist of 20 reviews for each of the 20 most popular chicago hotels (see [1] for more details). the files are named according to the following conventions: directories prefixed with fold correspond to a single fold from the cross-validation experiments reported in [1] and [2].\n",
      "hotels included in this dataset\n",
      "affinia: affinia chicago (now milenorth, a chicago hotel)\n",
      "allegro: hotel allegro chicago - a kimpton hotel\n",
      "amalfi: amalfi hotel chicago\n",
      "ambassador: ambassador east hotel (now public chicago)\n",
      "conrad: conrad chicago\n",
      "fairmont: fairmont chicago millennium park\n",
      "hardrock: hard rock hotel chicago\n",
      "hilton: hilton chicago\n",
      "homewood: homewood suites by hilton chicago downtown\n",
      "hyatt: hyatt regency chicago\n",
      "intercontinental: intercontinental chicago\n",
      "james: james chicago\n",
      "knickerbocker: millennium knickerbocker hotel chicago\n",
      "monaco: hotel monaco chicago - a kimpton hotel\n",
      "omni: omni chicago hotel\n",
      "palmer: the palmer house hilton\n",
      "sheraton: sheraton chicago hotel and towers\n",
      "sofitel: sofitel chicago water tower\n",
      "swissotel: swissotel chicago\n",
      "talbott: the talbott hotel\n",
      "references\n",
      "[1] m. ott, y. choi, c. cardie, and j.t. hancock. 2011. finding deceptive opinion spam by any stretch of the imagination. in proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies.\n",
      "[2] m. ott, c. cardie, and j.t. hancock. 2013. negative deceptive opinion spam. in proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: human language technologies.\n",
      "acknowledgements\n",
      "if you use any of this data in your work, please cite the appropriate associated paper (described above). please direct questions to myle ott (myleott@cs.cornell.edu).\n",
      "context:\n",
      "fraudulent e-mails contain criminally deceptive information, usually with the intent of convincing the recipient to give the sender a large amount of money. perhaps the best known type of fraudulent e-mails is the nigerian letter or “419” fraud.\n",
      "content:\n",
      "this dataset is a collection of more than 2,500 \"nigerian\" fraud letters, dating from 1998 to 2007.\n",
      "these emails are in a single text file. each e-mail has a header which includes the following information:\n",
      "return-path: address the email was sent from\n",
      "x-sieve: the x-sieve host (always cmu-sieve 2.0)\n",
      "message-id: a unique identifier for each message\n",
      "from: the message sender (sometimes blank)\n",
      "reply-to: the email address to which replies will be sent\n",
      "to: the email address to which the e-mail was originally set (some are truncated for anonymity)\n",
      "date: date e-mail was sent\n",
      "subject: subject line of e-mail\n",
      "x-mailer: the platform the e-mail was sent from\n",
      "mime-version: the multipurpose internet mail extension version\n",
      "content-type: type of content & character encoding\n",
      "content-transfer-encoding: encoding in bits\n",
      "x-mime-autoconverted: the type of autoconversion done\n",
      "status: r (read) and o (opened)\n",
      "acknowledgements:\n",
      "if you use this collection of fraud email in your research, please include the following citation in any resulting papers:\n",
      "radev, d. (2008), clair collection of fraud email, acl data and code repository, adcr2008t001, http://aclweb.org/aclwiki\n",
      "inspiration:\n",
      "this dataset contains fraudulent e-mails sent over a period of years. has the language used in fraudulent e-mails changed over time?\n",
      "are there any words or phrases that are particularly common in this type of e-mail? (you might compare it with the enron email corpus, linked below)\n",
      "related datasets:\n",
      "https://www.kaggle.com/wcukierski/enron-email-dataset\n",
      "https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
      "context:\n",
      "when children are born they don’t know any words. by the time they’re three, most children know 200 words or more. these words aren’t randomly selected from the language they’re learning, however. a two-year-old is much more likely to know the word “bottle” than the word “titration”. what words do children learn first, and what qualities do those words have? this dataset was collected to explore this question.\n",
      "content:\n",
      "the main dataset includes information for 732 norwegian words. a second table also includes measures of how frequently each word is used in norwegian, both on the internet (as observed in the norwegian web as corpus dataset) and when an adult is talking to a child. the latter is commonly called “child directed speech” and is abbreviated as “cds”.\n",
      "main data\n",
      "id_cdi_i: word id from the norwegian adaptation of the macarthur-bates communicative development inventories, version 1\n",
      "id_cdi_ii: word id from the norwegian adaptation of the macarthur-bates communicative development inventories, version 2\n",
      "word_nw: the word in norwegian\n",
      "word_cdi: the form of the word found in the norwegian adaptation of the macarthur-bates communicative development inventories\n",
      "translation: the english translation of the norwegian word\n",
      "aoa: how old a child generally is was when they this this word, in months (estimated from the macarthur-bates communicative development inventories)\n",
      "vsoa: how many other words a child generally knows when they learn this word (rounded up to the nearest 10)\n",
      "lex_cat: the specific part of speech of the word\n",
      "broad_lex: the broad part of speech of the word\n",
      "freq: a measure of how commonly this word occurs in norwegian\n",
      "cds_freq: a measure of how commonly this word occurs when a norwegian adult is talking to a norwegian child\n",
      "norwegian cds frequency\n",
      "word_cdi: the word from, as found in the norwegian adaptation of the macarthur-bates communicative development inventories\n",
      "translation: the english translation of the norwegian word\n",
      "freq_nowac: how often this word is used on the internet\n",
      "freq_cds: how often this word is used when talking to children (based on two norwegian childes corpora)\n",
      "acknowledgements:\n",
      "this dataset was collected by pernille hansen. if you use this data, please cite the following paper:\n",
      "hansen (2016). what makes a word easy to acquire? the effects of word class, frequency, imageability and phonological neighbourhood density on lexical development. first language. advance online publication. doi: 10.1177/0142 723716679956 http://dx.doi.org/10.1177/0142723716679956\n",
      "inspiration:\n",
      "how well can you predict which words a child will learn first?\n",
      "are some sounds or letters found more often than chance in words learned early?\n",
      "can you build topic models on earlier-acquired and later-acquired words? which topics are over-represented in words learned very early?\n",
      "context:\n",
      "word vectors, also called word embeddings, are a multi-dimensional representation of words based on which words are used in similar contexts. they can capture some elements of words’ meanings. for example, documents which use a lot of words that are clustered together in a vector space representation are more likely to be on similar topics.\n",
      "word vectors are very computationally intensive to train, and the vectors themselves will vary based on the documents or corpora they are trained on. for these reasons, it is often convenient to use word vectors which have been pre-trained rather than training them from scratch for each project.\n",
      "content:\n",
      "this dataset contains 1,000,653 word embeddings of dimension 300 trained on the spanish billion words corpus. these embeddings were trained using word2vec.\n",
      "parameters for embeddings training:\n",
      "word embeddings were trained using the following parameters:\n",
      "the selected algorithm was the skip-gram model with negative-sampling.\n",
      "the minimum word frequency was 5.\n",
      "the amount of “noise words” for the negative sampling was 20.\n",
      "the 273 most common words were downsampled.\n",
      "the dimension of the final word embedding was 300.\n",
      "the original corpus had the following amount of data:\n",
      "a total of 1420665810 raw words.\n",
      "a total of 46925295 sentences.\n",
      "a total of 3817833 unique tokens.\n",
      "after the skip-gram model was applied, filtering of words with less than 5 occurrences as well as the downsample of the 273 most common words, the following values were obtained:\n",
      "a total of 771508817 raw words.\n",
      "a total of 1000653 unique tokens.\n",
      "acknowledgements:\n",
      "this dataset was created by cristian cardellino. if you use this dataset in your work, please reference the following citation:\n",
      "cristian cardellino: spanish billion words corpus and embeddings (march 2016), http://crscardellino.me/sbwce/\n",
      "inspiration:\n",
      "word vector representations are widely-used in natural language processing tasks.\n",
      "can you improve an existing part of speech tagger in spanish by using word vectors? you might find it helpful to check out this paper.\n",
      "can you use these word embeddings to improve existing parsers for spanish? this paper outlines some approaches for this.\n",
      "there has been quite a bit of work recently on how word embeddings might encode implicit gender bias. for example, this paper show how embeddings can capture stereyoptyes about career fields and gender. however, this recent paper suggests that for languages with grammatical gender (like spanish), grammatical gender is more influential than gender bias. do these word embeddings support that claim?\n",
      "you may also like:\n",
      "glove: global vectors for word representation. pre-trained english word vectors from wikipedia 2014 + gigaword 5\n",
      "20 million word spanish corpus. the spanish language portion of the wikicorpus (v 1.0)\n",
      "context\n",
      "we are building a data set that can be used for building useful reports, understanding the difference between data and information, and multivariate analysis. the data set we are building is similar to that used in several academic reports and what may be found in erp hr subsystems.\n",
      "we will update the sample data set as we gain a better understanding of the data elements using the calculations that exist in scholarly journals. specifically, we will use the correlation tables to rebuild the data sets.\n",
      "content\n",
      "the fields represent a fictitious data set where a survey was taken and actual employee metrics exist for a particular organization. none of this data is real.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "prabhjot singh contributed a portion of the data (the columns on the right before the survey data was added). https://www.kaggle.com/prabhjotindia https://www.kaggle.com/prabhjotindia/visualizing-employee-data/data\n",
      "about this dataset why are our best and most experienced employees leaving prematurely? have fun with this database and try to predict which valuable employees will leave next. fields in the dataset include:\n",
      "satisfaction level last evaluation number of projects average monthly hours time spent at the company whether they have had a work accident whether they have had a promotion in the last 5 years departments salary\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "stock market data -- and particularly intraday price data -- can be very expensive to buy. to help more people gain access to it, here i provide daily as well as intraday price and volume data for all u.s.-based stocks and etfs trading on the nyse, nasdaq, and nyse mkt.\n",
      "content\n",
      "the dataset (last updated 12/06/2017) is presented in csv format as follows:\n",
      "intraday data: date,time,open,high,low,close,volume,openint\n",
      "daily data: date,open,high,low,close,volume,openint\n",
      "acknowledgements\n",
      "the dataset belongs to me. i’m sharing it here for free. you may do with it as you wish.\n",
      "inspiration\n",
      "many have tried, but most have failed, to predict the stock market's ups and downs. can you do any better?\n",
      "context\n",
      "i collected this data from incubators and accelerators to find out what they have been talking about in 2017.\n",
      "content\n",
      "the data contains the twitter usernames of various organizations and tweets for the year 2017 collected on 28th dec 2017.\n",
      "acknowledgements\n",
      "much appreciation to @emmanuelkens for helping in thinking through this\n",
      "inspiration\n",
      "i am very curious to find out what the various organizations have been talking about in 2017. i would also like to find out the most popular organization by tweets and engagement. i am also curious to find out if there is any relationship between the number of retweets a tweet gets and the time of day it was posted!\n",
      "context\n",
      "this database is managed by the us environmental protection agency and contains information reported annually by some industry groups as well as federal facilities. each year, companies across a wide range of industries (including chemical, mining, paper, oil and gas industries) that produce more than 25,000 pounds or handle more than 10,000 pounds of a listed toxic chemical must report it to the tri. the tri threshold was initially set at 75,000 pounds annually. if the company treats, recycles, disposes, or releases more than 500 pounds of that chemical into the environment (as opposed to just handling it), then they must provide a detailed inventory of that chemical's inventory.\n",
      "content\n",
      "there are roughly 100 columns in this dataset; please see the tri_basic_data_file_format_v15.pdf for details. you may also wish to consult factors_to_consider_6.15.15_final.pdf for general background about interpreting the data.\n",
      "i've merged all of the tri basic data files into a single large csv. you will probably need to process it in batches or use a tool like dask to stay within kernel memory limits.\n",
      "please note that the 2016 data remains preliminary at the time of this release.\n",
      "acknowledgements\n",
      "this dataset was released by the us epa. you can find the original dataset, more detailed versions of the data, and a great deal of background information here: https://www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools\n",
      "inspiration\n",
      "the epa runs an annual university contest. their list of previous winners contains a lot of great ideas that people have had for this dataset in the past. the 2017 competition is already over, but you can find the rules here.\n",
      "context\n",
      "web data extraction or web scraping can be a great business tool for trend spotting via media monitoring. essentially, leading media outlets can be tracked to unveil the top buzzwords and the number of mentions companies (including their products) garner over specific time period. we wanted to apply this method to understand the tech landscape and its coverage in 2017. hence, we deployed promptcloud’s in-house web crawler to extract the article titles from two popular outlets (techcrunch and venturebeat) and performed text mining on the dataset to uncover the top buzzwords, companies and products.\n",
      "content\n",
      "the dataset contains following 3 fields:\n",
      "url\n",
      "title\n",
      "date of publication\n",
      "acknowledgements\n",
      "this dataset was created by using promptcloud's in-house web scraping service.\n",
      "inspiration\n",
      "initial analysis can be found here. it includes the following findings:\n",
      "top companies/products that were covered by media over the year\n",
      "top tech trends over the year\n",
      "the freight analysis framework (faf) integrates data from a variety of sources to create a comprehensive picture of freight movement among states and major metropolitan areas by all modes of transportation. starting with data from the 2012 commodity flow survey (cfs) and international trade data from the census bureau, faf incorporates data from agriculture, extraction, utility, construction, service, and other sectors. faf version 4 (faf4) provides estimates for tonnage (in thousand tons) and value (in million dollars) by regions of origin and destination, commodity type, and mode. data are available for the base year of 2012, the recent years of 2013 - 2015, and forecasts from 2020 through 2045 in 5-year intervals.\n",
      "inspiration\n",
      "this dataset should be great for map-based visualizations.\n",
      "context\n",
      "the online job market is a good indicator of overall demand for labor in an economy. this dataset consists of 19,000 job postings from 2004 to 2015 posted on careercenter, an armenian human resource portal. since postings are text documents and tend to have similar structures, text mining can be used to extract features like posting date, job title, company name, job description, salary, and more. postings that had no structure or were not job-related were removed. the data was originally scraped from a yahoo! mailing group.\n",
      "inspiration\n",
      "students, job seekers, employers, career advisors, policymakers, and curriculum developers use online job postings to explore the nature of today's dynamic labor market. this dataset can be used to:\n",
      "understand the demand for certain professions, job titles, or industries\n",
      "identify skills that are most frequently required by employers, and how the distribution of necessary skills changes over time\n",
      "help education providers with curriculum development\n",
      "acknowledgements\n",
      "the data collection and initial research were funded by the american university of armenia’s research grant (2015).\n",
      "habet madoyan, ceo at datamotus, compiled this dataset and has granted us permission to republish. the republished dataset is identical to the original dataset, which can be found here. datamotus also published a report detailing the text mining techniques used, plus analyses and visualizations of the data.\n",
      "content\n",
      "this dataset documents all united nations general assembly votes since its establishment in 1946. the data is broken into three different files: the first lists each un resolution, subject, and vote records; the second records individual member state votes per resolution; and the third provides an annual summary of member state voting records with affinity scores and an ideal point estimate in relation to the united states.\n",
      "acknowledgements\n",
      "the un general assembly voting data was compiled and published by professor erik voeten of georgetown university.\n",
      "context\n",
      "what did the expansion of the london underground, the world’s first underground railway which opened in 1863, look like? what about the transportation system in your home city? citylines collects data on transportation lines across the world so you can answer questions like these and more.\n",
      "content\n",
      "this dataset, originally shared and updated here, includes transportation line data from a number of cities from around the world including london, berlin, mexico city, barcelona, washington d.c., and others covering many thousands of kilometers of lines.\n",
      "inspiration\n",
      "you can explore geometries to generate maps and even see how lines have changed over time based on historical records. want to include shapefiles with your analysis? simply publish a shapefile dataset here and then create a new kernel (r or python script/notebook), adding your shapefile as an additional datasource.\n",
      "the financial statement data sets below provide numeric information from the face financials of all financial statements. this data is extracted from exhibits to corporate financial reports filed with the commission using extensible business reporting language (xbrl). as compared to the more extensive financial statement and notes data sets, which provide the numeric and narrative disclosures from all financial statements and their notes, the financial statement data sets are more compact.\n",
      "the information is presented without change from the \"as filed\" financial reports submitted by each registrant. the data is presented in a flattened format to help users analyze and compare corporate disclosure information over time and across registrants. the data sets also contain additional fields including a company's standard industrial classification to facilitate the data's use.\n",
      "content\n",
      "each quarter's data is stored as a json of the original text files. this was necessary to limit the overall number of files. the num.txt file will likely be of most interest.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the sec. you can find the original dataset, which is updated quarterly, here.\n",
      "we are updating this dataset everymonth. access updated data for every month here\n",
      "context\n",
      "the dataset is proprietary data of yun solutions, collected from beta testing phase. the dataset contains sensor and obd data for over 4 months and around 30 vehicles.\n",
      "content\n",
      "the dataset contains vehicle telematics and driving data. metadata is explained in the file description on the data tab.\n",
      "acknowledgements\n",
      "we look forward to analysis on the data.\n",
      "inspiration\n",
      "we want to make this data accessible for learning and analysis.\n",
      "context\n",
      "most countries of the world define poverty as a lack of money. yet poor people themselves consider their experience of poverty much more broadly. a person who is poor can suffer from multiple disadvantages at the same time – for example they may have poor health or malnutrition, a lack of clean water or electricity, poor quality of work or little schooling. focusing on one factor alone, such as income, is not enough to capture the true reality of poverty.\n",
      "multidimensional poverty measures can be used to create a more comprehensive picture. they reveal who is poor and how they are poor – the range of different disadvantages they experience. as well as providing a headline measure of poverty, multidimensional measures can be broken down to reveal the poverty level in different areas of a country, and among different sub-groups of people.\n",
      "content\n",
      "ophi researchers apply the af method and related multidimensional measures to a range of different countries and contexts. their analyses span a number of different topics, such as changes in multidimensional poverty over time, comparisons in rural and urban poverty, and inequality among the poor. for more information on ophi’s research, see our working paper series and research briefings.\n",
      "ophi also calculates the global multidimensional poverty index mpi, which has been published since 2010 in the united nations development programme’s human development report. the global mpi is an internationally-comparable measure of acute poverty covering more than 100 developing countries. it is updated by ophi twice a year and constructed using the af method.\n",
      "the alkire foster (af) method is a way of measuring multidimensional poverty developed by ophi’s sabina alkire and james foster. building on the foster-greer-thorbecke poverty measures, it involves counting the different types of deprivation that individuals experience at the same time, such as a lack of education or employment, or poor health or living standards. these deprivation profiles are analysed to identify who is poor, and then used to construct a multidimensional index of poverty (mpi). for free online video guides on how to use the af method, see ophi’s online training portal.\n",
      "to identify the poor, the af method counts the overlapping or simultaneous deprivations that a person or household experiences in different indicators of poverty. the indicators may be equally weighted or take different weights. people are identified as multidimensionally poor if the weighted sum of their deprivations is greater than or equal to a poverty cut off – such as 20%, 30% or 50% of all deprivations.\n",
      "it is a flexible approach which can be tailored to a variety of situations by selecting different dimensions (e.g. education), indicators of poverty within each dimension (e.g. how many years schooling a person has) and poverty cut offs (e.g. a person with fewer than five years of education is considered deprived).\n",
      "the most common way of measuring poverty is to calculate the percentage of the population who are poor, known as the headcount ratio (h). having identified who is poor, the af method generates a unique class of poverty measures (mα) that goes beyond the simple headcount ratio. three measures in this class are of high importance:\n",
      "adjusted headcount ratio (m0), otherwise known as the mpi: this measure reflects both the incidence of poverty (the percentage of the population who are poor) and the intensity of poverty (the percentage of deprivations suffered by each person or household on average). m0 is calculated by multiplying the incidence (h) by the intensity (a). m0 = h x a.\n",
      "find out about other ways the af method is used in research and policy.\n",
      "additional data here.\n",
      "acknowledgements\n",
      "alkire, s. and robles, g. (2017). “multidimensional poverty index summer 2017: brief methodological note and results.” ophi methodological note 44, university of oxford.\n",
      "alkire, s. and santos, m. e. (2010). “acute multidimensional poverty: a new index for developing countries.” ophi working papers 38, university of oxford.\n",
      "alkire, s. jindra, c. robles, g. and vaz, a. (2017). ‘multidimensional poverty index – summer 2017: brief methodological note and results’. ophi mpi methodological notes no. 44, oxford poverty and human development initiative, university of oxford.\n",
      "inspiration\n",
      "which countries exhibit the largest subnational disparities in mpi?\n",
      "which countries have high per-capita incomes yet still rank highly in mpi?\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "when india got independence from british in 1947 the literacy rate was 12.2% and as per the recent census 2011 it is 74.0%. although it looks an accomplishment, still many people are there without access to education.\n",
      "it would be interesting to know the current status of the indian education system.\n",
      "content\n",
      "this dataset contains district and state wise indian primary and secondary school education data for 2015-16.\n",
      "granularity: annual\n",
      "list of files:\n",
      "2015_16_districtwise.csv ( 680 observations and 819 variables )\n",
      "2015_16_statewise_elementary.csv ( 36 observations and 816 variables )\n",
      "2015_16_statewise_secondary.csv ( 36 observations and 630 variables )\n",
      "acknowledgements\n",
      "ministry of human resource development (dise) has shared the dataset here and also published some reports.\n",
      "inspiration\n",
      "this dataset provides the complete information about primary and secondary education. there are many inferences can be made from this dataset. there are few things i would like to understand from this dataset.\n",
      "drop out ratio in primary and secondary education. (govt. has made law that every child under age 14 should get free compulsary education.)\n",
      "various factors affecting examination results of the students.\n",
      "what are all the factors that makes the difference (in literacy rate) between kerala and bihar?\n",
      "what could be done to improve the female literacy rate and literacy rate in rural area?\n",
      "context\n",
      "information reproduced from the national archives:\n",
      "\"the vietnam conflict extract data file of the defense casualty analysis system (dcas) extract files contains records of 58,220 u.s. military fatal casualties of the vietnam war. these records were transferred into the custody of the national archives and records administration in 2008. the earliest casualty record contains a date of death of june 8, 1956, and the most recent casualty record contains a date of death of may 28, 2006. the defense casualty analysis system extract files were created by the defense manpower data center (dmdc) of the office of the secretary of defense. the records correspond to the vietnam conflict statistics on the dmdc web site, which is accessible online at https://www.dmdc.osd.mil/dcas/pages/main.xhtml .\n",
      "a full series description for the defense casualty analysis system (dcas) extract files is accessible online via the national archives catalog under the national archives identifier 2163536. the vietnam conflict extract data file is also accessible for direct download via the national archives catalog file-level description, national archives identifier 2240992. \"\n",
      "content\n",
      "the raw data files have been cleaned and labelled as best as i can with reference to the accompanying supplemental code lists. names and id numbers have been removed out of respect and to provide anonymity.\n",
      "acknowledgements\n",
      "data provided by the u.s. national archives and records administration.\n",
      "raw data can be accessed via the following link: https://catalog.archives.gov/id/2240992\n",
      "inspiration\n",
      "by cleaning the data i hope to give wider access to this resource.\n",
      "context\n",
      "this dataset was obtained from facebook groups as part of my postgraduate thesis. the objective of the thesis was to extract posts from groups related to rare diseases and compare them with the spanish association of rare diseases (feder). if you want to use this open dataset or the code you should cite our paper:\n",
      "reguera, n., subirats, l., armayones, m. mining facebook data of people with rare diseases. ieee computer-based medical systems (ieee cbms 2017), thessaloniki, greece, 22-24th june, 2017.\n",
      "content\n",
      "the file contains information 3917 records from 5 facebook groups and was extracted using netvizz. the posts were generated since each group started (as far as 2009) until the 26/11/2016.\n",
      "the content is as follows:\n",
      "type: facebook's post classification (e.g. photo, status, etc.)\n",
      "by: either\"post_page_pageid\" (post by page) or \"post_user_pageid\" (post by user);\n",
      "post_id: id of the post;\n",
      "post_link: direct link to the post;\n",
      "post_message: text of the post;\n",
      "picture: the picture scraped from any link included with the post;\n",
      "full_picture: the picture scraped from any link included with the post (full size);\n",
      "link: link url (if the post points to external content);\n",
      "link_domain: domain name of link;\n",
      "post_published: publishing date\n",
      "post_published_unix: publishing date as unix timestamp (for easy conversion and ranking);\n",
      "post_published_sql: publishing date in sql format (some analysis tools prefer this);\n",
      "likes_count_fb: facebook provided like count for posts;\n",
      "comments_count_fb: facebook provided comment count for posts;\n",
      "reactions_count_fb: facebook provided reactions count for posts (includes likes);\n",
      "shares_count_fb: facebook provided share count for posts;\n",
      "engagement_fb: sum of comment, reaction, and share counts;\n",
      "acknowledgements\n",
      "i would like to thank my thesis directors, who guided me through all the process: laia subirats maté and manuel armayones ruiz.\n",
      "inspiration\n",
      "during my research (code available on https://github.com/natt77/uoc---tfp) several insights were found on the relation between the information posted on the gropus and the information in feder. text analytics was performed together with sentiment analysis. it would be interesting to deepen the analysis, create models to predict engagement or any other action that can help improving the life quality of people with rare diseases.\n",
      "content\n",
      "the data comes from the vancouver open data catalogue. it was extracted on 2017-07-18 and it contains 530,652 records from 2003-01-01 to 2017-07-13. the original data set contains coordinates in utm zone 10 (columns x and y). i also included latitude and longitude, which i converted using this spreadsheet that can be found here.\n",
      "there's also a google trends data that shows how often a search-term is entered relative to the total search-volume. from google trends:\n",
      "\"numbers represent search interest relative to the highest point on the chart for the given region and time. a value of 100 is the peak popularity for the term. a value of 50 means that the term is half as popular. likewise a score of 0 means the term was less than 1% as popular as the peak.\"\n",
      "original data for search term \"crime\" location british columbia: https://trends.google.com/trends/explore?date=2004-01-01%202017-06-30&geo=ca-bc&q=crime\n",
      "acknowledgements\n",
      "photo by charles de jesus [cc by 3.0 (http://creativecommons.org/licenses/by/3.0)], via wikimedia commons\n",
      "context\n",
      "an interview was videotaped to analyze the facial expressions of emotion. this interview lasts for 21:30 minutes.\n",
      "content\n",
      "an interview with a duration of 21:30 minutes was videotaped. emotional facial expressions were analyzed every 0.12 seconds with facereader software. emotions were neutral, joy, fear, anger, surprise, fear and contemp, positive-negative valences, arousal, degrees of head direction, facial action units, among others.\n",
      "acknowledgements\n",
      "acknowledgements to the clinic of borderline personality disorder for contributing the material, also to the laboratory of chronoecology and human ethology.\n",
      "inspiration\n",
      "how does the direction of the gaze relate to the expressions of emotions?\n",
      "from what score of any emotion is considered open, closed and / or neutral in the right eye, left eye, right eyebrow and left eyebrow?\n",
      "in addition to descriptive statistical analyzes, what other analyzes can be performed?\n",
      "context\n",
      "rio de janeiro is one of the most beautiful and famous city in the world. unfortunately, it's also one of the most dangerous. for the last years, in a scenario of economical and political crisis in brazil, the state of rio de janeiro was one of the most affected. since 2006, the instituto de segurança pública do rio de janeiro (institue of public security of rio de janeiro state) publishes reports of each police station.\n",
      "content\n",
      "three datasets are available: basedpevolucaomensalcisp - monthly evolution of statistics by police station populacaoevolucaomensalcisp - monthly evolution of population covered by police station delegacias - info about each police station\n",
      "most of the data are in brazilian portuguese because it was extracted directly from government sites.\n",
      "acknowledgements\n",
      "this dataset is provided by the instituto de segurança pública. delegacias.csv was compiled by myself.\n",
      "inspiration\n",
      "what is the most unsafe city in rio de janeiro state? and the safest? which events can be correlated with the numbers in dataset? (elections, crisis...) how crime correlates with population?\n",
      "this dataset was created to as part of an ongoing research on what information about the human mind could be possibly hidden in word-word associations.\n",
      "inspiration\n",
      "here are a few questions you might try to answer with this dataset:\n",
      "how well can we classify word pairs as originating from a specific online community?\n",
      "what are the properties of the graph/network of word associations?\n",
      "what are frequently used words? are there differences among communities?\n",
      "content\n",
      "the data was scraped from 10 public internet forums. the data is anonymous (all usernames are converted to unique user ids) and cleaned[script].\n",
      "most topics were still active at the time of scraping (june 2017), thus rescraping will result in a (slightly) bigger dataset.\n",
      "the dataset contains 4 columns, {author, word1, word2, source}, where author is the person who wrote the second word as reaction to the first word. a word can also be a phrase or (in some cases) a sentence.\n",
      "the national oceanic and atmospheric administration (noaa) national status and trends (ns&t) mussel watch program is a contaminant monitoring program that started in 1986. it is the longest running continuous contaminant monitoring program of its kind in the united states. mussel watch monitors the concentration of contaminants in bivalves (mussels and oysters) and sediments in the coastal waters of the u.s., including the great lakes, to monitor bivalve health and by extension the health of their local and regional environment.\n",
      "mussel watch consults with experts to determine appropriate contaminants to monitor; these include dichlorodiphenyltrichloroethane (ddt), polycyclic aromatic hydrocarbons (pahs), and polychlorinated biphenyls (pcbs). as of 2008, mussel watch monitors approximately 140 analytes. in addition to the effects of contaminants, mussel watch is able to assess the effects of natural disasters, such as the 2005 hurricane katrina, and environmental disasters, such as the 2010 deepwater horizon oil spill. data collected by mussel watch can also be used to monitor the effectiveness of coastal remediation. the mussel watch program utilized its 20 years of monitoring data to effectively analyze the impacts of hurricane katrina and has affected regulatory decisions based on the data it has collected on bivalve parasites.\n",
      "you can find additional details about the history of the program here.\n",
      "data notes\n",
      "this version has been consolidated and lightly cleaned from its original format.\n",
      "it was not possible to acquire data for all sites in mussel watch while preparing the dataset.\n",
      "the pdf manuals are technically for specific sites and may not map perfectly to the data here. you can find manuals specific to each site here if need be.\n",
      "acknowledgements\n",
      "this dataset is the result of the work of generations of scientists working for noaa. you can find the original data here.\n",
      "context\n",
      "the leipzig corpora collection presents corpora in different languages using the same format and comparable sources. all data are available as plain text files and can be imported into a mysql database by using the provided import script. they are intended both for scientific use by corpus linguists as well as for applications such as knowledge extraction programs.\n",
      "content\n",
      "this dataset contains 3 million sentences taken from newspaper texts in 2015. non-sentences and foreign language material was removed. in addition to the sentences themselves, this dataset contains information on the frequency of each word. more information about the format and content of these files can be found here.\n",
      "the corpora are automatically collected from carefully selected public sources without considering in detail the content of the contained text. no responsibility is taken for the content of the data. in particular, the views and opinions expressed in specific parts of the data remain exclusively with the authors.\n",
      "acknowledgements\n",
      "this dataset is released under a cc-by 4.0 license. if you use this dataset in your work, please cite the following paper:\n",
      "d. goldhahn, t. eckart & u. quasthoff: building large monolingual dictionaries at the leipzig corpora collection: from 100 to 200 languages. in: proceedings of the 8th international language resources and evaluation (lrec'12), 2012\n",
      "context:\n",
      "pubs, or public houses, are popular traditional british gathering places where alcohol and food is served.\n",
      "content:\n",
      "this dataset includes information on 51,566 pubs. this dataset contains the following columns:\n",
      "fsa_id (int): food standard agency's id for this pub.\n",
      "name (string)l name of the pub\n",
      "address (string): address fields separated by commas.\n",
      "postcode (string): postcode of the pub.\n",
      "easting (int)\n",
      "northing (int)\n",
      "latitude (decimal)\n",
      "longitude (decimal)\n",
      "local_authority (string): local authority this pub falls under.\n",
      "acknowledgements:\n",
      "the data was derived from the food standard agency's food hygiene ratings and the ons postcode directory. the data is licensed under the open government licence. (see the included .html file.)\n",
      "inspiration:\n",
      "you could use this data as the basis for a real-life travelling salesman problem and plan the world’s longest pub crawl.\n",
      "context:\n",
      "“russian is an east slavic language and an official language in russia, belarus, kazakhstan, kyrgyzstan and many minor or unrecognised territories. it is an unofficial but widely spoken language in ukraine and latvia, and to a lesser extent, the other countries that were once constituent republics of the soviet union and former participants of the eastern bloc.” -- “russian language” on wikipedia\n",
      "russian has around 150 million native speakers and 110 million non-native speakers. russian in written in cyrillic script. this dataset is a morphologically, syntactically and semantically annotated corpus of texts in russian, fully accessible to researchers and edited by users.\n",
      "content:\n",
      "this dataset is encoded in utf-8. there are two files included in this dataset: the corpus and the dictionary. the corpus is in .json format, while the dictionary is in plain text.\n",
      "dictionary\n",
      "in the dictionary, each entry is a lemma, presented with all of its tagged derivations. the tags depend on the part of speech of the lemma. some examples are:\n",
      "nouns: part of speech, animacy, gender & number, case\n",
      "verbs: part of speech, aspect, transitivity, gender & number, person, tense, mood\n",
      "adjectives: part of speech (adjf), gender, number, case\n",
      "a python script to convert the tags in this corpus to this set more commonly used in english-language linguistics can be found here.\n",
      "sample dictionary entries:\n",
      "1\n",
      "ёж  noun,anim,masc sing,nomn\n",
      "ежа noun,anim,masc sing,gent\n",
      "ежу noun,anim,masc sing,datv\n",
      "ежа noun,anim,masc sing,accs\n",
      "ежом    noun,anim,masc sing,ablt\n",
      "еже noun,anim,masc sing,loct\n",
      "ежи noun,anim,masc plur,nomn\n",
      "ежей    noun,anim,masc plur,gent\n",
      "ежам    noun,anim,masc plur,datv\n",
      "\n",
      "41\n",
      "ёрничаю verb,impf,intr sing,1per,pres,indc\n",
      "ёрничаем    verb,impf,intr plur,1per,pres,indc\n",
      "ёрничаешь   verb,impf,intr sing,2per,pres,indc\n",
      "ёрничаете   verb,impf,intr plur,2per,pres,indc\n",
      "ёрничает    verb,impf,intr sing,3per,pres,indc\n",
      "ёрничают    verb,impf,intr plur,3per,pres,indc\n",
      "ёрничал verb,impf,intr masc,sing,past,indc\n",
      "ёрничала    verb,impf,intr femn,sing,past,indc\n",
      "ёрничало    verb,impf,intr neut,sing,past,indc\n",
      "ёрничали    verb,impf,intr plur,past,indc\n",
      "ёрничай verb,impf,intr sing,impr,excl\n",
      "ёрничайте   verb,impf,intr plur,impr,excl\n",
      "corpous\n",
      "in this corpus, each word has been grammatically tagged. you can access individual tokens using the following general path:\n",
      "json > text > paragraphs > paragraph > [paragraph number] > sentence > [sentence number] > tokens > [token number]\n",
      "each token has: * a unique id number (@id) * the text of the token (@text) * information on the lemma (under “l”), including the id number of the lemma as found in the dictionary\n",
      "you can see an example of the token portion of the .json structure below:\n",
      "             {\n",
      "                \"@id\": 1714292,\n",
      "                \"@text\": \"сват\",\n",
      "                \"tfr\": {\n",
      "                  \"@t\": \"сват\",\n",
      "                  \"@rev_id\": 3754311,\n",
      "                  \"v\": {\n",
      "                    \"l\": {\n",
      "                      \"@id\": 314741,\n",
      "                      \"@t\": \"сват\",\n",
      "                      \"g\": [\n",
      "                        {\n",
      "                          \"@v\": \"noun\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"@v\": \"anim\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"@v\": \"masc\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"@v\": \"sing\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"@v\": \"nomn\"\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  }\n",
      "            }\n",
      "          }\n",
      "acknowledgements:\n",
      "this dataset was collected and annotated by, among others, svetlana alekseeva, anastasia bodrova, victor bocharov, dmitry granovsky, irina krylova, maria nikolaeva, catherine protopopova, alexander chuchunkov, anastasia shimorina, vasily alekseev, natalia ostapuk, maria stepanova and alexey surikov. the code used to collect and clean this data is available online.\n",
      "it is reproduced here under a cc-by-sa license.\n",
      "more information on this corpus and its most recent version can be found here (in russian.)\n",
      "context:\n",
      "traffic management is a critical concern for policymakers, and a fascinating data question. this ~2gb dataset contains daily volumes of traffic, binned by hour. information on flow direction and sensor placement is also included.\n",
      "content:\n",
      "two datasets are included:\n",
      "dot_traffic_2015.txt.gz\n",
      "daily observation of traffic volume, divided into 24 hourly bins\n",
      "station_id, location information (geographical place), traffic flow direction, and type of road\n",
      "dot_traffic_stations_2015.txt.gz\n",
      "deeper location and historical data on individual observation stations, cross-referenced by station_id\n",
      "acknowledgements:\n",
      "this dataset was compiled by the us department of transportation and available on google bigquery\n",
      "inspiration:\n",
      "where are the heaviest traffic volumes? by time of day? by type of road?\n",
      "any interesting seasonal patterns to traffic volumes?\n",
      "context\n",
      "this dataset is a list of people who have been involved in an accident in the city of barcelona (spain) from year 2010 till 2016. this data is managed by the police in the city of barcelona and includes several information described below.\n",
      "content\n",
      "this dataset is composed by 7 files, each one containing between 10k-12k lines.\n",
      "every row contains several information, like the type of injury (slightly wounded, serious injuries or death). it includes a description of the person (driver, passenger or pedestrian), sex, age, location, etc...\n",
      "important: this dataset is uploaded as it is, so it's possible that some data in some rows is missing/not correct.\n",
      "description of each column:\n",
      "número d'expedient: case file number\n",
      "codi districte: district code where the accident was. barcelona is divided in several districts\n",
      "nom districte: name of the district\n",
      "codi barri: hood code where the accident was. every district in barcelona has several hoods\n",
      "nom barri: name of the hood\n",
      "codi carrer: street code (every street has a code)\n",
      "nom carrer: name of the street\n",
      "num postal caption: postal number of the street\n",
      "descripció dia setmana: day of the week in text (written in catalan)\n",
      "dia setmana: shortcode of the previous field (also in catalan)\n",
      "descripció tipus dia: description of the type of the day, it can be \"labor\" or \"festive\" (also in catalan)\n",
      "nk any: number of the year\n",
      "mes de any: number of the month (1-12)\n",
      "nom mes: name of the month (in catalan)\n",
      "dia de mes: day of the month\n",
      "descripció torn: type of round of the police. it can be \"matí\" (morning), \"tarda\" (evening) or \"nit\" (night)\n",
      "hora de dia: hour of the day (0-23)\n",
      "descripció causa vianant: text in catalan. describes the accident in case the victim is a pedestrian. if not, it says \"no és causa del vianant\"\n",
      "desc. tipus vehicle implicat: type of vehicle in the accident. also in catalan.\n",
      "descripció sexe: sex of the victim. \"home\" means man, \"dona\" means woman.\n",
      "descripció tipus persona: type of role in the accident. it describes if the victim is the pilot (conductor ), passenger (passatger), pedestrian (vianant)\n",
      "edat: age of the victim\n",
      "descripció victimització: type of injury in catalan (slightly wounded (ferit lleu), serious injuries (ferit greu) or death (mort))\n",
      "coordenada utm (y): utm coordinate y\n",
      "coordenada utm (x): utm coordinate x\n",
      "as you can see, some columns could be removed and we wouldn't loose information. my experience working with these files tells me that some rows have no correct data or no data at all. so, be careful!\n",
      "acknowledgements\n",
      "this data can be found in \"open data bcn - barcelona's city hall open data service\", which is the owner of the csv files.\n",
      "inspiration\n",
      "i have uploaded this information here because i believe that data should be shared with everybody! so, do your own \"research\" and share it also! i'm always happy to get some feedback and help each other!\n",
      "context\n",
      "achieving the appropriate balance of intellectual property (ip) protection through patent litigation is critical to economic growth. examining the interplay between us patent law and economic effect is of great interest to many stakeholders. published in march 2017, this dataset is the most comprehensive public body of information on uspto patent litigation.\n",
      "content\n",
      "the dataset covers over 74k cases across 52 years. five different files (attorneys.csv, cases.csv, documents.csv, names.csv, pacer_cases.csv) detail the litigating parties, their attorneys, results, locations, and dates. the large documents.csv file covers more than 5 million relevant documents (a tool like split might be your friend here).\n",
      "acknowledgements\n",
      "this data was collected by the office of the chief economist at the uspto. data was collected from both the public access to court electronics records (pacer), as well as recap, an independent pacer repository. further documentation available via this paper.\n",
      "inspiration\n",
      "patent litigation is a tug of war between patent holders, competing parties using similar ip, and government policy. which industries see the most litigation? any notable changes over time? is there a positive (or negative) correlation between litigation, and a company’s economic fortunes?\n",
      "license\n",
      "public domain mark 1.0 also see source.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 4.7 million job listings) that was created by extracting data from monster.com, a leading job board.\n",
      "content\n",
      "this dataset has following fields:\n",
      "country\n",
      "country_code\n",
      "date_added\n",
      "has_expired - always false.\n",
      "job_description - the primary field for this dataset, containing the bulk of the information on what the job is about.\n",
      "job_title\n",
      "job_type - the type of tasks and skills involved in the job. for example, \"management\".\n",
      "location\n",
      "organization\n",
      "page_url\n",
      "salary\n",
      "sector - the industry sector the job is in. for example, \"medical services\".\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "what kinds of jobs titles correspond with what kinds of wages?\n",
      "what can you learn about the moster.com-based us job market based on analyzing the contents of the job descriptions?\n",
      "how do job descriptions different between different industry sectors?\n",
      "context:\n",
      "starbucks is an american coffee chain founded in seattle. it serves both beverages and food.\n",
      "content:\n",
      "this dataset includes the nutritional information for starbucks’ food and drink menu items. all nutritional information for drinks are for a 12oz serving size.\n",
      "acknowledgements:\n",
      "food composition data is in the public domain, but product names marked with ® or ™ remain the registered trademarks of starbucks.\n",
      "inspiration:\n",
      "can you train a markov chain to generate new starbucks drink or food items?\n",
      "can you design an easy-to-interpret visualization for the nutrition of each item?\n",
      "how to starbucks menu items compare to mcdonald’s menu items (see link to dataset below) in terms of nutrition?\n",
      "you may also like:\n",
      "nutrition facts for mcdonald's menu\n",
      "starbucks locations worldwide\n",
      "context\n",
      "the carbon dioxide record from mauna loa observatory, known as the “keeling curve,” is the world’s longest unbroken record of atmospheric carbon dioxide concentrations. scientists make atmospheric measurements in remote locations to sample air that is representative of a large volume of earth’s atmosphere and relatively free from local influences.\n",
      "content\n",
      "this dataset includes a monthly observation of atmospheric carbon dioxide (or co2) concentrations from the mauna loa observatory (hawaii) at a latitude of 19.5, longitude of -155.6, and elevation of 3397 meters.\n",
      "columns 1-3: provide the date in the following redundant formats: year, month and decimal date\n",
      "column 4: monthly co2 concentrations in parts per million (ppm) measured on the 08a calibration scale and collected at 24:00 hours on the fifteenth of each month.\n",
      "column 5: the fifth column provides the same data after a seasonal adjustment, which involves subtracting from the data a 4-harmonic fit with a linear gain factor to remove the seasonal cycle from carbon dioxide measurements\n",
      "column 6: the sixth column provides the data with noise removed, generated from a stiff cubic spline function plus 4-harmonic functions with linear gain\n",
      "column 7: the seventh column is the same data with the seasonal cycle removed.\n",
      "acknowledgements\n",
      "the carbon dioxide data was collected and published by the university of california's scripps institution of oceanography under the supervision of charles david keeling with support from the us department of energy, earth networks, and the national science foundation.\n",
      "inspiration\n",
      "how have atmospheric carbon dioxide levels changed in the past sixty years? how do carbon dioxide concentrations change seasonally? what do you think causes this seasonal cycle? when will the carbon dioxide levels exceed 450 parts per million?\n",
      "context\n",
      "this are the official datasets used on the medicare.gov hospital compare website provided by the centers for medicare & medicaid services. these data allow you to compare the quality of care at over 4,000 medicare-certified hospitals across the country.\n",
      "content\n",
      "dataset fields:\n",
      "provider id\n",
      "hospital name\n",
      "address\n",
      "city\n",
      "state\n",
      "zip code\n",
      "county name\n",
      "phone number\n",
      "hospital type\n",
      "hospital ownership\n",
      "emergency services\n",
      "meets criteria for meaningful use of ehrs\n",
      "hospital overall rating\n",
      "hospital overall rating footnote\n",
      "mortality national comparison\n",
      "mortality national comparison footnote\n",
      "safety of care national comparison\n",
      "safety of care national comparison footnote\n",
      "readmission national comparison\n",
      "readmission national comparison footnote\n",
      "patient experience national comparison\n",
      "patient experience national comparison footnote\n",
      "effectiveness of care national comparison\n",
      "effectiveness of care national comparison footnote\n",
      "timeliness of care national comparison\n",
      "timeliness of care national comparison footnote\n",
      "efficient use of medical imaging national comparison\n",
      "efficient use of medical imaging national comparison\n",
      "acknowledgements\n",
      "dataset was downloaded from [https://data.medicare.gov/data/hospital-compare]\n",
      "inspiration\n",
      "if you just broke your leg, you might need to use this dataset to find the best hospital to get that fixed!\n",
      "the poll\n",
      "as part of cards against humanity saves america, this poll is funded for one year of monthly public opinion polls. cards against humanity is asking the american people about their social and political views, what they think of the president, and their pee-pee habits.\n",
      "to conduct their polls in a scientifically rigorous manner, they partnered with survey sampling international — a professional research firm — to contact a nationally representative sample of the american public. for the first three polls, they interrupted people’s dinners on both their cell phones and landlines, and a total of about 3,000 adults didn’t hang up immediately. they examined the data for statistically significant correlations which can be found here: https://thepulseofthenation.com/\n",
      "content\n",
      "polls are released each month (they are still polling so this will be updated each month)\n",
      "row one is the header and contains the questions\n",
      "each row is one respondent's answers\n",
      "questions in the sep 2017 poll:\n",
      "income\n",
      "gender\n",
      "age\n",
      "age range\n",
      "political affiliation\n",
      "do you approve or disapprove of how donald trump is handling his job as president?\n",
      "what is your highest level of education?\n",
      "what is your race?\n",
      "what is your marital status?\n",
      "what would you say is the likelihood that your current job will be entirely performed by robots or computers within the next decade?\n",
      "do you believe that climate change is real and caused by people, real but not caused by people, or not real at all?\"\n",
      "how many transformers movies have you seen?\n",
      "do you agree or disagree with the following statement: scientists are generally honest and are serving the public good.\n",
      "do you agree or disagree with the following statement: vaccines are safe and protect children from disease.\n",
      "\"how many books, if any have you read in the past year?\"\n",
      "do you believe in ghosts?\n",
      "what percentage of the federal budget would you estimate is spent on scientific research?\n",
      "\"is federal funding of scientific research too high too low or about right?\"\n",
      "true or false: the earth is always farther away from the sun in the winter than in the summer.\n",
      "\"if you had to choose: would you rather be smart and sad or dumb and happy?\"\n",
      "do you think it is acceptable or unacceptable to urinate in the shower?\n",
      "questions from oct 2017 poll\n",
      "income\n",
      "gender\n",
      "age\n",
      "age range\n",
      "political affiliation\n",
      "do you approve or disapprove of how donald trump is handling his job as president?\n",
      "what is your highest level of education?\n",
      "what is your race?\n",
      "from what you have heard or seen do you mostly agree or mostly disagree with the beliefs of white nationalists?\n",
      "if you had to guess what percentage of republicans would say that they mostly agree with the beliefs of white nationalists?\n",
      "would you say that you love america?\n",
      "if you had to guess, what percentage of democrats would say that they love america?\n",
      "do you think that government policies should help those who are poor and struggling in america?\n",
      "if you had to guess, what percentage of republicans would say yes to that question?\n",
      "do you think that most white people in america are racist?\n",
      "if you had to guess, what percentage of democrats would say yes to that question?\n",
      "have you lost any friendships or other relationships as a result of the 2016 presidential election?\n",
      "do you think it is likely or unlikely that there will be a civil war in the united states within the next decade?\n",
      "have you ever gone hunting?\n",
      "have you ever eaten a kale salad?\n",
      "if dwayne \"the rock\" johnson ran for president as a candidate for your political party, would you vote for him?\n",
      "who would you prefer as president of the united states, darth vader or donald trump?\n",
      "questions from nov 2017 poll\n",
      "income\n",
      "gender\n",
      "age\n",
      "age range\n",
      "in politics today, do you consider yourself a democrat, a republican or independent?\n",
      "would you say you are liberal, conservative, or moderate?\n",
      "what is your highest level of education? (high school or less, some college, college degree, graduate degree)\n",
      "what is your race? (white, black, latino, asian, other)\n",
      "do you live in a city, suburb, or small town?\n",
      "do you approve, disapprove, or neither approve nor disapprove of how donald trump is handling his job as president?\n",
      "do you think federal funding for welfare programs in america should be increased, decreased, or kept the same?\n",
      "do you think poor black people are more likely to benefit from welfare programs than poor white people?\n",
      "do you think poor people in cities are more likely to benefit from welfare programs than poor people in small towns?\n",
      "if you had to choose, would you rather live in a more equal society or a more unequal society?\n",
      "acknowledgements\n",
      "these polls are from cards against humanity saves america and the raw data can be found here: https://thepulseofthenation.com/#future\n",
      "context\n",
      "this dataset contains 1.3 million sarcastic comments from the internet commentary website reddit. the dataset was generated by scraping comments from reddit (not by me :)) containing the \\s ( sarcasm) tag. this tag is often used by redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic comment content.\n",
      "content\n",
      "data has balanced and imbalanced (i.e true distribution) versions. (true ratio is about 1:100). the corpus has 1.3 million sarcastic statements, along with what they responded to as well as many non-sarcastic comments from the same source.\n",
      "labelled comments are in the train-balanced-sarcasm.csv file.\n",
      "acknowledgements\n",
      "the data was gathered by: mikhail khodak and nikunj saunshi and kiran vodrahalli for their article \"a large self-annotated corpus for sarcasm\". the data is hosted here.\n",
      "citation:\n",
      "@unpublished{sarc,\n",
      "  authors={mikhail khodak and nikunj saunshi and kiran vodrahalli},\n",
      "  title={a large self-annotated corpus for sarcasm},\n",
      "  url={https://arxiv.org/abs/1704.05579},\n",
      "  year=2017\n",
      "}\n",
      "annotation of files in the original dataset: readme.txt.\n",
      "inspiration\n",
      "predicting sarcasm and relevant nlp features (e.g. subjective determinant, racism, conditionals, sentiment heavy words, \"internet slang\" and specific phrases).\n",
      "sarcasm vs sentiment\n",
      "unusual linguistic features such as caps, italics, or elongated words. e.g., \"yeahhh, i'm sure that is the right answer\".\n",
      "topics that people tend to react to sarcastically\n",
      "context\n",
      "this dataset was originally published by the university of zurich robotics and perception group here. a sample of the data along with accompanying descriptions is provided here for research uses.\n",
      "this presents the world's first dataset recorded on-board a camera equipped micro aerial vehicle (mav) flying within urban streets at low altitudes (i.e., 5-15 meters above the ground). the 2 km dataset consists of time synchronized aerial high-resolution images, gps and imu sensor data, ground-level street view images, and ground truth data. the dataset is ideal to evaluate and benchmark appearance-based topological localization, monocular visual odometry, simultaneous localization and mapping (slam), and online 3d reconstruction algorithms for mav in urban environments.\n",
      "content\n",
      "the entire dataset is roughly 28 gigabyte. we also provide a sample subset less than 200 megabyte, representing the first part of the dataset. you can download the entire dataset from this page.\n",
      "the dataset contains time-synchronized high-resolution images (1920 x 1080 x 24 bits), gps, imu, and ground level google-street-view images. the high-resolution aerial images were captured with a rolling shutter gopro hero 4 camera that records each image frame line by line, from top to bottom with a readout time of 30 millisecond. a summary of the enclosed files is given below.\n",
      "the data from the on-board barometric pressure sensor barometricpressure.csv, accelerometer rawaccel.csv, gyroscope rawgyro.csv, gps receiver onbordgps.csv, and pose estimation onboardpose.csv is logged and timesynchronized using the clock of the px4 autopilot board. the on-board sensor data was spatially and temporally aligned with the aerial images. the first column of every file contains the timestamp when the data was recorded expressed in microseconds. in the next columns the sensor readings are stored. the second column in onbordgps.csv encodes the identification number (id) of every aerial image stored in the /mav images/ folder. the first column in groundtruthagl.csv is the id of the aerial image, followed by the ground truth camera position of the mav and the raw gps data. the second column in groundtruthagm.csv is the id of of the aerial image, followed by the id of the first, second and third best match ground-level street view image in the /street view img/ folder.\n",
      "ground truth\n",
      "two types of ground truth data are provided in order to evaluate and benchmark different vision-based localization algorithms. firstly, appearance-based topological localization algorithms, that match aerial images to street level ones, can be evaluated in terms of precision rate and recall rate. secondly, metric localization algorithms, that computed the ego-motion of the mav using monocular visual slam tools, can be evaluated in terms of standard deviations from the ground truth path of the vehicle.\n",
      "see more details here.\n",
      "past research\n",
      "the work listed below inspired the recording of this dataset. in these papers a much smaller dataset was used, that did not contain time synchronized gps (except a small street segment ), imu data and accurate metric ground truth. if you used this dataset, please send your paper to majdik (at) ifi (dot) uzh (dot) ch.\n",
      "a.l. majdik, d. verda, y. albers-schoenberg, d. scaramuzza. air-ground matching: appearance-based gps-denied urban localization of micro aerial vehicles journal of field robotics, 2015.\n",
      "a. l. majdik, d. verda, y. albers-schoenberg, d. scaramuzza micro air vehicle localization and position tracking from textured 3d cadastral models ieee international conference on robotics and automation (icra), hong kong, 2014.\n",
      "a. majdik, y. albers-schoenberg, d. scaramuzza. mav urban localization from google street view data ieee/rsj international conference on intelligent robots and systems (iros), tokyo, 2013.\n",
      "license\n",
      "these datasets are released under the creative commons license (cc by-nc-sa 3.0), which is free for non-commercial use (including research).\n",
      "acknowledgements\n",
      "this dataset was recorded with the help of karl schwabe, mathieu noirot-cosson, and yves albers-schoenberg. to record the dataset we used a fotokite mav offered to our disposal by perspective robotics ag—http://fotokite.com.\n",
      "this work was supported by the national centre of competence in research robotics (nccr) through the swiss national science foundation and by the hungarian scientific research fund (no. otka/nkfih 120499).\n",
      "context\n",
      "epilepsy is a group of neurological disorders characterized by epileptic seizures. epileptic seizures are episodes that can vary from brief and nearly undetectable to long periods of vigorous shaking.\n",
      "content\n",
      "the original dataset consists of 5 different folders, each with 100 files, with each file representing a single subject/person. each file is a recording of brain activity for 23.6 seconds. the corresponding time-series is sampled into 4097 data points. each data point is the value of the eeg recording at a different point in time.\n",
      "the column y contains the category of the 178-dimensional input vector. specifically y in {1, 2, 3, 4, 5}\n",
      "5- eyes open, means when they were recording the eeg signal of the brain the patient had their eyes open 4- eyes closed, means when they were recording the eeg signal the patient had their eyes closed 3- yes they identify where the region of the tumor was in the brain and recording the eeg activity from the healthy brain area 2- they recorder the eeg from the area where the tumor was located\n",
      "all subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure\n",
      "1- recording of seizure activity\n",
      "the explanatory variables x1, x2, ..., x178\n",
      "each 178-dimensional vector contained in a row, represents a randomly selected 1-second long sample picked from the single file. recall that each file is a recording of brain activity for 23.6 seconds. the corresponding time-series is sampled into 4097 data points.\n",
      "acknowledgements\n",
      "andrzejak rg, lehnertz k, rieke c, mormann f, david p, elger ce (2001) indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity: dependence on recording region and brain state, phys. rev. e, 64, 061907\n",
      "inspiration\n",
      "can you help epileptic people live a better life ?\n",
      "all recorded police reports as taken from https://data.seattle.gov/public-safety/seattle-police-department-police-report-incident/7ais-f98f\n",
      "bug triage.\n",
      "this is a dataset of tweets from various active scientists and personalities ranging from donald trump and hillary clinton to neil degrasse tyson. more are forthcoming.\n",
      "they were obtained through javascript scraping of the browser twitter timeline rather than a tweepy python api or the twitter timeline api.\n",
      "the inspiration for this twitter dataset is comparing tweets in my own twitter analysis to find who tweets like whom, e.g. does trump or hillary tweet more like kim kardashian than one another?\n",
      "thus, this goes further back in time than anything directly available from twitter.\n",
      "the data is in json format rather than csv, which will be forthcoming as well.\n",
      "kim kardashian, adam savage, billnye, neil degrasse tyson, donald trump, and hillary clinton have been collected up to 2016-10-14 richard dawkins, commander scott kelly, barack obama, nasa, and the onion, tweets up to 2016-10-15.\n",
      "for your own pleasure, with special thanks to the trump twitter archive for providing some of the code, here is the javascript used to scrape tweets off of a timeline and output the results to the clipboard in json format:\n",
      "1) construct the query with from:twitterhandle since:date until:date\n",
      "2) in the browser console set up automatic scrolling with: setinterval(function(){ scrollto(0, document.body.scrollheight) }, 2500)\n",
      "3) scrape the resulting timeline with: var alltweets = []; var tweetelements = document.queryselectorall('li.stream-item');\n",
      "for (var i = 0; i < tweetelements.length; i++) { try { var el = tweetelements[i]; var text = el.queryselector('.tweet-text').textcontent; alltweets.push({ id: el.getattribute('data-item-id'), date: el.queryselector('.time a').textcontent, text: text, link: el.queryselector('div.tweet').getattribute('data-permalink-path'), retweet: text.indexof('\\\"@') == 0 && text.includes(':') ? true : false }); } catch(err) {} }; copy(alltweets);\n",
      "have fun!\n",
      "context\n",
      "there is a well-documented phenomenon of increased suicide rates among united states military veterans. one recent analysis, published in 2016, found the suicide rate amongst veterans to be around 20 per day. the widespread nature of the problem has resulted in efforts by and pressure on the united states military services to combat and address mental health issues in and after service in the country's armed forces.\n",
      "in 2013 news21 published a sequence of reports on the phenomenon, aggregating and using data provided by individual states to typify the nationwide pattern. this dataset is the underlying data used in that report, as collected by the news21 team.\n",
      "content\n",
      "the data consists of six files, one for each year between 2005 and 2011. each year's worth of data includes the general population of each us state, a count of suicides, a count of state veterans, and a count of veteran suicides.\n",
      "acknowledgements\n",
      "this data was originally published by news21. it has been converted from an xls to a csv format for publication on kaggle. the original data, visualizations, and stories can be found at the source.\n",
      "inspiration\n",
      "what is the geospatial pattern of veterans in the united states? how much more vulnerable is the average veteran to suicide than the average citizen? is the problem increasing or decreasing over time?\n",
      "context\n",
      "the dataset reflects the votes for the different areas in germany for the 2017 federal elections. see german federal election 2017 for more details\n",
      "content\n",
      "the data was aquired from govdata.de, which is state website offering interesting datasets. the original dataset was not easy to use, therefore i did some reshaping without changing the data\n",
      "acknowledgements\n",
      "the dataset is originally from https://www.govdata.de/web/guest/apps/-/details/bundestagswahl-2017\n",
      "inspiration\n",
      "the data is interesting as it reflects the votes for all areas in germany\n",
      "comcast is notorious for terrible customer service and despite repeated promises to improve, they continue to fall short. only last month (october 2016) the fcc fined them a cool $2.3 million after receiving over 1000 consumer complaints. after dealing with their customer service for hours yesterday, i wanted to find out more about others' experiences.\n",
      "this will serve as a repository of public customer complaints filed against comcast as i scrape them from the web. the data should not only provide a fun source for analysis, but it will help to pin down just what is wrong with comcast's customer service.\n",
      "context\n",
      "a collection of slc end-of-year crime reports geocoded to standard gps coordinates.\n",
      "content\n",
      "2016 crime statistics for salt lake city, ut. includes:\n",
      "case numbers\n",
      "offence codes for categorization\n",
      "descriptions for context\n",
      "ibr (national incident-based reporting system number)\n",
      "occurrence date\n",
      "report date\n",
      "day of the week (1 = monday, 7 = sunday)\n",
      "location (addresses in slc)\n",
      "city council district\n",
      "slcpd police zones\n",
      "slcpd grid\n",
      "x_coordinate: note that this is based on epsg:32043 projections\n",
      "y_coordinate: note that this is based on epsg:32043 projections\n",
      "x_gps_coords (added by yours truly, converted to epsg:4326)\n",
      "y_gps_coords (added by yours truly, converted to epsg:4326)\n",
      "data accuracy notes\n",
      "some data wrangling will still likely be required to clean up null columns.\n",
      "i went ahead and lowercased column names (and corrected a spelling mistake in the y-coordinate column).\n",
      "epsg:32043 projections were converted to epsg:4326 projections using pyplot with distances preserved.\n",
      "multiple year munging performed here: https://github.com/octaflop/slcpd/blob/master/develop/2017-08-16-crunch.ipynb\n",
      "still awaiting dataset owner clarification of calls vs cases\n",
      "acknowledgements\n",
      "taken from the slc open data web site.\n",
      "thank you dean larson, the original dataset owner.\n",
      "thank you to the city of salt lake government and the utah.gov catalog for providing this data for public use.\n",
      "thanks to the dat science edex course for inspiring me to take a look at my own city's crime stats.\n",
      "thank you to the slcpd for keeping salt lake city citizens safe and enforcing an internal discipline of open data-collection.\n",
      "inspiration\n",
      "crime report locations by season?\n",
      "cross reference of city council districts\n",
      "time of day\n",
      "offence descriptions\n",
      "moving centroids based on time of day / season?\n",
      "holiday rowdiness?\n",
      "coming soon\n",
      "full 2016 reports (eta spring 2017) ✔\n",
      "3-year combined reports (eta summer 2017) ✔\n",
      "3-year combined cases vs calls (eta summer 2017)\n",
      "year-by-year files (eta fall 2017)\n",
      "context\n",
      "every marathoner has a time goal in mind, and this is the result of all the training done in months of exercises. long runs, strides, kilometers and phisical exercise, all add improvement to the result. marathon time prediction is an art, generally guided by expert physiologists that prescribe the weekly exercises and the milestones to the marathon.\n",
      "unfortunately, runners have a lot of distractions while preparing the marathon, work, family, illnes, and therefore each one of us arrives to the marathon with his own story. the \"simple\" approach is to look at data after the competition, the leaderboard.\n",
      "but what if we could link the marathon result to the training history of the athlete? could we find that \"non orthodox\" training plans give good results?\n",
      "the athlete training history\n",
      "as a start, i'll take just two data from the athlete history, easy to extract. two meaningful data, the average km run during the 4 weeks before the marathon, and the average speed that the athlete has run these km.\n",
      "meaningful, because in the last month of the training i have the recap of all the previous months that brought me to the marathon.\n",
      "easy to extract, because i can go to strava and i have a \"side-by-side\" comparison, myself and the reference athlete. i said easy, well, that's not so easy, since i have to search every athlete and write down those numbers, the exact day the marathon happened, otherwise i will put in the average the rest days after the marathon.\n",
      "i've set my future work in extracting more data and build better algorithms. thank you for helping me to understand or suggest.\n",
      "content\n",
      "id:\n",
      "simple counter\n",
      "marathon:\n",
      "the marathon name where the data were extracted. i use the data coming out from strava \"side by side comparison\" and the data coming from the final marathon result\n",
      "name:\n",
      "the athlete's name, still some problems with utf-8, i'll fix that soon\n",
      "category:\n",
      "the sex and age group of a runner - mam male athletes under 40 years - wam women under 40 years - m40 male athletes between 40 and 45 years\n",
      "km4week\n",
      "this is the total number of kilometers run in the last 4 weeks before the marathon, marathon included. if, for example, the km4week is 100, the athlete has run 400 km in the four weeks before the marathon\n",
      "sp4week\n",
      "this is the average speed of the athlete in the last 4 training weeks. the average counts all the kilometers done, included the slow kilometers done before and after the training. a typic running session can be of 2km of slow running, then 12-14km of fast running, and finally other 2km of slow running. the average of the speed is this number, and with time this is one of the numbers that has to be refined\n",
      "cross training:\n",
      "if the runner is also a cyclist, or a triathlete, does it counts? use this parameter to see if the athlete is also a cross trainer in other disciplines\n",
      "wall21: in decimal. the tricky field. to acknowledge a good performance, as a marathoner, i have to run the first half marathon with the same split of the second half. if, for example, i run the first half marathon in 1h30m, i must finish the marathon in 3h (for doing a good job). if i finish in 3h20m, i started too fast and i hit \"the wall\". my training history is, therefore, less valid, since i was not estimating my result\n",
      "marathon time:\n",
      "in decimal. this is the final result. based on my training history, i must predict my expected marathon time\n",
      "category:\n",
      "this is an ancillary field. it gives some direction, so feel free to use or discard it. it groups in:\n",
      "- a results under 3h\n",
      "- b results between 3h and 3h20m\n",
      "- c results between 3h20m and 3h40m\n",
      "- d results between 3h40 and 4h\n",
      "acknowledgements\n",
      "thank you to the main athletes data sources, garmin and strava\n",
      "the goal of this competition:\n",
      "based on my training history, i must predict my expected marathon time. which other relevant data could help me to be more precise? heart rate, cadence, speed training, what else? and how could i get those data?\n",
      "context\n",
      "emergent.info was a major rumor tracker, created by veteran journalist craig silverman. it has been defunct for a while, but its well-structured format and well-documented content provides an opportunity for analyzing rumors on the web.\n",
      "snopes.com is one of the oldest rumors trackers on the web. originally launched by barbara and david mikkelson, it is now run by a team of editors who investigate urban legends, myths, viral rumors and fake news. the investigators try to provide a detailed explanation for why they have chosen to confirm or debunk a rumor, often citing several web pages and other external sources.\n",
      "politifact.com is a fact-checker that is focused on statements made by politicians and claims circulated by political campaigns, blogs and similar websites. politifact's labels range from \"true,\" to \"pants on fire!\"\n",
      "content\n",
      "this dataset consists of three files. one file is a collection of all webpages cited in emergent.info, and the second is a collection of webpages cited in snopes.com, and the third is a similar collection from politifact.com. the webpages were often cited because they had started a rumor, shared a rumor, or debunked a rumor.\n",
      "emergent.info\n",
      "emergent.info often provides a clean timeline of the rumor's propagation on the web, and identifies which page was for the rumor, which page was against it, and which page was simply observing it. please refer to the image below to learn more about the fields in this dataset.\n",
      "snopes.com\n",
      "the structure of posts on snopes.com is not as well-defined. please refer to the image below to learn more about the fields in the snopes dataset.\n",
      "politifact.com\n",
      "similar to emergent.info, politifact.com follows a well-structured format in reporting and documenting rumors. there is a sidebar on the right side of each page that lists all of the sources cited within the page. the top link is the likeliest to be the original source of the rumor. for this link, page_is_first_citation is set to true.\n",
      "inspiration\n",
      "i created this dataset in order to study domains that frequently start, propagate, or debunk rumors. by studying these domains and people who follow them, i hope to gain some insight into the dynamics of rumor propagation on the web, as well as social media.\n",
      "notes/disclaimer\n",
      "when using the snopes dataset, please keep the following in mind:\n",
      "in addition to debunking rumors, snopes.com occasionally reports news and other types of content. this collection only includes data from \"fact check\" posts on snopes.\n",
      "snopes.com was launched years ago. some of the older posts on the website do not follow the current format of the site, therefore some of the fields might be missing.\n",
      "snopes.com used to use a service named \"donotlink.com\" for citation purposes. that service is no longer active and as a result some of the links are missing from older posts on snopes.\n",
      "in addition, some of the shortened links would time-out prior to resolution, in which case they would not be added to the dataset.\n",
      "occasionally, a website that has been cited has not maliciously started a rumor. for instance, andy borowitz is a humorist who writes for the new yorker. his satirical column is sometimes mistaken for real news; as a result, the new yorker may be cited as a source of fake news on snopes.com. this does not mean that the new yorker is a fake news website.\n",
      "when using the politifact dataset, please keep the following in mind:\n",
      "the data included in this dataset are collected from the \"truth-o-meter\" page of politifact.com.\n",
      "politifact often fact-checks statements made by politicians. since this dataset is focused on websites, i have ignored all the posts in which the rumor was attributed to a person, a political party, a campaign, or an organization. instead, i have only included rumors attributed explicitly to websites or blogs.\n",
      "useful tips for using the snopes collection\n",
      "as opposed to the emergent collection where each page is flagged with whether it was for or against a rumor, no such information is available for the snopes dataset. to avoid manually labeling the data, you may use the following heuristics to identify which page started a rumor:\n",
      "webpages that are cited in the \"examples\" section of a post are often \"observing\" the rumor, i.e. they have not started it, but they are repeating it. in the snopes.csv file, these webpages have been flagged as \"page_is_example.\"\n",
      "webpages that are cited in the \"featured image\" section of a post are often not related to the rumor. the editors on snopes have simply extracted an image from those pages to embed in their posts. in the snopes.csv file, these webpages have been flagged as \"page_is_image_credit.\"\n",
      "webpages that are cited through a secondary service (such as archive.is) are likelier to be rumor-propagators. editors do not link to them directly so that a record of their page is available, even if it is later deleted.\n",
      "if neither of these hints help, very often (but not always) the first link cited on the page (for which \"page_is_example\" and \"page_is_image_credit\" are false) is the link to a page that started the rumor. this link is identified by the \"page_is_first_citation\" field. pages for which both \"page_is_first_citation\" and \"page_is_archived\" are true are very likely to be rumor propagators.\n",
      "to identify satirical websites that are mistaken for real news, it's useful to inspect the way they are cited on snopes. to demonstrate that a website contains satire or humor, snopes writers often cite the \"about us\" page of the site. therefore it's useful to see which domains often contain a uri to their \"about\" page (e.g. \"http://politicops.com/about-us/\").\n",
      "context\n",
      "satellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. new commercial imagery providers, such as planet and blacksky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.\n",
      "this flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.\n",
      "the aim of this dataset is to help address the difficult task of detecting the location of airplanes in satellite images. automating this process can be applied to many issues including monitoring airports for activity and traffic patterns, and defense intelligence.\n",
      "continusouly updates will be made to this dataset as new planet imagery released. current images were collected as late as july 2017.\n",
      "content\n",
      "provided is a zipped directory planesnet.zip that contains the entire dataset as .png image chips. each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png\n",
      "label: valued 1 or 0, representing the \"plane\" class and \"no-plane\" class, respectively.\n",
      "scene id: the unique identifier of the planetscope visual scene the image chip was extracted from. the scene id can be used with the planet api to discover and download the entire scene.\n",
      "longitude_latitude: the longitude and latitude coordinates of the image center point, with values separated by a single underscore.\n",
      "the dataset is also distributed as a json formatted text file planesnet.json. the loaded object contains data, label, scene_ids, and location lists.\n",
      "the pixel value data for each 20x20 rgb image is stored as a list of 1200 integers within the data list. the first 400 entries contain the red channel values, the next 400 the green, and the final 400 the blue. the image is stored in row-major order, so that the first 20 entries of the array are the red channel values of the first row of the image.\n",
      "the list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.\n",
      "class labels\n",
      "the \"plane\" class includes 8000 images. images in this class are near-centered on the body of a single airplane, with the majority of the plane's wings, tail, and nose also visible. examples of different aircraft sizes, orientations, and atmospheric collection conditions are included. example images from this class are shown below.\n",
      "the \"no-plane\" class includes 24000 images. a third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an airplane. the next third are \"partial planes\" that contain a portion of an airplane, but not enough to meet the full definition of the \"plane\" class. the last third are \"confusers\" - chips with bright objects or strong linear features that resemble a plane - that have previously been mislabeled by machine learning models. example images from this class are shown below.\n",
      "acknowledgements\n",
      "satellite imagery used to build planesnet is made available through planet's open california dataset, which is openly licensed. as such, this dataset is also available under the same cc-by-sa license. users can sign up for a free planet account to search, view, and download thier imagery and gain access to their api.\n",
      "context:\n",
      "halloween begins frenetic candy consumption that continues into the christmas holidays and new year’s day, when people often make (usually short-lived) resolutions to lose weight. but all this consumption first needs production. the graph shows the relevant data from the industrial production index and its stunning seasonality\n",
      "content:\n",
      "the industrial production (ip) index measures the real output of all relevant establishments located in the united states, regardless of their ownership, but not those located in u.s. territories. this dataset tracks industrial production every month from january 1972 to august 2017.\n",
      "acknowledgements:\n",
      "board of governors of the federal reserve system (us), industrial production: nondurable goods: sugar and confectionery product [ipg3113n], retrieved from fred, federal reserve bank of st. louis; https://fred.stlouisfed.org/series/ipg3113n, october 13, 2017.\n",
      "inspiration:\n",
      "can you correct for the seasonality in this data?\n",
      "which months have the highest candy production?\n",
      "can you predict production for september through december 2017?\n",
      "context\n",
      "the canadian disaster database the canadian disaster database (cdd) contains detailed disaster information on more than 1000 natural, technological and conflict events (excluding war) that have happened since 1900 at home or abroad and that have directly affected canadians.\n",
      "content\n",
      "data description copied from: https://www.publicsafety.gc.ca/cnt/rsrcs/cndn-dsstr-dtbs/index-en.aspx\n",
      "dataset date range: 1900 - present\n",
      "the cdd tracks \"significant disaster events\" which conform to the emergency management framework for canada definition of a \"disaster\" and meet one or more of the following criteria:\n",
      "10 or more people killed\n",
      "100 or more people affected/injured/infected/evacuated or homeless\n",
      "an appeal for national/international assistance\n",
      "historical significance\n",
      "significant damage/interruption of normal processes such that the community affected cannot recover on its own\n",
      "the database describes where and when a disaster occurred, the number of injuries, evacuations, and fatalities, as well as a rough estimate of the costs. as much as possible, the cdd contains primary data that is valid, current and supported by reliable and traceable sources, including federal institutions, provincial/territorial governments, non-governmental organizations and media sources.\n",
      "data is updated and reviewed on a semi-annual basis.\n",
      "data field description\n",
      "disaster type the type of disaster (e.g. flood, earthquake, etc.) that occurred.\n",
      "date of event the date a specific event took place.\n",
      "specific location the city, town or region where a specific event took place.\n",
      "description of event a brief description of a specific event, including pertinent details that may not be captured in other data fields (e.g. amount of precipitation, temperatures, neighbourhoods, etc.)\n",
      "fatalities the number of people killed due to a specific event.\n",
      "injured/infected the number of people injured or infected due to a specific event.\n",
      "evacuees the number of individuals evacuated by the government of canada due to a specific event.\n",
      "latitude & longitude the exact geographic location of a specific event.\n",
      "province/territory the province or territory where a specific event took place.\n",
      "estimated total cost a roll-up of all the costs listed within the financial data fields for a specific event.\n",
      "dfaa payments the amount, in dollars, paid out by disaster financial assistance arrangements (public safety canada) due to a specific event.\n",
      "insurance payments the amount, in dollars, paid out by insurance companies due to a specific event.\n",
      "provincial/territorial costs/payments the amount, in dollars, paid out by a province or territory due to a specific event.\n",
      "utility costs/losses the amount of people whose utility services (power, water, etc.) were interrupted/affected by a specific event.\n",
      "magnitude a measure of the size of an earthquake, related to the amount of energy released.\n",
      "other federal institution costs the amount, in dollars, paid out by other federal institutions.\n",
      "acknowledgements\n",
      "data gathered from: http://cdd.publicsafety.gc.ca terms of use for commercial and non-comerical reproduction: https://www.publicsafety.gc.ca/cnt/ntcs/trms-en.aspx\n",
      "inspiration\n",
      "this dataset provides valuable insight to natural and non-natrual disasters which have affected canada.\n",
      "possible explorations: * where do different types of disasters occur more frequently? * which province / location in canada has been hit the hardest in terms of fatalities, number of injuries, estimated total cost, etc.?\n",
      "spatial-temporal correlations between natural/artifical distasters *\n",
      "i think that this can be used to produce some interesting data visualizations. some of the questions i look forward to answering include:\n",
      "can any spatial-temporal correlations between disasters be found in this dataset?\n",
      "which locations in canada have been hit the hardest, in terms of people injured, fatalities, financial impact, etc.\n",
      "context\n",
      "is the movie industry dying? is netflix the new entertainment king? those were the first questions that lead me to create a dataset focused on movie revenue and analyze it over the last decades. but, why stop there? there are more factors that intervene in this kind of thing, like actors, genres, user ratings and more. and now, anyone with experience (you) can ask specific questions about the movie industry, and get answers.\n",
      "content\n",
      "there are 6820 movies in the dataset (220 movies per year, 1986-2016). each movie has the following attributes:\n",
      "budget: the budget of a movie. some movies don't have this, so it appears as 0\n",
      "company: the production company\n",
      "country: country of origin\n",
      "director: the director\n",
      "genre: main genre of the movie.\n",
      "gross: revenue of the movie\n",
      "name: name of the movie\n",
      "rating: rating of the movie (r, pg, etc.)\n",
      "released: release date (yyyy-mm-dd)\n",
      "runtime: duration of the movie\n",
      "score: imdb user rating\n",
      "votes: number of user votes\n",
      "star: main actor/actress\n",
      "writer: writer of the movie\n",
      "year: year of release\n",
      "acknowledgements\n",
      "this data was scraped from imdb.\n",
      "contribute\n",
      "you can contribute via github.\n",
      "opensprayer.com\n",
      "open sprayer will hopefully be an open sourced autonomous land drone that will propel itself across the fields spraying weeds it can see with its mounted cameras. the project should involve a mix of mechanical engineering, classical software design and machine learning to achieve its goal. the project is meant to be a diy effort to compete with the big companies like john deere currently developing similar tech. the benefit of an open design is cheaper capital and maintenance cost. the ability to fix, update and repair your own sprayer would offer a great alternative to the usual high running costs of branded machines.\n",
      "the data set includes around 150 photos with annotations (bounding box coordinates) locating the broad leaved docks. 70% were taken by me with the remaining being collected on google. i plan to update the images to better reflect the images that the sprayer drone will produce when operating. when the drone is running photos will most likely be taken from a height looking straight down at the ground, therefore the google images may be useless or not? give me feedback and i can take more pictures to improve the dataset.\n",
      "the 2014 american community survey public use microdata sample\n",
      "context\n",
      "the american community survey (acs) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.\n",
      "frequency: annual\n",
      "period: 2014\n",
      "content\n",
      "through the acs, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. public officials, planners, and entrepreneurs use this information to assess the past and plan the future. when you respond to the acs, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. the data dictionary can be found here.\n",
      "inspiration\n",
      "kernels created using the 2013 acs can serve as excellent starting points for working with the 2014 acs. for example, the following analyses were created using acs data:\n",
      "work arrival times and earnings in the usa\n",
      "inequality in stem careers\n",
      "acknowledgements\n",
      "the american community survey (acs) is administered, processed, researched and disseminated by the u.s. census bureau within the u.s. department of commerce.\n",
      "full text of questions and answers from cross validated, the statistics and machine learning q&a site from the stack exchange network.\n",
      "this is organized as three tables:\n",
      "questions contains the title, body, creation date, score, and owner id for each question.\n",
      "answers contains the body, creation date, score, and owner id for each of the answers to these questions. the parentid column links back to the questions table.\n",
      "tags contains the tags on each question\n",
      "for space reasons only non-deleted and non-closed content are included in the dataset. the dataset contains questions up to 19 october 2016 (utc).\n",
      "license\n",
      "all stack exchange user contributions are licensed under cc-by-sa 3.0 with attribution required.\n",
      "context\n",
      "the vision zero data contained in this layer pertain to parking violations issued by the district of columbia's metropolitan police department (mpd) and partner agencies with the authority. parking violation locations are summarized ticket counts based on time of day, week of year, year, and category of violation. data was originally downloaded from the district department of motor vehicle's etims meter work order management system.\n",
      "content\n",
      "this dataset contains 132,850 rows of:\n",
      "objectid (unique id)\n",
      "rowid_ day_of_week (text)\n",
      "holiday (number)\n",
      "week_of_year (number)\n",
      "month_of_year (number)\n",
      "issue_time (number)\n",
      "violation_code (text)\n",
      "violation_description (text)\n",
      "location (text)\n",
      "rp_plate_state (text)\n",
      "body_style (text)\n",
      "address_id (number)\n",
      "streetsegid (number)\n",
      "xcoord (number)\n",
      "ycoord (number)\n",
      "ticket_issue_date (date or time)\n",
      "x\n",
      "y\n",
      "acknowledgements\n",
      "the dataset is shared by dcgisopendata, and the original dataset and metadata can be found here.\n",
      "inspiration\n",
      "can you use the dataset to determine:\n",
      "which area has the highest concentration of parking violations? how about by type of violation?\n",
      "do areas with high concentration of parking violations change throughout the month of december? can you identify any trends?\n",
      "the greenhouse gas (ghg) inventory data contains the most recently submitted information, covering the period from 1990 to the latest available year, to the extent the data have been provided. the ghg data contain information on anthropogenic emissions by sources and removals by sinks of the following ghgs (carbon dioxide (co2), methane (ch4), nitrous oxide (n2o), hydrofluorocarbons (hfcs), perfluorocarbons (pfcs), unspecified mix of hfcs and pfcs, sulphur hexafluoride (sf6) and nitrogen triflouride (nf3)) that are not controlled by the montreal protocol.\n",
      "ghg emission inventories are developed by parties to the convention using scientific and methodological guidance from the intergovernmental panel on climate change (ipcc), such as 2006 ipcc guidelines for national greenhouse gas inventories, revised guidelines for national greenhouse gas inventories (1996), ipcc good practice guidance and uncertainty management in national greenhouse gas inventories (2000) and ipcc good practice guidance on land use, land-use change and forestry (2003). last update in undata: 23 mar 2017 with data released in nov 2016.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nation on the undata site. you can find the original dataset here.\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "this data set comprises events for major league baseball, provided by http://retrosheet.org.\n",
      " the information used here was obtained free of\n",
      " charge from and is copyrighted by retrosheet.  interested\n",
      " parties may contact retrosheet at \"www.retrosheet.org\".\n",
      "roughly speaking an event is an outcome in a baseball game. this includes the end result of a plate appearance (strikeout, out in the field, hit, base on balls), events that occur within a plate appearance (stolen bases, caught stealing), and rare other occurrences. the retrosheet event data prior to 1955 are not complete. the data subsequent to 1988 include pitch counts while the data prior do not. the data here cover the years 1970-2015, in three divisions (1970-1992, 1993-2004, 2005-2015) that correspond, roughly, to distinct eras with different run-scoring environments. these data have specifically been obtained with a mix of the data dumps available at baseball heatmaps and with the py-retrosheet python package, available on github.\n",
      "i have augmented the data provided by retrosheet with some additional fields. most substantively the rows include the woba value of the event, in the field woba_pts, and an estimated time stamp, in units of seconds since jan. 1, 1900 (time_since_1900).\n",
      "the conversion from retrosheet files to sql and csv is done by the chadwick software. a detailed description of all of the fields is available on the documentation for chadwick, http://chadwick.sourceforge.net/doc/cwevent.html. in order to keep the file sizes down, i have limited the fields in this data set to a subset of the fields described in the chadwick documentation.\n",
      "the master.csv file is a subset of the baseball databank data and is released under a creative commons attribution-sharealike 3.0 unported license.https://creativecommons.org/licenses/by-sa/3.0/. more details are available on the baseball databank github https://github.com/chadwickbureau/baseballdatabank\n",
      "content\n",
      "data sources : - crime (2016): https://www.icpsr.umich.edu/icpsrweb/ - population (2013): https://census.gov\n",
      "ucla housing dataset\n",
      "this dataset is a result of my research production in machine learning and android security. the data were obtained by a process that consisted to create a binary vector of permissions used for each application analyzed {1=used, 0=no used}. moreover, the samples of malware/benign were devided by \"type\"; 1 malware and 0 non-malware.\n",
      "one important topic to work is to create a good set of malware, because it is difficult to find one updated and with a research work to support it .\n",
      "if your papers or other works use our dataset, please cite our colcom 2016 paper as follows. urcuqui, c., & navarro, a. (2016, april). machine learning classifiers for android malware analysis. in communications and computing (colcom), 2016 ieee colombian conference on (pp. 1-6). ieee.\n",
      "if you need an article in english, you can download another of my works: urcuqui, christian., & navarro, andres. (2016). framework for malware analysis in android. sistemas & telemática, 14(37), 45-56.\n",
      "ccurcuqui@icesi.edu.co\n",
      "context\n",
      "a few years ago, i investigated a korean corpus in order to find the most frequent 1000 words. subsequently, i asked native speakers to translate those words and their example sentences into english, japanese, spanish, and indonesian. i've totally forgotten this data since then, but it flashed on me this might be helpful for some people. undoubtedly, 1000 sentences are a pretty small corpus, but it is also true that parallel corpora are hard to get.\n",
      "content\n",
      "it's a csv file. as you expect, the first line is the heading.\n",
      "id: id of the headword. it is arranged by alphabetical order.\n",
      "headword: 1000 most frequent korean words.\n",
      "pos: part of speech.\n",
      "english: english meaning or equivalent.\n",
      "japanese: japanese meaning or equivalent.\n",
      "spanish: spanish meaning or equivalent.\n",
      "indonesian: indonesian meaning or equivalent.\n",
      "example (ko): an example sentence\n",
      "example (en): english translation\n",
      "example (ja): japanese translation\n",
      "example (es): spanish translation\n",
      "example (id): indonesian translation\n",
      "inspiration\n",
      "for now, i'm not sure how this small corpus can be used. hopefully this will be helpful for some pilot linguistic project.\n",
      "visit my github for detailed description and code.\n",
      "also, here's an app, a recommendation system for museum selection, i created with this data set: http://216.230.228.88:3838/bootcamp006_project/project5-capstone/museo/app/\n",
      "inspiration for further analysis\n",
      "predict if a museum will be featured or not\n",
      "discover the important factors that results in a higher rating\n",
      "apply natural language processing to discover insights from review/quote/tag data\n",
      "cluster museums based on review/quote polarity or subjectivity (sentiment analysis)\n",
      "apply association rules to uncover relationship of different combinations of review tags\n",
      "processed and merged data\n",
      "tripadvisor_merged.csv: a file containing museum data collected from tripadvisor including tag/ type/ review/ quote features\n",
      "raw data scraped from tripadvisor (us/world)\n",
      "tripadvisor_museum: general museum data scraped from tripadvisor\n",
      "traveler_type: {'museum': ['families','couples','solo','business','friends']}\n",
      "traveler_rating: {'museum': ['excellent','very good','average','poor','terrible']}\n",
      "tag_clouds: {'museum': ['tag 1', 'tag 2', 'tag 3' ...]}\n",
      "review_quote: {'museum': ['quote 1', 'quote 2', ... ,'quote 10']}\n",
      "review_content: {'museum': ['review 1', 'review 2', ... ,'review 10']}\n",
      "museum_categories: {'museum': ['museum type 1','museum type 2', ...]}\n",
      "context\n",
      "data set contains the recorded number of dengue cases per 100,000 population per region of the philippines from 2008 to 2016\n",
      "content\n",
      "this is a small data set that is a good starting point for beginners that wants to play around with small scale temporal and spatial data set\n",
      "acknowledgements\n",
      "publisher would like to thank the department of health of the philippines for providing the raw data\n",
      "inspiration\n",
      "what is the trend of dengue cases in the philippines? what region/s recorded the highest prevalence of dengue cases? in what specific years do we observe the highest dengue cases? when and where will a possible dengue outbreak occur?\n",
      "context\n",
      "this dataset contains the metadata of over 50,000 tweets from christmas eve and christmas. we are hoping the data science and research community can use this to develop new and informative conclusions about this holiday season.\n",
      "content\n",
      "we acquired this data through a web crawler written in java. the first field is the id of the tweet, and the second is the html metadata. we recommend using beautifulsoup or another library to parse this data and extract information from each tweet.\n",
      "inspiration\n",
      "we would especially like to see research on the use of emojis in tweets, the type of sentiment there is on christmas (maybe determine how grateful each country is), or some kind of demographic on the age or nationality of active twitter users during christmas.\n",
      "overview\n",
      "only two major-party candidates are competing in the 2016 us presidential election, but there was tough competition to get to the general election. this dataset contains transcripts of every democratic, republican, and republican undercard debate held during the 2016 primary season.\n",
      "this dataset is meant to be a complement to megan risdal's transcripts of the 2016 us presidential (general election) debates.\n",
      "so you can now take all of the questions (who talks the most? who has a wider vocabulary?) that you answered in the general election debates, and apply the same procedures to see how your favorite (or least favorite) candidate has changed over time.\n",
      "the column names (and order) in this dataset are a superset of those found in the general election dataset, and non-speech annotations (such as \"(applause)\") in this dataset are also a superset. kernels uploaded for the general election dataset should be compatible with this dataset as well; please let me know if you have any compatibility issues.\n",
      "what in the world is an \"undercard\" debate?\n",
      "the field of republicans running for president in the primaries was (yuuuuge!) pretty big: 17 candidates threw their hat in the ring at one point or another. debate organizers realized that having 17 people on stage (each with a set amount of time to answer a question / respond to a criticism / interrupt each other) would, in the best case, lead to a three-to-four-hour-long debate (and, in the worst case, lead to a chaotic shout-fest as candidates tried to talk over each other for three to four hours).\n",
      "to alleviate this issue, many of the republican debate nights were split into two separate debates: the main debate with the top party contenders, aired live during primetime; and the undercard debate, which typically aired a few hours earlier than the main debate.\n",
      "the criteria for a candidate to be allowed into the main debate (rather than the \"kids' table\" debate, as some pundits derisively called the undercard event) varied a bit by event. typically a poll showing of 1% in one of several specified polls was sufficient to gain admission to the undercard. to get into the main debate, candidates had to either (a) be polling above a different, higher, percentage in specific polls, or (b) be among the top n republican candidates in said polls.\n",
      "the details get a little thorny (certain debates had multiple criteria, of which candidates had to meet at least one), so i refer questions to the individual undercard debate pages at the american presidency project for detailed criteria.\n",
      "in this dataset, the split between republican debates is made in the party column: republican is used for the primetime main events, and republican undercard is used for the undercard.\n",
      "acknowledgements\n",
      "all transcripts were scraped from the presidential debates page of the american presidency project at the university of california, santa barbara. individual lines in the dataset contain links to the particular page for that debate.\n",
      "updates\n",
      "v2 contains a few minor changes related to normalizing parenthesized elements (non-speech) within the text field, and adding a few lines of interruptions that persisted in the text field\n",
      "the data\n",
      "the fields are described more fully in the file description. this section describes the particular elements that can appear in the speaker and text fields.\n",
      "who's who?\n",
      "(aka, the speaker column)\n",
      "the primary debates had a ton of participants. this dataset contains utterances from 22 politicians and 49 moderators, not to mention the occasional audience laughter or 2-minute timer.\n",
      "almost all speaker columns contain either a single title-case name (listed below in the participants and moderators subsections) or a single upper-case word indicating non-speaker noise (listed below in the none-speaker turns subsection); the exceptions to this are cases where a name is concatenated with a space and a parenthesized tag, as follows:\n",
      "spkr (video): transcriptions of pre-recorded material of any of the candidates or moderators\n",
      "spkr (translated): in the univision/telemundo debate, some questions and answers are translated into english from the original spanish; the transcript reflects the translations as spoken by a translator\n",
      "non-speaker turns\n",
      "the non-speaker turns in the speaker column are:\n",
      "audience: any laughter, booing, applause, etc. from the audience\n",
      "candidates: cross-talk between candidates\n",
      "other: non-speaker, non-audience noise (such as commercial break, timer bell, national anthem, etc.)\n",
      "question: a question from an audience member (or a prerecorded question)\n",
      "unknown: cases where the transcriber could hear a phrase but could not determine who said it\n",
      "here are the various speakers who appear in the dataset:\n",
      "candidates\n",
      "democratic\n",
      "chafee: former governor lincoln chafee (ri)\n",
      "clinton: former secretary of state hillary clinton\n",
      "o'malley: former governor martin o'malley (md)\n",
      "sanders: senator bernie sanders (vt)\n",
      "webb: former senator jim webb (va)\n",
      "republican\n",
      "bush: former governor jeb bush (fl)\n",
      "carson: ben carson\n",
      "cruz: senator ted cruz (tx)\n",
      "kasich: governor john kasich (oh)\n",
      "paul: senator rand paul (ky)\n",
      "rubio: senator marco rubio (fl)\n",
      "trump: donald trump\n",
      "walker: governor scott walker (wi)\n",
      "republican (undercard only)\n",
      "gilmore: former governor jim gilmore (va)\n",
      "graham: senator lindsey graham (sc)\n",
      "jindal: governor bobby jindal (la)\n",
      "pataki: former governor george pataki (ny)\n",
      "perry: former governor rick perry (tx)\n",
      "santorum: former senator rick santorum (pa)\n",
      "republican (main and undercard)\n",
      "christie: governor chris christie (nj)\n",
      "fiorina: carly fiorina\n",
      "huckabee: former governor mike huckabee (ar)\n",
      "all candidates in a python list for easy copy/paste\n",
      "['bush', 'carson', 'chafee', 'christie', 'clinton', 'cruz', 'fiorina', 'gilmore', 'graham', 'huckabee', 'jindal', 'kasich', \"o'malley\", 'pataki', 'paul', 'perry', 'rubio', 'sanders', 'santorum', 'trump', 'walker', 'webb']\n",
      "moderators\n",
      "note: some moderators are seen across various debates; in particular, the republican main debates and undercard debates on a given day tend to share the same moderators. some moderators are public figures who are seen only in videos (with the (video) tag).\n",
      "moderators\n",
      "arrarás: maría celeste arrarás (telemundo)\n",
      "baier: bret baier (fox news)\n",
      "baker: gerard baker (the wall street journal)\n",
      "bartiromo: maria bartiromo (fox business network)\n",
      "bash: dana bash (cnn)\n",
      "blitzer: wolf blitzer (cnn)\n",
      "cavuto: neil cavuto (fox business network)\n",
      "cooney: kevin cooney (cbs news)\n",
      "cooper: anderson cooper (cnn)\n",
      "cordes: nancy cordes (cbs news)\n",
      "cramer: jim cramer (cnbc)\n",
      "cuomo: governor andrew cuomo (ny)\n",
      "dickerson: john dickerson (cbs news)\n",
      "dinan: stephen dinan (washington times)\n",
      "epperson: sharon epperson (cnbc)\n",
      "garrett: major garrett (cbs news)\n",
      "ham: mary katharine ham (hot air)\n",
      "hannity: sean hannity (fox news)\n",
      "harwood: john harwood (cnbc)\n",
      "hemmer: bill hemmer (fox news)\n",
      "hewitt: hugh hewitt (salem radio network)\n",
      "holt: lester holt (nbc news)\n",
      "ifill: gwen ifill (pbs)\n",
      "kelly: megyn kelly (fox news)\n",
      "lemon: don lemon (cnn)\n",
      "levesque: neil levesque (new hampshire institute of politics)\n",
      "lopez: juan carlos lopez (cnn en espanol)\n",
      "louis: errol louis (ny1)\n",
      "maccallum: martha maccallum (fox news)\n",
      "maddow: rachel maddow (msnbc)\n",
      "mcelveen: josh mcelveen (wmur-tv)\n",
      "mitchell: andrea mitchell (nbc news)\n",
      "muir: david muir (abc news)\n",
      "o'reilly: bill o'reilly (fox news)\n",
      "obradovich: kathie obradovich (the des moines register)\n",
      "quick: becky quick (cnbc)\n",
      "quintanilla: carl quintanilla (cnbc)\n",
      "raddatz: martha raddatz (abc news)\n",
      "ramos: jorge ramos (univision)\n",
      "regan: trish regan (fox business network)\n",
      "salinas: maría elena salinas (univision)\n",
      "santelli: rick santelli (cnbc)\n",
      "seib: gerald seib (the wall street journal)\n",
      "strassel: kimberly strassel (the wall street journal)\n",
      "tapper: jake tapper (cnn)\n",
      "todd: chuck todd (msnbc)\n",
      "tumulty: karen tumulty (the washington post)\n",
      "wallace: chris wallace (fox news)\n",
      "woodruff: judy woodruff (pbs)\n",
      "all moderators in a python list for easy copy/paste\n",
      "['arrarás', 'baier', 'baker', 'bartiromo', 'bash', 'blitzer', 'cavuto', 'cooney', 'cooper', 'cordes', 'cramer', 'cuomo', 'dickerson', 'dinan', 'epperson', 'garrett', 'ham', 'hannity', 'harwood', 'hemmer', 'hewitt', 'holt', 'ifill', 'kelly', 'lemon', 'levesque', 'lopez', 'louis', 'maccallum', 'maddow', 'mcelveen', 'mitchell', 'muir', \"o'reilly\", 'obradovich', 'quick', 'quintanilla', 'raddatz', 'ramos', 'regan', 'salinas', 'santelli', 'seib', 'strassel', 'tapper', 'todd', 'tumulty', 'wallace', 'woodruff']\n",
      "what's what?\n",
      "(aka, the text column)\n",
      "in general, the text column contains fully punctuated and appropriately capitalized speech transcriptions. most parenthesized elements are non-speech elements, with the following exceptions:\n",
      "(c) and (4): are spoken in reference to 501(c)(4)s (tax-exempt lobbying groups)\n",
      "(k): spoken in reference to 401(k)s (individual pension accounts)\n",
      "the non-speech elements that can be found in parentheses in the text column are:\n",
      "(anthem): the national anthem is played\n",
      "(applause): the audience expresses approval\n",
      "(bell): a bell or buzzer indicating that a candidate's time has expired\n",
      "(booing): the audience expresses disapproval\n",
      "(commercial): the televised debate breaks for a commercial advertisement\n",
      "(crosstalk): more than one candidate or moderator are speaking at the same time\n",
      "(laughter): the audience expresses a sense of humor\n",
      "(moment.of.silence): the debate pauses for a moment of silence\n",
      "(spanish): the utterance is in spanish (for the democrats' univision-hosted debate on 3/9/16 in miami)\n",
      "(video.end): a video clip ends\n",
      "(video.start): a video clip begins\n",
      "(inaudible): the utterance was inaudible, off-mike, or too indecipherable to transcribe\n",
      "context\n",
      "movebank is a free, online database of animal tracking data hosted by the max planck institute for ornithology. it is designed to help animal tracking researchers to manage, share, protect, analyze, and archive their data. movebank is an international project with over 11,000 users, including people from research and conservation groups around the world. the animal tracking data accessible through movebank belongs to researchers all over the world. these researchers can choose to make part or all of their study information and animal tracks visible to other registered users, or to the public.\n",
      "animal tracking\n",
      "animal tracking data helps us understand how individuals and populations move within local areas, migrate across oceans and continents, and evolve through millennia. this information is being used to address environmental challenges such as climate and land use change, biodiversity loss, invasive species, and the spread of infectious diseases. read more.\n",
      "context\n",
      "this dataset contains energy usage information for every building owned and managed by nyc dcas. dcas, or the department of citywide administrative services, is the arm of the new york city municipal government which handles ownership and management of the city's office facilities and real estate inventory. the organization voluntarily publicly discloses self-measured information about the energy use of its buildings.\n",
      "content\n",
      "this data contains information on the name, address, location, and 2015 financial cycle energy usage of every building managed at that time by dcas.\n",
      "acknowledgements\n",
      "this dataset is published as-is by of the city of new york (here).\n",
      "inspiration\n",
      "by combining this dataset with the new york city buildings database, what can you learn about the energy usage of buildings in new york city?\n",
      "can you use this data to model the energy consumption for city's office space at large?\n",
      "short track speed skating database for sports data analysis\n",
      "what is short track speed skating?\n",
      "maybe some people have never heard of this sport. short track is a competitive and strategic game in which skaters race on ice. sometimes the smartest or the luckiest guy, rather than the strongest, wins the game (for example).\n",
      "what's in the data?\n",
      "the database covers all the international short track games in the last 5 years. currently it contains only men's 500m, but i will keep updating it.\n",
      "detailed lap data including personal time and ranking in each game from seasons 2012/2013 to present .\n",
      "the final time results, ranking, starting position, qualified or penalized information of each athlete in each game.\n",
      "all series of world cup, world championship, european championship and olympic games.\n",
      "original data source\n",
      "the data is collected from the isu's (international skating union) official website. i have already done the cleaning procedure.\n",
      "please make sure that the data are only for personal and non-commercial use.\n",
      "explore the data\n",
      "interesting questions may be like:\n",
      "what will happen in a game when there are more than one athlete from the same team? are there performance all improved?\n",
      "how does the performance of athletes change within a season and over seasons?\n",
      "do some athletes have special patterns in terms of time allocation and surpassing opportunity?\n",
      "what is the influence of the implementation of \"no toe starts\" rules on athletes since july 2015?\n",
      "is there also home advantage like in other sports?\n",
      "who are the most \"dangerous\" guys that always get penalty?\n",
      "context\n",
      "« bixi montréal is a public bicycle sharing system serving montréal, quebec, canada.\n",
      "launched in may 2009, it is north america's first large-scale bike sharing system and the original bixi brand of systems.\n",
      "the location of a bixi bike station is determined by several parameters, including population density, points of interest and activities (universities, bike paths, other transportation networks, and data on travel patterns of the general public. in 2009, 5,000 bikes were deployed in montreal through a network of pay stations located mainly in the boroughs of rosemont–la petite-patrie, the plateau-mont-royal and ville-marie, spilling over into parts of outremont and the south west. as of 2011, the system has spread to hochelaga-maisonneuve, villeray–saint-michel–parc-extension, ahuntsic, côte-des-neiges–notre-dame-de-grâce, westmount and verdun. » [1]\n",
      "content\n",
      "1. bixi - movements history\n",
      "datasets containing the details of the travels made via the bixi montréal self-service bike network. each year is a .zip file (ex. biximontrealrentals2014.zip) containing several .csv files (ex. od_2014-04.csv) for each months.\n",
      "\n",
      "version 2 : all .csv are merged per year (od-year.csv).\n",
      "\n",
      "the data is extracted from the bixi montréal station and bike management system. trips of less than 1 minute or more than 2 hours are excluded. the station identifiers used correspond to those of the station status data set\n",
      "data sructure\n",
      "start_date: date and time of the start of the trip ( aaaa-mm-jj hh:mm )\n",
      "start_station_code: start station id\n",
      "end_date: date and time of the start of the trip ( aaaa-mm-jj hh:mm )\n",
      "end_station_code : end station id\n",
      "is_member : type users. (1 : suscriber, 0 : non-suscriber)\n",
      "duration_sec: total travel time in seconds\n",
      "\n",
      "2. bixi - the condition of stations\n",
      "this dataset presents the list of stations in the bixi montréal self-service bicycle network, including the geographic position, the number of bicycles available and the number of terminals available. it is a .json file, which corresponds to a dictionary.\n",
      "the data is produced by the bixi montréal station management system with a refresh rate of 5 minutes. station locations are generally stable over time, but may be subject to change during the season, particularly when the city of montreal is carrying out work or as part of special events. temporary storage stations are not included in station status. the data is automatically generated on the bixi servers, so the date of last update of this dataset does not represent the actual date of update. information about bikes and bad terminals is available in the json format file.\n",
      "data sructure\n",
      "id: unique station id\n",
      "s: name of the station\n",
      "n: station terminal id\n",
      "st: station status\n",
      "b: boolean value (true or false) specifying whether the station is blocked\n",
      "su: boolean value (true or false) specifying whether the station is suspended\n",
      "m: boolean value (true or false) specifying whether the station is displayed as out of service\n",
      "read: timestamp of the last update of the data in milliseconds since january 1, 1970.\n",
      "lc: timestamp of the last communication with the server in milliseconds since january 1, 1970.\n",
      "bk: (for future use)\n",
      "bl: (for future use)\n",
      "la: latitude of the station according to the geodesic datum wgs84\n",
      "lo: longitude of the station according to the wgs84 datum\n",
      "da: number of available terminals at this station\n",
      "dx: number of unavailable terminals at this station\n",
      "ba: number of available bikes at this station\n",
      "bx: number of unavailable bicycles at this station\n",
      "3. geographical boundaries of montreal (borough and related city)\n",
      "this dataset is optional and will be useful mostly for ploting data and doing some choropleth maps.\n",
      "acknowledgements\n",
      "creative commons attribution 4.0 international\n",
      "for more details :\n",
      "http://donnees.ville.montreal.qc.ca/dataset/bixi-historique-des-deplacements\n",
      "http://donnees.ville.montreal.qc.ca/dataset/bixi-etat-des-stations\n",
      "http://donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements\n",
      "inspiration\n",
      "can you find pattern in the behavior of bixi users?\n",
      "are there any inefficient stations ?\n",
      "what insights can we use from this data for decision making ?\n",
      "[1] https://en.wikipedia.org/wiki/bixi_montr%c3%a9al\n",
      "context\n",
      "elon musk is an american business magnate. he was one of the founders of paypal in the past, and the founder and/or cofounder and/or ceo of spacex, tesla, solarcity, openai, neuralink, and the boring company in the present. he is known as much for his extremely forward-thinking ideas and huge media presence as he is for his extremely business savvy.\n",
      "musk is famously active on twitter. this dataset contains all tweets made by @elonmusk, his official twitter handle, between november 16, 2012 and september 29, 2017.\n",
      "content\n",
      "this dataset includes the body of the tweet and the time it was made, as well as who it was re-tweeted from (if it is a retweet).\n",
      "inspiration\n",
      "can you figure out elon musk's opinions on various things by studying his twitter statements?\n",
      "how elon musk's post rate increased, decreased, or stayed about the same over time?\n",
      "context\n",
      "data scientists often use crowdsourcing platforms, such as amazon mechanical turk or crowdflower to collect labels for their data. controlling high quality and timeless execution of tasks is an important part of such collection process. it is not possible (or not efficient) to manually check every worker assignment. there is an intuition that there quality could be predicted based on workers task browser behaviour (e.g. key presses, scrolling, mouse clicks, tab switching). in this dataset there are assignment results for 3 different crowdsourcing tasks launched on crowdflower, along with associated workers behaviour.\n",
      "content\n",
      "we collected data running 3 tasks:\n",
      "image labelling,\n",
      "receipt transcription,\n",
      "business search.\n",
      "tasks are described in tasks.csv. results for corresponding tasks are given in files: results_{task_id}.csv. workers's activity could be found in the following files:\n",
      "activity_keyboard.csv - timestamps of keyboard keys pressed\n",
      "activity_mouse.csv - timestamps of mouse clicks with associated html elements\n",
      "activity_tab.csv - timestamps of event task browser tab changes (opened, active, hidden, closed)\n",
      "activity_page.csv - a summary of events happened in the task page every 2 seconds (boolean keyboard activity, boolean mouse movement activity, boolean scrolling activity, the position of the screen, boolean if text was selected)\n",
      "result files have a similar structure to the original one given by crowdflower:\n",
      "_unit_id: a unique id number created by the system for each row\n",
      "_created_at: the time the contributor submitted the judgement\n",
      "_golden: this will be \"true\" if this is a test question, otherwise it is \"false\"\n",
      "_id: a unique id number generated for this specific judgment\n",
      "_missed: this will be \"true\" if the row is an incorrect judgment on a test question.\n",
      "_started_at: the time at which the contributor started working on the judgement\n",
      "_tainted: this will be \"true\" if the contributor has been flagged for falling below the required accuracy. this judgment will not be used in the aggregation.\n",
      "_channel: the work channel that the contributor accessed the job through\n",
      "_trust: the contributor's accuracy. learn more about trust here\n",
      "_worker_id: a unique id number assigned to the contributor (in the current dataset md5 value is given)\n",
      "_country: the country the contributor is from\n",
      "_region: a region code for the area the contributor is from\n",
      "_city: the city the contributor is from\n",
      "_ip: the ip address for the contributor (in the current dataset md5 value is given)\n",
      "{{field}}: there will be a column for each field in the job, with a header equal to the field's name.\n",
      "{{field}}_gold: the correct answer for the test question\n",
      "acknowledgements\n",
      "we thank crowd workers who accomplished our not always exciting tasks on crowdflower.\n",
      "context\n",
      "this database was used in the paper: \"covert online ethnography and machine learning for detecting individuals at risk of being drawn into online sex work\". https://www.flinders.edu.au/centre-crime-policy-research/illicit-networks-workshop\n",
      "content\n",
      "the database includes data scraped from a european online adult forum. using covert online ethnography we interviewed a small number of participants and determined their risk to either supply or demand sex services through that forum. this is a great dataset for semi-supervised learning.\n",
      "inspiration\n",
      "how can we identify individuals at risk of being drawn into online sex work? the spread of online social media enables a greater number of people to be involved into online sex trade; however, detecting deviant behaviors online is limited by the low available of data. to overcome this challenge, we combine covert online ethnography with semi-supervised learning using data from a popular european adult forum.\n",
      "context\n",
      "cars dataset with features including make, model, year, engine, and other properties of the car used to predict its price.\n",
      "content\n",
      "scraped from edmunds and twitter.\n",
      "acknowledgements\n",
      "edmunds and twitter for providing info sam keene\n",
      "inspiration\n",
      "effects of features on the price how does the brand affect the price? what cars can be consider overpriced? price vs. popularity?\n",
      "context\n",
      "the federal election commission (fec) is an independent regulatory agency established in 1975 to administer and enforce the federal election campaign act (feca), which requires public disclosure of campaign finance information. the fec publishes campaign finance reports for presidential and legislative election campaign candidates on the campaign finance disclosure portal.\n",
      "content\n",
      "the finance summary report contains one record for each financial report (form 3p) filed by the presidential campaign committees during the 2016 primary and general election campaigns. presidential committees file quarterly prior to the election year and monthly during the election year. the campaign expenditures file contains individual operating expenditures made by the campaign committee and reported on form 3p line 23 during the same period. operating expenditures consist of the routine costs of campaigning for president, which include staffing, travel, advertising, voter outreach, and other activities.\n",
      "the new york public library is digitizing and transcribing its collection of historical menus. the collection includes about 45,000 menus from the 1840s to the present, and the goal of the digitization project is to transcribe each page of each menu, creating an enormous database of dishes, prices, locations, and so on. as of early november, 2016, the transcribed database contains 1,332,279 dishes from 17,545 menus.\n",
      "the data\n",
      "this dataset is split into four files to minimize the amount of redundant information contained in each (and thus, the size of each file). the four data files are menu, menupage, menuitem, and dish. these four files are described briefly here, and in detail in their individual file descriptions below.\n",
      "menu\n",
      "the core element of the dataset. each menu has a unique identifier and associated data, including data on the venue and/or event that the menu was created for; the location that the menu was used; the currency in use on the menu; and various other fields.\n",
      "each menu is associated with some number of menupage values.\n",
      "menupage\n",
      "each menupage refers to the menu it comes from, via the menu_id variable (corresponding to menu:id). each menupage also has a unique identifier of its own. associated menupage data includes the page number of this menupage, an identifier for the scanned image of the page, and the dimensions of the page.\n",
      "each menupage is associated with some number of menuitem values.\n",
      "menuitem\n",
      "each menuitem refers to both the menupage it is found on -- via the menu_page_id variable -- and the dish that it represents -- via the dish_id variable. each menuitem also has a unique identifier of its own. other associated data includes the price of the item and the dates when the item was created or modified in the database.\n",
      "dish\n",
      "a dish is a broad category that covers some number of menuitems. each dish has a unique id, to which it is referred by its affiliated menuitems. each dish also has a name, a description, a number of menus it appears on, and both date and price ranges.\n",
      "inspiration\n",
      "what are some things we can look at with this dataset?\n",
      "how has the median price of restaurant dishes changed over time? are there particular types of dishes (alcoholic beverages, seafood, breakfast food) whose price changes have been greater than or less than the average change over time?\n",
      "can we predict anything about a dish's price based on its name or description?\n",
      "-- there's been some work on how the words used in advertisements for potato chips are reflective of their price; is that also true of the words used in the name of the food?\n",
      "-- are, for example, french or italian words more likely to predict a more expensive dish?\n",
      "acknowledgments\n",
      "this dataset was downloaded from the new york public library's what's on the menu? page. the what's on the menu? data files are updated twice monthly, so expect this dataset to go through multiple versions.\n",
      "context\n",
      "i am interested in indoor positioning technology and had been playing with blue-tooth beacons. typically blue-tooth beacons have accuracy of a range between 1 to 4 meters. i am thinking of maybe using machine learning can produce results of greater accuracy compared to using traditional filtering methods e.g. kalman filters, particle filters.\n",
      "content\n",
      "3 kontakt blue-tooth beacons are mounted in a 2.74 meters wide x 4.38 meters long (width x length) room. the 3 beacons are transmitting at a transmit power of -12dbm. a sony xperia e3 smartphone with blue-tooth turned on is used as a receiver to the record the data. recordings are done on several positions in the room of an interval of 30-60 seconds in the same position.\n",
      "beacons location # beacon x(meters) y(meters) beacona 0.10 1.80 beaconb 2.74 2.66 beaconc 1.22 4.54\n",
      "fields:\n",
      "the distance in meters between beacon a and the device calculated by using the rssi of this blue-tooth beacon.\n",
      "the distance in meters between beacon b and the device calculated by using the rssi of this blue-tooth beacon.\n",
      "the distance in meters between beacon c and the device calculated by using the rssi of this blue-tooth beacon.\n",
      "x coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy.\n",
      "y coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy.\n",
      "date and time of the recording.\n",
      "acknowledgements\n",
      "the data is collected solely by me.\n",
      "inspiration\n",
      "to try and improve on the accuracy of indoor position using blue-tooth beacons which typically have accuracy of range between 1 meters to 4 meters.\n",
      "context\n",
      "this dataset was collected using our app ec taximeter.\n",
      "an easy to use tool developed to compare fees, giving the user an accurate fee based on gps to calculate a cost of the taxi ride. due to the ability to verify that you are charged fairly, our app is very popular in several cities. we encourage our users to send us urls with the taxi/transportation fees in their cities to keep growing our database.\n",
      "★ our app gets the available fares for your location based on your gps, perfect when traveling and not getting scammed.\n",
      "★ users can start a taximeter in their own phone and check they are charged fairly\n",
      "★ several useful information is displayed to the user during the ride: speed, wait time, distance, gps update, gps precision, range of error.\n",
      "★ each fare has information available for reference like: schedule, minimum fee, source, last update.\n",
      "★ it’s possible to surf through several cities and countries which fares are available for use. if a fare is not in the app, now it’s easier than ever to let us know thanks to questbee apps.\n",
      "we invite users to contribute to our project and expect this data set to be useful, please don't hesitate to contact us to info@ashkadata.com to add your city or to contribute with this project.\n",
      "content\n",
      "the data is collected from june 2016 until july 20th 2017. the data is not completely clean, many users forget to turn off the taximeter when done with the route. hence, we encourage data scientist to explore it and trim the data a little bit\n",
      "acknowledgements\n",
      "we have to acknowledge the valuable help of our users, who have contributed to generate this dataset and have push our growth by mouth to mouth recommendation.\n",
      "inspiration\n",
      "our first inspiration for the app was after being scammed in our home city quito. we started it as a tool for people to be fairly charged when riding a taxi. currently with other transportation options available, we also help user to compare fares in their cities or the cities which they are visiting.\n",
      "file descriptions\n",
      "mex_clean.csv - the dataset contains information of routes in mexico city\n",
      "uio_clean.csv - the dataset contains information of routes in quito ecuador\n",
      "bog_clean.csv - the dataset contains information of routes in bogota\n",
      "all-data_clean.csv - the dataset contains information of routes in different cities\n",
      "presidential elections have just finished in france, with two rounds on april 23rd and may 7th 2017. the results of the elections are available online for each ~67000 polling stations around the country. this data can be used to gain insights about both the electorate and the candidates. examples of basic questions worth looking at are:\n",
      "how are candidates' scores correlated with each other? can you infer 'political affinity' of the candidates just looking at the data, without any other a priori knowledge?\n",
      "how are scores of various candidates correlated with the turnout? supporters of which candidate are the most 'participative'?\n",
      "how did the votes get redistributed from the first to the second round? can you build an a posteriori predictive model of the second round results taking as an input the results of the first round ?\n",
      "the data provides coordinates of each polling station. can you gain any insight from the geography?\n",
      "some preliminary analysis is described in two blog posts here and here. this dataset is inspired by an analogous one for the 2016 us elections by ben hamner.\n",
      "predict molecular properties\n",
      "context\n",
      "this dataset contains molecular properties scraped from the pubchem database. each file contains properties for thousands of molecules , made up of the elements h, c, n, o, f, si, p, s, cl, br, and i. the dataset is related to a previous one which had fewer number of molecules, where the features were preconstructed.\n",
      "instead, this dataset is a challenging case for feature engineering and is subject of active research (see references below).\n",
      "data description\n",
      "the utilities used to download and process the data can be accessed from my github repo.\n",
      "each json file contains a list of molecular data. an example molecule is given below:\n",
      "{\n",
      "'en': 37.801,\n",
      "'atoms': [\n",
      "{'type': 'o', 'xyz': [0.3387, 0.9262, 0.46]},\n",
      "{'type': 'o', 'xyz': [3.4786, -1.7069, -0.3119]},\n",
      "{'type': 'o', 'xyz': [1.8428, -1.4073, 1.2523]},\n",
      "{'type': 'o', 'xyz': [0.4166, 2.5213, -1.2091]},\n",
      "{'type': 'n', 'xyz': [-2.2359, -0.7251, 0.027]},\n",
      "{'type': 'c', 'xyz': [-0.7783, -1.1579, 0.0914]},\n",
      "{'type': 'c', 'xyz': [0.1368, -0.0961, -0.5161]},\n",
      "{'type': 'c', 'xyz': [-3.1119, -1.7972, 0.659]},\n",
      "{'type': 'c', 'xyz': [-2.4103, 0.5837, 0.784]},\n",
      "{'type': 'c', 'xyz': [-2.6433, -0.5289, -1.426]},\n",
      "{'type': 'c', 'xyz': [1.4879, -0.6438, -0.9795]},\n",
      "{'type': 'c', 'xyz': [2.3478, -1.3163, 0.1002]},\n",
      "{'type': 'c', 'xyz': [0.4627, 2.1935, -0.0312]},\n",
      "{'type': 'c', 'xyz': [0.6678, 3.1549, 1.1001]},\n",
      "{'type': 'h', 'xyz': [-0.7073, -2.1051, -0.4563]},\n",
      "{'type': 'h', 'xyz': [-0.5669, -1.3392, 1.1503]},\n",
      "{'type': 'h', 'xyz': [-0.3089, 0.3239, -1.4193]},\n",
      "{'type': 'h', 'xyz': [-2.9705, -2.7295, 0.1044]},\n",
      "{'type': 'h', 'xyz': [-2.8083, -1.921, 1.7028]},\n",
      "{'type': 'h', 'xyz': [-4.1563, -1.4762, 0.6031]},\n",
      "{'type': 'h', 'xyz': [-2.0398, 1.417, 0.1863]},\n",
      "{'type': 'h', 'xyz': [-3.4837, 0.7378, 0.9384]},\n",
      "{'type': 'h', 'xyz': [-1.9129, 0.5071, 1.7551]},\n",
      "{'type': 'h', 'xyz': [-2.245, 0.4089, -1.819]},\n",
      "{'type': 'h', 'xyz': [-2.3, -1.3879, -2.01]},\n",
      "{'type': 'h', 'xyz': [-3.7365, -0.4723, -1.463]},\n",
      "{'type': 'h', 'xyz': [1.3299, -1.3744, -1.7823]},\n",
      "{'type': 'h', 'xyz': [2.09, 0.1756, -1.3923]},\n",
      "{'type': 'h', 'xyz': [-0.1953, 3.128, 1.7699]},\n",
      "{'type': 'h', 'xyz': [0.7681, 4.1684, 0.7012]},\n",
      "{'type': 'h', 'xyz': [1.5832, 2.901, 1.6404]}\n",
      "],\n",
      "'id': 1,\n",
      "'shapem': [259.66, 4.28, 3.04, 1.21, 1.75, 2.55, 0.16, -3.13, -0.22, -2.18, -0.56, 0.21, 0.17, 0.09]\n",
      "}\n",
      "en: this field is the molecular energy calculated using a force-field method. see references [1,2] for details. this is the target variable which is being predicted.\n",
      "atoms: this field contains the name of the element and the position (x,y,z coordinates) and needs to be used for feature engineering.\n",
      "id : this field is the pubchem id\n",
      "shapem : this field contains the shape multipoles and can be used for feature engineering. for definition of shape multipoles, see reference [3].\n",
      "notice that each molecule contains different number and types of atoms, so it is challenging to come up with features that can describe every molecule in a unique way. there are several approaches taken in the literature (see the references), one of which is to use the coulomb matrix for a given molecule defined by\n",
      "cij=\n",
      "zizj\n",
      "|ri−rj|\n",
      ",(i≠j)cij=z\n",
      "2.4\n",
      "i\n",
      ",(i=j)\n",
      "where $z_i$ are atomic numbers (can be looked up from the periodic table for each element), and ${\\vert r_i - r_j \\vert}$ is the distance between two atoms i and j. the previous dataset used these features for a subset of molecules given here, where the maximum number of elements in a given molecules was limited by 50. in this case, each molecule has a 50x50 coulomb matrix where zeroes were used as padding when the molecule had smaller than 50 number of atoms.\n",
      "there are around 100,000,000 molecules in the whole database. as more files are scraped, new data will be added in time.\n",
      "note: in the previous dataset, the molecular energies were computed by quantum mechanical simulations. here, the given energies are computed using another method, so their values are different.\n",
      "inspiration\n",
      "simulations of molecular properties are computationally expensive. the purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database. in the pubchem database, there are around 100,000,000 molecules. it could take years to do simulations on all of these molecules, however machine learning can be used to predict their properties much faster. as a result, this could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.\n",
      "this is a regression problem so mean squared error is minimized during training.\n",
      "i am looking for kagglers to find the best model and reduce mean squared error as much as possible!\n",
      "references\n",
      "[1] halgren ta. merck molecular force field: i. basis, form, scope, parameterization and performance of mmff94. j. comp. chem. 1996;17:490-519.\n",
      "[2] halgren ta. merck molecular force field: vi. mmff94s option for energy minimization studies. j. comp. chem. 1999;20:720-729.\n",
      "[3] kim, sunghwan, evan e bolton, and stephen h bryant. “pubchem3d: shape compatibility filtering using molecular shape quadrupoles.” j. cheminf. 3 (2011): 25.\n",
      "[4] himmetoglu b.: tree based machine learning framework for predicting ground state energies of molecules, j. chem. phys 145, 134101 (2016)\n",
      "[5] rupp m., ramakrishnan r., von lilienfeld oa.: machine learning for quantum mechanical properties of atoms in molecules, j. phys. chem. lett. , 6(16): 3309–3313 (2015)\n",
      "[6] montavon g., rupp m., gobre v., vazquez-mayagoitia a., hansen k., tkatchenko a., müller k-r., von lilienfeld oa.: machine learning of molecular electronic properties in chemical compound space, new j. phys., 15(9): 095003 (2013)\n",
      "[7] hansen k., montavon g., biegler f., fazli s., rupp m., scheffler m., von lilienfeld oa., tkatchenko a., müller k-p.: assessment and validation of machine learning methods for predicting molecular atomization energies, j. chem. theory comput. , 9(8): 3543–3556 (2013)\n",
      "about this dataset\n",
      "you can use this fonts file to generate some chinese character. use this image can train a machine learning model to recognize text.\n",
      "dataset is updating\n",
      "tell me if you have other font file or anything related to this topic.\n",
      "context\n",
      "i have been a comic book fan for many years and, when i started writing web scrapers for practice, it was only natural that i did one inspired by my passion for marvel.\n",
      "content\n",
      "this dataset has 27.290 rows, each one representing a distinct marvel character, such as peter parker, tony stark or jean grey. as for columns, there are 1822 of them, one for each marvel universe. all the cells contain a boolean value: true if there is a version of that character from that universe or false otherwise.\n",
      "acknowledgements\n",
      "i would like to thank the marvel wikia for its amazing amount of information, as well as very practical api, and marvel for having such a huge and diverse multiverse that inspires many possibilities of analysis.\n",
      "inspiration\n",
      "this was my first attempt at data science. it was challenging but very fun and rewarding. i would really appreciate any feedback or suggestions for next works.\n",
      "context\n",
      "can you create a python symptom sorter, in bot style with these data ?\n",
      "symptom 1: i have an eye problem > the set selects actually all the diseases and symptoms related to the eyes ok\n",
      "then comes a question: is your nose congested ? and do you have red eye ? symptom 2: congested nose yes, red eye no ok\n",
      "then comes a second question do you cough ? and do you have chills ? symptom 3: cough yes, chills no ok\n",
      "the answer: with these symptoms i think you have...\n",
      "the primer shows the idea.\n",
      "primer to start with if you want a reduced data input, you download the us_df from the primer so the challenge i have here is, what classification method gives the best results for making a decision tree\n",
      "content\n",
      "its one of my databases.\n",
      "overview\n",
      "this is the original data from titanic competition plus some changes that i applied to it to be better suited for binary logistic regression:\n",
      "merged the train and test data.\n",
      "removed the 'ticket' and 'cabin' attributes.\n",
      "moved the 'survived' attribute to the last column.\n",
      "added extra zero columns for categorical inputs to be better suited for one-hot-encoding.\n",
      "substituted the values of 'sex' and 'embarked' attributes with binary and categorical values respectively.\n",
      "filled the missing values in 'age' and 'fare' attributes with the median of the data.\n",
      "context\n",
      "the death penalty was authorized by 32 states, the federal government, and the u.s. military. while connecticut, maryland, and new mexico no longer have death penalty statutes, they do currently incarcerate death-sentenced offenders. texas leads the nation in the number of executions since the death penalty was reinstated in 1976. california, florida, texas, and pennsylvania have the largest death row populations.\n",
      "the following crimes are capital murder in texas:\n",
      "murder of a peace officer or fireman who is acting in the lawful discharge of an official duty and who the person knows is a peace officer or fireman;\n",
      "murder during the commission or attempted commission of kidnapping, burglary, robbery, aggravated sexual assault, arson, obstruction or retaliation, or terroristic threat;\n",
      "murder for remuneration or promise of remuneration or employs another to commit murder for remuneration or promise of remuneration;\n",
      "murder during escape or attempted escape from a penal institution;\n",
      "murder, while incarcerated in a penal institution, of a correctional employee or with the intent to establish, maintain, or participate in a combination or in the profits of a combination;\n",
      "murder while incarcerated in a penal institution for a conviction of murder or capital murder;\n",
      "murder while incarcerated in a penal institution serving a life sentence or a 99 year sentence for a conviction of aggravated kidnapping, aggravated sexual assault, or aggravated robbery;\n",
      "murder of more than one person during the same criminal transaction or during different criminal transactions but the murders are committed pursuant to the same scheme or course of conduct;\n",
      "murder of an individual under ten years of age; or\n",
      "murder in retaliation for or on account of the service or status of the other person as a judge or justice of the supreme court, the court of criminal appeals, a court of appeals, a district court, a criminal district court, a constitutional county court, a statutory county court, a justice court, or a municipal court.\n",
      "content\n",
      "the texas department of criminal justice publishes various details, including the last words, of every inmate on death row they execute. this dataset includes information on the name, age, race, county, date, and last words of texas death row inmates from 1982 to 2017.\n",
      "acknowledgments\n",
      "this dataset on last statements by executed offenders was obtained here: https://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html\n",
      "start a new kernel\n",
      "context\n",
      "this is a general disclosure of donald trump's assets, debts, and sources of income. because the forms are only meant to reveal on potential conflicts of interests a filer might have, they provide far less specificity than tax returns would. this is an unpacked version of the pdf of trump's form that was made available by the federal election commission june 16th, 2017. it contains some information about his financial interests, but not enough to paint a complete picture of his net worth. it may be possible to use some of these forms to identify his foreign business partners.\n",
      "acknowledgements\n",
      "this dataset was kindly extracted from the original pdf and made publicly available by the center for responsive politics.\n",
      "inspiration\n",
      "previous work on similar disclosures by trump has often focused on identifying his foreign business ties. are you able to find any new ones?\n",
      "you might also like\n",
      "trump's 2016 financial disclosure\n",
      "trump's world\n",
      "trump's tweets\n",
      "trump campaign expenditures\n",
      "this dataset provides a complete snapshot of crime, outcome, and stop and search data, as held by the home office from late 2014 through mid 2017 for london, both the greater metro and the city.\n",
      "content\n",
      "the core fields are as follows:\n",
      "reported by: the force that provided the data about the crime.\n",
      "falls within: at present, also the force that provided the data about the crime. this is currently being looked into and is likely to change in the near future.\n",
      "longitude and latitude: the anonymised coordinates of the crime.\n",
      "lsoa code and lsoa name: references to the lower layer super output area that the anonymised point falls into, according to the lsoa boundaries provided by the office for national statistics.\n",
      "crime type: one of the crime types listed in the police.uk faq.\n",
      "last outcome category: a reference to whichever of the outcomes associated with the crime occurred most recently. for example, this crime's 'last outcome category' would be 'offender fined'.\n",
      "context: a field provided for forces to provide additional human-readable data about individual crimes. currently, for newly added csvs, this is always empty.\n",
      "for additional details, including the steps taken to anonymize the data, please see https://data.police.uk/about/#provenance.\n",
      "acknowledgements\n",
      "this dataset was kindly released by the british home office under the open government license 3.0 at https://data.police.uk/data/. if you are looking for more data, they cover much more than london! all major cities in england and wales are available, adding up to roughly 2gb of new data per month.\n",
      "context:\n",
      "a lemma is the uninflected form of a word. so while “tree” and “trees” are two words, they are the same lemma: “tree”. similarly, “go”, “went” and “going” are all forms of the underlying lemma “to go”. this dataset contains the most common lemmas in japanese.\n",
      "content:\n",
      "this dataset contains the most common japanese lemmas from the internet corpus, as tagged by the chasen morphological tagger for japanese (http://chasen.naist.jp/hiki/chasen/). for each lemma, both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided.\n",
      "the total corpus size is 253,071,774 tokens, with a lexicon of 451,963 types.\n",
      "acknowledgements:\n",
      "this dataset was developed at the university of leeds by centre for translation studies(more information: http://corpus.leeds.ac.uk/list.html), and is distributed under a cc-by license.\n",
      "inspiration:\n",
      "this dataset is an especially helpful resource for work on japanese texts.\n",
      "what is the distribution of hiragana, katakana and kanji characters among common lemmas?\n",
      "can you use machine translation to find the equivalent lemmas and their frequency in other languages? is there a lot of cross-linguistic difference between what concepts are the most frequent?\n",
      "which parts of speech are the most common in japanese? are these different across languages?\n",
      "context:\n",
      "building dialogue systems, where a human can have a natural-feeling conversation with a virtual agent, is a difficult task in natural language processing and the focus of much ongoing research. some of the challenges include linking references to the same entity over time, tracking what’s happened in the conversation previously, and generating appropriate responses. this corpus of naturally-occurring dialogues can be helpful for building and evaluating dialogue systems.\n",
      "content:\n",
      "the new ubuntu dialogue corpus consists of almost one million two-person conversations extracted from the ubuntu chat logs, used to receive technical support for various ubuntu-related problems. the conversations have an average of 8 turns each, with a minimum of 3 turns. all conversations are carried out in text form (not audio).\n",
      "the full dataset contains 930,000 dialogues and over 100,000,000 words and is available here. this dataset contains a sample of this dataset spread across .csv files. this dataset contains more than 269 million words of text, spread out over 26 million turns.\n",
      "folder: the folder that a dialogue comes from. each file contains dialogues from one folder .\n",
      "dialogueid: an id number for a specific dialogue. dialogue id’s are reused across folders.\n",
      "date: a timestamp of the time this line of dialogue was sent.\n",
      "from: the user who sent that line of dialogue.\n",
      "to: the user to whom they were replying. on the first turn of a dialogue, this field is blank.\n",
      "text: the text of that turn of dialogue, separated by double quotes (“). line breaks (\\n) have been removed.\n",
      "acknowledgements:\n",
      "this dataset was collected by ryan lowe, nissan pow , iulian v. serban† and joelle pineau. it is made available here under the apache license, 2.0. if you use this data in your work, please include the following citation:\n",
      "ryan lowe, nissan pow, iulian v. serban and joelle pineau, \"the ubuntu dialogue corpus: a large dataset for research in unstructured multi-turn dialogue systems\", sigdial 2015. url: http://www.sigdial.org/workshops/conference16/proceedings/pdf/sigdial40.pdf\n",
      "inspiration:\n",
      "can you use these chat logs to build a chatbot that offers help with ubuntu?\n",
      "context:\n",
      "understanding the distribution of anopheline vectors of malaria is an important prelude to the design of national malaria control and elimination programmes. a single, geo-coded continental inventory of anophelines using all available published and unpublished data has not been undertaken since the 1960s. we present the largest ever geo-coded database of anophelines in africa representing a legacy dataset for future updating and identification of knowledge gaps at national levels. the geo-coded and referenced database is made available with the related publication as a reference source for african national malaria control programmes planning their future control and elimination strategies. information about the underlying research studies can be found at http://kemri-wellcome.org/programme/population-health/.\n",
      "content:\n",
      "geocoded info on anopheline inventory. see key below.\n",
      "acknowledgements:\n",
      "kemri-wellcome trust assembled the data and distributed it on dataverse.\n",
      "inspiration:\n",
      "where have malarial mosquito populations grown or decreased?\n",
      "can you predict mosquito population growth trends?\n",
      "do you seen any correlation between mosquito populations and malaria deaths from this dataset?\n",
      "is the banner image mosquito capable of carrying malaria?\n",
      "context\n",
      "this dataset consists of 384 features extracted from ct images. the class variable is numeric and denotes the relative location of the ct slice on the axial axis of the human body. the data was retrieved from a set of 53500 ct images from 74 different patients (43 male, 31 female).\n",
      "content\n",
      "each ct slice is described by two histograms in polar space. the first histogram describes the location of bone structures in the image, the second the location of air inclusions inside of the body. both histograms are concatenated to form the final feature vector. bins that are outside of the image are marked with the value -0.25.\n",
      "the class variable (relative location of an image on the axial axis) was constructed by manually annotating up to 10 different distinct landmarks in each ct volume with known location. the location of slices in between landmarks was interpolated.\n",
      "field descriptions:\n",
      "patientid: each id identifies a different patient\n",
      "value[1-241]: histogram describing bone structures\n",
      "value[242 - 385]: histogram describing air inclusions\n",
      "386: relative location of the image on the axial axis (class value).\n",
      "values are in the range [0; 180] where 0 denotes the top of the head and 180 the soles of the feet.\n",
      "acknowledgements\n",
      "original dataset was downloaded from uci machine learning repository\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "banner image acknowledgement:\n",
      "self portre on cat scan, 1997\n",
      "title: \"soon i will be there\"\n",
      "date: 8 april 1997\n",
      "author: sérgio valle duarte\n",
      "license: cc by 3.0\n",
      "source: wikipedia\n",
      "inspiration\n",
      "predict the relative location of ct slices on the axial axis\n",
      "content\n",
      "the world glacier inventory contains information for over 130,000 glaciers. inventory parameters include geographic location, area, length, orientation, elevation, and classification. the wgi is based primarily on aerial photographs and maps with most glaciers having one data entry only. the data set can be viewed as a snapshot of the glacier distribution in the second half of the twentieth century. it was founded on the original wgi from the world glacier monitoring service.\n",
      "acknowledgements\n",
      "the national snow & ice data center continues to work with the world glacier monitoring service to update the glacier inventory database.\n",
      "content\n",
      "the corruption perceptions index scores and ranks countries/territories based on how corrupt a country’s public sector is perceived to be. it is a composite index, a combination of surveys and assessments of corruption, collected by a variety of reputable institutions. the cpi is the most widely used indicator of corruption worldwide.\n",
      "corruption generally comprises illegal activities, which are deliberately hidden and only come to light through scandals, investigations or prosecutions. there is no meaningful way to assess absolute levels of corruption in countries or territories on the basis of hard empirical data. possible attempts to do so, such as by comparing bribes reported, the number of prosecutions brought or studying court cases directly linked to corruption, cannot be taken as definitive indicators of corruption levels. instead, they show how effective prosecutors, the courts or the media are in investigating and exposing corruption. capturing perceptions of corruption of those in a position to offer assessments of public sector corruption is the most reliable method of comparing relative corruption levels across countries.\n",
      "acknowledgements\n",
      "the data sources used to calculate the corruption perceptions index scores and ranks were provided by the african development bank, bertelsmann stiftung foundation, the economist, freedom house, ihs markit, imd business school, political and economic risk consultancy, political risk services, world bank, world economic forum, world justice project, and varieties of democracy project.\n",
      "content\n",
      "the advanced placement exam scores for the class of 2016, highlighted in this dataset, show that students continue to demonstrate college-level skills and knowledge in increasing numbers. even as ap teachers deliver rigor to an ever-diversifying population of students, participation and performance continue to improve. behind and within these data are the daily sacrifices of ap students and teachers, including the late nights that students put in diligently studying and the weekends that teachers give up to help their students succeed. their hard work and effort are worth celebrating.\n",
      "acknowledgements\n",
      "this data was collected and released by the college board after the may 2016 exam administration.\n",
      "context\n",
      "between 1901 and 2016, the nobel prizes and the prize in economic sciences were awarded 579 times to 911 people and organizations. the nobel prize is an international award administered by the nobel foundation in stockholm, sweden, and based on the fortune of alfred nobel, swedish inventor and entrepreneur. in 1968, sveriges riksbank established the sveriges riksbank prize in economic sciences in memory of alfred nobel, founder of the nobel prize. each prize consists of a medal, a personal diploma, and a cash award.\n",
      "a person or organization awarded the nobel prize is called nobel laureate. the word \"laureate\" refers to being signified by the laurel wreath. in ancient greece, laurel wreaths were awarded to victors as a sign of honor.\n",
      "content\n",
      "this dataset includes a record for every individual or organization that was awarded the nobel prize since 1901.\n",
      "acknowledgements\n",
      "the nobel laureate data was acquired from the nobel prize api.\n",
      "inspiration\n",
      "which country has won the most prizes in each category? what words are most frequently written in the prize motivation? can you predict the age, gender, and nationality of next year's nobel laureates?\n",
      "context:\n",
      "the cfsan adverse event reporting system (caers) is a database that contains information on adverse event and product complaint reports submitted to fda for foods, dietary supplements, and cosmetics. the database is designed to support cfsan's safety surveillance program. adverse events are coded to terms in the medical dictionary for regulatory activities (meddra) terminology.\n",
      "content:\n",
      "see the metadata description in the accompanying readme.pdf below or here. approximately 90k reactions are recorded from 2004-mid 2017, with 12 columns of information regarding type of reaction and related event details.\n",
      "acknowledgements:\n",
      "this dataset is collected by the us food and drug administration.\n",
      "inspiration:\n",
      "what are the most commonly reported foodstuffs?\n",
      "what are the most commonly reported medical reactions to foods?\n",
      "where do people in the us most commonly report food-related conditions?\n",
      "introduction\n",
      "this is the keystroke dataset for the study titled 'high-accuracy detection of early parkinson's disease using multiple characteristics of finger movement while typing'. this research report is currently under review for publication by plos one.\n",
      "the dataset contains keystroke logs collected from over 200 subjects, with and without parkinson's disease (pd), as they typed normally on their own computer (without any supervision) over a period of weeks or months (having initially installed a custom keystroke recording app, tappy). this dataset has been collected and analyzed in order to indicate that the routine interaction with computer keyboards can be used to detect changes in the characteristics of finger movement in the early stages of pd.\n",
      "data\n",
      "the participants, from the u.s., canada, uk and australia, had visited the project website and agreed to participate in the study. the research was approved by the human research ethics committee at charles sturt university, australia, protocol number h17013.\n",
      "each data file collected includes the timing information from typing activity as the participants used their various windows applications (such as email, word processing, web searches and the like). the keystroke acquisition software ('tappy') provided timing accuracy of key press and release timestamps to within several milliseconds.\n",
      "the data files comprise two zip archives, one with the participant detail files and the other with the keystroke data files for each user.\n",
      "acknowledgements\n",
      "this dataset is from the research article \"high-accuracy detection of early parkinson's disease using multiple characteristics of finger movement while typing\" by warwick r. adams. read the article here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188226#sec008\n",
      "inspiration\n",
      "while this is a difficult dataset to work with, there is a rich trove of information. it is a great set to practice preprocessing, attempt to replicate the results of the article, or do your own analysis of keystroke data.\n",
      "deepsat sat-4\n",
      "\n",
      "\n",
      "originally, images were extracted from the national agriculture imagery program (naip) dataset. the naip dataset consists of a total of 330,000 scenes spanning the whole of the continental united states (conus). the authors used the uncompressed digital ortho quarter quad tiles (doqqs) which are geotiff images and the area corresponds to the united states geological survey (usgs) topographic quadrangles. the average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. the entire naip dataset for conus is ~65 terabytes. the imagery is acquired at a 1-m ground sample distance (gsd) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points.\n",
      "the images consist of 4 bands - red, green, blue and near infrared (nir). in order to maintain the high variance inherent in the entire naip dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of california. an image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.\n",
      "once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. we chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. care was taken to avoid interclass overlaps within a selected and labeled image patch.\n",
      "content\n",
      "each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared.\n",
      "the training and test labels are one-hot encoded 1x4 vectors\n",
      "the four classes represent the four broad land covers which include barren land, trees, grassland and a class that consists of all land cover classes other than the above three.\n",
      "training and test datasets belong to disjoint set of image tiles.\n",
      "each image patch is size normalized to 28x28 pixels.\n",
      "once generated, both the training and testing datasets were randomized using a pseudo-random number generator.\n",
      "csv files\n",
      "x_train_sat4.csv: 400,000 training images, 28x28 images each with 4 channels\n",
      "y_train_sat4.csv: 400,000 training labels, 1x4 one-hot encoded vectors\n",
      "x_test_sat4.csv: 100,000 training images, 28x28 images each with 4 channels\n",
      "y_test_sat4.csv: 100,000 training labels, 1x4 one-hot encoded vectors\n",
      "the original mat file\n",
      "train_x: 28x28x4x400000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)\n",
      "train_y: 400000x4 uint8 (containing 4x1 vectors having labels for the 400000 training samples)\n",
      "test_x: 28x28x4x100000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)\n",
      "test_y: 100000x4 uint8 (containing 4x1 vectors having labels for the 100000 test samples)\n",
      "acknowledgements\n",
      "the original matlab file was converted to multiple csv files\n",
      "the original sat-4 and sat-6 airborne datasets can be found here:\n",
      "http://csc.lsu.edu/~saikat/deepsat/\n",
      "thanks to:\n",
      "saikat basu, robert dibiano, manohar karki and supratik mukhopadhyay, louisiana state university sangram ganguly, bay area environmental research institute/nasa ames research center ramakrishna r. nemani, nasa advanced supercomputing division, nasa ames research center\n",
      "context\n",
      "welcome. this is a women’s clothing e-commerce dataset revolving around the reviews written by customers. its nine supportive features offer a great environment to parse out the text through its multiple dimensions. because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with “retailer”.\n",
      "content\n",
      "this dataset includes 23486 rows and 10 feature variables. each row corresponds to a customer review, and includes the variables:\n",
      "clothing id: integer categorical variable that refers to the specific piece being reviewed.\n",
      "age: positive integer variable of the reviewers age.\n",
      "title: string variable for the title of the review.\n",
      "review text: string variable for the review body.\n",
      "rating: positive ordinal integer variable for the product score granted by the customer from 1 worst, to 5 best.\n",
      "recommended ind: binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
      "positive feedback count: positive integer documenting the number of other customers who found this review positive.\n",
      "division name: categorical name of the product high level division.\n",
      "department name: categorical name of the product department name.\n",
      "class name: categorical name of the product class name.\n",
      "acknowledgements\n",
      "anonymous\n",
      "inspiration\n",
      "i look forward to come quality nlp! there is also some great opportunities for feature engineering, and multivariate analysis.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 5.8 million products) that was created by extracting data from flipkart.com, a leading indian ecommerce store.\n",
      "content\n",
      "this dataset has following fields:\n",
      "product_url\n",
      "product_name\n",
      "product_category_tree\n",
      "pid\n",
      "retail_price\n",
      "discounted_price\n",
      "image\n",
      "is_fk_advantage_product\n",
      "description\n",
      "product_rating\n",
      "overall_rating\n",
      "brand\n",
      "product_specifications\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of the pricing, product specification and brand can be performed.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 9.4 million job listings) that was created by extracting data from naukri.com, a leading job board.\n",
      "content\n",
      "this dataset has following fields:\n",
      "company\n",
      "education\n",
      "experience\n",
      "industry\n",
      "job description\n",
      "jobid\n",
      "joblocation_address\n",
      "job title\n",
      "number of positions\n",
      "pay rate\n",
      "postdate\n",
      "site_name\n",
      "skills\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of the pay rate, job title, industry and experience can be performed to name a few starting points.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 1.8 million restaurants) that was created by extracting data from tripadvisor.co.uk.\n",
      "content\n",
      "this dataset has following fields:\n",
      "uniq_id\n",
      "url\n",
      "restaurant_id\n",
      "restaurant_location\n",
      "name\n",
      "category\n",
      "title\n",
      "review_date\n",
      "review_text\n",
      "author\n",
      "author_url\n",
      "location\n",
      "rating\n",
      "food\n",
      "value\n",
      "service\n",
      "visited_on\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of the restaurant reviews and ratings can be performed.\n",
      "the tmy3s are data sets of hourly values of solar radiation and meteorological elements for a 1-year period. their intended use is for computer simulations of solar energy conversion systems and building systems to facilitate performance comparisons of different system types, configurations, and locations in the united states and its territories. because they represent typical rather than extreme conditions, they are not suited for designing systems to meet the worst-case conditions occurring at a location.\n",
      "please note that tmy3 is not the state of the art solar data. it was used as a key component of investment analyses for several years, but nrel has released a more recent version based on satellite data and updated meteorological models that provides coverage for the entire united states. that dataset is much too large to publish here, but is highly recommended if you need the best information.\n",
      "content\n",
      "please see the pdf manual for full details of each field; there are several dozen of them.\n",
      "it's important to know that nearly all of the solar data is modeled based on estimates of cloud cover; less than 1% of the stations directly measured sunlight.\n",
      "this data is not appropriate for time series analysis. a typical meteorological year is literally twelve months of real data from twelve different years. please see the manual for further details.\n",
      "acknowledgements\n",
      "this dataset was made available by the national renewable energy laboratory. you can find the original dataset here.\n",
      "if you like\n",
      "if you liked this dataset, you might also enjoy:\n",
      "google project sunroof\n",
      "30 years of european wind generation\n",
      "30 years of european solar generation\n",
      "we don't always think about industrial scale food, but cheese blocks the size of a small car are important.\n",
      "the mandatory price reporting act of 2010 (pdf) was passed on september 27, 2010, the act required usda to release dairy product sales information on or before wednesday at 3:00 pm est (unless affected by a federal holiday). the act also required the establishment of an electronic mandatory price reporting system for dairy products reported under public law 106-532. these dairy statistics will continue to be collected on a weekly basis, ams-dairy programs will collect, analyze, aggregate, and publish dairy product sales information for selected dairy commodities.\n",
      "acknowledgements\n",
      "this data is released by the us department of agriculture. you can find the original dataset here.\n",
      "inspiration\n",
      "can you predict changes in moisture content for 40 pound blocks of cheese?\n",
      "context:\n",
      "few events in american history are better known than the salem witchcraft trials of 1692. its popularity is doubtless attributable to a number of things: a persistent fascination with the occult; a perverse pleasure to expose the underbelly of an american culture that boasts of toleration, social harmony, and progress; and an appreciation for a compelling, dramatic narrative replete with heroes and villains. skeptics, like the preeminent twentieth-century historian perry miller, question whether the salem trials constituted anything more than an inconsequential episode in colonial history. but most historians consider salem worthy of continuing investigation even if it was less than a major turning point in history. indeed, salem has been an unusually fertile field for historical research because it readily lends itself to new approaches, insights, and methodologies. to understand what happened in salem, historians have profitably applied the perspectives of politics, anthropology, economic and social analysis, religion, social psychology, and demography. if the ultimate meaning of salem is still elusive, these investigations have broadened and deepened our understanding of the 1692 witchcraft outbreak.\n",
      "content:\n",
      "the salem witchcraft website contains eight data sets. they provide only a small portion of the historical record about salem. they do not contain transcripts of examinations or trials or contemporary narrative accounts, for example. instead, they provide information, primarily of a quantitative nature, about three major aspects of the outbreak: its chronology, its geographic spread, and the social and economic divisions in salem village that shaped events. the data were derived primarily from four published sources: paul boyer and stephen nissenbaum's three-volume transcription of the legal records of the witchcraft trials, the salem witchcraft papers; the new and now authoritative records of the salem witch-hunt, edited by bernard rosenthal, et. al.; boyer and nissenbaum's edited collection of documents, salem-village witchcraft; and salem village's book of record, which contain tax records and other information relating to salem village. photocopies of the original salem village record book and church records were examined at the danvers archival center.\n",
      "the accused witches data set contains information about those who were formally accused of witchcraft during the salem episode. this means that there exists evidence of some form of direct legal involvement, such as a complaint made before civil officials, an arrest warrant, an examination, or court record. accused witches were almost always detained in jail to await further action by a grand jury, which had the authority to indict and hold the accused for trial. trials by a special court of oyer and terminer began in june 1692. in october 1692, this court was discontinued due to mounting criticism of its methods. it was replaced by another court, the superior court of judicature, which held trials from january to may 1693.\n",
      "the \"accused witch\" column records the names of the 152 people mentioned in legal records as having been formally accused of witchcraft. their names are alphabetically arranged. spelling generally follows that of paul boyer and stephen nissenbaum, salem witchcraft papers but has been sometimes changed in accordance with the newer records of the salem witch-hunt and other sources.\n",
      "\"residence\" identifies the community in which the accused person was living when accused of witchcraft. in a few cases, the residence of an accused witch is problematic. for elizabeth how, for example, some records cite ipswich while others name topsfield as her home. in such cases, the most likely residence has been used. in a few instances, the residence entry does not reflect the actual geographic relationship of the accused with the trials. george burroughs was living in wells, maine, in 1692, but he had been a controversial minister in salem village in the early 1680s.\n",
      "\"month of accusation\" numerically expresses the month of the year in which an alleged witch was accused: \"1\"=january 1692; \"6\"=june 1692; and \"13\"=january 1693. a negative 1 (-1) indicates that the actual month of accusation is not known with sufficient certainty to be included. some of these \"unknowns\" can be approximated from available records, and users may choose to substitute their estimate. users should also recognize an artificial quality to this data: those accused in one month, may (5), for example, may have been charged only a day or two before someone in june (6).\n",
      "\"month of execution\" numerically expresses the month in which an alleged witch was executed as a result of the legal process. the data do not include entries for those who died as a result of their incarceration. in one case, giles corey, the month of execution does record the month in which he was pressed to death for refusing to plead to the charges against him.\n",
      "the towns data set provides a convenient way to construct histograms of the communities whose residents were charged with witchcraft in 1692. it contains 25 columns:\n",
      "twenty-four columns record each town for which at least one formal accusation occurred (salem village and salem town are listed separately). each cell lists the month of an accusation, numerically expressed: 1=january 1692, 2=february 1692, and so forth. the negative number, -1, indicates that the month of accusation is unknown.\n",
      "a \"bin\" column contains the range of months of witchcraft accusations, from 1 (january 1692) to 12 (december 1692), with -1 for unknown months of accusation.\n",
      "both the pro-parris and anti-parris data sets contain the same four columns:\n",
      "\"name\" identifies each signer of the pro-parris petition.\n",
      "\"identification\" indicates the category in the petition under which the signer was placed.\n",
      "\"sex\" indicates whether the signer was male or female.\n",
      "\"sort\" locates each signer in the data set so that it can be returned to its original order.\n",
      "to compare the social make-up of salem village's pro- and anti-parris factions to the village's general population, download the salem village data set. the data set contains four columns:\n",
      "\"name\" lists every person in salem village who appeared on any village tax assessments for 1690, 1695, and 1697. the 137 names are a good, though imperfect, indicator of the village's adult male population in the period of the witch hunt. only a few women, all widows, appear. young men not yet independent or paying taxes do not appear.\n",
      "\"petition\" notes whether the taxpayer signed either the pro- or anti-parris petition in 1695. \"nos\" (no signature) means that the person did not sign either petition.\n",
      "\"church to 1696\" indicates whether a person was a church member though 1696. no distinction is made as to whether a person was a member of the salem village church or another church. the list is compiled from the pro- and anti-parris petitions as well as from the records of the salem village church as recorded by samuel parris. additional information came from the published records of the first church in salem town.\n",
      "\"sort\" permits data to be easily restored to their original order after a statistical manipulation.\n",
      "the committee yearly data set contains information about salem village's committees from 1685 to 1698, a period that covers the last years deodat lawson's ministry and the entire tenure of samuel parris. the data set contains three columns of information for each committee:\n",
      "\"committee\" lists the names of committeemen for a particular year (in 1688, only four men were elected).\n",
      "\"petition\" indicates whether the committeeman signed either the pro- or anti-parris petition in 1695. \"nos\" (no signature) means that this committeeman did not sign either petition. signing a petition strongly suggests but does not conclusively establish a petitioner's earlier position regarding parris or the witchcraft outbreak.\n",
      "\"social\" indicates whether the committeeman was a church member or a householder. three committeemen (william sibley, james smith, and jacob fuller) have been listed as householders in the absence of information linking them to a church.)\n",
      "the committee list data set provides information about all salem village committee members who held office from 1685 to 1698. the data set contains 18 columns:\n",
      "\"committee members\" records the names of the thirty-one villagers who held committee office from 1685-1698. they are listed in the order in which they first appeared in the village's book of record.\n",
      "\"petition\" notes whether the committeeman signed either the pro- or anti-parris petition in 1695. \"nos\" (no signature) means that this committeeman did not sign either petition.\n",
      "\"social\" indicates whether the committeeman was a church member or a householder. (three committeemen, william sibley, james smith, and jacob fuller, have been listed as householders in the absence of information linking them to a church; the three did not sign either petition.)\n",
      "columns 4-17 indicate committee membership for each year.\n",
      "\"sort\" permits data to be easily restored to their original order.\n",
      "the tax comparison data set was compiled by listing all salem village taxpayers who were assessed rates in the period between 1681 and 1700. the rates are recorded in salem village's book of record (see bibliography).\n",
      "\"name\" lists in alphabetical order all assessments on salem village's tax lists for 1681, 1690, 1694, 1695, 1697, and 1700.\n",
      "\"tax\" records the taxpayer's assessment in shillings. since the village's revenue needs changed, the total assessment (and individual allocations) changed accordingly.\n",
      "\"petition\" indicates whether the taxpayer signed either the pro- or anti-parris petition in 1695. \"nos\" (no signature) means that this taxpayer did not sign either petition.\n",
      "\"sort\" permits data to be easily restored to their original order after a statistical manipulation.\n",
      "acknowledgements:\n",
      "users who copy, share, adapt, and re-publish any of the content in salem witchcraft dataset should credit professor richard latner of tulane university for making this material available. more information and guided exercises can be found on this website.\n",
      "inspiration:\n",
      "what was the relationship between economic success and support for parris?\n",
      "can you split the list of accused witches and predict who would be accused based on other acquisitions?\n",
      "context:\n",
      "how frequently a word occurs in a language is an important piece of information for natural language processing and linguists. in natural language processing, very frequent words tend to be less informative than less frequent one and are often removed during preprocessing. human language users are also sensitive to word frequency. how often a word is used affects language processing in humans. for example, very frequent words are read and understood more quickly and can be understood more easily in background noise.\n",
      "content:\n",
      "this dataset contains the counts of the 333,333 most commonly-used single words on the english language web, as derived from the google web trillion word corpus.\n",
      "acknowledgements:\n",
      "data files were derived from the google web trillion word corpus (as described by thorsten brants and alex franz, and distributed by the linguistic data consortium) by peter norvig. you can find more information on these files and the code used to generate them here.\n",
      "the code used to generate this dataset is distributed under the mit license.\n",
      "inspiration:\n",
      "can you tag the part of speech of these words? which parts of speech are most frequent? is this similar to other languages, like japanese?\n",
      "what differences are there between the very frequent words in this dataset, and the the frequent words in other corpora, such as the brown corpus or the timit corpus? what might these differences tell us about how language is used?\n",
      "content\n",
      "the contents of this data set comes from public data available on the city of portland website. each individual crime reported is lists the location, time and date of the incident as well as a the neighborhood in which the event occurred.\n",
      "all data prior to 2015 has the same general format but the newer 2015-17 data needs to be reformatted for easier comparison since it does not match the older organizational scheme. to this end i will be adding new .csv with 2015 , 2016, and 2017 ytd data broken out. coordinate data will also be added to make the data sets more easily comparable and mappable.\n",
      "update: i created new .csv for each year 2015-2017 changing the formatting from the portland police department's tab separated values to the standard comma separated values. the pre-2015 data still isn't comparable because of the differences in the crime categorization but i will work creating some sort of key so that the full data set can be analyzed as a single batch of information.\n",
      "acknowledgements\n",
      "banner image by zack spear on unsplash.\n",
      "all data gathered from portlandoregon.gov and civicapps.org\n",
      "context\n",
      "the monthly salary of the public workers of the state of são paulo in brazil is a public data available in the transparency portal of the state government at: http://www.transparencia.sp.gov.br/buscaremunera.html\n",
      "content\n",
      "the data is about the salary for all worker in the state for the month of october 2017. there are just over one million records. the names of the employee are anonymous represented by the variable id.\n",
      "inspiration\n",
      "this database may reveal:\n",
      "higher salaries\n",
      "the contribution of extra remuneration to higher salaries\n",
      "by the rules of the government no employee could receive more than the state governor salary: r$ 21,631.05\n",
      "«datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\n",
      "context\n",
      "en aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.\n",
      "hem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. és important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.\n",
      "content\n",
      "en aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.\n",
      "pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.\n",
      "pel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. els camps especialment destacats són:\n",
      "• data: data de la observació\n",
      "• nom: nom empresa o cryptomoneda, per identificar de quina moneda o index estem representant.\n",
      "• símbol: símbol de la moneda o del index borsatil, per realitzar gràfic posteriorment d’una forma mes senzilla que el nom.\n",
      "• preu: valor en euros d’una acció o una cryptomoneda (transformarem la moneda a euros en el cas de estigui en dòlars amb l'última cotització (un dollar a 0,8501 euro)\n",
      "• tipus_cotitzacio: valor nou que agregarem per discretitzar entre la cotització: baix (0 i 1), normal (1 i 100), alt (100 i 1000), molt_alt (&gt;1000)\n",
      "script r\n",
      "anàlisis de les observacions i el domini de les dades\n",
      "anàlisis en especial de bitcoin i la iota.\n",
      "test de levene per veure la homogeneitat\n",
      "kmeans per creació de cluster per veure la homegeneitat\n",
      "freqüències de les distribucions\n",
      "test de contrast d'hipòtesis de variables dependents (wilcoxon)\n",
      "test de shapiro-wilk per veure la normalitat de les dades, per normalitzar-les o no\n",
      "correlació d'índexs borsatils, per eliminar complexitat dels índexs amb grau més alt de correlació\n",
      "iteració de regressions lineals per obtenir el model amb més qualitat, observa'n el p-valor i l'índex de correlació\n",
      "validació de la qualitat del model\n",
      "representació grafica\n",
      "acknowledgements\n",
      "en aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:\n",
      "http://www.eleconomista.es\n",
      "https://coinmarketcap.com\n",
      "per aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. pel que fa a la terminologia financera podem veure vocabulari en renta4banco.\n",
      "[https://www.r4.com/que-necesitas/formacion/diccionario]\n",
      "inspiration\n",
      "hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:\n",
      "https://arxiv.org/pdf/1410.1231v1.pdf\n",
      "en aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.\n",
      "la comunitat podrà respondre, entre altres preguntes, a:\n",
      "està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'espanya?\n",
      "els efectes o agents externs afecten per igual a les accions o cryptomonedes?\n",
      "hi ha relacions cause efecte entre les acciones i cryptomonedes?\n",
      "project repository\n",
      "https://github.com/acostasg/scraping\n",
      "datasets\n",
      "els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:\n",
      "https://www.kaggle.com/acostasg/stock-index/\n",
      "https://www.kaggle.com/acostasg/crypto-currencies\n",
      "per una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. de forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció.\n",
      "context\n",
      "the dataset is used by \"a temperature-forecasting problem\" from the \"deep learning with python\" book\n",
      "content\n",
      "the data was downloaded from: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
      "it represents time period between 2009 and 2016\n",
      "acknowledgements\n",
      "the dataset recorded at the weather station at the max planck institute for biogeochemistry in jena, germany. https://www.bgc-jena.mpg.de/wetter/\n",
      "it was reassembled by françois chollet, the author of the \"deep learning with python\" book\n",
      "inspiration\n",
      "the main purpose of this dataset is to perform rnn exercise (6.3.1 a temperature-forecasting problem) from the \"deep learning with python\" book.\n",
      "the physician compare website was created by the centers for medicare & medicaid services (cms) in december 2010 as required by the affordable care act (aca) of 2010 to help patients assess and find doctors and hospitals. this dataset contains the information supplied to patients via that website, including patient satisfaction surveys and performance scores across over 100 metrics.\n",
      "acknowledgements\n",
      "this dataset was kindly released by the centers for medicare & medicaid services. you can find the original copy of the dataset here.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 4.6 million job listings) that was created by extracting data from dice.com, a prominent us-based technology job board.\n",
      "content\n",
      "this dataset has following fields:\n",
      "advertiserurl\n",
      "company\n",
      "employmenttype_jobstatus\n",
      "jobdescription\n",
      "joblocation_address\n",
      "jobtitle\n",
      "postdate\n",
      "shift\n",
      "skills\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of the job description with respect to the job title and skills can be performed.\n",
      "context:\n",
      "pluto is a master record of the locations and characteristics of buildings in new york city. it’s published by the new york city department of city planning on an approximately quarterly-to-half-yearly basis, and is one of the more important datasets for civic analysis in new york city.\n",
      "content:\n",
      "pluto includes information on building height, square footage, location, type, landmark status, number of units, owner, year of construction, and other related fields.\n",
      "acknowledgements:\n",
      "this dataset is published as-is by the new york city department of planning.\n",
      "inspiration:\n",
      "what is the distribution of the heights of buildings in new york city? the age?\n",
      "can you define neighborhoods by clustering similar buildings within them?\n",
      "what (and where) is the split between commercial, residential, and office space in new york city?\n",
      "context\n",
      "this dataset contains the salary, pay rate, and total compensation of every new york city employee. in this dataset this information is provided for the 2014, 2015, 2016, and 2017 fiscal years, and provides a transparent lens into who gets paid how much and for what.\n",
      "note that fiscal years in the new york city budget cycle start on july 1st and end on june 30th (see here). that means that this dataset contains, in its sum, compensation information for all city of new york employees for the period july 1, 2014 to june 30, 2017.\n",
      "content\n",
      "this dataset provides columns for fiscal year, employee name, the city department they work for, their job title, and various fields describing their compensation. the most important of these fields is \"regular gross pay\", which provides that employee's total compensation.\n",
      "acknowledgements\n",
      "this information was published as-is by the city of new york.\n",
      "inspiration\n",
      "how many people do the various city agencies employ, and how much does each department spend on salary in total?\n",
      "what are the most numerous job titles in civic government employment?\n",
      "where does overtime pay seem to be especially common? how much of it is there?\n",
      "how do new york city employee salaries compare against salaries of city employees in chicago? is the difference more or less than the difference in cost of living between the two cities?\n",
      "context\n",
      "the new york city department of transportation collects daily data about the number of bicycles going over bridges in new york city. this data is used to measure bike utilization as a part of transportation planning. this dataset is a daily record of the number of bicycles crossing into or out of manhattan via one of the east river bridges (that is, excluding bronx thruways and the non-bikeable hudson river tunnels) for a stretch of 9 months.\n",
      "content\n",
      "a count of the number of bicycles on each of the bridges in question is provided on a day-by-day basis, along with information on maximum and minimum temperature and precipitation.\n",
      "acknowledgements\n",
      "this data is published in an excel format by the city of new york (here). it has been processed into a csv file for use on kaggle.\n",
      "inspiration\n",
      "in this dataset, how many bicycles cross into and out of manhattan per day?\n",
      "how strongly do weather conditions affect bike volumes?\n",
      "what is the top bridge in terms of bike load?\n",
      "context:\n",
      "rats in new york city are prevalent, as in many densely populated areas. for a long time, the exact number of rats in new york city was unknown, and a common urban legend was that there were up to four times as many rats as people. in 2014, however, scientists more accurately measured the entire city's rat population to be approximately only 25% of the number of humans; i.e., there were approximately 2 million rats to new york's 8.4 million people at the time of the study.[1][2]\n",
      "content:\n",
      "new york city rodent complaints can be made online, or by dialing 3-1-1, and the new york city guide preventing rats on your property discusses how the new york city health department inspects private and public properties for rats. property owners that fail inspections receive a commissioner's order and have five days to correct the problem. if after five days the property fails a second inspection, the owner receives a notice of violation and can be fined. the property owner is billed for any clean-up or extermination carried out by the health department.\n",
      "data is from 2010-sept 16th, 2017 and includes date, location (lat/lon), type of structure, borough, and community board.\n",
      "acknowledgements:\n",
      "data was produced by the city of new york via their 311 portal.\n",
      "inspiration:\n",
      "where and when are rats most seen?\n",
      "can you predict rat sightings from previous data?\n",
      "are there any trends in rat sightings?\n",
      "all the data was taken from open data zurich (https://data.stadt-zuerich.ch/dataset/pd-stapo-hundebestand) with the idea of making a useful few kernel demos from it and let people look at information about the dogs that live here.\n",
      "german\n",
      "since german is the official language of zurich most of the columns are in german but the translations to english aren't too tricky\n",
      "alter -> age\n",
      "geschlecht -> gender\n",
      "stadtkreis -> city quarter or district\n",
      "rasse1 -> dog's primary breed\n",
      "rasse2 -> dog's secondary breed\n",
      "geburtsjahr_hund -> dog's year of birth\n",
      "geschlecht_hund -> dog's gender\n",
      "hundefarbe -> dog's color\n",
      "utility\n",
      "it might also help people trying to find apartments in areas with the right kind of dogs\n",
      "could be used to look at how dog trends have changed in time (by looking at the numbers by birth year)\n",
      "helpful for picking the right kind of dog to get your 90 year old grandmother (what kind of dogs do other 90 year old women have)\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "to understand the foreign direct investment in india for the last 17 years from 2000-01 to 2016-17.\n",
      "content\n",
      "this dataset contains sector and financial year wise data of fdi in india.\n",
      "acknowledgements\n",
      "ministry of commerce and industry has published financial year wise fdi equity inflows from 2000-01 to 2016-17 dataset in open government data platform india under govt. open data license - india.\n",
      "inspiration\n",
      "how much fdi has changed over the year?\n",
      "how much has varied since 2014 after narendra modi become pm of india?\n",
      "context\n",
      "the major league soccer union releases the salaries of every mls player each year. this is a collection of salaries from 2007 to 2017.\n",
      "content\n",
      "each file contains the following fields:\n",
      "club: team abbreviation\n",
      "last_name: player last name\n",
      "first_name: player first name\n",
      "position: position abbreviation\n",
      "base_salary: base salary\n",
      "guaranteed_compensation: guaranteed compensation\n",
      "acknowledgements\n",
      "jeremy singer-vine over at data is plural scraped the pdf's released by the mls union and put the data in a nice little package of csv files for everyone.\n",
      "i downloaded this dataset from: https://github.com/data-is-plural/mls-salaries mit license\n",
      "inspiration\n",
      "who in the mls makes the most money? are they worth it? i make about $900 bazillion each year, can i afford a soccer team?\n",
      "context\n",
      "this dataset is a collection newsgroup documents. the 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
      "content\n",
      "there is file (list.csv) that contains a reference to the document_id number and the newsgroup it is associated with. there are also 20 files that contain all of the documents, one document per newsgroup.\n",
      "in this dataset, duplicate messages have been removed and the original messages only contain \"from\" and \"subject\" headers (18828 messages total).\n",
      "each new message in the bundled file begins with these four headers:\n",
      "newsgroup: alt.newsgroup\n",
      "document_id: xxxxxx\n",
      "from: cat\n",
      "subject: meow meow meow\n",
      "the newsgroup and document_id can be referenced against list.csv\n",
      "organization - each newsgroup file in the bundle represents a single newsgroup - each message in a file is the text of some newsgroup document that was posted to that newsgroup.\n",
      "this is a list of the 20 newsgroups:\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "misc.forsale talk.politics.misc\n",
      "talk.politics.guns\n",
      "talk.politics.mideast talk.religion.misc\n",
      "alt.atheism\n",
      "soc.religion.christian\n",
      "acknowledgements\n",
      "ken lang is credited by the source for collecting this data. the source of the data files is here:\n",
      "http://qwone.com/~jason/20newsgroups/\n",
      "inspiration\n",
      "this dataset text can be used to classify text documents\n",
      "context\n",
      "during the 2016 us presidential election, the phrase “fake news” found its way to the forefront in news articles, tweets, and fiery online debates the world over after misleading and untrue stories proliferated rapidly. buzzfeed news analyzed over 1,000 stories from hyperpartisan political facebook pages selected from the right, left, and mainstream media to determine the nature and popularity of false or misleading information they shared.\n",
      "content\n",
      "this dataset supports the original story “hyperpartisan facebook pages are publishing false and misleading information at an alarming rate” published october 20th, 2016. here are more details on the methodology used for collecting and labeling the dataset (reproduced from the story):\n",
      "more on our methodology and data limitations\n",
      "“each of our raters was given a rotating selection of pages from each category on different days. in some cases, we found that pages would repost the same link or video within 24 hours, which caused facebook to assign it the same url. when this occurred, we did not log or rate the repeat post and instead kept the original date and rating. each rater was given the same guide for how to review posts:\n",
      "“mostly true: the post and any related link or image are based on factual information and portray it accurately. this lets them interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up. this rating does not allow for unsupported speculation or claims.\n",
      "“mixture of true and false: some elements of the information are factually accurate, but some elements or claims are not. this rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate. it should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link. finally, use this rating for news articles that are based on unconfirmed information.\n",
      "“mostly false: most or all of the information in the post or in the link being shared is inaccurate. this should also be used when the central claim being made is false.\n",
      "“no factual content: this rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim. this is also the category to use for posts that are of the “like this if you think...” variety.\n",
      "“in gathering the facebook engagement data, the api did not return results for some posts. it did not return reaction count data for two posts, and two posts also did not return comment count data. there were 70 posts for which the api did not return share count data. we also used crowdtangle's api to check that we had entered all posts from all nine pages on the assigned days. in some cases, the api returned urls that were no longer active. we were unable to rate these posts and are unsure if they were subsequently removed by the pages or if the urls were returned in error.”\n",
      "acknowledgements\n",
      "this dataset was originally published on github by buzzfeed news here: https://github.com/buzzfeednews/2016-10-facebook-fact-check\n",
      "inspiration\n",
      "here are some ideas for exploring the hyperpartisan echo chambers on facebook:\n",
      "how do left, mainstream, and right categories of facebook pages differ in the stories they share?\n",
      "which types of stories receive the most engagement from their facebook followers? are videos or links more effective for engagement?\n",
      "can you replicate buzzfeed’s findings that “the least accurate pages generated some of the highest numbers of shares, reactions, and comments on facebook”?\n",
      "start a new kernel\n",
      "context\n",
      "the journal of the american chemical society is the premier journal published by the american chemical society and one of the highest ranking journals in all of chemistry. with almost 60,000 papers and over 120,000 authors this collections of papers published between 1996 and 2016, represents the current state of chemistry research.\n",
      "content\n",
      "this dataset is presented in 3 database tables, one of published articles, and one of all authors, with a further table relating authors to the journal articles they have published.\n",
      "update 21-12-2017: the previous data was collected by a top level scrape of the table of contents pages from the journal.\n",
      "a couple of months ago i performed a page-level scrape and then forgot about it, but i had some positive reactions to the data-set this week, so i have processed some of the data (although more remains, the raw output from the scrape is an 18 gb csv file).\n",
      "this new data updates the articles table to contain many more data fields, including the paper abstract, number of citations and page views (page views are a relatively new feature, so is probably not for the lifetime of some of the older papers).\n",
      "one interesting project for this data would be to look at the term frequencies in the abstracts of the papers, and use that to see how the focus of chemistry research has changed over the years.\n",
      "if it is interesting to people, i have the following unprocessed data: - articles citing papers in this database inc, title, journal, year and author list - institutions of authors for each papers (this data is very complicated and requires some difficult parsing)\n",
      "acknowledgements\n",
      "this data was scraped from the table of contents section of the jacs website and is available online publicly.\n",
      "inspiration\n",
      "this data could be used to determine the average number of authors per paper, or the connections between authors, to determine if specific research fields can be grouped by the associated authors\n",
      "also see if you can find the two papers published by me in this year, and see who my co-authors were!\n",
      "context\n",
      "the story behind this data set and analysis is just a combination of my interest in data science and metal music. i was looking for interesting data that i could analyze and this happen to be one of the first i started exploring.\n",
      "content\n",
      "the world population information was a direct download so i did not have to do any work to get it. this data set consists of population information for countries on earth from the years 1960 to 2015.\n",
      "the metal band information was scraped from the website http://metalstorm.net/ and consists of the following - band name - how many fans the band has on the website - when the band formed - when the band split - the country of origin on the band - the styles of the band\n",
      "acknowledgements\n",
      "the metal information was compiled from information found on http://metalstorm.net/, the world population information is from http://www.worldbank.org/.\n",
      "inspiration\n",
      "there has already been some great analysis of metal bands on kaggle, i wanted to contribute to the discussion by adding some new data and looking at it from a slightly different angle. also i thought it might be useful to share the process by which i came up with the visualizations and data management since the community has been a big help to me.\n",
      "context\n",
      "observations of particles much smaller than us, and various understandings of those particles, have propelled mankind forward in ways once impossible to imagine. \"the elements\" are what we call the sequential patterns in which some of these particles manifest themselves.\n",
      "as a chemistry student and a coder, i wanted to do what came naturally to me and make my class a bit easier by coding/automating my way around some of the tedious work involved with calculations. unfortunately, it seems that chemical-related datasets are not yet a thing which have been conveniently formatted into downloadable databases (as far as my research went). i decided that the elements would be a good place to start data collection, so i did that, and i'd like to see if this is useful to others as well.\n",
      "other related data sets i'd like to coalesce are some large amount of standard entropies and enthalpies of various compounds, and many of the data sets from the crc handbook of chemistry and physics. i also think as many diagrams as possible should be documented in a way that can be manipulated and read via code.\n",
      "content\n",
      "included here are three data sets. each data set i have included is in three different formats (csv, json, excel), for a total of nine files.\n",
      "table of the elements:\n",
      "this is the primary data set.\n",
      "118 elements in sequential order\n",
      "72 features\n",
      "reactivity series:\n",
      "33 rows (in order of reactivity - most reactive at the top)\n",
      "3 features (symbol, name, ion)\n",
      "electromotive potentials:\n",
      "284 rows (in order from most negative potential to most positive)\n",
      "3 features (oxidant, reductant, potential)\n",
      "acknowledgements\n",
      "all of the data was scraped from 120 pages on wikipedia using scripts. the links to those scripts are available in the dataset descriptions.\n",
      "extra\n",
      "if you are interested in trying the chemistry calculations code i made for completing some of my repetitive class work, it's publicly available on my github. (chemistry calculations repository) i plan to continue updating that as time goes on.\n",
      "context\n",
      "a dataset of wta matches including individual statistics.\n",
      "content\n",
      "in these datasets there are individual csv files for wta tournament from 2000 to 2016.\n",
      "the numbers in the last columns are absolute values, using them you can calculate percentages.\n",
      "acknowledgement\n",
      "thanks to jeff sackmann for the excellent work. be sure to visit his github profile\n",
      "https://github.com/jeffsackmann/tennis_wta\n",
      "inspiration\n",
      "this dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. i'm planning to add historical odds and injuries data as soon as i have the time to get them.\n",
      "context\n",
      "there are a number of kaggle datasets that provide spatial data around new york city. for many of these, it may be quite interesting to relate the data to the demographic and economic characteristics of nearby neighborhoods. i hope this data set will allow for making these comparisons without too much difficulty.\n",
      "exploring the data and making maps could be quite interesting as well.\n",
      "content\n",
      "this dataset contains two csv files:\n",
      "nyc_census_tracts.csv\n",
      "this file contains a selection of census data taken from the acs dp03 and dp05 tables. things like total population, racial/ethnic demographic information, employment and commuting characteristics, and more are contained here. there is a great deal of additional data in the raw tables retrieved from the us census bureau website, so i could easily add more fields if there is enough interest.\n",
      "i obtained data for individual census tracts, which typically contain several thousand residents.\n",
      "census_block_loc.csv\n",
      "for this file, i used an online fcc census block lookup tool to retrieve the census block code for a 200 x 200 grid containing new york city and a bit of the surrounding area. this file contains the coordinates and associated census block codes along\n",
      "with the state and county names to make things a bit more readable to users.\n",
      "each census tract is split into a number of blocks, so one must extract the census tract code from the block code.\n",
      "acknowledgements\n",
      "the data here was taken from the american community survey 2015 5-year estimates (https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml).\n",
      "the census block coordinate data was taken from the fcc census block conversions api (https://www.fcc.gov/general/census-block-conversions-api)\n",
      "as public data from the us government, this is not subject to copyright within the us and should be considered public domain.\n",
      "context\n",
      "with this dataset i hope to raise awareness on the trends in crime.\n",
      "content\n",
      "for nypd complaint data, each row represents a crime. for information on the columns, please see the attached csv, \"crime_column_description\". reported crime go back 5 years but i only attached reported crime from 2014-2015 due to file size. the full report can be found at nyc open data (https://data.cityofnewyork.us/public-safety/nypd-complaint-data-historic/qgea-i56i)\n",
      "acknowledgements\n",
      "i would like to thank nyc open data for the dataset.\n",
      "inspiration\n",
      "additional things i would like to better understand: 1. differences in crime that exist between the 5 boroughs 2. a mapping of the crimes per borough 3. where do the most dangerous crimes happen and what time?\n",
      "context\n",
      "rankings are a constant phenomenon in society, with a persistent interest in the stratification of items in a set across various disciplines. in sports, rankings are a direct representation of the performance of a team or player over a certain period. given the straightforward nature of rankings in sports (points based system) there is the opportunity to statistically explore rankings of sports disciplines.\n",
      "content\n",
      "the dataset comprises monthly rankings data of the top 100 chess players between july 2000 and june 2017 . the data is housed in a single csv file.\n",
      "acknowledgements\n",
      "data was sourced from the official site of the world chess federation: fide.com\n",
      "inspiration\n",
      "this dataset could be of use to anyone interested in the distribution of rankings in competitive events.\n",
      "context:\n",
      "the state of the union is an annual address by the president of the united states before a joint session of congress. in it, the president reviews the previous year and lays out his legislative agenda for the coming year.\n",
      "content:\n",
      "this dataset contains the full text of the state of the union address from 1989 (regan) to 2017 (trump).\n",
      "inspiration:\n",
      "this is a nice, clean set of texts perfect for exploring natural language processing techniques\n",
      "topic modelling: which topics have become more popular over time? which have become less popular?\n",
      "sentiment analysis: are there differences in tone between different presidents? presidents from different parties?\n",
      "parsing: can you train implement a parser to automatically extract the syntactic relationships between words?\n",
      "authorship identification: can you correctly identify the author of a previously unseen address?\n",
      "context:\n",
      "color terms are interesting in natural language processing because it’s an area where it’s possible to link distributional semantics (models of word meanings based on which words are used together in texts) to things in the world. this dataset was created to help link semantic models to images.\n",
      "content:\n",
      "this dataset is made up of two smaller files, but were both presented and discussed in the same paper (see acknowledgements). all data in this dataset is in english.\n",
      "concrete color terms\n",
      "this dataset contains a list of common items manually labeled with one of the 11 colors from the set: black, blue, brown, green, grey, orange, pink, purple, red, white, yellow.\n",
      "literal vs. nonliteral colors\n",
      "this dataset is made up of color adjective-noun phrases, randomly drawn from the most frequent 8k nouns and 4k adjectives in the concatenated ukwac, wackypedia, and bnc corpora. these were tagged by consensus by two human judges as literal (white towel, black feather) or nonliteral (white wine, white musician, green future). some phrases had both literal and nonliteral uses, such as blue book in “book that is blue” vs. “automobile price guide”. in these cases, only the most common sense (according to the judges) was taken into account for the present experiment. the dataset consists of 370 phrases.\n",
      "acknowledgements:\n",
      "if you use these datasets, please cite:\n",
      "bruni, e., g. boleda, m. baroni, n. k. tran. 2012. distributional semantics in technicolor. proceedings of acl 2012, pages 136-145, jeju island, korea.\n",
      "inspiration:\n",
      "are some colors used more often in a literal sense?\n",
      "is there a relationship between how many objects are a given color and how often that color is used in a literal sense?\n",
      "can you use the color of concrete and an image database of those objects to create an automatic color labeller?\n",
      "context:\n",
      "spanish is the second most widely-spoken language on earth; over one in 20 humans alive today is a native speaker of spanish. this medium-sized corpus contains 120 million words of modern spanish taken from the spanish-language wikipedia in 2010.\n",
      "content:\n",
      "this dataset is made up of 57 text files. each contains multiple wikipedia articles in an xml format. the text of each article is surrounded by tags. the initial tag also contains metadata about the article, including the article’s id and the title of the article. the text “endofarticle.” appears at the end of each article, before the closing tag.\n",
      "acknowledgements:\n",
      "this dataset was collected by samuel reese, gemma boleda, montse cuadros, lluís padró and german rigau. if you use it in your work, please cite the following paper:\n",
      "samuel reese, gemma boleda, montse cuadros, lluís padró, german rigau. wikicorpus: a word-sense disambiguated multilingual wikipedia corpus. in proceedings of 7th language resources and evaluation conference (lrec'10), la valleta, malta. may, 2010.\n",
      "inspiration:\n",
      "can you create a stop-word list for spanish based on this corpus? how does it compare to the one in this dataset?\n",
      "can you build a topic model to cluster together articles on similar topics?\n",
      "you may also like:\n",
      "brazilian portuguese literature corpus: 3.7 million word corpus of brazilian literature published between 1840-1908\n",
      "colonia corpus of historical portuguese: a 5.1 million word corpus of historical portuguese\n",
      "the national university of singapore sms corpus: a corpus of more than 67,000 sms messages in singapore english & mandarin\n",
      "context\n",
      "this dataset contains information about deep sea corals and sponges collected by noaa and noaa’s partners. amongst the data are geo locations of deep sea corals and sponges and the whole thing is tailored to the occurrences of azooxanthellates - a subset of all corals and all sponge species (i.e. they don't have symbiotic relationships with certain microbes). additionally, these records only consists of observations deeper than 50 meters to truly focus on the deep sea corals and sponges.\n",
      "content:\n",
      "column descriptions:\n",
      "catalognumber: unique record identifier assigned by the deep-sea coral research and technology program.\n",
      "dataprovider: the institution, publication, or individual who ultimately deserves credit for acquiring or aggregating the data and making it available.\n",
      "scientificname: taxonomic identification of the sample as a latin binomial.\n",
      "vernacularnamecategory: common (vernacular) name category of the organism.\n",
      "taxonrank: identifies the level in the taxonomic hierarchy of the scientificname term.\n",
      "observationdate: time as hh:mm:ss when the sample/observation occurred (utc).\n",
      "latitude (degrees north): latitude in decimal degrees where the sample or observation was collected.\n",
      "longitude (degrees east): longitude in decimal degrees where the sample or observation was collected.\n",
      "depthinmeters: best single depth value for sample as a positive value in meters.\n",
      "depthmethod: method by which best singular depth in meters (depthinmeters) was determined. \"averaged\" when start and stop depths were averaged. \"assigned\" when depth was derived from bathymetry at the location. \"reported\" when depth was reported based on instrumentation or described in literature.\n",
      "locality: a specific named place or named feature of origin for the specimen or observation (e.g., dixon entrance, diaphus bank, or sur ridge). multiple locality names can be separated by a semicolon, arranged in a list from largest to smallest area (e.g., gulf of mexico; west florida shelf, pulley ridge).\n",
      "identificationqualifier: taxonomic identification method and level of expertise. examples: “genetic id”; “morphological id from sample by taxonomic expert”; “id by expert from image”; “id by non-expert from video”; etc.\n",
      "samplingequipment: method of data collection. examples: rov, submersible, towed camera, scuba, etc.\n",
      "recordtype: denotes the origin and type of record. published literature (\"literature\"); a collected specimen (\"specimen\"); observation from a still image (\"still image\"); observation from video (\"video observation\"); notation without a specimen or image (\"notation\"); or observation from trawl surveys, longline surveys, and/or observer records (\"catch record\").\n",
      "acknowledgements\n",
      "big shout out to noaa and it's partners. thank you for being scientists! the original and probably more up-to-date dataset can be found here: https://deepseacoraldata.noaa.gov/website/agsviewers/deepseacorals/mapsites.htm\n",
      "this dataset hasn't been changed in anyway.\n",
      "noaa (2015) national database for deep-sea corals and sponges (version 20170324-0). https://deepseacoraldata.noaa.gov/; noaa deep sea coral research & technology program.\n",
      "inspiration\n",
      "who doesn't love coral and sponges?! i challenge you to find the best algorithm that successfully saves the world's corals 100% of the time!\n",
      "content\n",
      "the purpose of epa’s fuel economy estimates is to provide a reliable basis for comparing vehicles. most vehicles in the database (other than plug-in hybrids) have three fuel economy estimates: a “city” estimate that represents urban driving, in which a vehicle is started in the morning (after being parked all night) and driven in stop-and-go traffic; a “highway” estimate that represents a mixture of rural and interstate highway driving in a warmed-up vehicle, typical of longer trips in free-flowing traffic; and a “combined” estimate that represents a combination of city driving (55%) and highway driving (45%). estimates for all vehicles are based on laboratory testing under standardized conditions to allow for fair comparisons.\n",
      "the database provides annual fuel cost estimates, rounded to the nearest $50, for each vehicle. the estimates are based on the assumptions that you travel 15,000 miles per year (55% under city driving conditions and 45% under highway conditions) and that fuel costs $2.33/gallon for regular unleaded gasoline, $2.58/gallon for mid-grade unleaded gasoline, and $2.82/gallon for premium.\n",
      "epa’s fuel economy values are good estimates of the fuel economy a typical driver will achieve under average driving conditions and provide a good basis to compare one vehicle to another. however, your fuel economy may be slightly higher or lower than epa’s estimates. fuel economy varies, sometimes significantly, based on driving conditions, driving style, and other factors.\n",
      "acknowledgements\n",
      "fuel economy data are produced during vehicle testing at the environmental protection agency's national vehicle and fuel emissions laboratory in ann arbor, michigan, and by vehicle manufacturers with epa oversight.\n",
      "content\n",
      "the cingranelli-richards human rights database contains quantitative information on government recognition of 15 internationally recognized human rights in more than 200 countries from 1981-2011. it includes measures of the practices of governments that allow or impede citizens who wish to exercise their physical integrity rights like the rights not to be tortured, summarily executed, disappeared, or imprisoned for political beliefs; civil liberties such as free speech, freedom of association and assembly, freedom of movement, freedom of religion, and the right to participate in the selection of government leaders; employment rights; and rights of women to equal treatment politically, economically, and socially. the database is designed for use by scholars and students who seek to test theories about the causes and consequences of human rights violations, as well as policy makers and analysts who seek to estimate the human rights effects of a wide variety of institutional changes and public policies including democratization, economic aid, military aid, structural adjustment, and humanitarian intervention.\n",
      "the primary source of information about human rights practices is obtained from the annual united states department of state’s country reports on human rights practices. coders are instructed to use this source for all variables. for a group of four rights known as \"physical integrity rights\" (the rights to freedom from extrajudicial killing, disappearance, torture, and political imprisonment), coders use amnesty international’s annual report in addition the department of state reports. if discrepancies exist between the two sources, coders are instructed to treat the amnesty international report as authoritative; some scholars believe that this step is necessary to remove a potential bias in favor of american allies.\n",
      "acknowledgements\n",
      "the human rights database was developed, updated, and published by professor david cingranelli of binghamton university, suny, professor david richards of the university of connecticut's human rights institute, and professor k. chad clay of the university of georgia.\n",
      "content\n",
      "the data in this report consists of individuals accused of terrorism and related crimes since september 11, 2001, who are either american citizens or who engaged in terrorist activity within the united states. the data includes some individuals who died before being charged with a crime, but were widely reported to have engaged in terrorist activity.\n",
      "acknowledgements\n",
      "this report was produced by the international security program at new america.\n",
      "context\n",
      "the university of copenhagen’s zoological museum placed a light trap on their roof and for 18 years they documented the types of insects being caught. the data was collected as part of a study to determine insect responses to recent climate change.\n",
      "content\n",
      "this file contains the raw data from the light trapping study ordered by: order (lepidoptera/coleoptera), family, name (species), year, date1 (start), date2 (end) and number of individuals\n",
      "acknowledgements\n",
      "original publication: thomsen pf, jørgensen ps, bruun hh, pedersen j, riis-nielsen t, jonko k, słowińska i, rahbek c, karsholt o (2016) resource specialists lead local insect community turnover associated with temperature – analysis of an 18-year full-seasonal record of moths and beetles. journal of animal ecology 85(1): 251–261. http://dx.doi.org/10.1111/1365-2656.12452\n",
      "the original dataset can be found at http://datadryad.org/resource/doi:10.5061/dryad.s4945/1\n",
      "inspiration\n",
      "climate change is on everyone's mind for one reason or another and insects are susceptible to climate change just like humans. using these data, can you determine which species have become more or less prevalent over the 18 years of collection?\n",
      "sweden has a surprisingly large number of school fires for a small country (< 10m inhabitants), and many of these fires are due to arson. for instance, according to the division of fire safety engineering at lund university, \"almost every day between one and two school fires occur in sweden. in most cases arson is the cause of the fire.\" the associated costs can be up to a billion sek (around 120 million usd) per year.\n",
      "it is hard to say why these fires are so common in sweden compared to other countries, and this dataset doesn't address that question - but could it be possible, within a swedish context, to find out which properties and indicators of swedish towns (municipalities, to be exact) might be related to a high frequency of school fires?\n",
      "i have collected data on school fire cases in sweden between 1998 and 2014 through a web site with official statistics from the swedish civil contingencies agency (https://ida.msb.se/ida2#page=a0087). at least at the time when i collected the data, there was no api to allow easy access to schools fire data, so i had to collect them using a quasi-manual process, downloading xlsx report generated from the database year by year, after which i joined these with an r script into a single table of school fire cases where the suspected reason was arson. (full details on the data acquisition process are available.)\n",
      "the number of such cases is reported for each municipality (of which there are currently 290) and year (i e each row is a unique municipality/year combination). the population at the time is also reported.\n",
      "as a complement to these data, i provide a list of municipal kpi:s (key performance indicators) from 1998 to 2014. there are thousands of these kpi:s, and it would be a futile task for me to try to translate the descriptions from swedish to english, although i might take a stab at translating a small subset of them at some point. these kpis were extracted from kolada (a database of swedish municipality and county council statistics) by repeatedly querying its api (https://github.com/hypergene/kolada).\n",
      "i'd be very interested to hear if anyone finds some interesting correlations between schools fire cases and municipality indicators!\n",
      "context\n",
      "experiment to apply same strategy from beluga's keras dataset with pytorch models. this dataset has the weights for several models included in pytorch. to use these weights they need to be copied when the kernel runs, like in this example.\n",
      "content\n",
      "pytorch models included:\n",
      "densenet-161\n",
      "inception-v3\n",
      "resnet18\n",
      "resnet50\n",
      "squeezenet 1.0\n",
      "squeezenet 1.1\n",
      "acknowledgements\n",
      "beluga's keras dataset\n",
      "pytorch\n",
      "what is cdli?\n",
      "the cuneiform digital library initiative (cdli) is an international digital library project aimed at putting text and images of an estimated 500,000 recovered cuneiform tablets created from between roughly 3350 bc and the end of the pre-christian era online. the initiative is a joint project of the university of california, los angeles, the university of oxford, and the max planck institute for the history of science, berlin.\n",
      "this dataset includes the full cdli catalogue (metadata), transliterations of tablets in the catalogue, and word/sign lists from old akkadian and ur iii. this data was downloaded on the 9th of may 2017.\n",
      "transliterations are in .atf format, find out more about this format here: http://oracc.museum.upenn.edu/doc/help/editinginatf/cdliatf/index.html\n",
      "find more about cdli here: http://cdli.ucla.edu/\n",
      "what is cuneiform?\n",
      "cuneiform script, one of the earliest systems of writing, was invented by the sumerians. it is distinguished by its wedge-shaped marks on clay tablets, made by means of a blunt reed for a stylus. the name cuneiform itself simply means \"wedge shaped\".\n",
      "cuneiform is not a language, nor is it an alphabet. cuneiform uses between 600-1000 characters to write words or syllables. it has been used by many different cultural groups to represent many different languages, but it was primarily used to write sumerian and akkadian. deciphering cuneiform is very difficult to this day, though the difficulty varies depending on the language.\n",
      "https://en.wikipedia.org/wiki/cuneiform_script\n",
      "what is assyriology?\n",
      "assyriology is the study of the languages, history, and culture of the people who used the ancient writing system called cuneiform. cuneiform was used primarily in an area called the near east, centred on mesopotamia (modern iraq and eastern syria) where cuneiform was invented, but including the northern levant (western syria and lebanon), parts of anatolia, and western iran. the sources for assyriology are all archaeological, and include both inscribed and uninscribed objects. most assyriologists focus on the rich textual record from the ancient near east, and specialise in either the study of language, literature, or history of the ancient near east.\n",
      "assyriology began as an academic discipline with the recovery of the monuments of ancient assyria, and the decipherment of cuneiform, in the middle of the 19th century. large numbers of archaeological objects, including texts, were brought to museums in europe and later the us, following the early excavations of nineveh, kalhu, babylon, girsu, assur and so forth. today assyriology is studied in universities across the globe, both as an undergraduate and a graduate subject, and knowledge from the ancient near east informs students of numerous other disciplines such as the history of science, archaeology, classics, biblical studies and more.\n",
      "context\n",
      "it's possible, using r (and no doubt python), to 'listen' to twitter and capture tweets that match a certain description. i decided to test this out by grabbing tweets with the text 'good morning' in them over a 24 hours period, to see if you could see the world waking up from the location information and time-stamp. the main r package used was streamr\n",
      "content\n",
      "the tweets have been tidied up quite a bit. first, i've removed re-tweets, second, i've removed duplicates (not sure why twitter gave me them in the first place), third, i've made sure the tweet contained the words 'good morning' (some tweets were returned that didn't have the text in for some reason) and fourth, i've removed all the tweets that didn't have a longitude and latitude included. this latter step removed the vast majority. what's left are various aspects of just under 5000 tweets. the columns are,\n",
      "text\n",
      "retweet_count\n",
      "favorited\n",
      "truncated\n",
      "id_str\n",
      "in_reply_to_screen_name\n",
      "source\n",
      "retweeted\n",
      "created_at\n",
      "in_reply_to_status_id_str\n",
      "in_reply_to_user_id_str\n",
      "lang\n",
      "listed_count\n",
      "verified\n",
      "location\n",
      "user_id_str\n",
      "description\n",
      "geo_enabled\n",
      "user_created_at\n",
      "statuses_count\n",
      "followers_count\n",
      "favourites_count\n",
      "protected\n",
      "user_url\n",
      "name\n",
      "time_zone\n",
      "user_lang\n",
      "utc_offset\n",
      "friends_count\n",
      "screen_name\n",
      "country_code\n",
      "country\n",
      "place_type\n",
      "full_name\n",
      "place_name\n",
      "place_id\n",
      "place_lat\n",
      "place_lon\n",
      "lat\n",
      "lon\n",
      "expanded_url\n",
      "url\n",
      "acknowledgements\n",
      "i used a few blog posts to get the code up and running, including this one\n",
      "code\n",
      "the r code i used to get the tweets is as follows (note, i haven't includes the code to set up the connection to twitter. see the streamr pfd and the link above for that. you need a twitter account),\n",
      "i = 1\n",
      "\n",
      "while (i <= 280) {\n",
      "\n",
      "filterstream(\"tw_gm.json\", timeout = 300, oauth = my_oauth, track = 'good morning', language = 'en')\n",
      "tweets_gm = parsetweets(\"tw_gm.json\")\n",
      "\n",
      "ex = grepl('rt', tweets_gm$text, ignore.case = false) #remove the rts\n",
      "tweets_gm = tweets_gm[!ex,]\n",
      "\n",
      "ex = grepl('good morning', tweets_gm$text, ignore.case = true) #remove anything without good morning in the main tweet text\n",
      "tweets_gm = tweets_gm[ex,]\n",
      "\n",
      "ex = is.na(tweets_gm$place_lat) #remove any with missing place_latitude information\n",
      "tweets_gm = tweets_gm[!ex,]\n",
      "\n",
      "tweets.all = rbind(tweets.all, tweets_gm) #add to the collection\n",
      "\n",
      "i=i+1\n",
      "\n",
      "sys.sleep(5)\n",
      "\n",
      "}\n",
      "context\n",
      "keystroke dynamics is the study of whether people can be distinguished by their typing rhythms, much like handwriting is used to identify the author of a written text. possible applications include acting as an electronic fingerprint, or in an access-control mechanism. a digital fingerprint would tie a person to a computer-based crime in the same manner that a physical fingerprint ties a person to the scene of a physical crime. access control could incorporate keystroke dynamics both by requiring a legitimate user to type a password with the correct rhythm, and by continually authenticating that user while they type on the keyboard.\n",
      "content\n",
      "the data are arranged as a table with 34 columns. each row of data corresponds to the timing information for a single repetition of the password by a single subject. the first column, subject, is a unique identifier for each subject (e.g., s002 or s057). even though the data set contains 51 subjects, the identifiers do not range from s001 to s051; subjects have been assigned unique ids across a range of keystroke experiments, and not every subject participated in every experiment. for instance, subject 1 did not perform the password typing task and so s001 does not appear in the data set. the second column, sessionindex, is the session in which the password was typed (ranging from 1 to 8). the third column, rep, is the repetition of the password within the session (ranging from 1 to 50).\n",
      "the remaining 31 columns present the timing information for the password. the name of the column encodes the type of timing information. column names of the form h.key designate a hold time for the named key (i.e., the time from when key was pressed to when it was released). column names of the form dd.key1.key2 designate a key down-key down time for the named digraph (i.e., the time from when key1 was pressed to when key2 was pressed). column names of the form ud.key1.key2 designate a key up-key down time for the named digraph (i.e., the time from when key1 was released to when key2 was pressed). note that ud times can be negative, and that h times and ud times add up to dd times.\n",
      "consider the following one-line example of what you will see in the data:\n",
      "subject sessionindex rep h.period dd.period.t ud.period.t ... s002 1 1 0.1491 0.3979 0.2488 ...\n",
      "the example presents typing data for subject 2, session 1, repetition 1. the period key was held down for 0.1491 seconds (149.1 milliseconds); the time between pressing the period key and the t key (key down-key down time) was 0.3979 seconds; the time between releasing the period and pressing the t key (key up-key down time) was 0.2488 seconds; and so on\n",
      "acknowledgements\n",
      "kevin s. killourhy and roy a. maxion\n",
      "inspiration\n",
      "to make measurable progress in the field of keystroke dynamics, i shared data. the anomaly-detection task was to discriminate between the typing of a genuine user trying to gain legitimate access to his or her account, and the typing of an impostor trying to gain access illegitimately to that same account. our intent with this is to share our resources—the typing data, with the research community, and to answer questions that they (or you) might have.\n",
      "for starters: 1. the typing patterns of different users. 2. the changing typing styles of a user over different attempts. 3. the difference in typing of left-side keys and right-side keys on the keyboard.\n",
      "and so on . . .\n",
      "*temporary note* some people are having problem with the main download button, please try downloading from the bottom of the page rather than the main button if issues observed.\n",
      "happy machine learning!\n",
      "the data set contains the details about all the atp matches played since 1968. the data set has a lot of missing values, especially for the period between 1968 - 1991.\n",
      "thanks to xiaming chen for making the data available to the online community.\n",
      "primarily, i would like to understand how tennis matches/players have evolved over time and any other insights.\n",
      "this dataset includes sp1 transcription factor binding and non-binding sites on human chromosome1. it can be used for binary classification tasks in bioinformatics. there are 1200 sequences for binding sites (bs) and 1200 sequences for non-biding sites (nbs) we have labeled sequences with 1 for bs and 0 for nbs. each sequence is 14 nucleobase length, which is converted to numeric string using codes below, assigned to each nucleobase 00 for a 01 for t 10 for c 11 for g\n",
      "the dataset contains cases from a study that was conducted between 1958 and 1970 at the university of chicago's billings hospital on the survival of patients who had undergone surgery for breast cancer.\n",
      "context\n",
      "software systems are composed of one or more software architectural styles. these styles define the usage patterns of a programmer in order to develop a complex project. these architectural styles are required to analyze for pattern similarity in the structure of multiple groups of projects. the researcher can apply different types of data mining algorithms to analyze the software projects through architectural styles used. the dataset is obtained from an online questionnaire delivered to the world 's best academic and software industry.\n",
      "content\n",
      "the content of this dataset are multiple architectural styles utilized by the system. he attributes are repository, client server, abstract machine,object oriented,function oriented,event driven,layered, pipes & filters, data centeric, blackboard, rule based, publish subscribe, asynchronous messaging, plug-ins, microkernel, peer-to-peer, domain driven, shared nothing.\n",
      "acknowledgements\n",
      "thanks to my honorable teacher prof.dr usman qamar for guiding me to accomplish this wonderful task.\n",
      "inspiration\n",
      "the dataset is capable of updating and refinements.any researcher ,who want to contribute ,plz feel free to ask.\n",
      "state energy data systems (seds) data for all us states, including dc, from 1960 to 2014f\n",
      "context\n",
      "this dataset is derived from my general interest in energy systems. it was originally composed for this exercise, as part of this coursera/john hopkins data science specilisation.\n",
      "the code that produced this dataset is in https://www.kaggle.com/nathanto/d/nathanto/seds-1960-2014f/data-wrangling-code-for-seds-1960-2014f\n",
      "content\n",
      "the data is a composition of the state energy data systems (seds) data for all us states, including dc, from 2016 to 2014f, for data released june 29, 2016. it has been tidied from a wide format to a long format, and includes unit codes for the values associated with the observations for each msn code for each state for each year.\n",
      "the \"f\" in the final year number indicates that these are the final observations. there is a lag of some 18 months after year end and final readings.\n",
      "the columns are:\n",
      "state - state postal code, composed from the function states.abb and including \"dc\".\n",
      "msn - a mnemonic series name identifying the value being observed.\n",
      "year - year of the observation.\n",
      "value - of the observation.\n",
      "units_code, representing the units of the value, e.g. bbtu is billion british thermal units.\n",
      "note that the units_codes are mostly my own invention, based on the eia writng style guide.\n",
      "acknowledgements\n",
      "thank you to the us energy information administration for making the data available.\n",
      "special thanks to yvonne taylor for guidance on style for the codes.\n",
      "inspiration\n",
      "the first goal for this data was to support some plotting and forecast testing exercises, which is a work in progress. to what extent do past observations predict future observations? since the data is readily available, and consistent, within limits, over a long period, this format is a good basis for experimenting with techniques in that space.\n",
      "context\n",
      "this is the dataset used in the section \"ann (artificial neural networks)\" of the udemy course from kirill eremenko (data scientist & forex systems expert) and hadelin de ponteves (data scientist), called deep learning a-z™: hands-on artificial neural networks. the dataset is very useful for beginners of machine learning, and a simple playground where to compare several techniques/skills.\n",
      "it can be freely downloaded here: https://www.superdatascience.com/deep-learning/\n",
      "the story: a bank is investigating a very high rate of customer leaving the bank. here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.\n",
      "the story of the story: i'd like to compare several techniques (better if not alone, and with the experience of several kaggle users) to improve my basic knowledge on machine learning.\n",
      "content\n",
      "i will write more later, but the columns names are very self-explaining.\n",
      "acknowledgements\n",
      "udemy instructors kirill eremenko (data scientist & forex systems expert) and hadelin de ponteves (data scientist), and their efforts to provide this dataset to their students.\n",
      "inspiration\n",
      "which methods score best with this dataset? which are fastest (or, executable in a decent time)? which are the basic steps with such a simple dataset, very useful to beginners?\n",
      "context\n",
      "this data set covers all aspects of the pre-wwi and interwar economies, including production, construction, employment, money, prices, asset market transactions, foreign trade, and government activity. many series are highly disaggregated, and many exist at the monthly or quarterly frequency. the data set has some coverage of the united kingdom, france and germany, although it predominantly covers the united states. for information see:\n",
      "improving the accessibility of the nber's historical data , by daniel feenberg and jeff miron. (nber working paper #5186). published in the journal of business and economic statistics, volume 15 number 3 (july 1997) pages 293-299.\n",
      "information about seasonal adjustments is available, but in most cases only unadjusted series have been made available here.\n",
      "content\n",
      "the data.csv is organized in a long format with columns for the date, variable, and value. the dates are always the beginning of period date for whatever period existed in the original data. this means that '1920' was converted to january 1st, 1920 while q2 1920 was converted to april 1, 1920. this is intended as a convenience to make it easier to work with multiple time series from the original mixed frequency data.\n",
      "the data is currently organized into 16 chapters:\n",
      "chapter1: production of commodities\n",
      "chapter2: construction\n",
      "chapter3: transportation and public utilities\n",
      "chapter4: prices\n",
      "chapter5: stocks of commodities\n",
      "chapter6: distribution of commodities\n",
      "chapter7: foreign trade\n",
      "chapter8: income and employment\n",
      "chapter9: financial status of business\n",
      "chapter10: savings and investment\n",
      "chapter11: security markets\n",
      "chapter12: volume of transactions\n",
      "chapter13: interest rates\n",
      "chapter14: money and banking\n",
      "chapter15: government and finance\n",
      "chapter16: leading indicators\n",
      "the dataset has been transformed from its original format. you can find the data preparation code here.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the national bureau of economic research (nber). you can find the original dataset here.\n",
      "inspiration\n",
      "which major historical events can you detect from the data?\n",
      "with roughly 3,500 time series in the dataset, finding relevant information can be challenging. can you find a better way of organizing or indexing the data?\n",
      "«datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\n",
      "context\n",
      "en aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.\n",
      "hem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. és important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.\n",
      "content\n",
      "en aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.\n",
      "pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.\n",
      "pel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. els camps especialment destacats són:\n",
      "• nom: nom empresa o cryptomoneda;\n",
      "• preu: valor en euros d’una acció o una cryptomoneda;\n",
      "• volum: en euros/volum 24 hores,acumulat de les transaccions diàries en milions d’euros\n",
      "• simbol: símbol o acrònim de la moneda\n",
      "• cap de mercat: valor total de totes les monedes en el moment actual\n",
      "• oferta circulant: valor en oportunitat de negoci\n",
      "• % 1h, % 2h i %7d, tant per cent del valor la moneda en 1h, 2h o 7d sobre la resta de cyprtomonedes.\n",
      "acknowledgements\n",
      "en aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:\n",
      "http://www.eleconomista.es\n",
      "https://coinmarketcap.com\n",
      "per aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. pel que fa a la terminologia financera podem veure vocabulari en renta4banco.\n",
      "[https://www.r4.com/que-necesitas/formacion/diccionario]\n",
      "inspiration\n",
      "hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:\n",
      "https://arxiv.org/pdf/1410.1231v1.pdf\n",
      "en aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.\n",
      "la comunitat podrà respondre, entre altres preguntes, a:\n",
      "està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'espanya?\n",
      "els efectes o agents externs afecten per igual a les accions o cryptomonedes?\n",
      "hi ha relacions cause efecte entre les acciones i cryptomonedes?\n",
      "project repository\n",
      "https://github.com/acostasg/scraping\n",
      "datasets\n",
      "els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:\n",
      "https://www.kaggle.com/acostasg/stock-index/\n",
      "https://www.kaggle.com/acostasg/crypto-currencies\n",
      "per una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. de forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció.\n",
      "this dataset, from crowdflower's data for everyone library, provides text of 5000 messages from politicians' social media accounts, along with human judgments about the purpose, partisanship, and audience of the messages.\n",
      "how was it collected?\n",
      "contributors looked at thousands of social media messages from us senators and other american politicians to classify their content. messages were broken down into audience (national or the tweeter’s constituency), bias (neutral/bipartisan, or biased/partisan), and finally tagged as the actual substance of the message itself (options ranged from informational, announcement of a media appearance, an attack on another candidate, etc.)\n",
      "acknowledgments\n",
      "data was provided by the data for everyone library on crowdflower.\n",
      "our data for everyone library is a collection of our favorite open data jobs that have come through our platform. they're available free of charge for the community, forever.\n",
      "inspiration\n",
      "here are a couple of questions you can explore with this dataset:\n",
      "what words predict partisan v. neutral messages?\n",
      "what words predict support messages v. attack messages?\n",
      "do politicians use twitter and facebook for different purposes? (e.g., twitter for attack messages, facebook for policy messages)?\n",
      "the data\n",
      "the dataset contains one file, with the following fields:\n",
      "_unit_id: a unique id for the message\n",
      "_golden: always false; (presumably whether the message was in crowdflower's gold standard)\n",
      "_unit_state: always \"finalized\"\n",
      "_trusted_judgments: the number of trusted human judgments that were entered for this message; an integer between 1 and 3\n",
      "_last_judgment_at: when the final judgment was collected\n",
      "audience: one of national or constituency\n",
      "audience:confidence: a measure of confidence in the audience judgment; a float between 0.5 and 1\n",
      "bias: one of neutral or partisan\n",
      "bias:confidence: a measure of confidence in the bias judgment; a float between 0.5 and 1\n",
      "message: the aim of the message. one of: -- attack: the message attacks another politician\n",
      "-- constituency: the message discusses the politician's constituency\n",
      "-- information: an informational message about news in government or the wider u.s.\n",
      "-- media: a message about interaction with the media\n",
      "-- mobilization: a message intended to mobilize supporters\n",
      "-- other: a catch-all category for messages that don't fit into the other\n",
      "-- personal: a personal message, usually expressing sympathy, support or condolences, or other personal opinions\n",
      "-- policy: a message about political policy\n",
      "-- support: a message of political support\n",
      "message:confidence: a measure of confidence in the message judgment; a float between 0.5 and 1\n",
      "orig__golden: always empty; presumably whether some portion of the message was in the gold standard\n",
      "audience_gold: always empty; presumably whether the audience response was in the gold standard\n",
      "bias_gold: always empty; presumably whether the bias response was in the gold standard\n",
      "bioid: a unique id for the politician\n",
      "embed: html code to embed this message\n",
      "id: unique id for the message within whichever social media site it was pulled from\n",
      "label: a string of the form \"from: firstname lastname (position from state)\"\n",
      "message_gold: always blank; presumably whether the message response was in the gold standard\n",
      "source: where the message was posted; one of \"facebook\" or \"twitter\"\n",
      "text: the text of the message\n",
      "context\n",
      "landslides are one of the most pervasive hazards in the world, causing more than 11,500 fatalities in 70 countries since 2007. saturating the soil on vulnerable slopes, intense and prolonged rainfall is the most frequent landslide trigger.\n",
      "content\n",
      "the global landslide catalog (glc) was developed with the goal of identifying rainfall-triggered landslide events around the world, regardless of size, impacts, or location. the glc considers all types of mass movements triggered by rainfall, which have been reported in the media, disaster databases, scientific reports, or other sources.\n",
      "acknowledgements\n",
      "the glc has been compiled since 2007 at nasa goddard space flight center.\n",
      "what is an independent expenditure?\n",
      "independent expenditures are what some refer to as \"hard money\" in politics -- spending on ads that specifically mention a candidate (either supporting or opposing). the money for these ads must come from pacs that are independent of the candidate and campaign, and the pacs cannot coordinate with the candidate.\n",
      "the federal election commission (fec) collects information on independent expenditures to ensure payers' independence from candidates.\n",
      "what can we look at?\n",
      "i'm super interested to see how much spending has increased over the years. the fec data only goes back to 2004, and it may be the case that the older data is spotty, but i don't doubt that political spending has gone up in the past few years (the 2016 presidential campaign reportedly involved the most political money since the 1970s).\n",
      "what does the data look like?\n",
      "this dataset includes a ton of information from the independent expenditure reports:\n",
      "committee_id : unique id of the pac that made the payment\n",
      "committee_name : name of the pac that made the payment\n",
      "report_year : the year the report was file\n",
      "report_type : one of 24 or 48; whether this is a 24-hour report or a 48-hour report\n",
      "image_number : unique id of the scanned image of the report\n",
      "line_number : line number in the report\n",
      "file_number : unique id of the report\n",
      "payee_name : who got paid\n",
      "payee_first_name : if an individual payee, their first name\n",
      "payee_middle_name : if an individual payee, their middle name\n",
      "payee_last_name : if an individual payee, their last name\n",
      "payee_street_1 : payee street address (1 of 2)\n",
      "payee_street_2 : payee street address (2 of 2)\n",
      "payee_city : payee city\n",
      "payee_state : payee state\n",
      "payee_zip : payee zip code\n",
      "expenditure_description : a string describing the expenditure\n",
      "expenditure_date : when was this expenditure made?\n",
      "dissemination_date : when was the advertisement disseminated?\n",
      "expenditure_amount : how much was spent?\n",
      "office_total_ytd : how much has this pac spent on this office, year-to-date?\n",
      "category_code : category of the expenditure (need to find categories!)\n",
      "category_code_full : category of the expenditure (need to find categories!)\n",
      "support_oppose_indicator : one of s or o; whether the ad is in support of or opposition to the candidate\n",
      "memo_code :\n",
      "memo_code_full :\n",
      "candidate_id : unique id of the candidate\n",
      "candidate_name : name of the candidate\n",
      "candidate_prefix : title or prefix of the candidate\n",
      "candidate_first_name : first name of the candidate\n",
      "candidate_middle_name : middle name of the candidate\n",
      "candidate_last_name : last name of the candidate\n",
      "candidate_suffix : suffix of the candidate's name\n",
      "candidate_office : office that the candidate is running for -- one of p (president), s (senate), or h (house)\n",
      "cand_office_state : if house or senate race, in what state?\n",
      "cand_office_district : if house or senate race, in what district?\n",
      "conduit_committee_id :\n",
      "conduit_committee_name :\n",
      "conduit_committee_street1 :\n",
      "conduit_committee_street2 :\n",
      "conduit_committee_city :\n",
      "conduit_committee_state :\n",
      "conduit_committee_zip :\n",
      "election_type : one of p (primary) or g (general)\n",
      "election_type_full : an id comprising the election type and the year, with no delimiter\n",
      "independent_sign_name :\n",
      "independent_sign_date :\n",
      "notary_sign_name :\n",
      "notary_sign_date :\n",
      "notary_commission_expiration_date :\n",
      "back_reference_transaction_id :\n",
      "back_reference_schedule_name :\n",
      "filer_first_name :\n",
      "filer_middle_name :\n",
      "filer_last_name :\n",
      "transaction_id : unique id identifying the transaction\n",
      "original_sub_id :\n",
      "action_code :\n",
      "action_code_full :\n",
      "schedule_type_full :\n",
      "filing_form :\n",
      "link_id :\n",
      "sub_id :\n",
      "payee_prefix :\n",
      "payee_suffix :\n",
      "is_notice :\n",
      "memo_text :\n",
      "filer_prefix :\n",
      "filer_suffix :\n",
      "schedule_type :\n",
      "pdf_url : link to the scanned form\n",
      "aedes aegypti and ae. albopictus are the main vectors transmitting dengue and chikungunya viruses. despite being pathogens of global public health importance, knowledge of their vectors’ global distribution remains patchy and sparse.\n",
      "a global geographic database of known occurrences of ae. aegypti and ae. albopictus between 1960 and 2014 was compiled. the database, which comprises occurrence data linked to point or polygon locations, was derived from peer-reviewed literature and unpublished studies including national entomological surveys and expert networks. the authors describe all data collection processes, as well as geo-positioning methods, database management and quality-control procedures in their 2015 paper cited below.\n",
      "this is the first comprehensive global database of ae. aegypti and ae. albopictus occurrence, consisting of 19,930 and 22,137 geo-positioned occurrence records respectively. the dataset can be used for a variety of mapping and spatial analyses of the vectors and, by inference, the diseases they transmit.\n",
      "citations\n",
      "kraemer mug, sinka me, duda ka, mylne a, shearer fm, brady oj, messina jp, barker cm, moore cg, carvalho rg, coelho ge, van bortel w, hendrickx g, schaffner f, wint grw, elyazar irf, teng h, hay si (2015) the global compendium of aedes aegypti and ae. albopictus occurrence. scientific data 2(7): 150035. http://dx.doi.org/10.1038/sdata.2015.35\n",
      "kraemer mug, sinka me, duda ka, mylne a, shearer fm, brady oj, messina jp, barker cm, moore cg, carvalho rg, coelho ge, van bortel w, hendrickx g, schaffner f, wint grw, elyazar irf, teng h, hay si (2015) data from: the global compendium of aedes aegypti and ae. albopictus occurrence. dryad digital repository. http://dx.doi.org/10.5061/dryad.47v3c\n",
      "about this data\n",
      "this is a list of over 3,500 pizzas from multiple restaurants provided by datafiniti's business database. the dataset includes the category, name, address, city, state, menu information, price range, and more for each pizza restaurant.\n",
      "what you can do with this data\n",
      "you can use this data to discover how much you can expect to pay for pizza across the country. e.g.:\n",
      "what are the least and most expensive cities for pizza?\n",
      "what is the number of restaurants serving pizza per capita (100,000 residents) across the u.s.?\n",
      "what is the median price of a large plain pizza across the u.s.?\n",
      "which cities have the most restaurants serving pizza per capita (100,000 residents)?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "this official dataset from the medicare.gov nursing home compare website allows for comparison of over 15,000 medicare and medicaid-certified nursing homes in the country.\n",
      "content\n",
      "separate data collections include:\n",
      "deficiencies, including fire safety, health, and inspection cycle types\n",
      "ownership details, including ownership percentage\n",
      "penalties, including filing date, fee, and payment date\n",
      "provider details, including non or for profit status, staff ratings, and survey scores\n",
      "quality msr (minimum savings rate) claims, including adjusted and observed scores\n",
      "mds (minimum data set) quality measures, scored on a quarterly basis\n",
      "state averages, including total number of quarterly deficiencies, and nurse staffing hours\n",
      "survey summaries for each nursing home\n",
      "inspiration\n",
      "how would you determine what the top ten best nursing homes in the country are? the least?\n",
      "which states have the best level of nursing home care? the least?\n",
      "in general, what are the most common types of complaints and deficiencies?\n",
      "acknowledgements\n",
      "this dataset was collected by medicare.gov, and the original files can be accessed here.\n",
      "content\n",
      "this dataset consists of a row for every accredited high school in new york city with its department id number, school name, borough, building code, street address, latitude/longitude coordinates, phone number, start and end times, student enrollment with race breakdown, and average scores on each sat test section for the 2014-2015 school year.\n",
      "acknowledgements\n",
      "the high school data was compiled and published by the new york city department of education, and the sat score averages and testing rates were provided by the college board.\n",
      "inspiration\n",
      "which public high school's students received the highest overall sat score? highest score for each section? which borough has the highest performing schools? do schools with lower student enrollment perform better?\n",
      "context\n",
      "this dataset presents speech files recorded for isolated words of urdu. language resources for urdu language are not well developed. in this work, we summarize our work on the development of urdu speech corpus for isolated words. the corpus comprises of 250 isolated words of urdu recorded by ten individuals. the speakers include both native and non-native, male and female individuals. the corpus can be used for both speech and speaker recognition tasks. the sampling frequency is 16000 hz.\n",
      "content\n",
      "each folder name refers to a single speaker. the folder name gives information about the characteristics of each speaker. each folder contains 250 isolated files i.e. 250 isolated words.\n",
      "speaker name aa ab ac . . . ak\n",
      "gender m for male f for female\n",
      "native /non-native y for native n for non-native\n",
      "age group g1, g2, g3, g4\n",
      "example: aamng1 speaker name = aa gender = male n = non-native g1 = age group g1\n",
      "acknowledgements\n",
      "all the volunteers community who recorded for us.\n",
      "context\n",
      "a hadith is a report describing the words, actions, intentions or habits of the last prophet and messenger muhammed (peace be upon him). the term literally means report, account or narrative.\n",
      "ṣaḥīḥ al-bukhārī (صحيح البخاري), is one of the six major hadith collections books. it was collected by a muslim scholar imam muhammad al-bukhari, after being transmitted orally for generations. there are 7,658 full hadiths in this collection narrated by 1,755 narrators/transmitters.\n",
      "imam bukhari finished his work around 846 a.d.\n",
      "content\n",
      "the two main sources of data regarding hadith are works of hadith and works containing biographical information about hadith narrators. the dataset contains 7,658 hadiths in arabic and the names of 1,755 transmitters. imam bukhari followed the following criterion to include a hadith in this book.\n",
      "quality and soundness of chain of narrators - the lifetime of a narrator should overlap with the lifetime of the authority from whom he narrates.\n",
      "verifiable - it should be verifiable that narrators have met with their source persons. they should also expressly state that they obtained the narrative from these authorities.\n",
      "piousness – he only accepted the narratives from only those who, according to his knowledge, not only believed in islam but practiced its teachings.\n",
      "acknowledgements\n",
      "more information on hadith and sahih bukhari can be found from this link - hadith books\n",
      "inspiration\n",
      "here are some ideas worth exploring:\n",
      "the traditional criteria for determining if a hadith is sahih (authentic) requires that there should be an uninterrupted chain of narrators; that all those narrators should be highly reliable and there should not be any hidden defects. can we make a social network graph of all the narrators and then timestamp it with their age and era to see who overlaps who?\n",
      "the name of a transmitter mentioned in a given hadith is not the full name, and many transmitters have similar names. so identifying who is the transmitter of a given hadith based on the names mentioned in the text might be a good problem to tackle\n",
      "can we analyze the chains of transmitters for entire collections using neo4j or some other graph database\n",
      "there exist different chains that reports the same hadith with little variation of words, can you identify those\n",
      "can you link the text with other external data sources?\n",
      "can we produce the word cloud for each chapter of the book?\n",
      "can we train a neural network to authenticate if the hadith is real or not?\n",
      "can we find out the specific style or vocabulary of each narrator?\n",
      "can we develop a system for comparing variant wordings for the same hadith to identify how reliable a given transmitter is.\n",
      "please also help me extend this dataset. if you have any other hadith book in csv or text format, please send me a message and i will add.\n",
      "context\n",
      "as i am trying to learn and build an lstm prediction model for equity prices, i have tried gold and then want to try crops which may have strong trends in times, so i prepared the dataset for the weekly corn prices.\n",
      "content\n",
      "the file composed of simply 2 columns. one is the date (weekend) and the other is corn close price. the period is from 2015-01-04 to 2017-10-01. the original data is downloaded from quantopian corn futures price.\n",
      "acknowledgements\n",
      "thanks to jason of his tutorial about lstm forecast: https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
      "inspiration\n",
      "william gann: time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. there is a definite relation between price and time.\n",
      "content\n",
      "the dataset consists of 167.053 examples and contains headlines, url of article, complete article and category. i gathered the summarized news from inshorts and only scraped the news articles from folha de são paulo - http://www.folha.uol.com.br/ (brazilian newspaper). time period ranges is between january 2015 and september 2017.\n",
      "context\n",
      "dataset was created by me for the major project of my dual degree course curriculum.\n",
      "content\n",
      "dataset comprises of all the odi cricket matches in the interval 1971-2017\n",
      "it consists of these files:\n",
      "- originaldataset.csv : raw dataset file which i scraped using pandas\n",
      "- categorialdataset.csv : categorial features suitable for models like mlpclassifier & dtclassifier\n",
      "- continousdataset.csv : purely for experimental purposes\n",
      "- labelleddataset.csv : suitable for support vector machines\n",
      "acknowledgements\n",
      "espn cricinfo\n",
      "description\n",
      "this is a collection of the santander (spain) bike-sharing open data facility, named tusbic, operated by jcdecaux.\n",
      "i will be updating this dataset form time to time, added new data as i collect it.\n",
      "format\n",
      "bike sharing system data\n",
      "the bikes.csv file contains the information from the bike sharing system. data is structured as follows:\n",
      "number number of the station\n",
      "contract_name name of the contract of the station\n",
      "name name of the station\n",
      "address address of the station (raw)\n",
      "lat latitude of the station in wgs84 format\n",
      "lng latitude of the station in wgs84 format\n",
      "banking indicates whether this station has a payment terminal\n",
      "bonus indicates whether this is a bonus station\n",
      "status indicates whether this station is closedor open\n",
      "bike_stands the number of operational bike stands at this station\n",
      "available_bike_stands the number of available bike stands at this station\n",
      "available_bikes the number of available and operational bikes at this station\n",
      "last_update timestamp indicating the last update time in milliseconds since epoch\n",
      "bike lane geometries\n",
      "the bike_lanes.csv file contains the geometries of bike lanes in santander city, as published by the santander city council in its open data platform.\n",
      "ayto:wkt contains the geometry in wkt format, using ed50 utm coordinates (zone 30n).\n",
      "wkt_wsg84 contains the geometry in wkt format, using wgs84 coordinates.\n",
      "ayto:estado shows the status of the bike lane. ejecutado means that is has been built and it is operative.\n",
      "license\n",
      "the bike sharing data is being collected from the jcdecaux developer open data platform and is licensed under the etalab open license, compatbile with the standards of open data licenses (odc-by, cc-by 2.0).\n",
      "the bike lane geometry is being collected form the santander open data platform and is licensed under a cc by 4.0 license.\n",
      "dataste kaggle logo is a photo licensed under a cc-by-sa 3.0, authored by tiia monto.\n",
      "gender info 2007 is a global database of gender statistics and indicators on a wide range of policy areas, including: population, families, health, education, work, and political participation. it can be used by governments, international organizations, advocacy groups, researchers and others in need of statistics for planning, analysis, advocacy and awareness-raising. users will find in gender info an easy-to-use tool to shed light on gender issues through customizable tables, graphs and maps. it is an initiative of the united nations statistics division, produced in collaboration with the united nations children’s fund (unicef) and the united nations population fund (unfpa).\n",
      "this dataset was last updated in 2008. if you need a more current version of the data please visit http://unstats.un.org/unsd/gender/data.html for other gender statistics.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nations on the undata site. you can find the original dataset here.\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "basic explanation\n",
      "it is important to know if a patient will be readmitted in some hospital. the reason is that you can change the treatment, in order to avoid a readmission.\n",
      "in this database, you have 3 different outputs:\n",
      "no readmission;\n",
      "a readmission in less than 30 days (this situation is not good, because maybe your treatment was not appropriate);\n",
      "a readmission in more than 30 days (this one is not so good as well the last one, however, the reason can be the state of the patient.\n",
      "in this context, you can see different objective functions for the problem. you can try to figure out situations where the patient will not be readmitted, or if their are going to be readmitted in less than 30 days (because the problem can the the treatment), etc... make your choice and let's help them creating new approaches for the problem.\n",
      "content\n",
      "\"the data set represents 10 years (1999-2008) of clinical care at 130 us hospitals and integrated delivery networks. it includes over 50 features representing patient and hospital outcomes. information was extracted from the database for encounters that satisfied the following criteria.\n",
      "it is an inpatient encounter (a hospital admission).\n",
      "it is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.\n",
      "the length of stay was at least 1 day and at most 14 days.\n",
      "laboratory tests were performed during the encounter.\n",
      "medications were administered during the encounter.\n",
      "the data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, hba1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.\"\n",
      "https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008\n",
      "source\n",
      "the data are submitted on behalf of the center for clinical and translational research, virginia commonwealth university, a recipient of nih ctsa grant ul1 tr00058 and a recipient of the cerner data. john clore (jclore '@' vcu.edu), krzysztof j. cios (kcios '@' vcu.edu), jon deshazo (jpdeshazo '@' vcu.edu), and beata strack (strackb '@' vcu.edu). this data is a de-identified abstract of the health facts database (cerner corporation, kansas city, mo).\n",
      "original source of the data set\n",
      "https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008\n",
      "the united states department of labor tells us that \"labor day, the first monday in september, is a creation of the labor movement and is dedicated to the social and economic achievements of american workers. it constitutes a yearly national tribute to the contributions workers have made to the strength, prosperity, and well-being of our country.\"\n",
      "this database of state-level union membership and coverage from 1983 to 2015 was originally compiled by barry hirsch (andrew young school of policy studies, georgia state university) and david macpherson (department of economics, trinity university). the database, available at unionstats.com provides private and public sector labor union membership, coverage, and density estimates compiled from the monthly household current population survey (cps) using bls methods.\n",
      "use of this data requires citation of the following paper which also includes a description of how the database was created: barry t. hirsch and david a. macpherson, \"union membership and coverage database from the current population survey: note,\" industrial and labor relations review, vol. 56, no. 2, january 2003, pp. 349-54. (pdf).\n",
      "context\n",
      "the panama paper case is the most publicized case in the history of pakistan. it was heard before the supreme court of pakistan between november 1st 2016 to february 23, 2017. it alleges the corruption, money laundering and wrong doing by the prime minister of pakistan, nawaz sharif.\n",
      "after subsequent formation of joint investigation committee and its recommendations, court finally disqualified the prime minister on july 28th 2017. since then, there is a hue and cry in the media by the incumbent political party and one question the ex-prime minister is keep asking is \"mujhe kiyun nikala (why they kicked me out). it seems the poor man has no idea what happened and why.\n",
      "pakistan is a country of 207 million people according to the recent census and no one is giving him the answer of his question. i feel pity for the ex-prime minister, and thought to launch this dataset to call my fellow data scientists to run the kernels, dig out the hidden meanings, find patterns and linkages with off-shore companies (i've posted the complete panama papers in another dataset as well) to help him understand \"unhen kiyun nikala - why did he kicked out?\"\n",
      "here is a good wikipedia article with further details. please do contribute more datasets regarding this case through discussion forums and i will keep updating it as the case/dataset progresses.\n",
      "please help me answer the most publicized question in the history of pakistan -\n",
      "mujhe kiyun nikala.\n",
      "content\n",
      "full text of panama case verdict is available through csv file in the data section. the original and subsequent review decisions by the supreme court of pakistan can be found through these links:\n",
      "civil review petition no. 297 of 2017 in const. petition no. 29 of 2016\n",
      "civil appeals no. 1406 and 1407 of 2016\n",
      "constitution petition no. 29 of 2016\n",
      "review decision\n",
      "full verdict in urdu\n",
      "full verdict in english\n",
      "acknowledgements\n",
      "supreme court of pakistan\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "who are the lawyers from both sides?\n",
      "what charges were filled against the ex-prime minister\n",
      "what charges were found not true\n",
      "what charges were found true\n",
      "what other references court has given that resembles this case\n",
      "how we can join this dataset with panama dataset\n",
      "how we can visually present the data\n",
      "how data science can answer the question - mujhe kiyun nikala\n",
      "context\n",
      "the first 100 rows of the data dump of parsed matches from opendota.com (formerly yasp.co) as of mid december 2015.\n",
      "content\n",
      "columns:\n",
      "match_id - integer, unique match id\n",
      "match_seq_num - integer, match sequence number\n",
      "radiant_win - string, boolean variable than indicates if radiant won or not in the match\n",
      "start_time - integer, start time of the match\n",
      "duration - integer, duration of the match\n",
      "tower_status_radiant - integer, remaining health of the towers of the radiant side\n",
      "tower_status_dire - integer, remaining health of the towers of the dire side\n",
      "barracks_status_radiant - integer, remaining health of the barracks of the radiant side\n",
      "barracks_status_dire - integer , remaining health of the towers of the direside\n",
      "cluster - integer,\n",
      "first_blood_time - integer, time when the first blood occured in the match\n",
      "lobby_type - integer, type of the looby of the match\n",
      "human_players - integer, number of human players in the match leagueid - integer, league id\n",
      "positive_votes - integer, number of positive votes\n",
      "negative_votes - integer, number of negative votes\n",
      "game_mode - integer, game mode\n",
      "engine - integer, engine\n",
      "picks_bans - string, picks and bans\n",
      "parse_status - integer, parse status\n",
      "item - string, a complex json that also include all the columns mentioned but may need more processing since the more interesting data are found here (e.g. chats, teamfights, purchase logs, etc. )\n",
      "past research\n",
      "there are already several sites (see dotabuff and onedota) that analyze dota 2 matches.\n",
      "inspiration\n",
      "the data set could be used by dota 2 players but are data science enthusiasts as well.\n",
      "there are lots of questions that can be formulated in this dataset.\n",
      "it will be helpful if somebody can produce a better friendlier version of the dataset.\n",
      "acknowledgements\n",
      "citation:\n",
      "source: https://yasp.co/blog/33\n",
      "readme: https://github.com/yasp-dota/yasp/issues/924\n",
      "type: dataset\n",
      "tags: dota 2, games, moba\n",
      "abstract: this is a data dump of all the parsed matches from yasp.co (as of mid december 2015). this is about 3.5 million matches.\n",
      "license: cc by-sa 4.0\n",
      "terms: we ask that you attribute yasp.co if you create or publish anything related to our data. also, please seed for as long as possible.\n",
      "https://www.opendota.com/\n",
      "https://bigquery.cloud.google.com/table/fh-bigquery:public_dump.dota2_yasp_v1\n",
      "how the dataset was compiled\n",
      "the details of how the dataset was complied is detailed here: https://github.com/odota/core/issues/924\n",
      "suggested way of reading the file:\n",
      "in r, the best way to read the file is through the use of read.delim() function since the values are tab separated.\n",
      "example: dota = read.delim(\"dota2_yasp_v1.txt\", sep=\"\\t\")\n",
      "image source:\n",
      "http://media.steampowered.com/apps/dota2/images/blogfiles/keyart_ezalor.jpg\n",
      "context\n",
      "over nine thousand developers took part in the first edition of the state of javascript survey.\n",
      "they answered questions on topics ranging from front-end frameworks and state management, to build tools and testing libraries.\n",
      "you'll find out which libraries developers most want to learn next, and which have the highest satisfaction ratings. and hopefully, this data will help you make sense of the ever-changing javascript ecosystem.\n",
      "content\n",
      "http://stateofjs.com/2016/introduction/\n",
      "acknowledgements\n",
      "thanks to http://stateofjs.com/ open the data for download.\n",
      "context\n",
      "when brazilian consumers need to resolve a dispute with business the first step is to go to a local procon (consumer protection agency) and file a complaint. the procon assists the consumer and intermediates the resolution with the company.\n",
      "content\n",
      "this dataset contains information about complaints filed in procons between 2012 and 2016. this data was download from official brazilian government open data website\n",
      "context:\n",
      "birds use songs and calls of varying length and complexity to attract mates, warn of nearby danger and mark their territory. this dataset contains a recordings of different birdsongs from bird species that can be found in britain (although the recordings themselves are from many different locations).\n",
      "content:\n",
      "this is a dataset of bird sound recordings, a specific subset gathered from the xeno canto collection to form a balanced dataset across 88 species commonly heard in the united kingdom.\n",
      "the copyright in each audio file is owned by the user who donated the file to xeno canto. please see \"birdsong_metadata.tsv\" for the full listing, which gives the authors' names and the cc licences applicable for each file. the audio files are encoded as .flac files.\n",
      "acknowledgements:\n",
      "these recordings were collected by 68 separate birding enthusiasts and uploaded to and stored by xeno-canto: www.xeno-canto.org. if you make use of these recordings in your work, please cite the specific recording and include acknowledgement of and a link to the xeno-canto website.\n",
      "inspiration:\n",
      "can you build a classifier to identify birds based on their songs?\n",
      "can you visualize the songs of specific birds?\n",
      "can you generate new birdsongs based on this data?\n",
      "context\n",
      "while the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like eeg, skin resistance, ...), surprisingly little is known about the exhalation of so-called volatile organic compounds (vocs) at quite low concentrations in response to such stimuli. vocs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us. the project introduces a new field of application for data mining, where trace gas responses of people reacting on-line to films shown in cinemas (or movie theaters) are related to the semantic content of the films themselves. to do so, we measured the vocs from a movie theater over a whole month in intervals of thirty seconds, and annotated the screened films by a controlled vocabulary compiled from multiple sources.\n",
      "content\n",
      "the data set consists of two parts, first the measured vocs, and second the information about the movies. the vocs are given in the file tof_co2_data_30sec.arff which is simply the time of the measurement in the first column, then all measured 400+ vocs in the other columns. roughly one measurement was carried out every 30 seconds. the information which movies were shown is given in the file screenings.csv. it gives start time, end time, movie title and how many visitors were in the screening. additionally, the folder labels_aggregated give a consensus labelling of multiple annotators for the movies. the labels describe the scenes, each label represented by a row, then each column showing if the label is active (1) or not (0). this is available for 6 movies in the data set.\n",
      "the goal of our initial analysis was the identification of markers, that is, finding certain vocs that have a relation to certain labels and therefore emotions. for example, given the scene label blood, is there any increase or decrease in the concentration of a specific voc?\n",
      "further information is available in our publications https://doi.org/10.1145/2783258.2783404 and https://doi.org/10.1038/srep25464\n",
      "acknowledgements\n",
      "if you use this data set, please cite:\n",
      "jörg wicker, nicolas krauter, bettina derstorff, christof stönner, efstratios bourtsoukidis, thomas klüpfel, jonathan williams, and stefan kramer. 2015. cinema data mining: the smell of fear. in proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining (kdd '15). acm, new york, ny, usa, 1295-1304. doi: https://doi.org/10.1145/2783258.2783404\n",
      "inspiration\n",
      "while the first analysis gave already interesting results, we believe that this data set has a high potential for further analysis. we are currently working on increasing the size of the data. additionally, multiple follow-up publications are being prepared. there are many posssible tasks, we focus mainly on the identification of markers in the voc data, but there are many potential interesting findings in the data set. are movies related based on the vocs? could we identify similar scenes based on the vocs?\n",
      "i need help to analyze this data set with r code, if someone can help me i'd appreciate a lot and i'd send some money for his kindness. i really need how to do a regression and clustering manipulating this data. sorry about the format, it's in text file. thanks in advance :)\n",
      "context: measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. different electrical quantities and some sub-metering values are available.\n",
      "data set characteristics:\n",
      "multivariate, time-series\n",
      "associated tasks: regression, clustering\n",
      "data set information:\n",
      "this archive contains 2075259 measurements gathered between december 2006 and november 2010 (47 months). notes: 1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\n",
      "2.the dataset contains some missing values in the measurements (nearly 1,25% of the rows). all calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. for instance, the dataset shows missing values on april 28, 2007.\n",
      "attribute information: 1.date: date in format dd/mm/yyyy\n",
      "2.time: time in format hh:mm:ss\n",
      "3.global_active_power: household global minute-averaged active power (in kilowatt)\n",
      "4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
      "5.voltage: minute-averaged voltage (in volt)\n",
      "6.global_intensity: household global minute-averaged current intensity (in ampere)\n",
      "7.sub_metering_1: energy sub-metering no. 1 (in watt-hour of active energy). it corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
      "8.sub_metering_2: energy sub-metering no. 2 (in watt-hour of active energy). it corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
      "9.sub_metering_3: energy sub-metering no. 3 (in watt-hour of active energy). it corresponds to an electric water-heater and an air-conditioner.\n",
      "patent assignment daily xml (padx)\n",
      "this dataset contains daily patent assignment (ownership) text (no drawings/images) for 10/18/2016 derived from patent assignment recordations made at the uspto.\n",
      "context\n",
      "each day, the us patent and trademark office (uspto) records patent assignments (changes in ownership). these assignments can be used to track chain-of-ownership for patents and patent applications. for more information about ownership/assignability of patents and applications, please see the uspto manual of patent examining procedure (mpep), section 301.\n",
      "frequency: weekly (mon-sun)\n",
      "period: 10/18/2016\n",
      "inspiration\n",
      "this dataset provides insight into where new knowledge and technological advances originate in the united states. to get started working with xml files, fork the kernel exploring daily patent assignment files. you can use this dataset to understand what sector is currently up-and-coming or which companies are filing a lot of patents, which can proxy their level of innovation, a corporate strength, or a focus of new research and development.\n",
      "acknowledgements\n",
      "the uspto owns the dataset. these files are extracted nightly from the assignment historical database (ahd).\n",
      "license\n",
      "creative commons - public domain mark 1.0\n",
      "context\n",
      "in 2013 alone, international migrants sent $413 billion home to families and friends. this money is known as \"remittance money\", and the total is more than three times that afforded by total global foreign aid ($135 billion). remittances are traditionally associated with poor migrants moving outside of their home country to find work, supporting their families back home on their foreign wages; as a result, they make up a significant part of the economic picture for many developing countries in the world.\n",
      "this dataset, published by the world bank, provides estimates of 2016 remittance movements between various countries. it also provides historical data on the flow of such money going back to 1970.\n",
      "for a look at how remittances play into the global economy, watch \"the hidden force in global economics: sending money home\".\n",
      "content\n",
      "this dataset contains three files:\n",
      "bilateral-remittance.csv --- estimated remittances between world countries in the year 2016.\n",
      "remittance-inflow.csv --- historical remittance money inflow into world countries since 1970. typically high in developing nations.\n",
      "remittance-outflow.csv --- historical remittance money outflow from world countries since 1970. typically high in more developed nations.\n",
      "all monetary values are in terms of millions of us dollars.\n",
      "for more information on how this data was generated and calculated, refer to the world bank remittance data faq.\n",
      "acknowledgements\n",
      "this dataset is a republished version of three of the tables published by the world bank which has been slightly cleaned up for use on kaggle. for the original source, and other complimentary materials, check out the dataset home page.\n",
      "inspiration\n",
      "what is the historical trend in remittance inflows and outflows for various countries? how does this relate to the developmental character of the countries in question?\n",
      "what countries send to most money abroad? what countries receive the most money from abroad? try combining this dataset with a demographics dataset to see what countries are most and least reliant on income from abroad.\n",
      "how far do workers migrate for a job? are they staying near home, or going half the world away? are there any surprising facts about who send money to who?\n",
      "this database contains a subset of the memetracker dataset collected by snap.\n",
      "the full memetracker dataset has observations broken into months. because of size considerations, however, this version consists of one-half of a month: the first 15 days of memetracker observations from november 2008.\n",
      "about\n",
      "memetracker tracks the quotes and phrases that appear most frequently over time across the entire online news spectrum. this makes it possible to see how different stories compete for news and blog coverage each day, and how certain stories persist while others fade quickly.\n",
      "overall memetracker tracks more than 17 million different phrases and about 54% of the total phrase/quote mentions appear on blogs and 46% in news media.\n",
      "acknowledgments\n",
      "this dataset was collected by the stanford network analysis project. detailed information about the data and its analysis can be found at the website here.\n",
      "an analysis of this dataset was published here:\n",
      "j. leskovec, l. backstrom, j. kleinberg. meme-tracking and the dynamics of the news cycle. acm sigkdd intl. conf. on knowledge discovery and data mining, 2009.\n",
      "the data\n",
      "the sqlite database contains three tables:\n",
      "articles: 4,542,920 records, with the following fields:\n",
      "article_id: a unique id for the article (int)\n",
      "url: the url of the article (text)\n",
      "date: the date of the article (text), in the strptime format '%y-%m-%d %h:%m:%s'\n",
      "quotes: 7,956,125 records, with the following fields:\n",
      "article_id: unique id for the article that this quote was found in (int)\n",
      "phrase: the high-frequency phrase found in the article (text)\n",
      "links: 16,727,125 records, with the following fields:\n",
      "article_id: unique id for the article that this link was found in (int)\n",
      "link_out: the url of the link out (text)\n",
      "link_out_id: unique id for the target article (int), if it exists; else null\n",
      "these are the top trending \"how to\" searches on google, ranked by their spike value. trending searches are searches with the biggest increase in search interest since the previous time period. data covers the past 5 years.\n",
      "context\n",
      "this dataset is created as a part of my dissertation work for the fulfillment of the master's degree in computer science (tribhuvan university, nepal, 2012).\n",
      "content\n",
      "the dataset contains three individual categories. samples are collected from 40 individuals (persons) from different fields and cropped for character boundary.\n",
      "numerals (288 samples per class, 10 classes)\n",
      "vowels (221 samples per class, 12 classes)\n",
      "consonants (205 samples per class, 36 classes)\n",
      "citation\n",
      "please cite in your publications if it helps your research:\n",
      "@inproceedings{pant2012off,\n",
      "  title={off-line nepali handwritten character recognition using multilayer perceptron and radial basis function neural networks},\n",
      "  author={pant, ashok kumar and panday, sanjeeb prasad and joshi, shashidhar ram},\n",
      "  booktitle={2012 third asian himalayas international conference on internet},\n",
      "  pages={1--5},\n",
      "  year={2012},\n",
      "  organization={ieee}\n",
      "}\n",
      "context\n",
      "the dataset is a csv file compiled using a python scrapper developed using reddit's praw api. the raw data is a list of 3-tuples of [username,subreddit,utc timestamp]. each row represents a single comment made by the user, representing about 5 days worth of reddit data. note that the actual comment text is not included, only the user, subreddit and comment timestamp of the users comment. the goal of the dataset is to provide a lens in discovering user patterns from reddit meta-data alone. the original use case was to compile a dataset suitable for training a neural network in developing a subreddit recommender system. that final system can be found here\n",
      "a very unpolished eda for the dataset can be found here. note the published dataset is only half of the one used in the eda and recommender system, to meet kaggle's 500mb size limitation.\n",
      "content\n",
      "user - the username of the person submitting the comment\n",
      "subreddit - the title of the subreddit the user made the comment in\n",
      "utc_stamp - the utc timestamp of when the user made the comment\n",
      "acknowledgements\n",
      "the dataset was compiled as part of a school project. the final project report, with my collaborators, can be found here\n",
      "inspiration\n",
      "we were able to build a pretty cool subreddit recommender with the dataset. a blog post for it can be found here, and the stand alone jupyter notebook for it here. our final model is very undertuned, so there's definitely improvements to be made there, but i think there are many other cool data projects and visualizations that could be built from this dataset. one example would be to analyze the spread of users through the reddit ecosystem, whether the average user clusters in close communities, or traverses wide and far to different corners. if you do end up building something on this, please share! and have fun!\n",
      "released under reddit's api licence\n",
      "context\n",
      "this data was compiled as part of our undergrad project that used machine learning to classify songs based on themes or activities songs are associated with. for the project we, four activities were choose.\n",
      "dinner: songs that sound good when played in a dinner setting or at a restaurant.\n",
      "sleep: songs that promote sleep when they are played.\n",
      "party: songs that sound good when played at a party.\n",
      "workout: songs that sound good when one is exercising/ working out.\n",
      "the collection of data started with collecting playlist details form spotify. spotify web api was used for the collection of the playlist of each category. track title, album name and artist names were used to extract low level and high level audio features like mfcc, spectral centroid, spectral roll-off, spectral bandwidth, tempo, spectral contrast and root mean square energy of the songs. for ease of computation, the mean of the values were calculated and added to the tables.\n",
      "data was also curated using spotify's audio analysis api. a larger set of songs is part of this data set.\n",
      "content\n",
      "the data set has eight tables.\n",
      "four tables with names playlist_audio_features have the signal processing features like mfcc, spectral centroid etc.\n",
      "four more tables with names playlist_spotify_features have the data extracted from spotify's audio feature api. these tables have larger number of features. the data set size is quite large.\n",
      "description of the \"playlist\"_audio_features columns:\n",
      "the first column has the simple integer id if the track. (this id is local to that file).\n",
      "the second column has the name of the track.\n",
      "the third column name mfcc has the mean of the calculated mfcc for that track. 20 mfc coefficients were extracted from one frame of the track.\n",
      "the forth column is named scem: this is the mean of spectral centroid. spectral centroid was calculated for each frame.\n",
      "the fifth column is named scom: this is the mean of spectral contrast. spectral contrast was calculated for each frame.\n",
      "the sixth column is named srom: this is the mean of spectral roll-off. spectral roll-off was calculated for each frame.\n",
      "the seventh column is named sbwm: this is the mean of spectral bandwidth. spectral bandwidth was calculated for each frame.\n",
      "the eight column is name tempo: this is the estimated tempo of the track.\n",
      "the ninth column is name rmse: this is the mean of the rsme was calculated for each frame.\n",
      "description of the _spotify_features columns:\n",
      "id: this is the spotify id of the track.\n",
      "name: this is the name of the track.\n",
      "url: this is a spotify uri of the track.\n",
      "artist: this is a one or more artists who worked on the track. 5-13: description of each of the column can be found at https://developer.spotify.com/web-api/get-audio-features/\n",
      "acknowledgements\n",
      "we would like to thank librosa an opensource audio feature extraction library in python for developing a great tool. we would also thank the large research done on music genre classification using audio feature which helped us in developing this data set as well as the classification. a special thanks to spotify\n",
      "content and context\n",
      "this dataset is a collection of biomedicine and life science bibliometric data obtained from medline.\n",
      "the data covers 26,759,425 papers available thought medline from 1946 through 2016.\n",
      "this dataset has been created by processing the publicly available data dump at nih.gov. the data dump consists of ca 23 million xml articles. from these xml files i have extracted some meta data into more accessible csv files. the processed csvs contain both very basic metadata (i.e. creation date, number of authors, and ids) and the \"medical subject headings\" mesh classification.\n",
      "the dataset is divided in two files,\n",
      "paper_details.csv\n",
      "mesh.csv\n",
      "the paper_details.csv file contains the following columns:\n",
      "pmid: this is the unique article identified within the dataset, it is also the official identifier used on pubmed\n",
      "doi: digital object identifier, this id allows to uniquely identify a paper through the widely used doi scheme\n",
      "num_authors: number of authors on the paper\n",
      "year: year the document has been created\n",
      "month: month the document has been created\n",
      "day: day the document has been created\n",
      "title: title of the document\n",
      "issn: the issn number of the journal the document has been published in\n",
      "issn_type: printed or electronic\n",
      "volume: volume of the journal the document has been published in\n",
      "issue: issue of the journal the document has been published in\n",
      "journal_title: name of journal the document has been published in\n",
      "journal_title_iso: iso abbreviation of the journal title.\n",
      "5 gb, and 26,759,425 rows\n",
      "in the mesh.csv file the following fields are available:\n",
      "pmid: this is the unique article identified within the dataset, it is also the official identifier used on pubmed\n",
      "descriptor_id: the id of the mesh descriptor for the given document\n",
      "descriptor_name: name of the mesh descriptor\n",
      "descriptor_major_topic: y/n, indicate if the descriptor is a major topic in mesh\n",
      "qualifiers: a list of mesh qualifiers (aka subheadings). subheadings are attached to mesh headings to describe a specific aspect of a concept.\n",
      "nb: a document can and has more often then not more then one mesh descriptor associated to it.\n",
      "11 gb, and 247,415,857 rows\n",
      "other resources\n",
      "an introduction to mesh can be found here and short tutorials from the u.s. national library of medicine on how to use mesh can be found here.\n",
      "factsheets describing mesh and medline:\n",
      "https://www.nlm.nih.gov/pubs/factsheets/mesh.html\n",
      "https://www.nlm.nih.gov/pubs/factsheets/medline.html\n",
      "further datasets, which might be useful to work with mesh data:\n",
      "https://mbr.nlm.nih.gov/downloads.shtml\n",
      "acknowledgements\n",
      "the data is freely available for download from medline. specifically through their ftp service at ftp://ftp.ncbi.nlm.nih.gov/pub/.\n",
      "read the disclaimer from the u.s. national library of medicine regarding public use and redistribution of the data here\n",
      "the data extraction would not have been possible without access to the computing facilities of imt school for advanced studies lucca.\n",
      "inspiration\n",
      "questions which one might want to look into using this dataset could be:\n",
      "prediction of mesh headings\n",
      "descriptive statistics on the rise and fall of mesh descriptors over time.\n",
      "which journals ˙have had the most meteoric rise?\n",
      "compute the co-occurrence probability of a given mesh descriptor pair over time\n",
      "how to open the files:\n",
      "due to the limit on kaggle for files to be at most 500mb in size, the files have been split. more specifically the two files have been compressed with zip and split. to recombine them do the following. if you are on linux/macos enter the following commands in the terminal\n",
      "cat paper_details.csv.zip.part-* > papers_details.csv.zip\n",
      "unzip papers_details.csv.zip\n",
      "for windows machines, this stock overflow answer will help.\n",
      "image credit\n",
      "the beautiful cover image has been made by olsztyn-poland over at unsplash.com.\n",
      "link: https://unsplash.com/collections/610433/medical?photo=nss2erzqwgw\n",
      "context\n",
      "study for poem classification. trying to classified poems with targets age and type. i use two xgboost predictors to predict target and type separately.\n",
      "content\n",
      "please refer to the website https://www.poetryfoundation.org/ for now i only crawl the data of\n",
      "renaissance love\n",
      "modern love\n",
      "renaissance nature\n",
      "modern nature\n",
      "renaissance mythology & folklore\n",
      "modern mythology & folklore\n",
      "some have copyrights. i only use for studying :)\n",
      "acknowledgements\n",
      "https://www.poetryfoundation.org/ has the copyright\n",
      "inspiration\n",
      "classification is fun!!\n",
      "indian premier league database for data analysis.\n",
      "csv version for the same dataset\n",
      "context\n",
      "► 577 matches\n",
      "► 469 players\n",
      "► seasons 2008 to 2016\n",
      "► detailed player details (player_dob, player_batting_style,player_bowling_style, etc...) for 469 players.\n",
      "► detailed match events (toss,stadium, date of the match, who captained the match, who was the keeper, extras, etc...) for 577 matches.\n",
      "example sql queries on ipl database\n",
      "microsoft sql server version\n",
      "database diagram\n",
      "content :\n",
      "dataset includes 21 includes as mentioned below. some tables created intentionally to practice sql queries. you can use boolean values in some situations instead of joining tables like toss_table\n",
      "acknowledgements :\n",
      "thanks to stackoverflow community\n",
      "inspiration\n",
      "aim is to provide the database for all the indian premier league matches. where people can run their own queries to analyze the data.\n",
      "some of the analysis we most often heard in tv commentary are below.\n",
      "dhoni's strike rate against left-ball spinners.\n",
      "what will be the result percentage when team lost more than three wickets in powerplay.\n",
      "below are the some websites where you can find inspiration for your analysis.\n",
      "knoema cricket analysis\n",
      "pandimi cricket analysis\n",
      "data on party strength in each us state\n",
      "the repository contains data on party strength for each state as shown on each state's corresponding party strength wikipedia page (for example, here is virginia )\n",
      "each state has a table of a detailed summary of the state of its governing and representing bodies on wikipedia but there is no data set that collates these entries. i scraped each state's wikipedia table and collated the entries into a single dataset. the data are stored in the state_party_strength.csv and state_party_strength_cleaned.csv. the code that generated the file can be found in corresponding python notebooks.\n",
      "data contents:\n",
      "the data contain information from 1980 on each state's: 1. governor and party 2. state house and senate composition 3. state representative composition in congress 4. electoral votes\n",
      "clean version\n",
      "data in the clean version has been cleaned and processed substantially. namely: - all columns now contain homogenous data within the column - names and wiki-citations have been removed - only the party counts and party identification have been left the notebook that created this file is here\n",
      "uncleaned data version\n",
      "the data contained herein have not been altered from their wikipedia tables except in two instances: - forced column names to be in accord across states - any needed data modifications (ie concatenated string columns) to retain information when combining columns\n",
      "to use the data:\n",
      "please note that the right encoding for the dataset is \"iso-8859-1\", not 'utf-8' though in future versions i will try to fix that to make it more accessible.\n",
      "this means that you will likely have to perform further data wrangling prior to doing any substantive analysis. the notebook that has been used to create this data file is located here\n",
      "raw scraped data\n",
      "the raw scraped data can be found in the pickle. this file contains a python dictionary where each key is a us state name and each element is the raw scraped table in pandas dataframe format.\n",
      "hope it proves as useful to you in analyzing/using political patterns at the state level in the us for political and policy research.\n",
      "context\n",
      "the complete global firepower list for 2017 puts the military powers of the world into full perspective.\n",
      "content\n",
      "globalfirepower.csv contains all columns without categories. globalfirepower_multiindex.csv has 2 level columns, see the kernel for a parsing example.\n",
      "fields list\n",
      "manpower\n",
      "total populations\n",
      "available manpower\n",
      "manpower fit-for-service\n",
      "manpower reaching military age annually\n",
      "active military manpower\n",
      "active reserve military manpower\n",
      "air power\n",
      "total aircraft strength\n",
      "fighters & interceptors\n",
      "attack aircraft\n",
      "transports\n",
      "trainers\n",
      "total helicopters\n",
      "attack helicopters\n",
      "serviceable airports\n",
      "army strengths\n",
      "combat tanks\n",
      "armored fighting vehicles\n",
      "self-propelled artillery\n",
      "towed artillery\n",
      "rocket projectors\n",
      "naval power\n",
      "total naval strength\n",
      "aircraft carriers\n",
      "frigates\n",
      "destroyers\n",
      "corvettes\n",
      "submarines\n",
      "patrol craft\n",
      "mine warfare\n",
      "financial resources\n",
      "annual defense budgets\n",
      "external debt\n",
      "reserves of foreign exchange and gold\n",
      "purchasing power parity\n",
      "logistical resources\n",
      "labor force strength\n",
      "merchant marine strength\n",
      "major ports & terminals\n",
      "roadway coverage\n",
      "railway coverage\n",
      "natural resources\n",
      "oil production\n",
      "oil consumption\n",
      "proven oil reserves\n",
      "geography\n",
      "square land areas\n",
      "coastline\n",
      "shared borders\n",
      "waterway coverage\n",
      "the finalized global firepower ranking relies on over 50 factors to determine a given nation's powerindex ('pwrindx') score. our formula allows smaller, though more technologically-advanced, nations to compete with larger, lesser-developed ones. modifiers (in the form of bonuses and penalties) are added to further refine the list. some items to observe in regards to the finalized ranking:\n",
      "ranking does not simply rely on the total number of weapons available to any one country but rather focuses on weapon diversity within the number totals to provide a better balance of firepower available (i.e. fielding 100 minesweepers does not equal the strategic and tactical value of fielding 10 aircraft carriers).\n",
      "nuclear stockpiles are not taken into account but recognized / suspected nuclear powers receive a bonus.\n",
      "geographical factors, logistical flexibility, natural resources and local industry influence the final ranking.\n",
      "available manpower is a key consideration; nations with large populations tend to rank higher.\n",
      "land-locked nations are not penalized for lack of a navy; naval powers are penalized for lack of diversity in available assets.\n",
      "nato allies receive a slight bonus due to the theoretical sharing of resources.\n",
      "current political / military leadership is not taken into account.\n",
      "for 2017 there are a total of 133 countries included in the gfp database.\n",
      "acknowledgements\n",
      "the csv files were parsed from http://www.globalfirepower.com/countries-listing.asp .\n",
      "©2017 www.globalfirepower.com • content ©2003-2017 globalfirepower.com • all rights reserved • the globalfirepower.com logo are trademarks and protected by all applicable domestic and international intellectual property laws. all material presented on this site is \"as is\" without any guarantee. part of the militaryfactory.com network of sites.\n",
      "content\n",
      "this dataset is scraped from https://www.thenews.com.pk website. it has news articles from 2015 till date related to business and sports. it contains the heading of the particular article, its content and its date. the content also contains the place from where the statement or article was published.\n",
      "importance\n",
      "this dataset can be used to detect main patterns between writing pattern of different types of articles. one more thing that can be extracted from it is that we could also detect the main locations from where the different types of articles originate.\n",
      "improvements\n",
      "some data cleaning could still be done specially in the content area of the dataset. one more thing that could be done is that we could extract the locations from the content and make a separated table for it.\n",
      "acknowledgements\n",
      "i'd like to thanks developer of selenium library. that helped a lot in retrieving the data.\n",
      "content\n",
      "dog: licensetype, breed, color, dogname, ownerzip, expyear, validdate\n",
      "i've included all the data from 2007 to 2017\n",
      "inspiration\n",
      "i love dogs to see trends in dog breeds, why are some more popular than others?\n",
      "content\n",
      "tweet id, contains tweet-stamp\n",
      "date + time, date and time of day (24hr)\n",
      "tweet text, text of tweet, remove 'b'\n",
      "usage\n",
      "what's someone going to do with a bunch of tweets?\n",
      "maybe someone would want to generate text using this dataset\n",
      "or do sentiment analysis\n",
      "or find out the most likely time of day elon would tweet.\n",
      "pie his tweets per month, its data!!\n",
      "either way its up to you!\n",
      "inspiration:\n",
      "content\n",
      "attached is a list of twitter users who regularly report on natural and man-made disasters, violence or crime. the accounts may belong to journalists, news media, local fire or police departments, other local authorities, or disaster monitors. disaster reporting may not be the primary function of the accounts, nevertheless they are a prolific source of disaster/accident reporting, especially at the location they are associated with.\n",
      "background\n",
      "details of the curation of this dataset, once published, will be added to this entry.\n",
      "disclaimer\n",
      "the dataset does not include a measure of credibility for the users. the stories reported by them may or may not be true. further vetting and verification is required to confirm if the stories that they report are credible.\n",
      "context\n",
      "these data are from a couple of sensors of my dad's house.\n",
      "content\n",
      "the data are from motion sensors at the front door, which also regularly logs brightness.\n",
      "the front door motion detection data also includes motions of three cats.\n",
      "data structure is pretty self explanatory. load the csv files.\n",
      "data files\n",
      "i already added day-of-year, year, weekday and time of day to the data for easier handling.\n",
      "brightness\n",
      "these time-stamped values resemble the brightness readings. they range from dark (low numbers) to bright day light (high numbers).\n",
      "the brightness nightly mean values differ. the reasons are: christmas decoration during the winter, and solar lights being set up some time in mid april this year.\n",
      "contacts\n",
      "these data indicate door and window openings. they are time-stamped. the boolean value isclosed indicates wether the contact has been closed.\n",
      "motion\n",
      "there is a motion sensor at the front door which indicates movement. movement detections also are time-stamped data.\n",
      "questions\n",
      "at what time of the day the newspaper is delivered?\n",
      "how can i tell from the brightness value, what kind of weather it was on this day?\n",
      "additional information\n",
      "newspaper is not delivered on sundays\n",
      "newspaper is in the mailbox by 6:30 am\n",
      "newspaper is usually taken out of the mailbox around 7:00 am (not on saturdays)\n",
      "there is regular front door activity - someon leaving the house - at 7:30 (not on saturdays and sundays)\n",
      "there are three cats living at the house\n",
      "if you play the game, you might understand the columns by themselves, otherwise sometime in this week, ill update the information and provide all details. (note: the encoding for names is in utf-8).\n",
      "if you want to contribute, send me a message.\n",
      "all data is collected from the game : football manager 2017.\n",
      "try to predict the next messi/ronaldo ?\n",
      "context:\n",
      "irony in language is when a statement is produced with one meaning but the intended meaning is exactly the opposite. for instance, someone who has burned toast might serve it and say ironically “it’s a little underdone”. automatically detecting when language is ironic is an especially difficult task in natural language processing.\n",
      "content:\n",
      "this dataset contains 1950 comments, which have been labeled as ironic (1) or not ironic (-1) by human annotators. the text was taken from reddit comments.\n",
      "acknowledgements:\n",
      "this dataset and analysis of it is presented in the following paper.\n",
      "wallace, b. c., do kook choe, l. k., kertz, l., & charniak, e. (2014, april). humans require context to infer ironic intent (so computers probably do, too). in acl (2) (pp. 512-516). url: http://www.byronwallace.com/static/articles/wallace-irony-acl-2014.pdf\n",
      "made possible by support from the army research office (aro), grant 64481-ma / w9111f-13-1-0406 \"sociolinguistically informed natural language processing: automating irony detection\"\n",
      "inspiration:\n",
      "is irony more likely when discussing certain topics?\n",
      "does ironic text tend to have more positive or more negative sentiment?\n",
      "what novel features can you develop to help detect irony?\n",
      "context:\n",
      "“compositionality” is a concept from linguistics where the meaning of a phrase is made up of the meaning of each of its individual words. so “the red apple” refers, literally, to an apple that is red. sometimes when you combine words, however, the meaning of the phrase isn’t the same as the combined meanings of the individual words. “the big apple”, for example, means “new york city”, not a literal large apple.\n",
      "this dataset contains human judgements of the compositionality of common english phrases with two nouns.\n",
      "content:\n",
      "this dataset contains two files: summary data of the human judgements and the annotations by individual judges. all judgements are on a scale of 0 to 5, with 0 being “not literal” and 5 being “literal”.\n",
      "acknowledgements:\n",
      "this dataset was collected by siva reddy, diana mccarthy and suresh manandhar. if you use this dataset in your work, please cite the following paper:\n",
      "reddy, s., mccarthy, d., & manandhar, s. (2011, november). an empirical study on compositionality in compound nouns. in ijcnlp (pp. 210-218).\n",
      "inspiration:\n",
      "is there a relationship between how frequent a word is and how often it’s used literally? (you can estimate word frequency using the corpora included in the natural language toolkit, which is already ready to run in any python kernel!)\n",
      "given this dataset, can you predict whether other compound nouns will be literal or not?\n",
      "which phrases are the most literal? which are the least literal? are there any patterns you notice?\n",
      "context\n",
      "from 1934 to 1963, san francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of alcatraz. today, the city is known more for its tech scene than its criminal past. but, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding bart to work, there is no scarcity of crime in the city by the bay. from sunset to soma, and marina to excelsior, this dataset provides nearly 12 years of crime reports from across all of san francisco's neighborhoods.\n",
      "this dataset was featured in our completed playground competition entitled san francisco crime classification. the goals of the competition were to:\n",
      "predict the category of crime that occurred, given the time and location\n",
      "visualize the city and crimes (see mapping and visualizing violent crime for inspiration)\n",
      "content\n",
      "this dataset contains incidents derived from sfpd crime incident reporting system. the data ranges from 1/1/2003 to 5/13/2015. the training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. there are 9 variables:\n",
      "dates - timestamp of the crime incident\n",
      "category - category of the crime incident (only in train.csv).\n",
      "descript - detailed description of the crime incident (only in train.csv)\n",
      "dayofweek - the day of the week\n",
      "pddistrict - name of the police department district\n",
      "resolution - how the crime incident was resolved (only in train.csv)\n",
      "address - the approximate street address of the crime incident\n",
      "x - longitude\n",
      "y - latitude\n",
      "acknowledgements\n",
      "this dataset is part of our completed playground competition entitled san francisco crime classification. visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. if you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!\n",
      "the original dataset is from sf opendata, the central clearinghouse for data published by the city and county of san francisco.\n",
      "context\n",
      "electoral integrity refers to international standards and global norms governing the appropriate conduct of elections. these standards have been endorsed in a series of authoritative conventions, treaties, protocols, and guidelines by agencies of the international community and apply universally to all countries throughout the electoral cycle, including during the pre-electoral period, the campaign, on polling day, and in its aftermath.\n",
      "content\n",
      "the perceptions of electoral integrity (pei) survey asks experts to evaluate elections according to 49 indicators, grouped into eleven categories reflecting the whole electoral cycle. the pei dataset is designed to provide a comprehensive, systematic and reliable way to monitor the quality of elections worldwide. it includes disaggregated scores for each of the individual indicators, summary indices for the eleven dimensions of electoral integrity, and a pei index score out of 100 to summarize the overall integrity of the election.\n",
      "acknowledgements\n",
      "this study was conducted by pippa norris, alessandro nai, and max grömping for harvard university's electoral integrity project.\n",
      "context\n",
      "on april 23, 2014, the department of justice, at the behest of president obama, announced the clemency initiative, inviting petitions for commutation of sentence from nonviolent offenders who, among other criteria, likely would have received substantially lower sentences if convicted of the same offenses today. as expected, the announcement resulted in a record number of petitions – including thousands of petitions involving crimes not included in the initiative, such as terrorism, murder, sex crimes, public corruption, and financial fraud.\n",
      "in the federal system, commutation of sentence and pardon are different forms of executive clemency, which is a broad term that applies to the president’s constitutional power to exercise leniency toward persons who have committed federal crimes.\n",
      "a commutation of sentence reduces a sentence, either totally or partially, that is then being served, but it does not change the conviction, signify innocence, or remove civil disabilities from the criminal conviction. a commutation may include remission (or release) of the financial obligations that are imposed as part of a sentence, such as payment of a fine or restitution; a remission applies only to the part of the financial obligation that has not already been paid. to be eligible to apply for commutation of sentence, a person must have reported to prison to begin serving his sentence and may not be challenging his conviction in the courts.\n",
      "a pardon is an expression of the president’s forgiveness and is granted in recognition of the applicant’s acceptance of responsibility for the crime and established good conduct for a significant period of time after conviction or completion of sentence. it does not signify innocence. it does, however, remove civil disabilities – such as restrictions on the right to vote, hold state or local office, or sit on a jury – imposed because of the conviction. a person is not eligible to apply for a presidential pardon until a minimum of five years has elapsed since his release from any form of confinement imposed upon him as part of a sentence for his most recent criminal conviction.\n",
      "acknowledgements\n",
      "the data was compiled and published by the office of the pardon attorney. the office of the pardon attorney receives and reviews petitions for all forms of executive clemency, including pardon, commutation (reduction) of sentence, remission of fine or restitution, and reprieve, initiates the necessary investigations of clemency requests, and prepares the report and recommendation of the attorney general to the president.\n",
      "context\n",
      "the president can declare an emergency for any occasion or instance when the president determines federal assistance is needed. emergency declarations supplement state and local or indian tribal government efforts in providing emergency services, such as the protection of lives, property, public health, and safety, or to lessen or avert the threat of a catastrophe in any part of the united states. the total amount of assistance provided for in a single emergency may not exceed $5 million.\n",
      "the president can declare a major disaster for any natural event, including any hurricane, tornado, storm, high water, wind-driven water, tidal wave, tsunami, earthquake, volcanic eruption, landslide, mudslide, snowstorm, or drought, or, regardless of cause, fire, flood, or explosion, that the president determines has caused damage of such severity that it is beyond the combined capabilities of state and local governments to respond. a major disaster declaration provides a wide range of federal assistance programs for individuals and public infrastructure, including funds for both emergency and permanent work.\n",
      "content\n",
      "this dataset includes a record for every federal emergency or disaster declared by the president of the united states since 1953.\n",
      "acknowledgements\n",
      "the disaster database was published by the federal emergency management agency with data from the national emergency management information system.\n",
      "inspiration\n",
      "what type of disaster is the most commonly declared by fema? which disasters or emergencies have lasted the longest? what disaster was declared in the most counties or states? has the number of disasters declared by fema risen or fallen over time?\n",
      "context\n",
      "this is the sentiment140 dataset. it contains 1,600,000 tweets extracted using the twitter api . the tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\n",
      "content\n",
      "it contains the following 6 fields:\n",
      "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
      "ids: the id of the tweet ( 2087)\n",
      "date: the date of the tweet (sat may 16 23:58:44 utc 2009)\n",
      "flag: the query (lyx). if there is no query, then this value is no_query.\n",
      "user: the user that tweeted (robotickilldozr)\n",
      "text: the text of the tweet (lyx is cool)\n",
      "acknowledgements\n",
      "the official link regarding the dataset with resources about how it was generated is here the official paper detailing the approach is here\n",
      "citation: go, a., bhayani, r. and huang, l., 2009. twitter sentiment classification using distant supervision. cs224n project report, stanford, 1(2009), p.12.\n",
      "inspiration\n",
      "to detect severity from tweets. you may have a look at this.\n",
      "context\n",
      "the data here is from the global health observatory (gho) who provide data on malaria incidence, death and prevention from around the world. i have also included malaria net distribution data the against malaria foundation (amf). the amf has consistently been ranked as the most cost effective charity by charity evaluators give well - http://www.givewell.org/charities/top-charities\n",
      "content\n",
      "gho data is all in narrow format, with variables for a country in a given year being found on different rows.\n",
      "gho data (there are a number or superfluous columns):\n",
      "gho (code)\n",
      "gho (display) - this is the variable being measured\n",
      "gho (url)\n",
      "publishstate (code)\n",
      "publishstate (display)\n",
      "publishstate (url)\n",
      "year (code)\n",
      "year (display)\n",
      "year (url)\n",
      "region (code)\n",
      "region (display)\n",
      "region (url)\n",
      "country (code) - can be used to join this data with the amf data\n",
      "country (display)\n",
      "country (url)\n",
      "display value - this is the measured value\n",
      "low - lower confidence interval\n",
      "high - higher confidence interval\n",
      "comments\n",
      "amf distribution data:\n",
      "#_llins - total number of malaria nets distributed\n",
      "location - the specific area that received the nets, within the target country\n",
      "country - the country in which the nets were distributed\n",
      "when - the period the distribution\n",
      "by_whom - the organisation(s) which partnered with the amf to perform the distribution\n",
      "country_code - the country's gho country code (this will allow joining with the gho data)\n",
      "for the current version all data was downloaded 20-08-17 the gho data covers the years from 2000 to 2015 (not all files have data in all years) the amf data runs from 2006 - the present.\n",
      "the gho data is taken as is from the csv (lists) available here: http://apps.who.int/gho/data/node.main.a1362?lang=en the source of the amf's distribution data is here: https://www.againstmalaria.com/distributions.aspx - it was assembled into a single csv using excel (mea culpa)\n",
      "inspiration\n",
      "malaria is one of the world's most devastating diseases, not least because it largely affects some of the poorest people. over the past 15 years malaria rates and mortality have dropped (http://www.who.int/malaria/media/world-malaria-report-2016/en/), but there is still a long way to go. understanding the data is generally one of the most important steps in solving any large problem. i'm excited to see what the kaggle community can find out about the global trends in malaria over this period, and if we can find out anything about the impact of organisations such as the amf.\n",
      "nan\n",
      "context\n",
      "computer network traffic data - a ~500k csv with summary of some real network traffic data from the past. the dataset has ~21k rows and covers 10 local workstation ips over a three month period. half of these local ips were compromised at some point during this period and became members of various botnets.\n",
      "content\n",
      "each row consists of four columns:\n",
      "date: yyyy-mm-dd (from 2006-07-01 through 2006-09-30)\n",
      "l_ipn: local ip (coded as an integer from 0-9)\n",
      "r_asn: remote asn (an integer which identifies the remote isp)\n",
      "f: flows (count of connnections for that day)\n",
      "reports of \"odd\" activity or suspicions about a machine's behavior triggered investigations on the following days (although the machine might have been compromised earlier)\n",
      "date : ip 08-24 : 1 09-04 : 5 09-18 : 4 09-26 : 3 6\n",
      "acknowledgements\n",
      "this public dataset was found on http://statweb.stanford.edu/~sabatti/data.html\n",
      "inspiration\n",
      "can you discover when a compromise has occurred by a change in the pattern of communication?\n",
      "context\n",
      "i was searching for a master degree program in data-science when i found this awesome website mastersportal, so i just scrapped it to take my time analysing all master programs available around the world.\n",
      "content\n",
      "this dataset contains 60442 different master's degree programs from 99 countries around the world.\n",
      "scrapping code\n",
      "https://github.com/anasfullstack/masters-portal-scrapper\n",
      "context\n",
      "this dataset is a cleaned-up and modernized version of \"caa database of battles, version 1990\", shortnamed \"cdb90\". it contains information on over 600 battles that were fought between 1600 ad and 1973 ad. descriptive data include battle name, date, and location; the strengths and losses on each side; identification of the victor; temporal duration of the battle; and selected environmental and tactical environment descriptors (such as type of fortifications, type of tactical scheme, weather conditions, width of front, etc.).\n",
      "content\n",
      "the data contained therein is split across several files. the most important of these is battles.csv, which is lists and gives information about the battles themselves. files marked enum describe the keys used by specific fields. other files provide additional context.\n",
      "acknowledgements\n",
      "the original version of this database was distributed by the u.s. army concepts analysis agency. the version of this dataset you see here is a cleaned-up version created by jeffrey arnold. this dataset, cleanup code, and source data are all available here.\n",
      "inspiration\n",
      "how often were battles fought in various weather conditions?\n",
      "how often did an attacker or defender achieve the element of surprise? did it have a significant effect on the outcome?\n",
      "did prepared fortifications have a significant effect on outcomes?\n",
      "context\n",
      "the calcofi data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. it includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. the physical, chemical, and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the california current and a range of biological responses to them. calcofi research drew world attention to the biological response to the dramatic pacific-warming event in 1957-58 and introduced the term “el niño” into the scientific literature.\n",
      "the california cooperative oceanic fisheries investigations (calcofi) are a unique partnership of the california department of fish & wildlife, noaa fisheries service and scripps institution of oceanography. the organization was formed in 1949 to study the ecological aspects of the sardine population collapse off california. today our focus has shifted to the study of the marine environment off the coast of california, the management of its living resources, and monitoring the indicators of el nino and climate change. calcofi conducts quarterly cruises off southern & central california, collecting a suite of hydrographic and biological data on station and underway. data collected at depths down to 500 m include: temperature, salinity, oxygen, phosphate, silicate, nitrate and nitrite, chlorophyll, transmissometer, par, c14 primary productivity, phytoplankton biodiversity, zooplankton biomass, and zooplankton biodiversity.\n",
      "content\n",
      "each table has several dozen columns. please see this page for the table details.\n",
      "the state contract and procurement registration system (scprs) was established in 2003, as a centralized database of information on state contracts and purchases over $5000. escprs represents the data captured in the state's eprocurement (ep) system, bidsync, as of march 16, 2009. the data provided is an extract from that system for fiscal years 2012-2013, 2013-2014, and 2014-2015\n",
      "acknowledgements\n",
      "this dataset was kindly released by the state of california. you can find the original copy here.\n",
      "context\n",
      "if you've ever had to typeset mathematical expressions, you might have thought: wouldn’t it be great if i could just take a picture of a handwritten expression and have it be recognized automatically? this dataset has all the data you’ll need to build a system to do just that.\n",
      "description\n",
      "the dataset provide more than 11,000 expressions handwritten by hundreds of writers from different countries, merging the data sets from 4 crohme competitions. writers were asked to copy printed expressions from a corpus of expressions. the corpus has been designed to cover the diversity proposed by the different tasks and chosen from an existing math corpus and from expressions embedded in wikipedia pages. different devices have been used (different digital pen technologies, white-board input device, tablet with sensible screen), thus different scales and resolutions are used. the dataset provides only the on-line signal.\n",
      "in the last competition crohme 2013 the test part is completely original and the train part is using 5 existing data sets:\n",
      "mathbrush (university of waterloo),\n",
      "hamex (university of nantes),\n",
      "mfrdb (czech technical university),\n",
      "expressmatch (university of sao paulo),\n",
      "the kaist data set.\n",
      "in crohme 2014 a new test set has been created with 987 new expressions and 2 new tasks has been added: isolated symbol recognition and matrix recognition. train and test files as the evaluation scripts for these new tasks are provided. for the isolated symbol datasets, elements are extracted from full expression using the existing datasets, which also includes segmentation errors. for the matrix recognition task, 380 new expressions have been labelled and split into training and test sets.\n",
      "furthermore, 6 participants of the 2012 competition provide their recognized expressions for the 2012 test part. this data allows research on decision fusion or evaluation metrics.\n",
      "technical details\n",
      "the ink corresponding to each expression is stored in an inkml file. an inkml file mainly contains three kinds of information:\n",
      "the ink: a set of traces made of points;\n",
      "the symbol level ground truth: the segmentation and label information of each symbol in the expression;\n",
      "the expression level ground truth: the mathml structure of the expression.\n",
      "the two levels of ground truth information (at the symbol as well as at the expression level) are entered manually. furthermore, some general information is added in the file:\n",
      "the channels (here, x and y);\n",
      "the writer information (identification, handedness (left/right), age, gender, etc.), if available;\n",
      "the latex ground truth (without any reference to the ink and hence, easy to render);\n",
      "the unique identification code of the ink (ui), etc.\n",
      "the inkml format makes references between the digital ink of the expression, its segmentation into symbols and its mathml representation. thus, the stroke segmentation of a symbol can be linked to its mathml representation.\n",
      "the recognized expressions are the outputs of the recognition competitors' systems. it uses the same inkml format, but without the ink information (only segmentation, label and mathml structure).\n",
      "more details available on crohme website.\n",
      "acknowledgements\n",
      "this dataset was compiled by harold mouchère and is licensed under a creative commons attribution-noncommercial-sharealike 3.0 unported license. if you use this dataset in your work, please include the following citation:\n",
      "harold mouchère, icfhr 2014 crohme: fourth international competition on recognition of online handwritten mathematical expressions (crohme-2014) ,2,id:crohme-2014_2,url:http://tc11.cvc.uab.es/datasets/crohme-2014_2\n",
      "you might also like:\n",
      "handwritten math symbols dataset: over 100 000 image samples\n",
      "arabic handwritten digits dataset\n",
      "context:\n",
      "sentimentwortschatz, or sentiws for short, is a publicly available german-language resource for sentiment analysis, opinion mining etc. it lists positive and negative polarity bearing words weighted within the interval of [-1; 1] plus their part of speech tag, and if applicable, their inflections. the current version of sentiws (v1.8b) contains 1,650 positive and 1,818 negative words, which sum up to 15,649 positive and 15,632 negative word forms incl. their inflections, respectively. it not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one.\n",
      "format:\n",
      "sentiws is organised in two utf8-encoded text files structured the following way:\n",
      "| \\t \\t ,..., \\n\n",
      "where \\t denotes a tab, and \\n denotes a new line.\n",
      "acknowledgement:\n",
      "sentiws is licensed under a creative commons attribution-noncommercial-share alike 3.0 unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). if you use sentiws in your work, please cite the following paper:\n",
      "r. remus, u. quasthoff & g. heyer: sentiws - a publicly available german-language resource for sentiment analysis. in: proceedings of the 7th international language ressources and evaluation (lrec'10), 2010\n",
      "this version of the data set was last updated in march 2012.\n",
      "context:\n",
      "“the universal product code (upc) is a barcode symbology that is widely used in the united states, canada, united kingdom, australia, new zealand, in europe and other countries for tracking trade items in stores.\n",
      "“upc (technically refers to upc-a) consists of 12 numeric digits, that are uniquely assigned to each trade item. along with the related ean barcode, the upc is the barcode mainly used for scanning of trade items at the point of sale, per gs1 specifications.[1] upc data structures are a component of gtins and follow the global gs1 specification, which is based on international standards. but some retailers (clothing, furniture) do not use the gs1 system (rather other barcode symbologies or article number systems). on the other hand, some retailers use the ean/upc barcode symbology, but without using a gtin (for products, brands, sold at such retailers only).”\n",
      "-- tate. (n.d.). in wikipedia. retrieved august 18, 2017, from https://en.wikipedia.org/wiki/plagiarism. text reproduced here under a cc-by-sa 3.0 license.\n",
      "content:\n",
      "this dataset contains just over 1 million upc codes and the names of the products associated with them.\n",
      "acknowledgements:\n",
      "while upc’s themselves are not copyrightable, the brand names and trademarks in this dataset remain the property of their respective owners.\n",
      "inspiration:\n",
      "can you use this dataset to generate new product names?\n",
      "can you use this in conjunction with other datasets to disambiguate products?\n",
      "context:\n",
      "the santa barbara corpus of spoken american english is based on hundreds of recordings of natural speech from all over the united states, representing a wide variety of people of different regional origins, ages, occupations, and ethnic and social backgrounds. it reflects many ways that people use language in their lives: conversation, gossip, arguments, on-the-job talk, card games, city council meetings, sales pitches, classroom lectures, political speeches, bedtime stories, sermons, weddings, and more. the corpus was collected by the university of california, santa barbara center for the study of discourse, director john w. du bois (ucsb), associate editors: wallace l. chafe (ucsb), charles meyer (umass, boston), and sandra a. thompson (ucsb).\n",
      "each speech file is accompanied by a transcript in which phrases are time stamped with respect to the audio recording. personal names, place names, phone numbers, etc., in the transcripts have been altered to preserve the anonymity of the speakers and their acquaintances and the audio files have been filtered to make these portions of the recordings unrecognizable. pitch information is still recoverable from these filtered portions of the recordings, but the amplitude levels in these regions have been reduced relative to the original signal. the audio data consists of mp3 format speech files, recorded in two-channel pcm, at 22050hz.\n",
      "contents:\n",
      "this dataset contains part one of the corpus. the other three parts and additional information can be found here. the following information is included in this dataset:\n",
      "recordings: 14 recordings as .mp3 files\n",
      "transcripts: time-aligned transcripts for all 14 recordings, in the chat format\n",
      "metadata: a .csv with demographic information on speakers, as well as which recordings they appear in. (some talkers appear in more than one recording.)\n",
      "acknowledgements:\n",
      "the santa barbara corpus was compiled by researchers in the linguistics department of the university of california, santa barbara. the director of the santa barbara corpus is john w. du bois, working with associate editors wallace l. chafe and sandra a. thompson (all of uc santa barbara), and charles meyer (umass, boston). for the publication of parts 3 and 4, the authors are john w. du bois and robert englebretson.\n",
      "it is distributed here under an cc by-nd 3.0 us license.\n",
      "inspiration:\n",
      "currently, the transcriptions are close transcriptions and include disfluencies and overlaps. can you use nlp to convert them to broad transcriptions without this information?\n",
      "can you create a phone-aligned transcription of this dataset? you might find it helpful to use forced alignment.\n",
      "context:\n",
      "bike shares are becoming a popular alternative means of transportation. the city of austin makes data available on >649k bike trips over 2013-2017.\n",
      "content:\n",
      "this data includes information on bike trip start location, stop location, duration, type of bike share user. bike station location data is also provided.\n",
      "dataset description\n",
      "use this dataset with bigquery\n",
      "you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too.\n",
      "*austin_bikeshare_trips.csv*\n",
      "bikeid: integer id of bike\n",
      "checkout_time: hh:mm:ss, see start time for date stamp\n",
      "duration_minutes: int minutes of trip duration\n",
      "end_station_id: integer id of end station\n",
      "end_station_name: string of end station name\n",
      "month: month, integer\n",
      "start_station_id: integer id of start station\n",
      "start_station_name: string of start station name\n",
      "start_time: yyyy-mm-dd hh:mm:ss\n",
      "subscriber_type: membership typ e.g. walk up, annual, other bike share, etc\n",
      "trip_id: unique trip id int\n",
      "year: year of trip, int\n",
      "*austin_bikeshare_stations.csv*\n",
      "latitude: geospatial latitude, precision to 5 places\n",
      "location: (lat, long)\n",
      "longitude: geospatial longitude, precision to 5 places\n",
      "name: station name, str\n",
      "station_id: unique station id, int\n",
      "status: station status (active, closed, moved, acl-only)\n",
      "acknowledgements:\n",
      "this dataset is available from google public data.\n",
      "inspiration:\n",
      "what stations are most popular? at certain times?\n",
      "what are the average user trip?\n",
      "can you predict station usage to improve the ability of bike share employees to supply high-use stations?\n",
      "context:\n",
      "occupational safety and health administration aka osha requires employers to report all severe work-related injuries, defined as an amputation, in-patient hospitalization, or loss of an eye. the requirement began on january 1, 2015.\n",
      "content:\n",
      "this dataset provides information from those reports, including a description of the incident and the name and address of the establishment where it happened. injuries are coded using the occupational injury and illness classification system. data covers ~22k incidents jan 1 2015-feb 28 2017. 26 columns describe incident, parties involved, employer, injury sustained, and final outcome.\n",
      "acknowledgements:\n",
      "this dataset was created by [osha](https://www.osha.gov/severeinjury/index.html ) and released to the public.\n",
      "inspiration:\n",
      "which industries have the highest rate of worker injuries? most severe injuries?\n",
      "can you predict injuries for 2016 based on 2015 data?\n",
      "in which regions are injuries most common?\n",
      "telecom customer churn prediction\n",
      "this data set consists of 100 variables and approx 100 thousand records. this data set contains different variables explaining the attributes of telecom industry and various factors considered important while dealing with customers of telecom industry. the target variable here is churn which explains whether the customer will churn or not. we can use this data set to predict the customers who would churn or who wouldn't churn depending on various variables available.\n",
      "context:\n",
      "restaurant inspections ensure that food served to the public at licensed food establishments follows food safety guidelines. the food protection division of the chicago department of public health (cdph) is committed to maintaining the safety of food bought, sold, or prepared for public consumption in chicago by carrying out science-based inspections of all retail food establishments. these inspections promote public health in areas of food safety and sanitation and prevent the occurrence of food-borne illness. cdph's licensed, accredited sanitarians inspect retail food establishments such as restaurants, grocery stores, bakeries, convenience stores, hospitals, nursing homes, day care facilities, shelters, schools, and temporary food service events. inspections focus on food handling practices, product temperatures, personal hygiene, facility maintenance, and pest control. all restaurants are subject to certain recurring inspections. each year a restaurant is subject to annual inspections to ensure continued compliance with city ordinances and regulations. in addition to recurring inspections, restaurants may also be inspected in response to a complaint. some of these recurring inspections, such as the inspection by the buildings department, will be scheduled, while others will not.\n",
      "generally inspections are conducted by the health department for sanitation and safe food handling practices, the buildings department to ensure the safety of the structure, and the fire department to ensure safe fire exits.the city's dumpster task force, a collaborative effort between the health department and streets and sanitation department, also inspects restaurants to ensure compliance with sanitation regulations.\n",
      "content:\n",
      "data includes inspection date, results, violations noted, business name and lat/lon, license# and risk. data covers 01/02/2013-08/28/17.\n",
      "acknowledgements:\n",
      "data was collected by city of chicago department of health.\n",
      "inspiration:\n",
      "can you predict restaurant closings?\n",
      "do restaurants in certain neighborhoods gather more/less violations?\n",
      "any seasonal or time anomalies in the data?\n",
      "context\n",
      "this dataset contains the name, job title, department, and salary of every employee that was on the city of chicago payroll at the time of capture in mid-2017. it provides a transparent lens into who gets paid how much and for what.\n",
      "content\n",
      "this dataset provides columns for employee name, the city department they work for, their job title, and various fields describing their compensation. most employee salaries are covered by the annual salary field, but some employees paid hourly are covered by a combination of typical hours and hourly rate fields.\n",
      "acknowledgements\n",
      "this dataset is published as-is by the city of chicago (here).\n",
      "inspiration\n",
      "how many people do the various city agencies employ, and how much does each department spend on salary in total?\n",
      "what are the most numerous job titles in civic government employment?\n",
      "how do chicago employee salaries compare against salaries of city employees in new york city? is the difference more or less than the difference in cost of living between the two cities?\n",
      "about this data\n",
      "this is a list of over 1,500 consumer reviews for amazon products like the kindle, fire tv stick, and more provided by datafiniti's product database. the dataset includes basic product information, rating, review text, and more for each product.\n",
      "what you can do with this data\n",
      "you can use this data to analyze amazon’s most successful consumer electronics product launches; discover insights into consumer reviews and assist with machine learning models. e.g.:\n",
      "what are the most reviewed amazon products?\n",
      "what are the initial and current number of customer reviews for each product?\n",
      "how do the reviews in the first 90 days after a product launch compare to the price of the product?\n",
      "how do the reviews in the first 90 days after a product launch compare to the days available for sale?\n",
      "map the keywords in the review text against the review ratings to help train sentiment models.\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "this is a database of airports, train stations, and ferry terminals around the world. some of the data come from public sources and some of it comes from openflights.org user contributions.\n",
      "content\n",
      "airport id unique openflights identifier for this airport.\n",
      "name name of airport. may or may not contain the city name.\n",
      "city main city served by airport. may be spelled differently from name.\n",
      "country country or territory where airport is located. see countries.dat to cross-reference to iso 3166-1 codes.\n",
      "iata 3-letter iata code. null if not assigned/unknown.\n",
      "icao 4-letter icao code.\n",
      "null if not assigned.\n",
      "latitude decimal degrees, usually to six significant digits. negative is south, positive is north.\n",
      "longitude decimal degrees, usually to six significant digits. negative is west, positive is east.\n",
      "altitude in feet.\n",
      "timezone hours offset from utc. fractional hours are expressed as decimals, eg. india is 5.5.\n",
      "dst daylight savings time. one of e (europe), a (us/canada), s (south america), o (australia), z (new zealand), n (none) or u (unknown). see also: help: time\n",
      "tz database time zone timezone in \"tz\" (olson) format, eg. \"america/los_angeles\".\n",
      "type type of the airport. value \"airport\" for air terminals, \"station\" for train stations, \"port\" for ferry terminals and \"unknown\" if not known.\n",
      "source source of this data. \"ourairports\" for data sourced from ourairports, \"legacy\" for old data not matched to ourairports (mostly dafif), \"user\" for unverified user contributions. in airports.csv, only source=ourairports is included.\n",
      "acknowledgements\n",
      "this dataset was downloaded from openflights.org under the open database license. this is an excellent resource and there is a lot more on their website, so check them out!\n",
      "context:\n",
      "a record of every public wifi hotspot in new york city.\n",
      "content:\n",
      "the dataset consists of records for every public wifi hotspot (ones provided by or in partnership with the city) in new york city. it contains over 2500 records overall, and is current as of august 11, 2017.\n",
      "acknowledgements:\n",
      "this dataset was published as-is by the new york city department of information technology & telecommunications.\n",
      "inspiration:\n",
      "does free public wifi tend to cluster around certain (more affluent) areas? who are the free wifi providers, and where do they do it? how does nyc wifi compare to wifi in other cities, like buenos aires? (https://www.kaggle.com/octaviog/buenos-aires-public-wifi-access-points)\n",
      "deepsat sat-6\n",
      "\n",
      "\n",
      "originally, images were extracted from the national agriculture imagery program (naip) dataset. the naip dataset consists of a total of 330,000 scenes spanning the whole of the continental united states (conus). the authors used the uncompressed digital ortho quarter quad tiles (doqqs) which are geotiff images and the area corresponds to the united states geological survey (usgs) topographic quadrangles. the average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. the entire naip dataset for conus is ~65 terabytes. the imagery is acquired at a 1-m ground sample distance (gsd) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points.\n",
      "the images consist of 4 bands - red, green, blue and near infrared (nir). in order to maintain the high variance inherent in the entire naip dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of california. an image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.\n",
      "once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. we chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. care was taken to avoid interclass overlaps within a selected and labeled image patch.\n",
      "content\n",
      "each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared.\n",
      "the training and test labels are one-hot encoded 1x6 vectors\n",
      "the six classes represent the six broad land covers which include barren land, trees, grassland, roads, buildings and water bodies.\n",
      "training and test datasets belong to disjoint set of image tiles.\n",
      "each image patch is size normalized to 28x28 pixels.\n",
      "once generated, both the training and testing datasets were randomized using a pseudo-random number generator.\n",
      "csv files\n",
      "x_train_sat6.csv: 324,000 training images, 28x28 images each with 4 channels\n",
      "y_train_sat6.csv: 324,000 training labels, 1x6 one-hot encoded vectors\n",
      "x_test_sat6.csv: 81,000 training images, 28x28 images each with 4 channels\n",
      "y_test_sat6.csv: 81,000 training labels, 1x6 one-hot encoded vectors\n",
      "the original mat file\n",
      "train_x: 28x28x6x324000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)\n",
      "train_y: 324,000x6 uint8 (containing 6x1 vectors having labels for the 400000 training samples)\n",
      "test_x: 28x28x6x18000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)\n",
      "test_y: 81,000x6 uint8 (containing 6x1 vectors having labels for the 100000 test samples)\n",
      "acknowledgements\n",
      "the original matlab file was converted to multiple csv files\n",
      "the original sat-4 and sat-6 airborne datasets can be found here:\n",
      "http://csc.lsu.edu/~saikat/deepsat/\n",
      "thanks to:\n",
      "saikat basu, robert dibiano, manohar karki and supratik mukhopadhyay, louisiana state university sangram ganguly, bay area environmental research institute/nasa ames research center ramakrishna r. nemani, nasa advanced supercomputing division, nasa ames research center\n",
      "english word vectors from common crawl\n",
      "about fasttext\n",
      "fasttext is a library for efficient learning of word representations and sentence classification. one of the key features of fasttext word representation is its ability to produce vectors for any words, even made-up ones. indeed, fasttext word vectors are built from vectors of substrings of characters contained in it. this allows you to build vectors even for misspelled words or concatenation of words.\n",
      "about the vectors\n",
      "these pre-trained vectors contain 2 million word vectors trained on common crawl (600b tokens).\n",
      "the first line of the file contains the number of words in the vocabulary and the size of the vectors. each line contains a word followed by its vectors, like in the default fasttext text format. each value is space separated. words are ordered by descending frequency.\n",
      "acknowledgements\n",
      "these word vectors are distributed under the creative commons attribution-share-alike license 3.0.\n",
      "p. bojanowski*, e. grave*, a. joulin, t. mikolov, enriching word vectors with subword information\n",
      "a. joulin, e. grave, p. bojanowski, t. mikolov, bag of tricks for efficient text classification\n",
      "a. joulin, e. grave, p. bojanowski, m. douze, h. jégou, t. mikolov, fasttext.zip: compressing text classification models\n",
      "\n",
      "(* these authors contributed equally.)\n",
      "vgg16\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "resnet-50\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "the convention on international trade in endangered species of wild fauna and flora, or cites for short, is an international treaty organization tasked with monitoring, reporting, and providing recommendations on the international species trade. cites is a division of the iucn, which is one of the principal international organization focused on wildlife conversation at large. it is not a part of the un (though its reports are read closely by the un).\n",
      "cites is one of the oldest conservation organizations in existence. participation in cites is voluntary, but almost every member nation in the un (and, therefore, almost every country worldwide) participates. countries participating in cites are obligated to report on roughly 5000 animal species and 29000 plant species brought into or exported out of their countries, and to honor limitations placed on the international trade of these species.\n",
      "protected species are organized into three appendixes. appendix i species are those whose trade threatens them with extinction. two particularly famous examples of class i species are the black rhinoceros and the african elephant, whose extremely valuable tusks are an alluring target for poachers exporting ivory abroad. there are 1200 such species. appendix ii species are those not threatened with extinction, but whose trade is nevertheless detrimental. most species in cites, around 21000 of them, are in appendix ii. finally, appendix iii animals are those submitted to cites by member states as a control mechanism. there are about 170 such species, and their export or import requires permits from the submitting member state(s).\n",
      "this dataset records all legal species imports and exports carried out in 2016 (and a few records from 2017) and reported to cites. species not on the cites lists are not included; nor is the significant, and highly illegal, ongoing black market trading activity.\n",
      "content\n",
      "this dataset contains records on every international import or export conducted with species from the cites lists in 2016. it contains columns identifying the species, the import and export countries, and the amount and characteristics of the goods being traded (which range from live animals to skins and cadavers).\n",
      "for further details on individual rows and columns refer to the metadata on the /data tab. a much more detailed description of each of the fields is available in the original cites documentation.\n",
      "acknowledgements\n",
      "this dataset was originally aggregated by cites and made available online through this downloader tool. the cites downloader goes back to 1975, however it is only possible to download fully international data two years at a time (or so) due to limitations in the number of rows allowed by the data exporter. if you would like data going further back, check out the downloader. be warned, though, this data takes a long time to generate!\n",
      "this data is prepared for cites by unep, a division of the un, and hence likely covered by the un data license.\n",
      "inspiration\n",
      "what is the geospatial distribution of the international plant/animal trade?\n",
      "how much export/import activity is there for well-known species, like rhinos, elephants, etcetera?\n",
      "what percent of the trade is live, as opposed to animal products (ivory, skins, cadavers, etcetera)?\n",
      "context\n",
      "the database contains wav recordings from the same optical sensor inserted in-turn into six insectary boxes containing only one mosquito species of both sexes. as the mosquitoes fly randomly through the sensor their wingbeat partially occludes the light from the transmitter to the receiver. the light fluctuation recorded follows the rhythm of the wingbeat. insect biometrics, in the context of our work, is a measurable behavioral characteristic of flying insects. biometric identifiers are related to the shape of the body (main body size, wing shape, wingbeat frequency, pattern movement of the wings). biometric identification methods use biometric characteristics or traits to verify species/sex identities when insects access endpoint traps following a bait.\n",
      "content\n",
      "• 279,566 wingbeat recordings correctly labeled\n",
      "• 6 mosquito species (ae. aegypti, ae. albopictus, an. arabiensis, an. gambiae, cu. pipiens, cu. quinquefasciatus)\n",
      "• 3 genera of mosquito species (aedes, anopheles, culex)\n",
      "acknowledgements\n",
      "the data have been recorded at the premises of biogents, regensburg, germany (https://www.biogents.com/) and with the help of irideon sa, spain (http://irideon.eu/ ). the data have been recorded using the device published in:\n",
      "potamitis i. and rigakis i., \"large aperture optoelectronic devices to record and time-stamp insects’ wingbeats,\" in ieee sensors journal, vol. 16, no. 15, pp. 6053-6061, aug.1, 2016. doi: 10.1109/jsen.2016.2574762\n",
      "the remosis project that supported the creation of the database has received funding from the european union’s horizon 2020 research and innovation program under grant agreement no 691131.\n",
      "we gratefully acknowledge the support of nvidia corporation with the donation of a titan-x gpu used for training the deep learning networks used to classify mosquitoes’ spectra.\n",
      "inspiration\n",
      "the point of having such recordings is to eventually embed optoelectronic sensors in automatic traps that will report counts, species and sex identity of captured mosquitoes. all species of this dataset can be dangerous as they are potential vectors of pathogens that cause serious illnesses. a widespread network of traps for insects of economic importance such as fruit flies and of hygienic importance such as mosquitoes allows the automatic creation of spatiotemporal maps and cuts down significantly the manual cost of visiting the traps. the creation of historical data can lead to the prediction of outbreaks and risk assessment in general.\n",
      "we provide code to read the data and extract the power spectral density signature of each wingbeat. we also extract mel-scaled, filter-bank features. how about wavelets and time-varying autoregressive models? the starter code using top-tier shallow classifiers achieves a mean accuracy of 81-84%. deep-learning performs better. can you classify genus, perform clustering, apply transfer learning to spectral data?\n",
      "come aboard and help humanity against killer mosquitoes!\n",
      "context\n",
      "dataset of starcraft 2 games, played in different leagues/levels.\n",
      "content\n",
      "screen movements aggregated into screen-fixations. -- time is recorded in terms of timestamps in the starcraft 2 replay file. when the game is played on 'faster', 1 real-time second is equivalent to roughly 88.5 timestamps.\n",
      "attribute information:\n",
      "gameid: unique id number for each game (integer)\n",
      "leagueindex: bronze, silver, gold, platinum, diamond, master, grandmaster, and professional leagues coded 1-8 (ordinal)\n",
      "age: age of each player (integer)\n",
      "hoursperweek: reported hours spent playing per week (integer)\n",
      "totalhours: reported total hours spent playing (integer)\n",
      "apm: action per minute (continuous)\n",
      "selectbyhotkeys: number of unit or building selections made using hotkeys per timestamp (continuous)\n",
      "assigntohotkeys: number of units or buildings assigned to hotkeys per timestamp (continuous)\n",
      "uniquehotkeys: number of unique hotkeys used per timestamp (continuous)\n",
      "minimapattacks: number of attack actions on minimap per timestamp (continuous)\n",
      "minimaprightclicks: number of right-clicks on minimap per timestamp (continuous)\n",
      "numberofpacs: number of pacs per timestamp (continuous)\n",
      "gapbetweenpacs: mean duration in milliseconds between pacs (continuous)\n",
      "actionlatency: mean latency from the onset of a pacs to their first action in milliseconds (continuous)\n",
      "actionsinpac: mean number of actions within each pac (continuous)\n",
      "totalmapexplored: the number of 24x24 game coordinate grids viewed by the player per timestamp (continuous)\n",
      "workersmade: number of scvs, drones, and probes trained per timestamp (continuous)\n",
      "uniqueunitsmade: unique unites made per timestamp (continuous)\n",
      "complexunitsmade: number of ghosts, infestors, and high templars trained per timestamp (continuous)\n",
      "complexabilitiesused: abilities requiring specific targeting instructions used per timestamp (continuous)\n",
      "acknowledgements\n",
      "source: 1. thompson jj, blair mr, chen l, henrey aj (2013) video game telemetry as a critical tool in the study of complex skill learning. plos one 8(9): e75129. [web link] -- results: -- skip league conditional inference forest classification (bronze-gold;silver-platinum;gold-diamond;platinum-masters;diamond-professional) showed changing patterns of variable importance with skill.\n",
      "http://archive.ics.uci.edu/ml/datasets/skillcraft1+master+table+dataset\n",
      "inspiration\n",
      "ordinal classification / regression model to determine league index (\"leagueindex\")\n",
      "suggest additional features to gather and analyze for predicting leagues/performance.\n",
      "are there features which do not increase/decrease linearly as we go up in the leagues?\n",
      "overwatch is a team-based multiplayer online first-person shooter video game developed and published by blizzard entertainment. it was released in may 2016 for windows, playstation 4, and xbox one. overwatch assigns players into two teams of six, with each player selecting from a roster of over 20 characters, known in-game as \"heroes\", each with a unique style of play, whose roles are divided into four general categories: offense, defense, tank, and support. players on a team work together to secure and defend control points on a map or escort a payload across the map in a limited amount of time.\n",
      "i discovered this dataset on the overwatch subreddit here: https://www.reddit.com/r/overwatch/comments/7o8hmg/my_friend_has_recorded_every_game_hes_played/\n",
      "it represents a ridiculous amount of effort in terms of manually recording game results. this data, whilst in some places incomplete, gives an unprecedented view into the experience of a single overwatch player over the course of years of gameplay. from it you can track the ups and downs, shifts in hero preference and all sorts of other exciting in game trends.\n",
      "thanks to justwingit for their amazing collecting this data.\n",
      "i cleaned the data a little and put it into a single csv.\n",
      "context\n",
      "this dataset focuses on public assistance in the united states with initial coverage of the wic program. the program is formally known as the special supplemental nutrition program for women, infants, and children (wic). the program allocates federal and state funds to help low-income women and children up to age five who are at nutritional risk. funds are used to provide supplemental foods, baby formula, health care, and nutrition education.\n",
      "content\n",
      "files include participation data and spending for state wic programs, and poverty data for each state. data is for fiscal years 2013-2016, which is actually october 2012 through september 2016.\n",
      "motivation\n",
      "my original purpose here is two-fold:\n",
      "explore various aspects of us public assistance. show trends over recent years and better understand differences across state agencies. although the federal government sponsors the program and provides funding, program are administered at the state level and can widely vary. indian nations (native americans) also administer their own programs.\n",
      "share with the kaggle community the joy - and pain - of working with government data. data is often spread across numerous agency sites and comes in a variety of formats. often the data is provided in excel, with the files consisting of multiple tabs. also, files are formatted as reports and contain aggregated data (sums, averages, etc.) along with base data.\n",
      "additional content ideas\n",
      "the dataset can benefit greatly from additional content. economics, additional demographics, administrative costs and more. i'd like to eventually explore the money trail from taxes and corporate subsidies, through the government agencies, and on to program participants. all community ideas are welcome!\n",
      "context\n",
      "freecodecamp is a web-based non-profit organization and learning platform which teaches programming newcomers how to code. it was founded by quincy larson in 2014, who in a 2015 interview stated that \"freecodecamp is my effort to correct the extremely inefficient and circuitous way i learned to code...all those things that made learning to code a nightmare to me are things that we are trying to fix with freecodecamp.\" the original curriculum took approximately 800 hours to complete; today, after several refreshes and additions, there is over 2000 hours of learner's content on the site.\n",
      "freecodecamp also provides several helpful secondary resources for learners. one of them is a freecodecamp gitter chatroom. gitter is an open-source instant messaging service that lets users share thoughts and ideas with one another. this dataset is a record of activity from this /freecodecamp gitter chatroom, containing posts from students, bots, moderators, and contributors between december 31, 2014 and december 9, 2017.\n",
      "content\n",
      "the data includes the usernames, screen names, timestamps, post content, and metadata of every post made to /freecodecamp in the aforementioned three year period. this comes out to nearly 5 million records overall.\n",
      "the data comes in the form of a set of three json files, each named freecodecamp_casual_chatroom_[01/02/03].json. the three files make a continuous dataset containing all posts sent during the aforementioned period, but note that they overlap in a few days. the three files were extracted on 01-06-2016, 09-03-2017, and 12-12-2017, respectively. the included convert.py file was used to concatenate these files into a unified csv file, freecodecamp_casual_chatroom.csv.\n",
      "some preliminary analyses using this dataset can be found at the github repository for the freecodecamp's open data initiative. for more details on specific elements of the dataset refer to the column metadata tab, or to the detailed documentation provided in the gitter api documentation.\n",
      "acknowledgements\n",
      "the datasets are a contribution from freecodecamp as part of the freecodecamp's open data initiative. more information about the rationale of this initiative can be found on this medium article.\n",
      "this dataset was extracted using python code over the gitter api. all the files were prepared by evaristo caraballo (github: @evaristoc).\n",
      "records are not anonymised or modified and are presented \"as they are\".\n",
      "thanks to freecodecamp team, specially to quincy larson for supporting the initiative. thanks to all freecodecamp students who kindly allowed to share their personal progress and to gitter for making these data available.\n",
      "if you have questions about this dataset, please contact quincy@freecodecamp.com or get in touch with us through https://gitter.im/freecodecamp/datascience (gitter registration might be required).\n",
      "inspiration\n",
      "this dataset presents three years of developer chat about web technologies. can you use it to trace the rise and fall over certain technologies, like angular and react, over time?\n",
      "what do programming newcomers tend to get stuck on the most? are there any canned responses that are particularly common in the freecodecamp chatroom?\n",
      "what other interesting insights into programmer culture can you glean from examining this dataset?\n",
      "context\n",
      "this is a protein data set retrieved from rcs pdb.\n",
      "the pdb archive is a repository of atomic coordinates and other information describing proteins and other important biological macromolecules. structural biologists use methods such as x-ray crystallography, nmr spectroscopy, and cryo-electron microscopy to determine the location of each atom relative to each other in the molecule. they then deposit this information, which is then annotated and publicly released into the archive by the wwpdb.\n",
      "the constantly-growing pdb is a reflection of the research that is happening in laboratories across the world. this can make it both exciting and challenging to use the database in research and education. structures are available for many of the proteins and nucleic acids involved in the central processes of life, so you can go to the pdb archive to find structures for ribosomes, oncogenes, drug targets, and even whole viruses. however, it can be a challenge to find the information that you need, since the pdb archives so many different structures. you will often find multiple structures for a given molecule, or partial structures, or structures that have been modified or inactivated from their native form.\n",
      "content\n",
      "there are two data sets. both are arranged on \"structureid\" of the protein\n",
      "pdb_data_no_dups.csv :- protein data set deatils of classification, extraction methods, etc. containing 141401 instances and 14 attributes.\n",
      "data_seq :- protein sequence information. containing 467304 instances and 5 attributes.\n",
      "acknowledgements\n",
      "original data set down loaded from http://www.rcsb.org/pdb/\n",
      "inspiration\n",
      "protein data base helped the life science community to study about different diseases and come with new drugs and solution that help the human survival.\n",
      "context\n",
      "it is not always easy to find databases from real world manufacturing plants, specially mining plants. so, i would like to share this database with the community, which comes from one of the most important parts of a mining process: a flotation plant!\n",
      "the main goal is to use this data to predict how much impurity is in the ore concentrate. as this impurity is measured every hour, if we can predict how much silica (impurity) is in the ore concentrate, we can help the engineers, giving them early information to take actions (empowering!). hence, they will be able to take corrective actions in advance (reduce impurity, if it is the case) and also help the environment (reducing the amount of ore that goes to tailings as you reduce silica in the ore concentrate).\n",
      "content\n",
      "the first column shows time and date range (from march of 2017 until september of 2017). some columns were sampled every 20 second. others were sampled on a hourly base.\n",
      "the second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. from column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. the last two columns are the final iron ore pulp quality measurement from the lab. target is to predict the last column, which is the % of silica in the iron ore concentrate.\n",
      "inspiration\n",
      "i have been working in this dataset for at least six months and would like to see if the community can help to answer the following questions:\n",
      "is it possible to predict % silica concentrate every minute?\n",
      "how many steps (hours) ahead can we predict % silica in concentrate? this would help engineers to act in predictive and optimized way, mitigatin the % of iron that could have gone to tailings.\n",
      "is it possible to predict % silica in concentrate whitout using % iron concentrate column (as they are highly correlated)?\n",
      "history\n",
      "i have made the database of photos sorted by products and brands. screenshots were performed only on official brand websites.\n",
      "content\n",
      "the main dataset (style.zip) is 894 color images (150x150x3) with 7 brands and 10 products, and the file with labels style.csv. photo files are in the .png format and the labels are integers and values.\n",
      "the file stylecolorimages.h5 consists of preprocessing images of this set: image tensors and targets (labels).\n",
      "acknowledgements\n",
      "i have published the data for absolutely free using by any site visitor. but this database contains the names of famous brands, so it can not be used for commercial purposes.\n",
      "usage\n",
      "classification, image recognition and colorizing, etc. in a case of a small number of images are useful exercises. the main question we can try to answer with the help of the data is whether the algorithms can recognize the unique design style well enough. to facilitate the task, i chose the most easily recognizable brands with a bright style.\n",
      "the example of usage\n",
      "improvement\n",
      "there are lots of ways for improving this set and the machine learning algorithms applying to it. at first, it needs to increase the number of photos.\n",
      "context\n",
      "kernels from porto seguro. some are more overfit than others.\n",
      "content\n",
      "copied by hand from kaggle. (may contain errors, obviously.) each kernel is identified by a link to the most recent version that has a verified score matching what is in the kernels tab. (notebooks are linked to the code tab, since that's where the scores are.) in many cases this is the most recent version, so if the kernel is subsequently revised, the link may be wrong (since it will then point to the subsequent most recent version, which may not be scored, or its score may not match the kernels tab). most of the fields should be self-explanatory, except \"adjusted\", which adjusts the public-private difference by subtracting the median difference over the whole leaderboard. (private scores are typically higher, presumably because the cases in private portion of the test data were easier to predict, so an \"adjusted\" value of zero represents a kernel that wouldn't have gained or lost much in the shake-up.)\n",
      "acknowledgements\n",
      "i would like to thank kaggle, porto seguro, and the authors of the kernels.\n",
      "inspiration\n",
      "what factors might help predict how much better or worse a kernel will do on the private leaderboard than on the public? it's one competition: just a start, but you've got to start somewhere.\n",
      "context\n",
      "i created this dataset to enable everyone to explore local businesses of pakistan. this dataset might help the local community in gathering information of local businesses. this might also help in local economic development of pakistan by bridging traders and manufacturers.\n",
      "content\n",
      "geography: pakistan\n",
      "time period: 1990-2017\n",
      "dataset: the dataset contains information of approx 67000 businesses in pakistan (~5000 in each csv file)\n",
      "features: the dataset has total 7 columns - business name - contact name - telephone - website - services (description of types of products/services provided by the business) - address - city\n",
      "acknowledgements\n",
      "this dataset was created by scraping this website. i wrote the script in python using beautifulsoup library. link to script: https://tinyurl.com/ybb4bdky\n",
      "inspiration\n",
      "a lot of questions can be answered and analysis can be done using this dataset. few interesting ideas i can think of are : - applying nlp techniques on services column to extract business category - clustering of categories of business according to cities\n",
      "context: well, we are a head of the largest football event(soccer, sorry for the american fellows) worldwide, as excited as we were. well, at least some of us were, by the draw, here comes the data. history of all previous matches, scores and titles of all participating teams to help us predict who will qualify and may even predict who will win. so, lets get our algorithms going.\n",
      "content: the dataset contains 32 entries(of the 32 participating teams of course), each team will have 3 matches in the group stage, so each match is mentioned vs whom, the history between those 2 teams with wins minus losses, let's say brazil has beaten argentina 14 times, argentina won 12 and there were 3 draws, so that will be +2 for brazil and -2 for argentina, the same goes for goals scored. you will find some entries with n/a and some with zeros, so what is the difference? the zeros mean that the 2 teams evened out, while n/as means that they have no previous history together and they have never faced each other. so, no data available. the matches history is up to 2012-2014. so, there is a couple of years missing here, and the fifa rank is up to date, which will be updated every month till we get there.\n",
      "acknowledgments: most of the data is pulled from fifa website except for the matches and scores history, they were pulled manually from various credible sources.\n",
      "inspiration: well, i think this data can help us predict who will head the groups and who comes second, then may be we can progress through round of 16....final. you can ask for more data and i'll be happy to search and update it.\n",
      "if you like the data or find it useful enough, don't forget the upvote ;)\n",
      "context\n",
      "spacex designs, manufactures and launches advanced rockets and spacecraft. the company was founded in 2002 to revolutionize space technology, with the ultimate goal of enabling people to live on other planets - spacex\n",
      "content\n",
      "the dataset contains mission information for rocket launches conducted by spacex (space exploration technologies corp).\n",
      "acknowledgements\n",
      "data was obtained via wikipedia's entry for falcon 9 and falcon heavy launches.\n",
      "inspiration\n",
      "do you anticipate an increase in launches with the introduction of the falcon heavy? how has launch rate increased over time? do you predict a shift in payload orbits for upcoming launches? how has the customer diversity changed over the years?\n",
      "context\n",
      "shanghai uses an auction system to sell a limited number of license plates to fossil-fuel car buyers every month. the average price of this license plate is about $13,000 and it is often referred to as \"the most expensive piece of metal in the world.\" so, our goal is to predict the avg price or the lowest price for the next month. this data set will be updated every month constantly. have fun!\n",
      "content\n",
      "this data set is gathered by myself with the aid of search engine.\n",
      "inspiration\n",
      "this data set could be used in your first toy example project. learning algorithms like rnn+lstm are well fitted into this time-series prediction problem. so, just have fun!\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com, a well known retailer.\n",
      "content\n",
      "this dataset has following fields:\n",
      "sku\n",
      "name_title\n",
      "description\n",
      "list_price\n",
      "sale_price\n",
      "category\n",
      "category_tree\n",
      "average_product_rating\n",
      "product_url\n",
      "product_image_urls\n",
      "brand\n",
      "total_number_reviews\n",
      "reviews\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of list price, sale price, rating and reviews can be performed.\n",
      "background\n",
      "nlp is a hot topic currently! team ai really want's to leverage the nlp research and this an attempt for all the nlp researchers to explore exciting insights from bilingual data\n",
      "the japanese-english bilingual corpus of wikipedia's kyoto articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation, information extraction, and other language processing technologies.\n",
      "unique features\n",
      "a precise and large-scale corpus containing about 500,000 pairs of manually-translated sentences. can be exploited for research and development of high-performance multilingual machine translation, information extraction, and so on.\n",
      "the three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation.\n",
      "translated articles concern kyoto and other topics such as traditional japanese culture, religion, and history. can also be utilized for tourist information translation or to create glossaries for travel guides.\n",
      "the japanese-english bilingual kyoto lexicon is also available. this lexicon was created by extracting the japanese-english word pairs from this corpus.\n",
      "sample\n",
      "one wikipedia article is stored as one xml file in this corpus, and the corpus contains 14,111 files in total.\n",
      "the following is a short quotation from a corpus file titled “ryoan-ji temple”. each tag has different implications. for example:\n",
      "<j>original japanese sentence<j> <e type=\"trans\" ver=\"1\">primary translation</e> <e type=\"trans\" ver=\"2\">secondary translation</e> <e type=\"check\" ver=\"1\">final translation</e> <cmt>comment added by translators</cmt>\n",
      "categories\n",
      "the files have been divided into 15 categories: school, railway, family, building, shinto, person name, geographical name, culture, road, buddhism, literature, title, history, shrines and temples, and emperor (click the link to view a sample file for each category).\n",
      "github\n",
      "https://github.com/venali/bilingualcorpus\n",
      "explains how to load the corpus\n",
      "acknowledgements\n",
      "the national institute of information and communications technology (nict) has created this corpus by manually translating japanese wikipedia articles (related to kyoto) into english.\n",
      "licence\n",
      "use and/or redistribution of the japanese-english bilingual corpus of wikipedia's kyoto articles and the japanese-english bilingual kyoto lexicon is permitted under the conditions of creative commons attribution-share-alike license 3.0. details can be found at http://creativecommons.org/licenses/by-sa/3.0/.\n",
      "link to web\n",
      "https://alaginrc.nict.go.jp/wikicorpus/index_e.html\n",
      "context\n",
      "this is data on new york city taxi cab trips. it should be useful for adding a lot more training data for the \"new york city taxi trip duration\". the data i am uploading is for 2014. the training data for the competition is for 2016. hopefully, the underlying data should be very similar over the two years. compressed, all of the data is over 5 gb, which is more than 10x the allowed data size of kaggle datasets. this represents a subset of the total data.\n",
      "content\n",
      "the data is in a csv format with the following fields. it was collected both from driver input and from the gps coordinates of the cab. it is downloaded from new york city's open data website.\n",
      "vendor_id pickup_datetime dropoff_datetime passenger_count trip_distance pickup_longitude pickup_latitude store_and_fwd_flag dropoff_longitude dropoff_latitude payment_type fare_amount mta_tax tip_amount tolls_amount total_amount imp_surcharge extra rate_code\n",
      "in the original dataset published by nyc, there is over 165 million trips. this is only 15 million. it was selected as the first 15 million available records of the year.\n",
      "acknowledgements\n",
      "this data comes from the city of new york, who have been leaders in making data available to the public. this comes from their nyc open data website (https://opendata.cityofnewyork.us/)\n",
      "inspiration\n",
      "the inspiration is the taxi competition: https://www.kaggle.com/c/nyc-taxi-trip-duration\n",
      "context\n",
      "forex is the largest market in the world, predicting the movement of prices is not a simple task, this dataset pretends to be the gateway for people who want to conduct trading using machine learning.\n",
      "content\n",
      "this dataset contains 4479 simulated winning transactions (real data, fictitious money) (3 years 201408-201708) with buy transactions (2771 operations 50.7%) and sell (2208 transactions, 49.3%), to generate this data a script of metatrader was used, operations were performed in time frame 4hour and fixed stop loss and take profits of 50 pips (4 digits) were used to determine if the operation is winning. each operation contains a set of classic technical indicators like rsi, mom, bb, emas, etc. (last 24 hours)\n",
      "acknowledgements\n",
      "thanks to kaggle for giving me the opportunity to share my passion for machine learning. my profile: https://www.linkedin.com/in/rsx2010/\n",
      "inspiration\n",
      "the problem of predicting price movement is reduced with this dataset to a classification problem:\n",
      "\"use the variables rsi1 to dayofweek to predict the type of correct operation to be performed (field=tipo)\"\n",
      "tipo = 0 ==> operation buy\n",
      "tipo= 1 ==> operation = sell:\n",
      "good luck\n",
      "rodrigo salas vallet-cendre.\n",
      "rasvc@hotmail.com\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context:\n",
      "*annual health survey : mortality schedule *\n",
      "this unit level dataset contains the details relating to death occurred to usual residents of sample household during the reference period and it includes information on sex of deceased, date of death, age at death, registration of death and source of medical attention received before death. for infant deaths, data related to symptoms preceding death is also provided. mortality schedule also includes information on various determinants of maternal mortality viz. case of deaths associated with pregnancy, information on factors leading/ contributing to death, symptoms preceding death, time between onset of complications and death, etc.\n",
      "there are total of 770k observations and 121 variables in this dataset.\n",
      "survey:\n",
      "base line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)\n",
      "the survey was conducted in the below 9 states.\n",
      "a. empowered action group (eag) states\n",
      "uttarakhand (05)\n",
      "rajasthan (08)\n",
      "uttar pradesh (09)\n",
      "bihar (10)\n",
      "jharkhand (20)\n",
      "odisha (21)\n",
      "chhattisgarh (22)\n",
      "madhya pradesh (23)\n",
      "b. assam. (18)\n",
      "these nine states, which account for about 48 percent of the total population, 59 percent of births, 70 percent of infant deaths, 75 percent of under 5 deaths and 62 percent of maternal deaths in the country, are the high focus states in view of their relatively higher fertility and mortality.\n",
      "content:\n",
      "the files contains the below columns.\n",
      "variable names:\n",
      "id\n",
      "m_id\n",
      "client_m_id\n",
      "hl_id\n",
      "house_no\n",
      "house_hold_no\n",
      "state\n",
      "district\n",
      "rural\n",
      "stratum_code\n",
      "psu_id\n",
      "m_serial_no\n",
      "deceased_sex\n",
      "date_of_death\n",
      "month_of_death\n",
      "year_of_death\n",
      "age_of_death_below_one_month\n",
      "age_of_death_below_eleven_month\n",
      "age_of_death_above_one_year\n",
      "treatment_source\n",
      "place_of_death\n",
      "is_death_reg\n",
      "is_death_certificate_received\n",
      "serial_num_of_infant_mother\n",
      "order_of_birth\n",
      "death_symptoms\n",
      "is_death_associated_with_pregnan\n",
      "death_period\n",
      "months_of_pregnancy\n",
      "factors_contributing_death\n",
      "factors_contributing_death_2\n",
      "symptoms_of_death\n",
      "time_between_onset_of_complicati\n",
      "nearest_medical_facility\n",
      "m_expall_status\n",
      "field38\n",
      "hh_id\n",
      "client_hh_id\n",
      "currently_dead_or_out_migrated\n",
      "hh_serial_no\n",
      "sex\n",
      "usual_residance\n",
      "relation_to_head\n",
      "member_identity\n",
      "father_serial_no\n",
      "mother_serial_no\n",
      "date_of_birth\n",
      "month_of_birth\n",
      "year_of_birth\n",
      "age\n",
      "religion\n",
      "social_group_code\n",
      "marital_status\n",
      "date_of_marriage\n",
      "month_of_marriage\n",
      "year_of_marriage\n",
      "currently_attending_school\n",
      "reason_for_not_attending_school\n",
      "highest_qualification\n",
      "occupation_status\n",
      "disability_status\n",
      "injury_treatment_type\n",
      "illness_type\n",
      "symptoms_pertaining_illness\n",
      "sought_medical_care\n",
      "diagnosed_for\n",
      "diagnosis_source\n",
      "regular_treatment\n",
      "regular_treatment_source\n",
      "chew\n",
      "smoke\n",
      "alcohol\n",
      "status\n",
      "hh_expall_status\n",
      "client_hl_id\n",
      "serial_no\n",
      "building_no\n",
      "house_status\n",
      "house_structure\n",
      "owner_status\n",
      "drinking_water_source\n",
      "is_water_filter\n",
      "water_filteration\n",
      "toilet_used\n",
      "is_toilet_shared\n",
      "household_have_electricity\n",
      "lighting_source\n",
      "cooking_fuel\n",
      "no_of_dwelling_rooms\n",
      "kitchen_availability\n",
      "is_radio\n",
      "is_television\n",
      "is_computer\n",
      "is_telephone\n",
      "is_washing_machine\n",
      "is_refrigerator\n",
      "is_sewing_machine\n",
      "is_bicycle\n",
      "is_scooter\n",
      "is_car\n",
      "is_tractor\n",
      "is_water_pump\n",
      "cart\n",
      "land_possessed\n",
      "hl_expall_status\n",
      "fid\n",
      "isdeadmigrated\n",
      "residancial_status\n",
      "iscoveredbyhealthscheme\n",
      "healthscheme_1\n",
      "healthscheme_2\n",
      "housestatus\n",
      "householdstatus\n",
      "isheadchanged\n",
      "fidh\n",
      "fidx\n",
      "as\n",
      "wt\n",
      "x\n",
      "schedule_id\n",
      "year\n",
      "file content: mortality_data_dictionary.xlsx : this data dictionary excel work book has the detailed information about each and every column and codes used in the data.\n",
      "acknowledgements\n",
      "department of health and family welfare, govt. of india has published this dataset in open govt data platform india portal under govt. open data license - india.\n",
      "the below information is from the project page: https://nlp.stanford.edu/projects/glove/\n",
      "context\n",
      "glove is an unsupervised learning algorithm for obtaining vector representations for words. training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
      "content\n",
      "due to size constraints, only the 25 dimension version is uploaded. please visit the project page for glove of other dimensions.\n",
      "this dataset (https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) contains glove extracted from wikipedia 2014 + gigaword 5.\n",
      "nearest neighbors the euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary.\n",
      "linear substructures the similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. this simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. for example, man may be regarded as similar to woman in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.\n",
      "in order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. a natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. glove is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words.\n",
      "acknowledgements\n",
      "jeffrey pennington, richard socher, and christopher d. manning. 2014.\n",
      "inspiration\n",
      "the dataset specifically includes tokens extracted from twitter, which unlike tokens from wikipedia, include many abbreviations that have interesting content.\n",
      "context\n",
      "this dataset comes from \"the exempt organization business master file extract\" (eo bmf) which includes cumulative information on tax-exempt organizations.\n",
      "content\n",
      "data is current up to: 8/14/2017\n",
      "ein: employer identification number (ein)\n",
      "name: primary name of organization\n",
      "ico: in care of name\n",
      "street: street address\n",
      "city: city\n",
      "state: state\n",
      "zip: zip code\n",
      "group: group exemption number\n",
      "subsection: subsection code\n",
      "affiliation: affiliation code\n",
      "classification: classification code(s)\n",
      "ruling: ruling date\n",
      "deductibility: deductibility code\n",
      "foundation: foundation code\n",
      "activity: activity codes\n",
      "organization: organization code\n",
      "status: exempt organization status code\n",
      "tax_period: tax period\n",
      "asset_cd: asset code\n",
      "income_cd: income code\n",
      "filing_req_cd: filing requirement code\n",
      "pf_filing_req_cd: pf filing requirement code\n",
      "acct_pd: accounting period\n",
      "asset_amt: asset amount\n",
      "income_amt: income amount (includes negative sign if amount is negative)\n",
      "revenue_amt: form 990 revenue amount (includes negative sign if amount is negative)\n",
      "ntee_cd: national taxonomy of exempt entities (ntee) code\n",
      "sort_name: sort name (secondary name line)\n",
      "there are six data files separated by regions\n",
      "eo1:\n",
      "ct\n",
      "ma\n",
      "me\n",
      "nh\n",
      "nj\n",
      "ny\n",
      "ri\n",
      "vt\n",
      "eo2:\n",
      "dc\n",
      "de\n",
      "ia\n",
      "il\n",
      "in\n",
      "ky\n",
      "md\n",
      "mi\n",
      "mn\n",
      "nc\n",
      "nd\n",
      "ne\n",
      "oh\n",
      "pa\n",
      "sc\n",
      "sd\n",
      "va\n",
      "wi\n",
      "wv\n",
      "eo3:\n",
      "ak\n",
      "al\n",
      "ar\n",
      "az\n",
      "ca\n",
      "co\n",
      "fl\n",
      "ga\n",
      "hi\n",
      "id\n",
      "ks\n",
      "la\n",
      "mo\n",
      "ms\n",
      "mt\n",
      "nm\n",
      "nv\n",
      "ok\n",
      "or\n",
      "tn\n",
      "tx\n",
      "ut\n",
      "wa\n",
      "wy\n",
      "eo4:\n",
      "aa\n",
      "ae\n",
      "ap\n",
      "as\n",
      "fm\n",
      "gu\n",
      "mh\n",
      "mp\n",
      "pr\n",
      "pw\n",
      "vi\n",
      "eo_pr:\n",
      "puerto rico\n",
      "eo_xx:\n",
      "various international non-profits (too many countries to list). see columns 5 and 6.\n",
      "acknowledgements\n",
      "more information and updated data an be found here\n",
      "context\n",
      "this data contains 'real-time' traffic information from locations where nycdot picks up sensor feeds within the five boroughs of nyc, mostly on major arterials and highways. nycdot uses this information for emergency response and management, see acknowledgements.\n",
      "content\n",
      "nyc real time traffic speed data feed for the year 2016, separated in monthly files of 5 minutes intervals of 'real-time' traffic information within the five boroughs of nyc. each row represents a given street section (link), for which the average speed, travel time, timestamp and an id of the street section (link) is given. for each link id, information about this link is given in the linkinfo.csv file, e.g., geo coordinates.\n",
      "acknowledgements\n",
      "http://data.beta.nyc/dataset/nyc-real-time-traffic-speed-data-feed-archived https://data.cityofnewyork.us/transportation/real-time-traffic-speed-data/xsat-x5sa\n",
      "context\n",
      "presenting a compendium of crowdsourced journalism from the psuedo-news site the examiner.\n",
      "this dataset contains the headlines of 3 million articles written by 21000+ authors over 6 years.\n",
      "while the examiner was never praised for its quality, it consistently churned out 1000s of articles per day over several years.\n",
      "at their height in 2011, the examiner was ranked highly in google search and had enormous shares on social media. at one point it was the 10th largest site on mobile and was attracting 20 m unique visitors a month.\n",
      "as a platform driven towards advert revenue, most of their content was rushed, unsourced and factually sparse. it still manages to capture in great detail, the trending topics over a long period of time.\n",
      "prepared by rohit kulkarni\n",
      "content\n",
      "format: csv rows: 3,089,781\n",
      "column 1: publish_date date when the article was published on the site in yyyymmdd format\n",
      "column 2: headline_text text of the headline in english with rare utf8 chars (<1k)\n",
      "start date: 2010-01-01 end date: 2015-21-31\n",
      "another copy of the file with headlines tokenised to lowercase ascii only is included.\n",
      "acknowledgements\n",
      "created using jsoup, java and bash.\n",
      "similar news datasets exploring other attributes, countries and topics can be seen on my profile.\n",
      "this dataset is free to use with citation:\n",
      "rohit kulkarni (2017), the examiner - spam clickbait news 2010-2015 [csv data files], doi:10.7910/dvn/i4hkoo, retrieved from [this url]\n",
      "inspiration\n",
      "the examiner had emerged as an early winner in the digital content landscape of the 2000's using catchy headlines.\n",
      "it changed many roles over the years, from leftist citizen news to a multiuser blogging platform to a content farm.\n",
      "with falling views its operations were absorbed by axs in 2014 and the website was finally shut down in june 2016.\n",
      "the original site and content no longer exists: http://www.examiner.com\n",
      "this is the last surviving record of its existence.\n",
      "this dataset contains a collection of interaction sequences between allies in online diplomacy [1] games. a sequence consists of consecutive game seasons during which the two players exchange messages and help each other in the game. half of the sequences end with betrayal, while the other half are part of lasting friendships.\n",
      "description\n",
      "diplomacy [1] is a popular and engaging strategic board game that is often played online [2, 3]. it is based heavily on communication between the players. due to its military domination setting, diplomacy is a well suited environment for studying naturally occurring betrayal and deception.\n",
      "from a collection of diplomacy game logs, we identified and extracted ongoing, established, and reciprocal friendships: relationships that contain at least two consecutive and reciprocated acts of support that span at least three seasons in game time, with no more than five seasons passing between two acts of friendship.\n",
      "we then identified 250 betrayals: the subset of friendships described above that are followed by at least two attacks. to match each betrayal, we selected a friendship that is not followed by any offensive action, but is otherwise nearly identical (in terms of length and relative time within the game). the current dataset consists of these selected betrayals and friendships only.\n",
      "each relationship contains a sequence of seasons. within each season, we provide features extracted from the messages sent by each player.\n",
      "acknowledgements:\n",
      "this dataset is distributed under the open data commons attribution (odc-by 1.0) license. it was collected by vlad niculae, srijan kumar, jordan boyd-graber and cristian danescu-niculescu-mizil.\n",
      "if you use this dataset, please cite this paper:\n",
      "niculae, v., kumar, s., boyd-graber, j., & danescu-niculescu-mizil, c. (2015). linguistic harbingers of betrayal: a case study on an online strategy game. in proceedings of the acl.\n",
      "data format\n",
      "the dataset is a utf-8 encoded json file, which can be loaded into a python kernel with the following code:\n",
      ">>> import json\n",
      ">>> from io import open\n",
      ">>> with open(\"diplomacy_data.json\", \"r\") as f:\n",
      "...     diplomacy = json.load(f)\n",
      "...\n",
      "it is structured as a list of dictionaries, one for each of the 500 sequences.\n",
      ">>> len(diplomacy)\n",
      "500\n",
      "this is an example of one such entry, with the fields explained:\n",
      ">>> entry = diplomacy[0]\n",
      ">>> entry\n",
      "{\n",
      "    'idx': 0,           # unique identifier of the dataset entry\n",
      "    'game': 74,         # unique identifier of the game it comes from\n",
      "    'betrayal': true,   # whether the friendship ended in betrayal\n",
      "    'people': u'at',    # the countries represented by the two players\n",
      "                        # (in this case, austria and turkey)\n",
      "    'seasons': ...\n",
      "}\n",
      "the 'seasons' field is again a list of dictionaries, one for each game season in the friendship sequence. in the example below, there are 8 seasons, each identified by the game year. decimal notation is used to denote the season in each year. for example, 1906.0 is the spring of 1906 and 1906.5 is the fall of 1906. each season is also marked with what interaction the two players have at the end of the discussion: whether the players supported one another ('support'), attacked one another ('attack'), or did not have explicit military interactions (null).\n",
      ">>> seasons = entry['seasons']\n",
      ">>> len(seasons)\n",
      "8\n",
      ">>> seasons[0]\n",
      "{\n",
      "    'season': 1906.5,           # fall of the year 1906 (game time)\n",
      "    'interaction': {\n",
      "        'victim': u'support',   # the victim supported the betrayer\n",
      "        'betrayer': u'support'  # the betrayer supported the victim\n",
      "    },\n",
      "    'messages': {\n",
      "        'victim': ...,\n",
      "        'betrayer': ...\n",
      "    }\n",
      "}\n",
      "the ['messages']['victim'] and ['messages']['betrayer'] fields are lists of features of each message sent by the victim to the betrayer, and by the betrayer to the victim, respectively:\n",
      ">>> msgs = seasons[0]['messages']['betrayer']\n",
      ">>> len(msgs)\n",
      "6\n",
      ">>> msgs[0]\n",
      "{\n",
      "    \"n_words\": 146,             # number of words in the message\n",
      "    \"n_sentences\": 9,           # number of sentences in the message\n",
      "\n",
      "    \"n_requests\": 7,            # number of request sentences\n",
      "    \"politeness\": 0.8320,       # politeness of the requests (from 0 to 1)\n",
      "                                # (using the stanford politeness\n",
      "                                # classifier available at [4])\n",
      "    \"sentiment\": {\n",
      "        \"positive\": 1,          # no. sentences with positive sentiment\n",
      "        \"neutral\": 3,           #      \"      \"      neutral sentiment\n",
      "        \"negative\": 5           #      \"      \"      negative sentiment\n",
      "    },                          # (using stanford sentiment analysis [5])\n",
      "\n",
      "    \"lexicon_words\": {          # words and phrases matching several\n",
      "        \"disc_expansion\": [     # linguistic and psycholinguistic lexicons\n",
      "            \"until\",            # (see below for details)\n",
      "            \"yet\",\n",
      "            \"instead\"\n",
      "        ],\n",
      "        \"premise\": [\n",
      "            \"for\",\n",
      "            \"for\"\n",
      "        ],\n",
      "        ...\n",
      "    },\n",
      "    \"frequent_words\": [         # frequent words in the message\n",
      "        \"more\",                 # (occurring in at least 50 messages\n",
      "        \"let\",                  # and 5 friendships overall)\n",
      "        \"keep\",\n",
      "        \"...\n",
      "    ]\n",
      "}\n",
      "the words in each list are in random order. the order of messages within a season is also randomized. this measure is in place to protect the privacy of the players and of their conversations.\n",
      "the lexicons used to construct the \"lexicon_words\" field are:\n",
      "'claim', 'premise': argumentation structure markers [6]\n",
      "'allsubj': subjective markers [7]\n",
      "'disc_*': discourse markers from the penn discourse treebank. [8] includes 'disc_comparison', 'disc_expansion', 'disc_contingency', 'disc_temporal_future' and 'disc_temporal_rest' (we manually split 'temporal' from pdt into 'temporal_future' and 'temporal_rest' to capture planning).\n",
      "references\n",
      "[1] https://en.wikipedia.org/wiki/diplomacy_%28game%29 [2] http://www.floc.net/dpjudge/ [3] http://usak.asciiking.com/ [4] http://politeness.mpi-sws.org/ [5] http://nlp.stanford.edu/sentiment/ [6] c. stab and i. gurevych. identifying argumentative discourse structures in persuasive essays. in: proceedings of emnlp, 2014. https://www.ukp.tu-darmstadt.de/data/argumentation-mining/ [7] e. riloff and j. wiebe. learning extraction patterns for subjective expressions. in: proceedings of emnlp, 2003. http://www.anthology.aclweb.org/w/w03/w03-1014.pdf [8] https://www.seas.upenn.edu/~pdtb/\n",
      "context:\n",
      "“a blog (a truncation of the expression \"weblog\") is a discussion or informational website published on the world wide web consisting of discrete, often informal diary-style text entries (\"posts\"). posts are typically displayed in reverse chronological order, so that the most recent post appears first, at the top of the web page. until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject or topic.” -- wikipedia article “blog”\n",
      "this dataset contains text from blogs written on or before 2004, with each blog being the work of a single user.\n",
      "content:\n",
      "the blog authorship corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in august 2004. the corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n",
      "each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (all are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
      "all bloggers included in the corpus fall into one of three age groups: * 8240 \"10s\" blogs (ages 13-17), * 8086 \"20s\" blogs(ages 23-27) * 2994 \"30s\" blogs (ages 33-47).\n",
      "for each age group there are an equal number of male and female bloggers.\n",
      "each blog in the corpus includes at least 200 occurrences of common english words. all formatting has been stripped with two exceptions. individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n",
      "acknowledgements\n",
      "the corpus may be freely used for non-commercial research purposes. any resulting publications should cite the following:\n",
      "j. schler, m. koppel, s. argamon and j. pennebaker (2006). effects of age and gender on blogging in proceedings of 2006 aaai spring symposium on computational approaches for analyzing weblogs. url: http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf\n",
      "inspiration:\n",
      "this dataset contains information on writers demographics, including their age, gender and zodiac sign. can you build a classifier to guess someone’s zodiac sign from blog posts they’ve written?\n",
      "which are bigger: differences between demographic groups or differences between blogs on different topics?\n",
      "you may also like:\n",
      "news and blog data crawl: content section from over 160,000 news and blog articles\n",
      "20 newsgroups: a collection of ~18,000 newsgroup documents from 20 different newsgroups\n",
      "context:\n",
      "sentiment analysis is the task of computationally labeling whether the content of text is positive or negative. one common approach to this is to compile lists of words which have a positive connotation (like “wonderful”, “lovely” and “best”) and a negative connotation (like “bad”, “horrible” or “awful”). then you count how many positive and how many negative\n",
      "content:\n",
      "this dataset contains three lists of thai words:\n",
      "swear words (94 words)\n",
      "positive words (512 words)\n",
      "negative words (1218 words)\n",
      "each list a .txt file with one word per line. the character encoding is utf-8.\n",
      "acknowledgements:\n",
      "this dataset was compiled by wannaphong phatthiyaphaibun and is reproduced here under a cc-by-sa 4.0 license. (you may also be interested in his translation of python 3 documentation into thai on this blog.)\n",
      "inspiration:\n",
      "can you analyze the sentiment in this corpus of thai? is there a difference in sentiment between the wikipedia and government parts of the corpus?\n",
      "context:\n",
      "anticipating public nuisances and allocating proper resources is a critical part of public duties.\n",
      "content:\n",
      "this dataset contains 5 years (2008-2011, 2016) worth of public incidents, both criminal and non-criminal. data includes time, location, description, and unique key.\n",
      "acknowledgements:\n",
      "this dataset was compiled by the city of austin and published on google cloud public data.\n",
      "dataset description\n",
      "use this dataset with bigquery you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too.\n",
      "inspiration:\n",
      "are there notable time variations in incidences?\n",
      "can you predict incidence patterns for 2016 based on the 2008-2011 training data?\n",
      "do weather patterns or other changes related to incidence changes?\n",
      "context\n",
      "at shadow robot, we are leaders in robotic grasping and manipulation. as part of our smart grasping system development, we're developing different algorithms using machine learning.\n",
      "this first public dataset was created to investigate the use of machine learning to predict the stability of a grasp. due to the limitations of the current simulation, it is a restricted dataset - only grasping a ball. the dataset is annotated with an objective grasp quality and contains the different data gathered from the joints (position, velocity, effort).\n",
      "you can find all the explanations for this dataset over on medium.\n",
      "inspiration\n",
      "i'll be more than happy to discuss this dataset as well as which dataset you'd like to have to try your hands at solving real world robotic problems focused on grasping using machine learning. let's connect on twitter (@ugocupcic)!\n",
      "context\n",
      "the data were collected as the scitos g5 navigated through the room following the wall in a clockwise direction, for 4 rounds. to navigate, the robot uses 24 ultrasound sensors arranged circularly around its \"waist\". the numbering of the ultrasound sensors starts at the front of the robot and increases in clockwise direction.\n",
      "the provided files comprise three diferent data sets.\n",
      "the first one contains the raw values of the measurements of all 24 ultrasound sensors and the corresponding class label (moving forward, turning left, etc). sensor readings are sampled at a rate of 9 samples per second.\n",
      "the second one contains four sensor readings named 'simplified distances' and the corresponding class label l (moving forward, turning left, etc). these simplified distances are referred to as the 'front distance', 'left distance', 'right distance' and 'back distance'. they consist, respectively, of the minimum sensor readings among those within 60 degree arcs located at the front, left, right and back parts of the robot.\n",
      "the third one contains only the front and left simplified distances and the corresponding class labell (moving forward, turning left, etc).\n",
      "it is worth mentioning that the 24 ultrasound readings and the simplified distances were collected at the same time step, so each file has the same number of rows (one for each sampling time step).\n",
      "the wall-following task and data gathering were designed to test the hypothesis that this apparently simple navigation task is indeed a non-linearly separable classification task. thus, linear classifiers, such as the perceptron network, are not able to learn the task and command the robot around the room without collisions. nonlinear neural classifiers, such as the mlp network, are able to learn the task and command the robot successfully without collisions.\n",
      "if some kind of short-term memory mechanism is provided to the neural classifiers, their performances are improved in general. for example, if past inputs are provided together with current sensor readings, even the perceptron becomes able to learn the task and command the robot successfully. if a recurrent neural network, such as the elman network, is used to learn the task, the resulting dynamical classifier is able to learn the task using less hidden neurons than the mlp network.\n",
      "files with different number of sensor readings were built in order to evaluate the performance of the classifiers with respect to the number of inputs.\n",
      "content\n",
      "file sensor_readings_24.csv:\n",
      "us1: ultrasound sensor at the front of the robot (reference angle: 180°) - (numeric: real)\n",
      "us2: ultrasound reading (reference angle: -165°) - (numeric: real)\n",
      "us3: ultrasound reading (reference angle: -150°) - (numeric: real)\n",
      "us4: ultrasound reading (reference angle: -135°) - (numeric: real)\n",
      "us5: ultrasound reading (reference angle: -120°) - (numeric: real)\n",
      "us6: ultrasound reading (reference angle: -105°) - (numeric: real)\n",
      "us7: ultrasound reading (reference angle: -90°) - (numeric: real)\n",
      "us8: ultrasound reading (reference angle: -75°) - (numeric: real)\n",
      "us9: ultrasound reading (reference angle: -60°) - (numeric: real)\n",
      "us10: ultrasound reading (reference angle: -45°) - (numeric: real)\n",
      "us11: ultrasound reading (reference angle: -30°) - (numeric: real)\n",
      "us12: ultrasound reading (reference angle: -15°) - (numeric: real)\n",
      "us13: reading of ultrasound sensor situated at the back of the robot (reference angle: 0°) - (numeric: real)\n",
      "us14: ultrasound reading (reference angle: 15°) - (numeric: real)\n",
      "us15: ultrasound reading (reference angle: 30°) - (numeric: real)\n",
      "us16: ultrasound reading (reference angle: 45°) - (numeric: real)\n",
      "us17: ultrasound reading (reference angle: 60°) - (numeric: real)\n",
      "us18: ultrasound reading (reference angle: 75°) - (numeric: real)\n",
      "us19: ultrasound reading (reference angle: 90°) - (numeric: real)\n",
      "us20: ultrasound reading (reference angle: 105°) - (numeric: real)\n",
      "us21: ultrasound reading (reference angle: 120°) - (numeric: real)\n",
      "us22: ultrasound reading (reference angle: 135°) - (numeric: real)\n",
      "us23: ultrasound reading (reference angle: 150°) - (numeric: real)\n",
      "us24: ultrasound reading (reference angle: 165°) - (numeric: real)\n",
      "classes: move-forward, slight-right-turn, sharp-right-turn, slight-left-turn\n",
      "file: sensor_readings_4.csv:\n",
      "sd_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\n",
      "sd_left: minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)\n",
      "sd_right: minimum sensor reading within a 60 degree arc located at the right of the robot - (numeric: real)\n",
      "sd_back: minimum sensor reading within a 60 degree arc located at the back of the robot - (numeric: real)\n",
      "classes: move-forward, slight-right-turn, sharp-right-turn, slight-left-turn\n",
      "file: sensor_readings_2.csv:\n",
      "sd_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\n",
      "sd_left: minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)\n",
      "classes: move-forward, slight-right-turn, sharp-right-turn, slight-left-turn\n",
      "acknowledgements\n",
      "these datasets were downlaoded from the uci machine learning repository\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "inspiration\n",
      "use these ultrasound readings to predict the class, i.e. given these readings, is the robot moving straight? turning left?\n",
      "context\n",
      "the drug-development process is time-consuming and expensive. in high-throughput screening (hts), batches of compounds are tested against a biological target to test the compound's ability to bind to the target. targets might be antibodies for example. if the compound binds to the target then it is active for that target and known as a hit.\n",
      "virtual screening is the computational or in silico screening of biological compounds and complements the hts process. it is used to aid the selection of compounds for screening in hts bioassays or for inclusion in a compound-screening library.\n",
      "drug discovery is the first stage of the drug-development process and involves finding compounds to test and screen against biological targets. this first stage is known as primary-screening and usually involves the screening of thousands of compounds.\n",
      "this dataset is a collection of 21 bioassays (screens) that measure the activity of various compounds against different biological targets.\n",
      "content\n",
      "each bioassay is split into test and train files.\n",
      "here are some descriptions of some of the assays compounds. the source, unfortunately, does not have descriptions for every assay. that's the nature of the beast for finding this kind data and was also pointed out in the original study.\n",
      "primary screens\n",
      "aid362 details the results of a primary screening bioassay for formylpeptide receptor ligand binding university from the new mexico center for molecular discovery. it is a relatively small dataset with 4279 compounds and with a ratio of 1 active to 70 inactive compounds (1.4% minority class). the compounds were selected on the basis of preliminary virtual screening of approximately 480,000 drug-like small molecules from chemical diversity laboratories.\n",
      "aid604 is a primary screening bioassay for rho kinase 2 inhibitors from the scripps research institute molecular screening center. the bioassay contains activity information of 59,788 compounds with a ratio of 1 active compound to 281 inactive compounds (1.4%). 57,546 of the compounds have known drug-like properties.\n",
      "aid456 is a primary screen assay from the burnham center for chemical genomics for inhibition of tnfa induced vcam-1 cell surface expression and consists of 9,982 compounds with a ratio of 1 active compound to 368 inactive compounds (0.27% minority). the compounds have been selected for their known drug-like properties and 9,431 meet the rule of 5 [19].\n",
      "aid688 is the result of a primary screen for yeast eif2b from the penn center for molecular discovery and contains activity information of 27,198 compounds with a ratio of 1 active compound to 108 inactive compounds (0.91% minority). the screen is a reporter-gene assay and 25,656 of the compounds have known drug-like properties.\n",
      "aid373 is a primary screen from the scripps research institute molecular screening center for endothelial differentiation, sphingolipid g-protein-coupled receptor, 3. 59,788 compounds were screened with a ratio of 1 active compound to 963 inactive compounds (0.1%). 57,546 of the compounds screened had known drug-like properties.\n",
      "aid746 is a primary screen from the scripps research institute molecular screening center for mitogen-activated protein kinase. 59,788 compounds were screened with a ratio of 1 active compound to 162 inactive compounds (0.61%). 57,546 of the compounds screened had known drug-like properties.\n",
      "aid687 is the result of a primary screen for coagulation factor xi from the penn center for molecular discovery and contains activity information of 33,067 compounds with a ratio of 1 active compound to 350 inactive compounds (0.28% minority). 30,353 of the compounds screened had known drug-like properties.\n",
      "primary and confirmatory\n",
      "aid604 (primary) with aid644 (confirmatory)\n",
      "aid746 (primary) with aid1284 (confirmatory)\n",
      "aid373 (primary) with aid439 (confirmatory)\n",
      "aid746 (primary) with aid721 (confirmatory)\n",
      "confirmatory\n",
      "aid1608 is a different type of screening assay that was used to identify compounds that prevent httq103-induced cell death. national institute of neurological disorders and stroke approved drug program. the compounds that prevent a release of a certain chemical into the growth medium are labelled as active and the remaining compounds are labelled as having inconclusive activity. aid1608 is a small dataset with 1,033 compounds and a ratio of 1 active to 14 inconclusive compounds (6.58% minority class).\n",
      "aid644\n",
      "aid1284\n",
      "aid439\n",
      "aid721\n",
      "aid1608\n",
      "aid644\n",
      "aid1284\n",
      "aid439\n",
      "aid721\n",
      "acknowledgements\n",
      "original study: https://www.ncbi.nlm.nih.gov/pmc/articles/pmc2820499/\n",
      "data downloaded form uci ml repository:\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "inspiration\n",
      "drug development is expensive. use this virtual bio assay data to classify compounds as hits (active) against their biological targets.\n",
      "fireballs and bolides are astronomical terms for exceptionally bright meteors that are spectacular enough to to be seen over a very wide area. a world map shows a visual representation of the data table that provides a chronological data summary of fireball and bolide events provided by u.s. government sensors. ground-based observers sometimes also witness these events at night, or much more rarely in daylight, as impressive atmospheric light displays. this website is not meant to be a complete list of all fireball events. only the brightest fireballs are noted.\n",
      "content\n",
      "the accompanying table provides information on the date and time of each reported fireball event with its approximate total optical radiated energy and its calculated total impact energy. when reported, the event’s geographic location, altitude and velocity at peak brightness are also provided. note that data are not provided in real-time and not all fireballs are reported. a blank (empty) field in the table indicates the associated value was not reported.\n",
      "for more information about fireballs and bolides, please see https://cneos.jpl.nasa.gov/fireballs/intro.html.\n",
      "field legend\n",
      "peak brightness date/time (ut) the date and time in ut (universal time) of this event's peak brightness.\n",
      "latitude (deg.) geodetic latitude in degrees north (n) or south (s) of the equator for this event.\n",
      "longitude (deg.) geodetic longitude in degrees east (e) or west (w) of the prime meridian for this event.\n",
      "altitude (km) altitude in kilometers (km) above the reference geoid for this event.\n",
      "velocity (km/s) the magnitude of the meteor's pre-impact velocity in kilometers per second (km/s).\n",
      "velocity components (km/s) the magnitude of the meteor's pre-impact velocity in a geocentric earth-fixed reference frame defined as follows: the z-axis is directed along the earth's rotation axis towards the celestial north pole, the x-axis lies in the earth's equatorial plane, directed towards the prime meridian, and the y-axis completes the right-handed coordinate system.\n",
      "total radiated energy (j) the approximate total radiated energy in the atmosphere in joules [a unit of energy given in kilograms times velocity squared, or kg × (m/s)2]\n",
      "calculated total impact energy (kt) the impact energy of the event in kilotons of tnt (kt) computed from an empirical expression relating radiated and impact energy\n",
      "acknowledgements\n",
      "this dataset was kindly made available by nasa. you can find the original dataset at https://cneos.jpl.nasa.gov/fireballs/\n",
      "you might also be interested in their planetary defense faq.\n",
      "about this data\n",
      "this is a list of 10,000 different food listings and their ingredients provided by datafiniti's product database. the dataset includes the brand, name, manufacturer, category, features, and more for each product.\n",
      "what you can do with this data\n",
      "a similar dataset was used to determine if it's cheaper to eat in or eat out. discover insights into ingredients used in various foods. e.g.:\n",
      "what's the distribution of the number of ingredients per listing?\n",
      "what are the most common ingredients used?\n",
      "what is the total cost of ingredients needed for a homemade meal versus restaurant meal?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com, a well known retailer.\n",
      "content\n",
      "this dataset has following fields:\n",
      "sku\n",
      "name_title\n",
      "description\n",
      "list_price\n",
      "sale_price\n",
      "category\n",
      "category_tree\n",
      "average_product_rating\n",
      "product_url\n",
      "product_image_urls\n",
      "brand\n",
      "total_number_reviews\n",
      "reviews\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of list price, sale price, rating and reviews can be performed.\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one data file for each of these countries:\n",
      "states included in this dataset:\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip.\n",
      "background\n",
      "nlp is a hot topic currently! team ai really want's to leverage the nlp research and this an attempt for all the nlp researchers to explore exciting insights from bilingual data\n",
      "the japanese-english bilingual corpus of wikipedia's kyoto articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation, information extraction, and other language processing technologies.\n",
      "unique features\n",
      "a precise and large-scale corpus containing about 500,000 pairs of manually-translated sentences. can be exploited for research and development of high-performance multilingual machine translation, information extraction, and so on.\n",
      "the three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation.\n",
      "translated articles concern kyoto and other topics such as traditional japanese culture, religion, and history. can also be utilized for tourist information translation or to create glossaries for travel guides.\n",
      "the japanese-english bilingual kyoto lexicon is also available. this lexicon was created by extracting the japanese-english word pairs from this corpus.\n",
      "sample\n",
      "one wikipedia article is stored as one xml file in this corpus, and the corpus contains 14,111 files in total.\n",
      "the following is a short quotation from a corpus file titled “ryoan-ji temple”. each tag has different implications. for example:\n",
      "<j>original japanese sentence<j> <e type=\"trans\" ver=\"1\">primary translation</e> <e type=\"trans\" ver=\"2\">secondary translation</e> <e type=\"check\" ver=\"1\">final translation</e> <cmt>comment added by translators</cmt>\n",
      "categories\n",
      "the files have been divided into 15 categories: school, railway, family, building, shinto, person name, geographical name, culture, road, buddhism, literature, title, history, shrines and temples, and emperor (click the link to view a sample file for each category).\n",
      "github\n",
      "https://github.com/venali/bilingualcorpus\n",
      "explains how to load the corpus\n",
      "acknowledgements\n",
      "the national institute of information and communications technology (nict) has created this corpus by manually translating japanese wikipedia articles (related to kyoto) into english.\n",
      "licence\n",
      "use and/or redistribution of the japanese-english bilingual corpus of wikipedia's kyoto articles and the japanese-english bilingual kyoto lexicon is permitted under the conditions of creative commons attribution-share-alike license 3.0. details can be found at http://creativecommons.org/licenses/by-sa/3.0/.\n",
      "link to web\n",
      "https://alaginrc.nict.go.jp/wikicorpus/index_e.html\n",
      "dw-nominate scores of congressional voting behavior regularly appears in media such as the new york times, washington post, and 538. this dataset contains the voting records used to generate those scores and additional features related to the dw-nominate calculations.\n",
      "content\n",
      "this dataset contains descriptive data as well as ideological data for congressional rollcalls, individual member votes, members of congress, and parties. you can find information such the descriptions of rollcalls, what proportion of voting members were correctly classified by the ideological cutting line for that rollcall, the ideological position of members of congress, and more.\n",
      "both the rollcall data and the data on members are split into chambers and congresses. the data on parties is a dataset with some metadata about all of the different parties as well as their average ideological position and membership size broken down by congress and chamber.\n",
      "the full details behind the dw-nominate calculations may be helpful in interpreting some of this data. the technical details of the dw-nominate model can be found in poole's spatial models of parliamentary voting. poole and rosenthal's ideology and congress explores the nature of voting in congress and the political history of the united states through the lens of the ideological dimensions recovered by dw-nominate.\n",
      "acknowledgements\n",
      "this dataset was prepared by the team at voteview. please visit their site if you require up-to-date records. you may also be interested in their blog.\n",
      "inspiration\n",
      "-using national scores as a training set, can you develop polarization scores for you own state legislature? -can you find correlates that help explain changes in dw-nominate scores?\n",
      "unlike other kaggle competitions, the leaderboard for march madness changes as the ncaa basketball tournament progresses. part of the fun is seeing how the rankings will change due to upcoming games. to enable easier analysis, i've cleansed and processed the predictions dataset that william cukierski created.\n",
      "context\n",
      "this dataset is created for the deep learning based devanagari character recognition research evaluation, 2015.\n",
      "content\n",
      "the dataset contains mixed categories for devanagari numerals (10 classes) and consonants (36 classes). dataset is explicitly separated into train and test set. train set contains total 78,200 samples with 1700 samples per class for total 46 classes and test set contains total 13,800 samples with 300 samples per class for total 46 classes. dataset is collected from the school level students.\n",
      "this dataset is collected and maintained by the following research members,\n",
      "shailesh acharya\n",
      "ashok kumar pant\n",
      "prashnna kumar gyawali\n",
      "citation\n",
      "please cite in your publications if it helps your research:\n",
      "@inproceedings{ashok2015deep, \n",
      "    author={s. acharya and a. k. pant and p. k. gyawali}, \n",
      "    booktitle={2015 9th international conference on software, knowledge, information management and applications (skima)}, \n",
      "    title={deep learning based large scale handwritten devanagari character recognition}, \n",
      "    year={2015}, \n",
      "    pages={1-6},\n",
      "    month={dec}\n",
      " }\n",
      "mercedes-benz greener manufacturing competition\n",
      "mercedes-benz hosted a kaggle competition in june-july 2017. participants were invited to predict the time it takes to test the car using an anonymized set of 377 variables.\n",
      "in this competition, daimler is challenging kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. competitors will work with a dataset representing different permutations of mercedes-benz car features to predict the time it takes to pass testing.\n",
      "what is in the dataset\n",
      "current dataset represents public and private leaderboard standings for every day of competition (from 31-may-2017 to 11-07-2017). data has been collected from kaggle leaderboard tables after the competition has ended.\n",
      "acknowledgements\n",
      "we would like to thank @breakfastpirat for inspiration, daimler team for hosting the competition, kaggle team for support and all fellow participants in the mercedes data challenge for their courage and perseverance.\n",
      "questions for inspiration\n",
      "1) validate the leaderboard shakeup statistics\n",
      "2) try to reproduce the leaderboard progression visualizations\n",
      "3) when did the teams start overfitting? what makes one prone to overfitting?\n",
      "4) investigate public submissions statistics. can the pattern of submissions predict the final score?\n",
      "5) can you detect which teams have has a robust cross-validation strategy from their submissions stats?\n",
      "from peter norvig's classic how to write a spelling corrector\n",
      "one week in 2007, two friends (dean and bill) independently told me they were amazed at google's spelling correction. type in a search like [speling] and google instantly comes back with showing results for: spelling. i thought dean and bill, being highly accomplished engineers and mathematicians, would have good intuitions about how this process works. but they didn't, and come to think of it, why should they know about something so far outside their specialty?\n",
      "i figured they, and others, could benefit from an explanation. the full details of an industrial-strength spell corrector are quite complex (you can read a little about it here or here). but i figured that in the course of a transcontinental plane ride i could write and explain a toy spelling corrector that achieves 80 or 90% accuracy at a processing speed of at least 10 words per second in about half a page of code.\n",
      "a kernel has been added with peter's basic spell.py and evaluation code to set a baseline. minimal modifications were made so that it runs on this environment.\n",
      "data files\n",
      "big.txt is required by the code. that's how it learns the probabilities of english words. you can prepend more text data to it, but be sure to leave in the little python snippet at the end.\n",
      "testing files\n",
      "the other files are for testing the accuracy. the baseline code should get 75% of 270 correct on spell-testset1.txt, and 68% of 400 correct on spell-testset2.txt.\n",
      "i've also added some other files for more extensive testing. the example kernel runs all of them but birkbeck.txt by default. here's the output:\n",
      "testing spell-testset1.txt\n",
      "75% of 270 correct (6% unknown) at 32 words per second \n",
      "testing spell-testset2.txt\n",
      "68% of 400 correct (11% unknown) at 28 words per second \n",
      "testing wikipedia.txt\n",
      "61% of 2455 correct (24% unknown) at 21 words per second \n",
      "testing aspell.txt\n",
      "43% of 531 correct (23% unknown) at 15 words per second \n",
      "the larger datasets take a few minutes to run. birkbeck.txt takes more than a few minutes.\n",
      "you can try adding other datasets, or splitting these ones in meaningful ways - for example a dataset of only words of 5 characters or less, or 10 characters or more, or without uppercase - to understand the effect of changes you make on different types of words.\n",
      "languages\n",
      "the data and testing files include english only for now. in principle it is easily generalisable to other languages.\n",
      "context\n",
      "goodreads is the world’s largest site for readers and book recommendations. their mission is to help people find and share books they love. in addition to tracking the books you're reading, have read, and want to read, users on goodreads can see what their friends are reading, get personalized recommendations, and browse community reviews. the website is also a great place to look for inspirational quotes from authors.\n",
      "content & inspiration\n",
      "this dataset contains the most popular and recent quotes shared on goodreads. along with each quote, you'll get its tags, the number of likes it received, and the author of the quote. you can use this dataset to study which tags receive the most likes, what characteristics the most popular quotes have in common, use nlp techniques like sentiment analysis, or even generate your own novel quotes (pun intended).\n",
      "before you begin, an inspirational quote from arthur conan doyle, sherlock holmes:\n",
      "\"it is a capital mistake to theorize before one has data. insensibly one begins to twist facts to suit theories, instead of theories to suit facts.\"\n",
      "this dataset consists of 5547 breast histology images of size 50 x 50 x 3, curated from andrew janowczyk website and used for a data science tutorial at epidemium. the goal is to classify cancerous images (idc : invasive ductal carcinoma) vs non-idc images.\n",
      "context\n",
      "the assembly election results for uttar pradesh(up) were surprising to say the least. never in the past has any single party secured a similar mandate. up with a population of around 220 million is as big as the whole of united states. it has 403 constituencies each having its own demographic breakup. the election was conducted in 7 phases.\n",
      "content\n",
      "the dataset has 8 variables:\n",
      "seat_allotment: as some of you might be aware that there was a coalition between inc and sp which materialized pretty late into the campaign. hence, in a few constituencies the high command of the 2 parties could not convince contestants to forfeit their nomination. in such constituencies, there is a situation that is called a friendly fight(ff) where candidates from both parties inc and sp are contesting instead of just one. these constituencies are marked by the flag ff (friendly fight). others are inc (contested by inc), sp(contested by sp) and dnc(contested by none)\n",
      "phase: the phase in which the election was conducted.\n",
      "ac_no: assembly constituency number\n",
      "ac: assembly constituency name\n",
      "district: district to which the ac belongs\n",
      "candidate: candidate name\n",
      "party: party name\n",
      "votes: votes for each candidate\n",
      "source: scraped from eciresults.nic.in\n",
      "this dataset was obtained here and their description is reproduced below.\n",
      "astronomical background\n",
      "galaxies are fundamental structures in the universe. our sun lives in the milky way galaxy we can see as a patchy band of light across the sky. the components of a typical galaxy are: a vast number of stars (total mass ~106-1011 mo where mo is the unit of a solar mass), a complex interstellar medium of gas and dust from which stars form (typically 1-100% of the stellar component mass), a single supermassive black hole at the center (typically <1% of the stellar component mass), and a poorly understood component called dark matter with mass ~5-10-times all the other components combined.\n",
      "over the ~14 billion years since the big bang, the rate at which galaxies convert interstellar matter into stars has not been constant, and thus the brightness and color of galaxies change with cosmic time. this phenomenon has several names in the astronomical community: the history of star formation in the universe, chemical evolution of galaxies, or simply galaxy evolution. a major effort over several decades has been made to quantify and understand galaxy evolution using telescopes at all wavelengths.\n",
      "the traditional tool for such studies has been optical spectroscopy which easily reveals signatures of star formation in nearby galaxies. however, to study star formation in the galaxies recently emerged after the big bang, we must examine extremely faint galaxies which are too faint for spectroscopy, even using the biggest available telescopes. a feasible alternative is to obtain images of faint galaxies at random locations in the sky in narrow spectral bands, and thereby construct crude spectra. first, statistical analysis of such multiband photometric datasets are used to classify galaxies, stars and quasars. second, for the galaxies, multivariate regression is made to develop photometric estimates of redshift, which is a measure both of distance from us and age since the big bang. third, one can examine galaxy colors as a function of redshift (after various corrections are made) to study the evolution of star formation. the present dataset is taken after these first two steps are complete.\n",
      "contents\n",
      "wolf et al. (2004) provide the first public catalog of a large dataset (63,501 objects) with brightness measurements in 17 bands in the visible band. (note that the sloan digital sky survey provides a much larger dataset of 108 objects with measurements in 5 bands.) we provide here a subset of their catalog with 65 columns of information on 3,462 galaxies. these are objects in the chandra deep field south field which wolf and colleagues have classified as `galaxies'. the column headings are formally described in their table 3, and the columns we provide are summarized here with brief commentary:\n",
      "col 1: nr, object number\n",
      "col 2-3: total r (red band) magnitude and its error. this was the band at which the basic catalog was constructed. magnitudes are inverted logarithmic measures of brightness. a galaxy with r=21 is 100-times brighter than one with r=26. the error is the standard deviation derived from detailed knowledge of the measurement process. this dataset is an excellent example of astronomical datasets where each variable is accompanied by heteroscedastic measurement errors of known variances.\n",
      "col 4-5: apdrmag is the difference between the total and aperture magnitude in the r band. this is a rough measure of the size of the galaxy in the image where apdrmag=0 corresponds to a point source. negative values are not physically meaningful. mu_max is the central surface brightness of the object in the r band. the difference between rmag and mu_max should also be an indicator of galaxy size.\n",
      "col 6-9: mcz and mczml are two redshift estimates. mcz is the preferred value. e.mcz is its estimated error, and chi2red is the reduced chi-squared value of the least-squares fit of the 17-band magnitudes to the best-fit template galaxy spectrum. galaxies with large e.mcz or chi2red might be omitted as unreliable.\n",
      "col 10-29: these give the absolute magnitudes (i.e. intrinsic luminosities) of the galaxy in 10 bands, with their measurement errors. they are based on the measured magnitudes and the redshifts, and represent the intrinsic luminosities of the galaxies; a galaxy with m=-15 is 100-times less luminous than one with m=-20. these magnitudes are not all independent of each others, but the are important for representing intrinsic properties of the galaxies. below is one of several redshift-stratified plots of the b-band absolute magnitude (abscissa) against the difference of magnitude (i.e. ratio of luminosities) between the 2800a ultraviolet and blue band, which is a sensitive indicator of star formation. a redshift-dependent bimodal distribution is seen.\n",
      "col 30-55: observed brightnesses in 13 bands in sequence from 420 nm in the ultraviolet to 915 nm in the far red. these are given in linear variables with units of photon flux densities, photons/m2/s/nm. again, each measurement is accompanied by a measurement error which can be used to distinguish measurement from intrinsic dispersions in the distributions.\n",
      "col 56-65: observed brightnesses in 5 traditional broad spectral bands, ubvri. these are largely redundant with the 13 bands in the previous columns.\n",
      "statistical exercises\n",
      "examine basic characteristics of the survey which are not of scientific interest. these include the absence of high-redshift (i.e. distant) high-absolute-magnitude (i.e. faint) galaxies; the dropoff in flux with redshift; the dropoff in image size with redshift.\n",
      "study these two populations as a function of redshift to investigate the evolution of star formation.\n",
      "context\n",
      "this dataset includes some of the basic information of the websites we daily use. while scrapping this info, i learned quite a lot in r programming, system speed, memory usage etc. and developed my niche in web scrapping. it took about 4-5 hrs for scrapping this data through my system (4gb ram) and nearly about 4-5 days working out my idea through this project.\n",
      "content\n",
      "the dataset contains top 50 ranked sites from each 191 countries along with their traffic (global) rank. here, country_rank represent the traffic rank of that site within the country, and traffic_rank represent the global traffic rank of that site.\n",
      "since most of the columns meaning can be derived from their name itself, its pretty much straight forward to understand this dataset. however, there are some instances of confusion which i would like to explain in here:\n",
      "1) most of the numeric values are in character format, hence, contain spaces which you might need to clean on.\n",
      "2) there are multiple instances of same website. for.e.g. yahoo. com is present in 179 rows within this dataset. this is due to their different country rank in each country.\n",
      "3)the information provided in this dataset is for the top 50 websites in 191 countries as on 25th may 2017 and is subjected to change in future time due to the dynamic structure of ranking.\n",
      "4) the dataset inactual contains 9540 rows instead of 9550(50*191 rows). this was due to the unavailability of information for 10 websites.\n",
      "ps: in case if there are anymore queries, comment on this, i'll add an answer to that in above list.\n",
      "acknowledgements\n",
      "i wouldn't have done this without the help of others. i've scrapped this information from publicly available (open to all) websites namely: 1) http://data.danetsoft.com/ 2) http://www.alexa.com/topsites , of which i'm highly grateful. i truly appreciate and thanks the owner of these sites for providing us with the information that i included today in this dataset.\n",
      "inspiration\n",
      "i feel that there this a lot of scope for exploring & visualization this dataset to find out the trends in the attributes of these websites across countries. also, one could try predicting the traffic(global) rank being a dependent factor on the other attributes of the website. in any case, this dataset will help you find out the popular sites in your area.\n",
      "context\n",
      "just throwing it out their for anyone and everyone that is interested in heart disease.\n",
      "content\n",
      "dataset consisting of 14 attributes and 303 observations that were used for the purpose of heart disease classification of a given patient.\n",
      "acknowledgements\n",
      "hungarian institute of cardiology. budapest: andras janosi, m.d. university hospital, zurich, switzerland: william steinbrunn, m.d. university hospital, basel, switzerland: matthias pfisterer, m.d. v.a. medical center, long beach and cleveland clinic foundation: robert detrano, m.d., ph.d\n",
      "inspiration\n",
      "i'd like to have outside input on this model and create a hybrid classifier that can be used by md's in underserved communities.\n",
      "# context\n",
      "first, i am new to ml, and just in case i slip up, apologies in advance!! so, i am doing an online ml course and this is an assignment where we are supposed to practice scikit-learn's pca routine. since the course has been archived - which means the discussion posts are not answered!! - hence my posting of the problem here.\n",
      "what better way to learn than to get so many experts giving me feedback ... right?\n",
      "# content\n",
      "the data was taken over a 2-month period in india with 25 features ( eg, red blood cell count, white blood cell count, etc). the target is the 'classification', which is either 'ckd' or 'notckd' - ckd=chronic kidney disease. there are 400 rows\n",
      "the data needs cleaning: in that it has nans and the numeric features need to be forced to floats. basically, we were instructed to get rid of all rows with nans, with no threshold - meaning, any row that has even one nan, gets deleted.\n",
      "part 1: we are asked to choose 3 features (bgr, rc, wc), visualize them, then run the pca with n_components=2. the pca is to be run twice: one with no scaling and the second run with scaling. and this is where my issue starts ... in that after scaling i can hardly see any difference!\n",
      "i will stop here for now till i get feedback and then move to part 2.\n",
      "acknowledgements\n",
      "the dataset is available at: https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\n",
      "inspiration\n",
      "i would like to get an intuitive and a practical understanding of pca.\n",
      "context\n",
      "nifty50.csv the nifty 50 index is national stock exchange of india's benchmark stock market index for indian equity market. it is a well diversified 50 stock index accounting for 22 sectors of the economy. it is used for a variety of purposes such as bench-marking fund portfolios, index based derivatives and index funds.\n",
      "banknifty.csv bank nifty represents the 12 most liquid and large capitalized stocks from the banking sector which trade on the national stock exchange (nse). it provides investors and market intermediaries a benchmark that captures the capital market performance of indian banking sector.\n",
      "content\n",
      "a data frame with 8 variables: index, date, time, open, high, low, close and id. for each year from 2013 to 2016, the number of trading data of each minute of given each date. the currency of the price is indian rupee (inr).\n",
      "index : market id\n",
      "date: numerical value (ex. 20121203- to be converted to 2012/12/03)\n",
      "time: factor (ex. 09:16)\n",
      "open: numeric (opening price)\n",
      "high: numeric (high price)\n",
      "low: numeric (low price)\n",
      "close: numeric (closing price)\n",
      "inspiration\n",
      "initial raw data sets are very complex and mixed datatypes. these are processed properly using r libraries like dplyr, stringr and other data munging packages. the desired outputs are then converted into a csv format to use for further analysis.\n",
      "context\n",
      "data gathered from the james comey testimony to the senate intelligence committee on june 8th, 2017 regarding possible russian influence in the 2016 u.s. presidential election.\n",
      "content\n",
      "content includes the full transcript in transcript.csv as well as a breakdown of questions asked by each senator, their political affiliation, and comey's response.\n",
      "all csvs are utf-8.\n",
      "acknowledgements\n",
      "rod castor - initial python script. appliedai.\n",
      "inspiration\n",
      "initially i did analysis to determine length of question by party, length of comey response by party, number of times each word is used and words with a large difference by party. (clinton used 16x more by republicans).\n",
      "further analysis to follow as time permits.\n",
      "context\n",
      "chemicals have health effects: some recognized and some suspected. a comprehensive list of those chemicals and their health effects would be beneficial to those wishing to associate those chemicals' health effects with other sets of data. the datasets uploaded here represent the \"blending\" of various individual chemical health effect datasets as compiled by scorecard and hosted by the goodguide web site.\n",
      "content\n",
      "as a general rule, each dataset has a chemical's chemical abstract society registry number (casrn), for example \"100-00-5\", its name (e.g. \"p-nitrochlorobenzene\"), one or more health effect categories (e.g. \"recognized\" or \"suspected\"), and one or more health effects (e.g. \"kidney\" and/or \"neurotoxicity\") , as well as the organization of provenance (e.g. \"hazmap\" and/or \"rtecs\") for the recognized and/or suspected health effects.\n",
      "acknowledgements\n",
      "a team from the environmental defense fund created the individual datasets that presently reside at goodguide's scorecard's health effects web page. the \"blended\" datasets uploaded herein are various compilations of those individual datasets into ones more suitable for inclusion in data analyses and visualizations.\n",
      "inspiration\n",
      "initially, the \"blended\" datasets were used to associate health effects with fracking well chemical disclosures, as such joining of data was not readily available to most data analysts. examples of such datasets can be found at frackingdata.org's fracfocus data web page.\n",
      "context\n",
      "behavioral context refers to a wide range of attributes describing what is going on with you: where you are (home, school, work, at the beach, at a restaurant), what you are doing (sleeping, eating, in a meeting, computer work, exercising, shower), who you are with (family, friends, co-workers), your body posture state (sitting, standing, walking, running), and so on. the ability to automatically (effortlessly, frequently, objectively) recognize behavioral context can serve many domains. medical applications can monitor physical activity or eating habits; aging-at-home programs can log older adults' physical, social, and mental behavior; personal assistant systems can better server the user if they are aware of the context. in-the-wild (in real life), natural behavior is complex, composed of different aspects, and has high variability. you can run outside at the beach, with friends with your phone in the pocket; you can also run indoors, at the gym, on a treadmill, with your phone motionless next to you. this high variability makes context-recognition a hard task to perform in-the-wild.\n",
      "content\n",
      "the extrasensory dataset was collected from 60 participants where each person participated approximately 7 days. we installed our data-collection mobile app on their personal phone and it was used to collect both sensor-measurements and context-labels. the sensor-measurements were recorded automatically for a window of 20-seconds every minute. this included accelerometer, gyroscope, magnetometer, audio, location, and phone-state from the person's phone, as well as accelerometer and compass from an additional smartwatch that we provided. in addition, the app's interface had many mechanisms for self-reporting the relevant context-labels, including reporting past context, near future, responding to notifications, and more. the flexible interface allowed to collect many labels with minimal effort and interaction-time, to avoid interfering with the natural behavior. the data was collected in-the-wild: participants used their phone in any way that was convenient to them, they engaged in their regular behavior and reported an combinations of labels that fit their context.\n",
      "for every participant (or \"user\"), the dataset has a csv file with pre-computed features that we extracted from the sensors and with labels. each row has a separate example (representing 1 minute) and is indexed by the timestamp (seconds since the epoch). there are columns for the sensor-features, with the prefix of the column name indicating the sensor it came from (e.g. prefix \"raw_acc:\" indicating a feature came from the raw phone accelerometer measurements). there are columns for 51 diverse context-labels and the value for an example-label pair is either 1 (the label is relevant for the example), 0 (the label is not relevant), or 'nan' (missing information).\n",
      "here, we provide data for 2 of the 60 participants. you can use this partial data to get familiar with the data and practice algorithms. the full dataset is publicly available at http://extrasensory.ucsd.edu. the website has additional parts of the data (such as a wider range of the original reported labels, location coordinates, mood labels from part of the participants). if you use the data for your publications, you are required to cite our original paper vaizman, y., ellis, k., and lanckriet, g. \"recognizing detailed human context in-the-wild from smartphones and smartwatches\". ieee pervasive computing, vol. 16, no. 4, october-december 2017, pp. 62-74. read the information at http://extrasensory.ucsd.edu and the original paper for more details.\n",
      "acknowledgements\n",
      "the dataset was collected by yonatan vaizman and katherine ellis, under the supervision of prof. gert lanckriet, all from the department of electrical and computer engineering, university of california, san diego.\n",
      "inspiration\n",
      "the extrasensory dataset can serve as a benchmark to compare methods for context-recognition (or context-awareness, activity recognition, daily activity detection). you can focus on specific sensors or on specific context-labels. you can suggest new models and classifiers, train them on the data and evaluate their performance on the data.\n",
      "context:\n",
      "the occupational employment statistics (oes) and national compensation survey (ncs) programs have produced estimates by borrowing from the strength and breadth of each survey to provide more details on occupational wages than either program provides individually. modeled wage estimates provide annual estimates of average hourly wages for occupations by selected job characteristics and within geographical location. the job characteristics include bargaining status (union and nonunion), part- and full-time work status, incentive- and time-based pay, and work levels by occupation.\n",
      "direct estimates are based on survey responses only from the particular geographic area to which the estimate refers. in contrast, modeled wage estimates use survey responses from larger areas to fill in information for smaller areas where the sample size is not sufficient to produce direct estimates. modeled wage estimates require the assumption that the patterns to responses in the larger area hold in the smaller area.\n",
      "the sample size for the ncs is not large enough to produce direct estimates by area, occupation, and job characteristic for all of the areas for which the oes publishes estimates by area and occupation. the ncs sample consists of 6 private industry panels with approximately 3,300 establishments sampled per panel, and 1,600 sampled state and local government units. the oes full six-panel sample consists of nearly 1.2 million establishments.\n",
      "the sample establishments are classified in industry categories based on the north american industry classification system (naics). within an establishment, specific job categories are selected to represent broader occupational definitions. jobs are classified according to the standard occupational classification (soc) system.\n",
      "content:\n",
      "summary: average hourly wage estimates for civilian workers in occupations by job characteristic and work levels. these data are available at the national, state, metropolitan, and nonmetropolitan area levels.\n",
      "frequency of observations: data are available on an annual basis, typically in may.\n",
      "data characteristics: all hourly wages are published to the nearest cent.\n",
      "acknowledgements:\n",
      "this dataset was taken directly from the bureau of labor statistics and converted to csv format.\n",
      "inspiration:\n",
      "this dataset contains the estimated wages of civilian workers in the united states. wage changes in certain industries may be indicators for growth or decline. which industries have had the greatest increases in wages? combine this dataset with the bureau of labor statistics consumer price index dataset and find out what kinds of jobs you would need to afford your snacks and instant coffee!\n",
      "content\n",
      "the supreme court database is the definitive source for researchers, students, journalists, and citizens interested in the united states supreme court. the database contains more than two hundred variables regarding each case decided by the court between the 1946 and 2015 terms. examples include the identity of the court whose decision the supreme court reviewed, the parties to the suit, the legal provisions considered in the case, and the votes of the justices. the database codebook is available here.\n",
      "acknowledgements\n",
      "the database was compiled by professor spaeth of washington university law and funded with a grant from the national science foundation.\n",
      "content\n",
      "the satellite database is a listing of active satellites currently in orbit around the earth. the database includes basic information about the satellites and their orbits, but does not contain the detailed information necessary to locate individual satellites. the information included in the database is publicly accessible and free and was collected from corporate, scientific, government, military, non-governmental, and academic websites available to the public. no copyrighted material was used, nor did we subscribe to any commercial databases for information.\n",
      "we have attempted to include all currently active satellites. however, satellites are constantly being launched, decommissioned, or simply abandoned, and the list may inadvertently contain some satellites that are no longer active but for which we have not yet received information.\n",
      "acknowledgements\n",
      "the satellite database is produced and updated quarterly by teri grimwood.\n",
      "context\n",
      "thousands gather at gobbler’s knob in punxsutawney, pennsylvania, on the second day of february to await the spring forecast from a groundhog known as punxsutawney phil. according to legend, if phil sees his shadow the united states is in store for six more weeks of winter weather. but, if phil doesn’t see his shadow, the country should expect warmer temperatures and the arrival of an early spring.\n",
      "acknowledgements\n",
      "the historical weather predictions were provided by the punxsutawney groundhog club, and the average monthly temperatures were published by noaa's national climatic data center.\n",
      "context:\n",
      "the new york philharmonic played its first concert on december 7, 1842. since then, it has merged with the new york symphony, the new/national symphony, and had a long-running summer season at new york's lewisohn stadium. the performance history database documents all known concerts of all of these organizations, amounting to more than 20,000 performances.\n",
      "content:\n",
      "dataset is a single csv with over 800k rows. data contains information on season, orchestra, venue, date, time, conductor, work title, composer, movement, and soloists.\n",
      "acknowledgements:\n",
      "this dataset was compiled by the new york philharmonic. original json files hosted here. original json files were flattened and joined on guid to form a single csv file. image courtesy of larisa birta.\n",
      "inspiration:\n",
      "nearly 175 years of performance history, covering over 11k unique works--which composers are most popular? have there been any trends in popularity by conductor or by season?\n",
      "the challenge\n",
      "the online sports gambling industry employs teams of data analysts to build forecast models that turn the odds at sports games in their favour. while several betting strategies have been proposed to beat bookmakers, from expert prediction models and arbitrage strategies to odds bias exploitation, their returns have been inconsistent and it remains to be shown that a betting strategy can outperform the online sports betting market. we designed a strategy to beat football bookmakers with their own numbers:\n",
      "\"beating the bookies with their own numbers - and how the online sports betting market is rigged\", by lisandro kaunitz, shenjun zhong and javier kreiner.\n",
      "here, we make the full dataset publicly available to the kaggle community. we also provide the codes, raw sql database and the online real-time dashboard that were used for our study on github.\n",
      "our strategy proved profitable in a 10-year historical simulation using closing odds, a 6-month historical simulation using minute to minute odds, and a 5-month period during which we staked real money with the bookmakers. we would like to challenge the kaggle community to improve our results:\n",
      "can your strategy consistently beat the sports betting market over thousands of bets across leagues around the world?\n",
      "do time series odds movements offer insightful information that a betting strategy can exploit?\n",
      "can you outperform the bookmakers’ predictions included in the odds data by creating a better model?\n",
      "what's inside the beat the bookie dataset\n",
      "10 year historical closing odds:\n",
      "479,440 football games from 818 leagues around the world\n",
      "games from 2005-01-01 to 2015-07-30.\n",
      "maximum, average and count of active odds at closing time (start of the match)\n",
      "betting odds from up to 32 providers\n",
      "details about the match: date and time, league, teams, 90-minute score\n",
      "14-months time series odds:\n",
      "92,647 football games from 1005 leagues around the world\n",
      "games from 2015-09-01 to 2016-11-22\n",
      "hourly sampled odds time series, from up to 32 bookmakers from 72 hours before the start of each game\n",
      "details about the match: date and time, league, teams, 90-minute score\n",
      "the dataset was assembled over months of scraping online sport portals.\n",
      "we hope you enjoy your sports betting simulations (but remember... the house always wins in the end).\n",
      "acknowledgements\n",
      "ben fulcher was of great help when we were drafting the paper. ben has also developed a very nice toolbox for time-series analysis, which might be relevant for the analysis of this dataset.\n",
      "this a blend dataset that contains historic swedish interest rates from 1908-2001 source/källa: sveriges riksbank and swedish inflation rate 1908-2001 fetched from sweden's statistic central bureau scb.\n",
      "content: blend of swedish historic central bank interest rate diskkonto and swedish scb consument price index\n",
      "acknowledgements / original data sets:\n",
      "swedish central bank interest rate diskkonto http://www.riksbank.se/sv/rantor-och-valutakurser/sok-rantor-och-valutakurser/\n",
      "consumer price index http://www.scb.se/sv_/hitta-statistik/statistik-efter-amne/priser-och-konsumtion/konsumentprisindex/konsumentprisindex-kpi/33772/33779/konsumentprisindex-kpi/33831/\n",
      "data set cover images: wikipedia https://sv.wikipedia.org/wiki/enkronan#/media/file:1_krona_1927,_1.jpg https://en.wikipedia.org/wiki/flag_of_sweden#/media/file:flag_of_sweden.svg\n",
      "inspiration: question: how does central bank interest rate effect inflation? what are the interest rate inflation rate delays? verify roc r^2 inflation/interest rate causation.\n",
      "content:\n",
      "interestrate and inflation sweden 1908-2001.csv\n",
      "columns\n",
      "period year\n",
      "central bank interest rate diskonto average percent\n",
      "inflation percent\n",
      "price level integer\n",
      "this dataset is taken from uci : https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
      "the dataset in uci is imbalanced. a balanced sample was taken from that dataset to create this. features remain the same as the original one.\n",
      "context\n",
      "the brazilian government compiles motor insurance data from ago/2000 to ago/2016 and makes it available for public consumption.\n",
      "content\n",
      "there's information about the performance of brazilian insurance motor like premium, claim and commission.\n",
      "acknowledgements\n",
      "susep is a governmental office responsible for collect, house and share this information: http://www2.susep.gov.br/menuestatistica/ses/principal.aspx\n",
      "inspiration\n",
      "this information can answer questions about performance and trends regarding to brazilian motor line of business and make user able to gain insight about this market.\n",
      "i know this data well but my inspiration to share this data on kaggle is to discuss and see different points of view.\n",
      "context\n",
      "audio classification is often proposed as mfcc classification problem. with this dataset, we intend to give attention to raw audio classification, as performed in the wavenet network.\n",
      "content\n",
      "the dataset consists in 50 wav files sampled at 16khz for 50 different classes.\n",
      "to each one of the classes, corresponds 40 audio sample of 5 seconds each. all of these audio files have been concatenated by class in order to have 50 wave files of 3 min. 20sec.\n",
      "in our example notebook, we show how to access the data and visualize a piece of it.\n",
      "acknowledgements\n",
      "we have not much credit in proposing the dataset here. much of the work have been done by the authors of the esc-50 dataset for environmental sound classification. in order to fit on kaggle, we processed the files with the to_wav.py file present in the original repository. you might also notice that we transformed the data from ogg to wav as the former didn't seem to be supported in anaconda.\n",
      "inspiration\n",
      "you might use this dataset to challenge your algorithms in classifying from raw audio ;)\n",
      "1.8 million positions of racing king chess variant\n",
      "racing kings is a popular chess variant.\n",
      "each player has a standard set of pieces without pawns. the opening setup is as below.\n",
      "in this game, check is entirely forbidden: not only is it forbidden to move ones king into check, but it is also forbidden to check the opponents king. the purpose of the game is to be the first player that moves his king to the eight row. when white moves their king to the eight row, and black moves directly after that also their king to the last row, the game is a draw (this rule is to compensate for the advantage of white that they may move first.) apart from the above, pieces move and capture precisely as in normal chess.\n",
      "to learn a little bit more about a game and to experience the evaluation of the position, you can play a couple of games here. do not forget to select racing kings chess variant and to analyse the game at the end with the machine. keep in mind that the evaluation score on lichess website is from -10 to 10 and slightly different than in my dataset.\n",
      "what you get:\n",
      "2 csv files train.csv and validate.csv with 1.5 mln and ~0.35 mln positions. both have an identical structure: fen of the position and the score.\n",
      "the score is real value in [-1, 1] range. the closer it is to 1/-1, the more probable is the win of a white/black player. due to the symmetry i will explain the score only for a white player (for black is the same just with a negative sign.\n",
      "1 means that white already won (the game is already finished)\n",
      "0.98 white has a guaranteed(*) win in maximum 1 move\n",
      "0.96 ... 2 moves\n",
      "0.94 ... 3 moves\n",
      "....\n",
      "0.82 ... in 9 moves\n",
      "0.80 ... in 10 or more moves\n",
      "from 0.4 to 0.8 - white has big advantage. for a good player it is not hard to win in such situation\n",
      "from 0.2 to 0.4 - white has some advantage. might be hard to convert it to a win\n",
      "from 0 to 0.2 - white has tiny advantage\n",
      "0 means that the position is either a draw or very close to a draw\n",
      "(*) guaranteed means that the machine has found a forced sequence of moves that allows white player to win no matter what moves the opponent will make. if the opponent makes the best moves - the game will finish in x moves, but it can finish faster if the black player makes a mistake.\n",
      "your goal is to use predict a score of the position knowing its fen.\n",
      "use train.csv to build your model and evaluate the performance on the validate.csv dataset (without looking/using it). i used mae score in my analysis.\n",
      "construction of the dataset\n",
      "dataset was constructed by me. i created a bot that plays many games against itself. the bot takes 1 second to analyse the position and selects the move based on the score of position. it took almost a month to generate these positions.\n",
      "what is the purpose?\n",
      "currently the plan is to use ml + reinforcement learning to build my own chess bot that will not use alpha-beta prunning for position evaluation and self-play. in a couple of days i will release my own findings as kernels.\n",
      "the following dataset contains data on blog posts from marginalrevolution.com. for posts from jan. 1, 2010 to 9/17/2016, the following attributes are gathered.\n",
      "author name\n",
      "post title\n",
      "post date\n",
      "post content (words)\n",
      "number of words in post\n",
      "number of comments in post\n",
      "dummy variable for several commonly used categories\n",
      "the data was scraped using python's beautiful soup package, and cleaned in r. see my github page (https://github.com/wnowak10/) for the python and r code.\n",
      "gives a bunch of data regarding parkinson's disease. some of these variables have a very high correlation with each other.\n",
      "context\n",
      "i was curious about the hot topics in quantum physics as reflected by the quant-ph category on arxiv. citation counts have a long lag, and so do journal publications, and i wanted a more immediate measure of interest. scirate is fairly well known in this community, and i noticed that after the initial two-three weeks, the number of scites a paper gets hardly increases further. so the number of scites is both immediate and near constant after a short while.\n",
      "content\n",
      "the main dataset (scirate_quant-ph.csv) is the metadata of all papers published in quant-ph between 2012-01-01 and 2016-12-31 that had at least ten scites, as crawled on 2016-12-31. it has six columns:\n",
      "the id column as exported by pandas.\n",
      "the arxiv id.\n",
      "the year of publication.\n",
      "the month of publication.\n",
      "the day of publication.\n",
      "the number of scites (this column defines the order).\n",
      "the title.\n",
      "all authors separates by a semicolon.\n",
      "the abstract.\n",
      "the author names were subjected to normalization and the chances are high that the same author only appears with a unique name.\n",
      "the name normalization was the difficult part in compiling this collection, and this is why the number of scites was lower bounded. a second file (scirate_quant-ph_unnormalized.csv) includes all papers that appeared between 2012-2016 irrespective of the number of scites, but the author names are not normalized. the actual number of scites for each paper may show a slight variation between the two datasets because the unnormalized version was compiled more than a month later.\n",
      "acknowledgements\n",
      "many thanks to scirate for tolerating my crawling trials and not blacklisting my ip address.\n",
      "inspiration\n",
      "unleash topic models and author analysis to find out what or who is hot in quantum physics today. build a generative model to write trendy fake titles like snarxiv does it for hep-th.\n",
      "creating a rental market database for data analysis and machine learning.\n",
      "how does it work ?\n",
      "you scrape the property ads (sale or rent) on internet and you get a dataset.\n",
      "then 3 fancy solutions are possible:\n",
      "run your webcrawler everyday for a specific place, upload the data in your data warehouse, and monitor the trends in real estate market prices.\n",
      "apply machine learning to your database and get a sense of the relative expensiveness of the properties.\n",
      "localize every property ads on a google map using color-coded points in order to visualize the most cheap and expensive neighborhoods.\n",
      "original data source\n",
      "for the sake of example, and for proximity reasons, we fetched information from a mid-sized swiss city, called lausanne, based in the south of switzerland. the country has the particularity that people get often puzzled by the level of prices swarming almost everywhere in the rental markets. this is mostly related to the very high living standards prevailing over here. so we used one of the public property ads available in this french-speaking part of the country : https://www.homegate.ch/\n",
      "because the booming swiss housing market is mainly a rental market (foreign investments have been riding high for the sales of property, and mortgage loans are closed to record low), i focused on real estate for rent ads in the homegate website.\n",
      "building a webcrawler\n",
      "in the kernels section, you will find out how the python looks like. i used beautifulsoup and urllib python libraries to grab data from the website. as you can figure out, the code is simple, but really efficient.\n",
      "what you get\n",
      "in this example, i extracted data as of 03/17/2017, and i named the dataframe \"output\", available in csv format to make the data compatible with most commonly preferred tools for analysis. it allows you to get a dataframe with 12 columns:\n",
      "the date\n",
      "is it a rent or a buy\n",
      "the location\n",
      "the address of the property\n",
      "the zip code\n",
      "the available description of the property\n",
      "the number of rooms\n",
      "the surface\n",
      "the floor\n",
      "the price\n",
      "the source\n",
      "machine learning\n",
      "in the kernels section, you will see a very simple ml algorithm applied to the dataset in order to the \"theoretical\" price of each asset, at the end of the code. for the sake of simplicity, i ran a very straightforward linear regression using only 3 features (the 3 only quantitative factors i have at hand) :\n",
      "the number of rooms\n",
      "the floor\n",
      "the surface\n",
      "i know what you're thinking right at the moment : those 3 features can barely explain the price of a property. other determinants, such as the location, the neighborhood, the fact that it is outdated, badly maintained by a students roommate partying every night, ... , are of interest when it comes to assessing an appartment. but straightaway, i reduced the model to this.\n",
      "google map display of the property ads and their relative expensiveness\n",
      "cf capture.png file\n",
      "upcoming improvements\n",
      "add new features to machine learning process, especially a dummy variable accounting for the neighborhood to which the property pertains.\n",
      "see to what extent a logistic regression could overcome a linear regressor.\n",
      "test more complex machine learning algorithms.\n",
      "display trends in rental property prices, for each neighborhood, after establishing a larger database (with a few weeks of scraped data).\n",
      "context\n",
      "i got all these .csv files using pandas data reader but getting every single kospi data through pandas data reader is annoying. so i decided to share this files.\n",
      "content\n",
      "files\n",
      "kospi.csv contains average kospi price. you can use this for checking whether if korean stock is day-off or not. xxxxxx.csv contains each single price records. xxxxxx is it's unique ticker.\n",
      "columns\n",
      "date\n",
      "format - \\d{4}-\\d{2}-\\d{2}\n",
      "open\n",
      "format - \\d{1,}\\.\\d{1}\n",
      "high\n",
      "format - \\d{1,}\\.\\d{1}\n",
      "low\n",
      "format - \\d{1,}\\.\\d{1}\n",
      "close\n",
      "format - \\d{1,}\\.\\d{1}\n",
      "adj close\n",
      "format - \\d{1,}\\.\\d{1}\n",
      "volume\n",
      "format - \\d+\n",
      "acknowledgements\n",
      "blog post which describes how i got these data's. you might need this to update csv files.\n",
      "git repository git repository\n",
      "inspiration\n",
      "good luck.\n",
      "context\n",
      "if you're in northern california, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. across the world in south korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. india’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. some of our strongest geographic and cultural associations are tied to a region's local foods.\n",
      "this dataset was featured in our completed playground competition entitled what's cooking? the objective of the competition was to predict the category of a dish's cuisine given a list of its ingredients.\n",
      "content\n",
      "the data are stored in json format.\n",
      "train.json - the training set containing recipes id, type of cuisine, and list of ingredients\n",
      "test.json - the test set containing recipes id, and list of ingredients\n",
      "an example of a recipe node in train.json can be found here or in the file preview section below.\n",
      "acknowledgements\n",
      "this unique dataset was provided by yummly and featured in a kaggle playground competition for fun and practice. visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. if you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!\n",
      "introduction\n",
      "on 9 august 2017, the treasurer, under the census and statistics act 1905, directed the australian statistician to collect and publish statistical information from all eligible australians on the commonwealth electoral roll, about their views on whether or not the law should be changed to allow same-sex couples to marry.\n",
      "the voluntary survey asked one question: should the law be changed to allow same-sex couples to marry? respondents were asked to mark one box – yes or no – on the survey form.\n",
      "survey materials were mailed to eligible australians on the commonwealth electoral roll as at 24 august 2017.\n",
      "a range of strategies were implemented to assist all eligible australians who wished to complete the survey to do so. a survey response was received from 12,727,920 (79.5%) eligible australians.\n",
      "the abs implemented robust systems and controls for the processing, coding and publication of statistical data. detailed information on the systems used as well as the accuracy and integrity of the data is available in the survey process and the quality and integrity statement.\n",
      "the official statistics include a count of responses (yes, no and response not clear) by federal electoral division (fed), state/territory and national. this also includes a count of eligible australians who have not participated in the survey.\n",
      "information from the commonwealth electoral roll has been used to independently produce a participation rate by age and gender for each fed, state/territory and national. this rate has been published by gender, for each of the following age groups: 18-19 years, 20-24 years, 25-29 years, 30-34 years, 35-39 years, 40-44 years, 45-49 years, 50-54 years, 55-59 years, 60-64 years, 65-69 years, 70-74 years, 75-79 years, 80-84 years, and 85+ years.\n",
      "national results\n",
      "should the law be changed to allow same-sex couples to marry?\n",
      "of the eligible australians who expressed a view on this question, the majority indicated that the law should be changed to allow same-sex couples to marry, with 7,817,247 (61.6%) responding yes and 4,873,987 (38.4%) responding no. nearly 8 out of 10 eligible australians (79.5%) expressed their view.\n",
      "all states and territories recorded a majority yes response. 133 of the 150 federal electoral divisions recorded a majority yes response, and 17 of the 150 federal electoral divisions recorded a majority no response.\n",
      "data source\n",
      "all data presented here comes from the official abs website: https://marriagesurvey.abs.gov.au/\n",
      "this data was cleaned by myles o'neill and richard dear to make it easier to work with.\n",
      "tatoeba sentences corpus\n",
      "this data is directly from the tatoeba project: https://tatoeba.org/ it is a large collection of sentences in multiple languages. many of the sentences are contained with translations in multiple languages. it is a valuable resource for machine translation and many natural language processing projects.\n",
      "context\n",
      "sadly, seoul, south korea has some of the most polluted air in the world. since seoul also represents 25-50% of the south korean population, the air quality is a concern to many.\n",
      "it used to be that in korea, we have bad air quality in spring (yellow wind blowing from the chinese yellow river), and clear air in autumn. now with more industries in china, the air is getting worse in korea in a different seasonality pattern. this is known as asian dust.\n",
      "content\n",
      "hourly measurement on several air pollutants in dozens of districts in seoul.\n",
      "acknowledgements\n",
      "data downloaded from here. http://data.seoul.go.kr/openinf/sheetview.jsp?infid=oa-2275&tmenu=11 we thank seoul open data plaza for making the datasets available. http://english.seoul.go.kr/policy-information/key-policies/informatization/seoul-open-data-plaza/\n",
      "the banner photos are via jeonguk ha on unsplash\n",
      "inspiration\n",
      "recently, fine dusts are posing a big problem in korea. https://www.ft.com/content/b49a9878-141b-11e7-80f4-13e067d5072c\n",
      "context:\n",
      "this dataset contains survey responses to a survey that people could complete when they signed up for the 5-day data challenge.\n",
      "on december 12, 2017 survey responses for the second 5-day data challenge were added. for this version of the challenge, participants could sign up for either an intro version or a more in-depth regression challenge.\n",
      "content:\n",
      "the optional survey included four multiple-choice questions:\n",
      "have you ever taken a course in statistics?\n",
      "yep\n",
      "yes, but i've forgotten everything\n",
      "nope\n",
      "do you have any previous experience with programming?\n",
      "nope\n",
      "i have a little bit of experience\n",
      "i have quite a bit of experience\n",
      "i have a whole lot of experience\n",
      "what's your interest in data science?\n",
      "just curious\n",
      "it will help me in my current job\n",
      "i want to get a job where i use data science\n",
      "other\n",
      "just for fun, do you prefer dogs or cat?\n",
      "dogs 🐶\n",
      "cats 🐱\n",
      "both 🐱🐶\n",
      "neither 🙅\n",
      "in order to protect privacy, the data has been shuffled (so there’s no temporal order to the responses) and a random 2% of the data has been removed (so even if you know that someone completed the survey, you cannot be sure that their responses are included in this dataset). in addition, all incomplete responses have been removed, and any text entered in the “other” free response field has been replaced with the text “other”.\n",
      "acknowledgements:\n",
      "thanks to everyone who completed the survey! :)\n",
      "inspiration:\n",
      "is there a relationship between how much programming experience someone has and why they’re interested in data science?\n",
      "are more experienced programmers more likely to have taken statistics?\n",
      "do people tend to prefer dogs, cats, both or neither? is there a relationship between what people prefer and why they’re interested in data science?\n",
      "context\n",
      "as a huge lotr fan, i was excited to have acquired this character data from the lord of the rings wiki. i scraped this data using f#; the repository can be found here: https://github.com/mokosan/fsharpadvent.\n",
      "content\n",
      "data consists of character names, the url in the wiki and the respective race.\n",
      "acknowledgements\n",
      "wouldn't have been able to publish this data set unless it was for the work done by the great people of the wiki page.\n",
      "context\n",
      "this data set is a collection of all starcraft pro-player 2 matches. the data is taken from the site - http://aligulac.com/\n",
      "content\n",
      "dataset data - 18 october 2017. you can parse actual data. just use my script (github)\n",
      "this dataset contains 10 variables:\n",
      "match_date -date of match in format mm/dd/yyyy\n",
      "player_1 - player 1 nickname\n",
      "player_1_match_status - match status for player 1: winner or loser\n",
      "score - match score (example: 1-0, 1-2 etc)\n",
      "player_2 - player 2 nickname\n",
      "player_2_match_status - match status for player 2: winner or loser\n",
      "player_1_race - player 1 race: z - zerg, p - protoss, t - terran\n",
      "player_2_race - player 2 race: z - zerg, p - protoss, t - terran\n",
      "addon - game addon: wol- wings of liberty, hots - heart of the swarm, lotv - legacy of the void\n",
      "tournament_type - online or offline\n",
      "acknowledgements\n",
      "the source is http://aligulac.com/\n",
      "inspiration\n",
      "questions worth exploring:\n",
      "predict the outcome of a match between two players\n",
      "or whatever you want ....\n",
      "i work with uk company information on a daily basis, and i thought it would be useful to publish a list of all active companies, in a way that could be used for machine learning.\n",
      "there are 3,801,733 rows in the dataset, one for each active company. the postcode which is included in the dataset has been geolocated, and the resultant latitude and longitudes have been included, along with the standard industrial classification code, and date of incorporation.\n",
      "the company list is from the publicly available 1st november 2017 companies house snapshot.\n",
      "the postcode geolocations and sic codes are from the gov.uk website.\n",
      "in the file allcompanies.csv each row is formatted as follows:\n",
      "companynumber - in the format of 99999999 for england/wales, sc999999 for scotland and ni999999 for northern ireland.\n",
      "incorporationdate - in british date format, dd/mm/yyyy\n",
      "registeredaddresspostcode - standard british format postcode\n",
      "latitude - to 6 decimal places\n",
      "longitude - to 6 decimal places\n",
      "sic - 5 digits or if not known, none - see separate file for description of each code.\n",
      "inspiration possible uses for this data is to see where certain types of companies are located in the uk, and how over time they multiply and spread throughout the uk.\n",
      "training ml algorithms to predict where there are a high (or low) density of certain types of companies, and where would be a good area for a company to be located, if it wanted minimal competition, or the inverse, where there are clusters of high densities, where it might be easier to recruit specialised staff.\n",
      "a useful addition would be to overlay population density, which i am currently working on as an option for this dataset.\n",
      "i am sure there are many more possible uses for this data in ways, that i cannot imagine.\n",
      "this is my first go at publishing a dataset on any medium, so any useful tips and hints would be extremely welcome.\n",
      "links to the raw data sources are here:\n",
      "companies house http://download.companieshouse.gov.uk/en_output.html\n",
      "postcode to geolocation https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk\n",
      "sic codes https://www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic\n",
      "context\n",
      "scielo (scientific electronic library online) is an international research communication program launched in 1998 and implemented through a decentralized network of national collections of peer reviewed journals from 15 countries – 12 from latin america, portugal, spain and south africa – that jointly publish over 1 thousand journals and about 50 thousand articles per year. a thematic collection on public health is also operated by the scielo program. all collections are accessible via the network portal – http://www.scielo.org.\n",
      "scielo aims at the progress of research through the improvement of peer reviewed journals from all disciplines published by scientific and professional associations, academic institutions and public or private research and development institutions. the specific objectives are to increase in a sustainable way the quality, visibility, usage, impact and credibility of the indexed journals and the research they communicate. a key characteristic of scielo is multilingual publishing, so journals can publish articles in one or multiple languages including the simultaneous publishing of the same article in more than one language.\n",
      "scielo program develops itself according to three principles. first, the conception that scientific knowledge is a public good and therefore should be available openly in the web. second, the network operation envisaging to strengthen collaboration and interchange of information and experience, creating scale and lessen the costs. third, quality control as an essential policy and practice at the level of articles, journals and collections, adoption and compliance with bibliographic and interoperability standards.\n",
      "scielo operation and development are carried out following three main action lines. the first is professionalization, which means to produce journals according to the state of art. the second is internationalization, which means to strengthen the active participation of scielo journals and program in the international flow of scientific information. the third is operational and financial sustainability, which means to develop conditions to assure journals to be published on time with a well-established financial model.\n",
      "all collections follow the scielo publishing model, which comprises three main functions. first, the indexing of journals with metadata of articles, including the bibliographic references of the indexed articles and of the articles they cite. second, the full text of articles which are available in html, pdf and progressively in xml jats compatible according the scielo publishing schema. third, the dissemination and interoperability of journals and articles with bibliographic indexes and systems.\n",
      "scielo brazil, led by scielo / fapesp program, acts as the scielo network secretariat and coordinates the maintenance of the methodological and technological platform, while the operation of the network collections and journals are decentralized and led by national research agencies.\n",
      "content\n",
      "this dataset contains publication and access reports for the documents of the scielo brazil collection between 1998 and october 2017.\n",
      "the reports present metadata of journals, totals of issues and published documents, thematic areas, authors' affiliation, bibliographic references, use licenses and more. further details can be found at http://docs.scielo.org/projects/scielo-processing/pt/latest/public_reports.html (in portuguese, with notes in english).\n",
      "context\n",
      "the dataset available for download on this webpage represents a 5x5x5µm section taken from the ca1 hippocampus region of the brain, corresponding to a 1065x2048x1536 volume. the resolution of each voxel is approximately 5x5x5nm.\n",
      "content\n",
      "two image datasets in 3d of electron microscopy data with accompanying labels. the data is provided as multipage tif files that can be loaded in fiji, r, knime, or python\n",
      "acknowledgements\n",
      "the dataset was copied from http://cvlab.epfl.ch/data/em directly and only placed here to utilize the kaggle's kernel and forum capabilities. please acknowledge the cv group dataset for publication or any other uses\n",
      "data citations\n",
      "a. lucchi y. li and p. fua, learning for structured prediction using approximate subgradient descent with working sets, conference on computer vision and pattern recognition, 2013.\n",
      "a. lucchi, k.smith, r. achanta, g. knott, p. fua, supervoxel-based segmentation of mitochondria in em image stacks with learned shape features, ieee transactions on medical imaging, vol. 30, nr. 11, october 2011.\n",
      "challenges\n",
      "how accurately can the segmentation be performed with neural networks?\n",
      "is 3d more accurate than 2d for segmentation?\n",
      "how can mistakes critical to structure or connectivity be penalized more heavily, how would a standard roc penalize them?\n",
      "lelú is a french dialog corpus that contains a rich collection of human-human, spontaneous written conversations, extracted from reddit’s public dataset available through google bigquery. our corpus is composed of 556,621 conversations with 1,583,083 utterances in total. the code to generate this dataset can be found in our github repository.\n",
      "the archive spf.tar.gz contains reddit discussions in an xml file with the following format:\n",
      "<dialog>\n",
      "    <s link_id=\"4rqtz\" subreddit_id=\"2qhjz\">\n",
      "        <utt uid=\"1\" comment_id=\"123\" parent_id=\"4rqtz\" score=\"1\" create_utc=\"12458129356\">hey, how are you?</utt>\n",
      "        <utt uid=\"2\" comment_id=\"124\" parent_id=\"123\" score=\"1\" create_utc=\"12458129486\">i’m fine thank you!</utt>\n",
      "        <utt uid=\"1\" comment_id=\"125\" parent_id=\"124\" score=\"1\" create_utc=\"12458139804\">nice!</utt>\n",
      "    </s>\n",
      "    <s link_id=\"8y1br\" subreddit_id=\"2qhjz\">\n",
      "        <utt uid=\"1\" comment_id=\"126\" parent_id=\"124\"  score=\"1\" create_utc=\"12458129310\">who’s around for lunch?</utt>\n",
      "        <utt uid=\"2\" comment_id=\"127\" parent_id=\"126\" score=\"1\" create_utc=\"12458139345\">me!</utt>\n",
      "        <utt uid=\"3\" comment_id=\"128\" parent_id=\"127\" score=\"1\" create_utc=\"12458149382\">me too!</utt>\n",
      "    </s>\n",
      "</dialog>\n",
      "the tag attributes can be described as follows:\n",
      "link_id: id of the parent reddit post.\n",
      "subreddit_id: id of the subreddit.\n",
      "uid: id of the comment author.\n",
      "comment_id: id of the reddit comment.\n",
      "parent_id: id of the parent reddit comment.\n",
      "we have split up the conversation trees into short sequential conversations using a heuristic described in our paper, lelú: a french dialog corpus from reddit, however the full conversation trees can be reconstructed using the comment_id and parent_id attributes of the <utt> tag.\n",
      "data was collected from the following subreddits: /r/france, /r/francaiscanadien, /r/truefrance, /r/paslegorafi, and /r/rance.\n",
      "nan\n",
      "this is the dataset vgdb-2016 built for the paper \"from impressionism to expressionism: automatically identifying van gogh's paintings\", which has been published on the 23rd ieee international conference on image processing (icip 2016).\n",
      "to the best of our knowledge, this is the very first public and open dataset with high quality images of paintings, which also takes density (in pixels per inch) into consideration. the main research question we wanted to address was: is it possible to distinguish vincent van gogh's paintings from his contemporaries? our method achieved a f1-score of 92.3%.\n",
      "there are many possibilities for future work, such as:\n",
      "increase the dataset. this includes wikimedia commons and wikiart. unfortunately, google art project does not allow downloads.\n",
      "deal with density normalization. there is a lot of data available without such normalization (e.g., painting-91 and painter by numbers). it is possible analyze how this affects accuracy.\n",
      "experiment with multi-class and open-set recognition.\n",
      "try to identify the painting style, movement, or school.\n",
      "maybe study painting authorship verification: given two paintings, are they from the same author?\n",
      "is it possible to detect artificially generated paintings? are they useful for dataset augmentation?\n",
      "the paper is available at ieee xplore (free access until october 6, 2016): https://dx.doi.org/10.1109/icip.2016.7532335\n",
      "the dataset has been originally published at figshare (cc by 4.0): https://dx.doi.org/10.6084/m9.figshare.3370627\n",
      "the source code is available at github (apache 2.0): https://github.com/gfolego/vangogh\n",
      "if you find this work useful in your research, please cite the paper! :-)\n",
      "@inproceedings{folego2016vangogh,\n",
      "    author = {guilherme folego and otavio gomes and anderson rocha},\n",
      "    booktitle = {2016 ieee international conference on image processing (icip)},\n",
      "    title = {from impressionism to expressionism: automatically identifying van gogh's paintings},\n",
      "    year = {2016},\n",
      "    month = {sept},\n",
      "    pages = {141--145},\n",
      "    doi = {10.1109/icip.2016.7532335}\n",
      "}\n",
      "keywords: art; feature extraction; painting; support vector machines; testing; training; visualization; cnn-based authorship attribution; painter attribution; data-driven painting characterization\n",
      "summary & usage license\n",
      "movielens data sets were collected by the grouplens research project at the university of minnesota.\n",
      "this data set consists of: * 100,000 ratings (1-5) from 943 users on 1682 movies. * each user has rated at least 20 movies. * simple demographic info for the users (age, gender, occupation, zip)\n",
      "the data was collected through the movielens web site (movielens.umn.edu) during the seven-month period from september 19th, 1997 through april 22nd, 1998. this data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. detailed descriptions of the data file can be found at the end of this file.\n",
      "neither the university of minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. the data set may be used for any research purposes under the following conditions:\n",
      " * the user may not state or imply any endorsement from the\n",
      "   university of minnesota or the grouplens research group.\n",
      "\n",
      " * the user must acknowledge the use of the data set in\n",
      "   publications resulting from the use of the data set\n",
      "   (see below for citation information).\n",
      "\n",
      " * the user may not redistribute the data without separate\n",
      "   permission.\n",
      "\n",
      " * the user may not use this information for any commercial or\n",
      "   revenue-bearing purposes without first obtaining permission\n",
      "   from a faculty member of the grouplens research project at the\n",
      "   university of minnesota.\n",
      "if you have any further questions or comments, please contact grouplens .\n",
      "citation\n",
      "to acknowledge use of the dataset in publications, please cite the following paper:\n",
      "f. maxwell harper and joseph a. konstan. 2015. the movielens datasets: history and context. acm transactions on interactive intelligent systems (tiis) 5, 4, article 19 (december 2015), 19 pages. doi=http://dx.doi.org/10.1145/2827872\n",
      "acknowledgements\n",
      "thanks to al borchers for cleaning up this data and writing the accompanying scripts.\n",
      "published work that has used this dataset\n",
      "herlocker, j., konstan, j., borchers, a., riedl, j.. an algorithmic framework for performing collaborative filtering. proceedings of the 1999 conference on research and development in information retrieval. aug. 1999.\n",
      "further information about the grouplens research project\n",
      "the grouplens research project is a research group in the department of computer science and engineering at the university of minnesota. members of the grouplens research project are involved in many research projects related to the fields of information filtering, collaborative filtering, and recommender systems. the project is lead by professors john riedl and joseph konstan. the project began to explore automated collaborative filtering in 1992, but is most well known for its world wide trial of an automated collaborative filtering system for usenet news in 1996. the technology developed in the usenet trial formed the base for the formation of net perceptions, inc., which was founded by members of grouplens research. since then the project has expanded its scope to research overall information filtering solutions, integrating in content-based methods as well as improving current collaborative filtering technology.\n",
      "further information on the grouplens research project, including research publications, can be found at the following web site:\n",
      "    http://www.grouplens.org/\n",
      "grouplens research currently operates a movie recommender based on collaborative filtering:\n",
      "    http://www.movielens.org/\n",
      "detailed descriptions of data files\n",
      "here are brief descriptions of the data.\n",
      "ml-data.tar.gz -- compressed tar file. to rebuild the u data files do this: gunzip ml-data.tar.gz tar xvf ml-data.tar mku.sh\n",
      "u.data -- the full u data set, 100000 ratings by 943 users on 1682 items. each user has rated at least 20 movies. users and items are numbered consecutively from 1. the data is randomly ordered. this is a tab separated list of user id | item id | rating | timestamp. the time stamps are unix seconds since 1/1/1970 utc\n",
      "u.info -- the number of users, items, and ratings in the u data set.\n",
      "u.item -- information about the items (movies); this is a tab separated list of movie id | movie title | release date | video release date | imdb url | unknown | action | adventure | animation | children's | comedy | crime | documentary | drama | fantasy | film-noir | horror | musical | mystery | romance | sci-fi | thriller | war | western | the last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once. the movie ids are the ones used in the u.data data set.\n",
      "u.genre -- a list of the genres.\n",
      "u.user -- demographic information about the users; this is a tab separated list of user id | age | gender | occupation | zip code the user ids are the ones used in the u.data data set.\n",
      "u.occupation -- a list of the occupations.\n",
      "u1.base -- the data sets u1.base and u1.test through u5.base and u5.test u1.test are 80%/20% splits of the u data into training and test data. u2.base each of u1, ..., u5 have disjoint test sets; this if for u2.test 5 fold cross validation (where you repeat your experiment u3.base with each training and test set and average the results). u3.test these data sets can be generated from u.data by mku.sh. u4.base u4.test u5.base u5.test\n",
      "ua.base -- the data sets ua.base, ua.test, ub.base, and ub.test ua.test split the u data into a training set and a test set with ub.base exactly 10 ratings per user in the test set. the sets ub.test ua.test and ub.test are disjoint. these data sets can be generated from u.data by mku.sh.\n",
      "allbut.pl -- the script that generates training and test sets where all but n of a users ratings are in the training data.\n",
      "mku.sh -- a shell script to generate all the u data sets from u.data.\n",
      "context\n",
      "this dataset is partly associated to the \"hand tremor based biometric recognition using leap motion device\" paper (doi: 10.1109/access.2017.2764471 ). objective is to investigate whether hand jitter can be treated as a new behavioral biometric recognition trait in the filed od security so that imitating and/or reproducing artificially can be avoided.\n",
      "content\n",
      "dataset contains five subjects. 1024 samples each subject's spatiotemporal hand tremor signals as a time series data were acquired via leap motion device. features are x, y, z and mixed (average) channels. channel represents displacement value of adjacent frames (difference between current and previous positions) and finally the last item is class label having value from 1 to 5.\n",
      "acknowledgements\n",
      "i would like to thanks to our volunteer donor who provides us valuable hand tremor data.\n",
      "inspiration\n",
      "please read the \"hand tremor based biometric recognition using leap motion device\" paper for more details and feature extraction methods. if you have any questions related to the preprocessing and/or processing the dataset please do not hesitate to contact with me via e-mail: hakmesyo@gmail.com . it should be noted that, data acquisition software was implemented in java (netbeans) and i utilized processing, open cezeri library and weka tools alongside.\n",
      "periodic table of elements & minerals with known and hidden relationships to stocks, etfs & options for additional signal boosting built with starmine.ai http://starmine.ai\n",
      "rows contain stock symbols. columns contain scores that represent known and hidden relationships between elements and stocks.\n",
      "how the scoring is calculated\n",
      "what kind of things can be done with custom concept columns/features?\n",
      "create unique sectors or clusters based on concepts and hidden relationships and compare their gains to the s&p\n",
      "determine if price correlations have similar concept or keyword correlations\n",
      "examine symbiotic, parasitic and sympathetic relationships between equities\n",
      "automatically create baskets of stocks based on concepts and/or keywords\n",
      "detach the custom columns and append them to other proprietary inhouse datasets\n",
      "select a data context (e.g. biological, chemical, geophysical and others) to derive different signals\n",
      "use stock symbols as custom concept column labels and model cross-correlations between equities\n",
      "create features using trending terms anywhere on the internet\n",
      "ofcom annual reports on the uk’s fixed broadband, mobile and wifi networks, digital television, digital radio and internet infrastructure.\n",
      "ofcom gathers data from the main fixed broadband internet service providers (bt, kcom, sky, talktalk and virgin media) on both their retail services and the services they provide to other isps as a wholesale service. more information can be found here.\n",
      "gla connectivity map shows a summary version of the download speed data. next generation broadband map - https://maps.london.gov.uk/webmaps/nextgenbroadband/\n",
      "content\n",
      "xiangqi, also known as chinese chess, is one of the most popular board game in china and southeastern asia that is played by millions of people every single day. more information on the rules and the history of xiangqi can be found from the xiangqi wikipedia page\n",
      "the dataset contains 10,000 game logs of blitz xiangqi played on playok.com, scraped off playok api with python. in particular, the games in the dataset have id numbers between 57380690 and 57390689. the game records are stored in two separate files:\n",
      "gameinfo.csv which contains players information and game result in\n",
      "gameid\n",
      "game_datetime\n",
      "blackid\n",
      "blackelo\n",
      "redid\n",
      "redelo\n",
      "winner\n",
      "moves.csv which contains game moves in\n",
      "gameid\n",
      "turn: a number denoting at which turn of the game the move was made.\n",
      "side\n",
      "move: moves are recorded with the wxf notation. explainations can be found at xqinenglish.com\n",
      "acknowledgements\n",
      "data is scraped from the playok.com game logs api. cover photo is from rosino under cc by-sa 2.0.\n",
      "misc.\n",
      "there are millions of game logs on playok.com but i decided to cut the data off at 10,000 games due to file size. if you need more games, check the github repository of my online xiangqi scraper.\n",
      "context\n",
      "for many years airplanes have been considered the second safest transport mean in the world - losing just to elevators. traveling great distances in short time, those aircrafts have brought several advantaged for the world, both in commercial and regular application. unfortunately, as any transport mean, aircrafts have their own count of tragedies. the last event envolving airplanes - to the publication date - was the accident envolving the brazilian soccer team chapecoense and a lamia's aircraft, which was transporting them to colombia for a championship. this tragedy brought back discussions and controversies about aircraft's security and human capacity during aeronautics occurrences.\n",
      "content\n",
      "this dataset was available by cenipa - centro de investigação e prevenção de acidentes aeronáuticos - or aeronautical accidents investigation and prevention center. such files contains informations about occurrences which envolved aircrafts in the last 10 years. you may access more updated data by visiting brazilian open data's official website, or clicking in the download links below.\n",
      "acknowledgements\n",
      "this dataset is available for studies and analysis thanks to cenipa.\n",
      "aggregated visitor interests compiled from https://www.jc-bingo.com/ web access logs. includes visitor ip address, user-agent string, visitor country, accessed page languages and topics. dataset is disseminated free of charge and used in basic customer preferences predictive modeling.\n",
      "methodology\n",
      "this is a data dump of the top 100 products (ordered by number of mentions) from every subreddit that has posted an amazon product. the data was extracted from google bigquery's reddit comment database. it only extracts amazon links, so it is certainly a subset of all products posted to reddit.\n",
      "the data is organized in a file structure that follows:\n",
      "reddits/<first lowercase letter of subreddit>/<subreddit>.csv\n",
      "an example of where to find the top products for /r/watches would be:\n",
      "reddits/w/watches.csv\n",
      "definitions\n",
      "below are the column definitions found in each <subreddit>.csv file.\n",
      "name the name of the product as found on amazon.\n",
      "category the category of the product as found on amazon.\n",
      "amazon_link the link to the product on amazon.\n",
      "total_mentions the total number of times that product was found on reddit.\n",
      "subreddit_mentions the total number of times that product was found on that subreddit.\n",
      "want more?\n",
      "you can search and discover products more easily on thingsonreddit\n",
      "acknowledgements\n",
      "this dataset was published by ben rudolph on github, and was republished as-is on kaggle.\n",
      "context\n",
      "this data set contains a list of video games with sales greater than 100,000 copies along with critic and user ratings. it is a combined web scrape from vgchartz and metacritic along with manually entered year of release values for most games with a missing year of release. the original coding was created by rush kirubi and can be found here, but it limited the data to only include a subset of video game platforms. not all of the listed video games have information on metacritic, so there data set does have missing values.\n",
      "content\n",
      "the fields include:\n",
      "name - the game's name\n",
      "platform - platform of the games release\n",
      "year_of_release - year of the game's release\n",
      "genre - genre of the game\n",
      "publisher - publisher of the game\n",
      "na_sales - sales in north america (in millions)\n",
      "eu_sales - sales in europe (in millions)\n",
      "jp_sales - sales in japan (in millions)\n",
      "other_sales - sales in the rest of the world (in millions)\n",
      "global_sales - total worldwide sales (in millions)\n",
      "critic_score - aggregate score compiled by metacritic staff\n",
      "critic_count - the number of critics used in coming up with the critic score\n",
      "user_score - score by metacritic's subscribers\n",
      "user_count - number of users who gave the user score\n",
      "rating - the esrb ratings\n",
      "acknowledgements\n",
      "again the main credit behind this data set goes to rush kirubi. i just commented out two lines of his code.\n",
      "also the original inspiration for this data set came from gregory smith who originally scraped the data from vgchartz, it can be found here.\n",
      "context\n",
      "swedish crime statistics from 1950 to 2015\n",
      "content\n",
      "this data set contains statistics on reported crimes in sweden (by 100.000) from 1950 to 2015. it contains the following columns:\n",
      "crimes.total: total number of reported crimes\n",
      "crimes.penal.code: total number of reported crimes against the criminal code\n",
      "crimes.person: total number of reported crimes against a person\n",
      "murder: total number of reported murder\n",
      "sexual.offences: total number of reported sexual offences\n",
      "rape: total number of reported rapes\n",
      "assault: total number of reported aggravated assaults\n",
      "stealing.general: total number of reported crimes involving stealing or robbery\n",
      "robbery: total number of reported armed robberies\n",
      "burglary: total number of reported armed burglaries\n",
      "vehicle.theft: total number of reported vehicle thefts\n",
      "house.theft: total number of reported theft inside a house\n",
      "shop.theft: total number of reported theft inside a shop\n",
      "out.of.vehicle.theft: total number of reported theft from a vehicle\n",
      "criminal.damage: total number of reported criminal damages\n",
      "other.penal.crimes: number of other penal crime offenses\n",
      "fraud: total number of reported frauds\n",
      "narcotics: total number of reported narcotics abuses\n",
      "drunk.driving: total number of reported drunk driving incidents\n",
      "year: the year\n",
      "population: the total estimated population of sweden at the time\n",
      "acknowledgements\n",
      "raw data taken from: https://www.bra.se/bra/bra-in-english/home/crime-and-statistics/crime-statistics.html\n",
      "context\n",
      "\"these are arrests, charges and citations of nfl players for crimes more serious than common traffic violations. almost all of the players belonged to an nfl roster at the time of the incident. in rare cases, a free agent is included only if that player later signs with an nfl team. the data comes from media reports and public records. it cannot be considered fully complete because records of some player arrests might not have been found for various reasons, including lack of media coverage or accessible public records. many resolutions to these cases also are pending or could not be immediately found.\" (source)\n",
      "content\n",
      "this data covers january 2000 to march 2017. like mentioned above, it is not fully complete. in the future i hope to add files to add dimensions like usa crime rates, team info, player info, team season records\n",
      "column name | description | example data\n",
      "date | date of the incident | 3/7/2017\n",
      "team | team identifier at time of incident | sea (35 total)\n",
      "name | player name | aldon smith (627 total)\n",
      "position | player's position at time of incident | te (18 total)\n",
      "case | incident type | cited (10 total)\n",
      "category | incident crime categories, a comma separated list of crime types | dui (103 unique sets)\n",
      "description | a short text description of the incident | suspected of stealing golf cart, driving drunk, resisting arrest in scottsdale, ariz.\n",
      "outcome | incident outcome description | resolution undetermined.\n",
      "acknowledgements\n",
      "the original database was conceived and created by sports writer brent schrotenboer of usa today. http://www.usatoday.com/sports/nfl/arrests/\n",
      "past research:\n",
      "the rate of domestic violence arrests among nfl players - benjamin morris (fivethirtyeight)\n",
      "i found this data set august of 2015 and created http://nflarrest.com/ that attempts to provide a visual tool to explore the data set and a restful api.\n",
      "inspiration\n",
      "can the next arrest team or crime or date be predicted?\n",
      "does the number of arrests in the previous season, pre-season, in season effect overall team season record(wins,losses,playoff progression).\n",
      "how does the nfl arrest rate compare to the nation on average?\n",
      "how does the nfl arrest rate compare to populations with similar affluency?\n",
      "how do crime rates (e.g dui rates) compare to the geographic area the team represents?\n",
      "context:\n",
      "pakistan has a rich multilingual and multicultural heritage, with about 70 spoken languages, deriving from a diverse set of indo-aryan, indo-iranian, sino-tibetan and dravidian language families. more than half of these languages also have a written form, employing (predominantly) perso-arabic nastalique and arabic naskh writing styles. gujarati, gurmuki and tibetan scripts are also used by some communities, while some others are in the process of defining their writing systems. these languages exhibit a diverse set of sounds and underlying linguistic structures which are both linguistically and computationally exciting and challenging. most of these languages are not well-studied or well-modeled, and present a vast training ground for researchers in linguistics and computer science.\n",
      "this dataset provides resources for two languages spoken in pakistan: nepali and urdu. urdu is the national language of pakistan, while nepali is mainly spoken in a small immigrant community.\n",
      "content:\n",
      "this corpus is made of two documents, one in nepali and one in urdu. each document is available with and without part of speech tags. they are parallel to the 100,000 words of common english source from penn treebank corpus, available through linguistic data consortium (ldc).\n",
      "the part of speech tags are those in the penn treebank, and additional information can be found in the included .csv file.\n",
      "acknowledgements:\n",
      "this dataset was collected and made available by the center for language engineering at the university of engineering and technology uet in lahore. the work has been supported by the language resource association (gsk) of japan and international development research center (idrc) of canada, through pan localization project (www.panl10n.net). it is distributed here under a cc-by-nc-sa 3.0 license.\n",
      "inspiration:\n",
      "nepali and urdu are written in two different scripts (usually devanagari and nastaʿlīq, respectively), but are in the same language family. can you identify congruent characters in each writing system?\n",
      "can you automatically identify which words in urdu and nepali are cognates (descended from a common root)?\n",
      "can you use these files to build a part of speech tagger for nepali? urdu?\n",
      "context\n",
      "this san francisco 311 dataset contains all 311 cases created since 7/1/2008 (~2m). sf311 is a way for citizens to obtain information, report problems, or submit service requests to the city and county of san francisco.\n",
      "potential question(s) to get started with!\n",
      "what are some effective visualizations for conveying 311 incidences and trends?\n",
      "how do 311 requests vary by neighborhood? or source? over time or seasonally?\n",
      "what attributes have the greatest effect on how long it takes a case to close?\n",
      "is there a way to identify duplicative reports (when multiple people create a 311 report for the same incidence)?\n",
      "fields\n",
      "please see datasf's 311 case data faq here\n",
      "caseid - (numeric) - the unique id of the service request created.\n",
      "opened - (timestamp) - the date and time when the service request was made\n",
      "closed - (timestamp) - the date and time when the service request was closed\n",
      "updated - (timestamp) - the date and time when the service request was last modified. for requests with status=closed, this will be the date the request was closed\n",
      "status - (text) - the current status of the service request.\n",
      "status notes - (text) - explanation of why status was changed to current state or more details on current status than conveyed with status alone\n",
      "responsible agency - (text) - the agency responsible for fulfilling or otherwise addressing the service request.\n",
      "category - (text) - the human readable name of the specific service request type\n",
      "request type - (text) - more specific description of the problem related to the category\n",
      "request details - (text) - more specific description of the problem related to the request type\n",
      "address - (text) - human readable address or description of location\n",
      "supervisor district - (numeric) - supervisor district\n",
      "neighborhood - (text) - neighborhood\n",
      "point - (geometry: point) - latitude and longitude using the (wgs84) projection.\n",
      "source - (text) - how the service request was made\n",
      "media url - (text) - url to media\n",
      "we have included the following commonly used geographic shapefile(s):\n",
      "supervisor districts as of april 2012\n",
      "neighborhoods\n",
      "acknowledgements\n",
      "data provided by sf311 via the san francisco open data portal at https://data.sfgov.org/d/vw6y-z8j6\n",
      "pddl 1.0 odc public domain dedication and licence (pddl)\n",
      "photo via flickr jeremy brooks (cc by-nc 2.0)\n",
      "this dataset contains over 12k observations of male-female baboon pairs from a population of baboons that has recently seen genetic admixture from a different (but closely-related) taxon. the data contains genetic and social information for the male and female baboons, whether they mated, and whether the mating resulted in conception of offspring.\n",
      "acknowledgements\n",
      "the original journal article that this data was collected for:\n",
      "tung j, charpentier mje, mukherjee s, altmann j, alberts sc (2012) genetic effects on mating success and partner choice in a social mammal. the american naturalist 180(1): 113-129. http://dx.doi.org/10.1086/665993\n",
      "the data dryad page that this data was downloaded from:\n",
      "tung j, charpentier mje, mukherjee s, altmann j, alberts sc (2012) data from: genetic effects on mating success and partner choice in a social mammal. dryad digital repository. http://dx.doi.org/10.5061/dryad.4r9h61v8\n",
      "abstract\n",
      "(from the original paper)\n",
      "mating behavior has profound consequences for two phenomena—individual reproductive success and the maintenance of species boundaries—that contribute to evolutionary processes. studies of mating behavior in relation to individual reproductive success are common in many species, but studies of mating behavior in relation to genetic variation and species boundaries are less commonly conducted in socially complex species. here we leveraged extensive observations of a wild yellow baboon (papio cynocephalus) population that has experienced recent gene flow from a close sister taxon, the anubis baboon (papio anubis), to examine how admixture-related genetic background affects mating behavior. we identified novel effects of genetic background on mating patterns, including an advantage accruing to anubis-like males and assortative mating among both yellow-like and anubis-like pairs. these genetic effects acted alongside social dominance rank, inbreeding avoidance, and age to produce highly nonrandom mating patterns. our results suggest that this population may be undergoing admixture-related evolutionary change, driven in part by nonrandom mating. however, the strength of the genetic effect is mediated by behavioral plasticity and social interactions, emphasizing the strong influence of social context on mating behavior in socially complex species.\n",
      "the data\n",
      "this dataset contains over 12,000 observations of the following variables:\n",
      "female_id: three letter \"short name\" id for the female in a potentially consorting pair; each female has a unique id\n",
      "male_id: three letter \"short name\" id for the male in a potentially consorting pair; each male has a unique id\n",
      "cycle_id: a unique number assigned to each female-estrus cycle combination\n",
      "consort: whether the female-male pair consorted (1) or not (0), given the opportunity to do so\n",
      "conceptive: whether the estrus cycle resulted in a conception (1) or not (0)\n",
      "female_hybridscore: an estimate of the proportion of the female's genome that represents anubis baboon ancestry; for details of the estimation procedure, see materials and methods and tung et al (2008)\n",
      "male_hybridscore: an estimate of the proportion of the male's genome that represents anubis baboon ancestry; for details of the estimation procedure, see materials and methods and tung et al (2008)\n",
      "female_gendiv: an estimate of the female's genetic diversity; for details of the estimation procedure, see materials and methods\n",
      "male_gendiv: an estimate of the male's genetic diversity; for details of the estimation procedure, see materials and methods\n",
      "gen_distance: an estimate of the genetic distance (queller-goodnight r) between the male and female of a potentially consorting pair\n",
      "female_age: the age of the female in a potentially consorting pair\n",
      "male_rank: the ordinal rank of the male in a potentially consorting pair\n",
      "female_rank: the ordinal rank of the female in a potentially consorting pair\n",
      "males_present: the number of adult males present in the group of the potentially consorting pair\n",
      "females_present: the number of adult females present in the group of the potentially consorting pair\n",
      "male_rank_transform: ordinal male rank transformed to reflect fit (given number of males in a group) to the priority-of-access model; see materials and methods and appendix for more details\n",
      "gen_distance_transform: genetic distance estimate transform to test whether consortship probabilities decrease with genetic distance as well as genetic similarity\n",
      "rank_interact: the multiplicative interaction of male rank and female rank in the potentially consorting pair\n",
      "assort_index: assortative mating index, calculated from the hybrid scores of the male and female of a potentially consorting pair; see materials and methods for additional detail\n",
      "female_age_transform: female age transformed to test for a higher probability of consortship behavior for maximally fertile (middle-aged) females\n",
      "inspiration\n",
      "here are a few ideas for things to look at in this dataset:\n",
      "does genetic distance affect mating probability?\n",
      "does age of the female baboon affect conception probability?\n",
      "does social rank of the male affect mating probability?\n",
      "context\n",
      "this dataset contains information on pesticide residues in food. the u.s. department of agriculture (usda) agricultural marketing service (ams) conducts the pesticide data program (pdp) every year to help assure consumers that the food they feed themselves and their families is safe. ultimately, if epa determines a pesticide is not safe for human consumption, it is removed from the market.\n",
      "the pdp tests a wide variety of domestic and imported foods, with a strong focus on foods that are consumed by infants and children. epa relies on pdp data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. usda uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance usda’s integrated pest management objectives. usda also works with u.s. growers to improve agricultural practices.\n",
      "content\n",
      "while the original 2013 ms access database can be found here, the data has been transferred to a sqlite database for easier, more open use. the database contains two tables, sample data and results data. each sampling includes attributes such as extraction method, the laboratory responsible for the test, and epa tolerances among others. these attributes are labeled with codes, which can be referenced in pdf format here, or integrated into the database using the included csv files.\n",
      "inspiration\n",
      "what are the most common types of pesticides tested in this study?\n",
      "do certain states tend to use one particular pesticide type over another?\n",
      "does pesticide type correspond more with crop type or location (state)?\n",
      "are any produce types found to have higher pesticide levels than assumed safe by epa standards?\n",
      "by combining databases from several years of pdp tests, can you see any trends in pesticide use?\n",
      "acknowledgement\n",
      "this dataset is part of the usda pdp yearly database, and the original source can be found here.\n",
      "content\n",
      "the execution database includes records of every execution performed in the united states since the supreme court reinstated the death penalty in 1976. federal executions are indicated by fe in the state field and included in the region in which the crime occurred. the information in this database was obtained from news reports, the department of corrections in each state, and the naacp legal defense fund.\n",
      "acknowledgements\n",
      "the execution database was compiled and published by the death penalty information center. victim details, including quantity, sex, and race, were acquired from the criminal justice project's death row usa report.\n",
      "context\n",
      "vaccinations provide people the ability to develop immunity to particular diseases. when the majority of a population is vaccinated, “herd immunity” protects those who have not been vaccinated by blocking the spread of these diseases. a medical research paper published by the lancet in 1998 suggested an association between the measles/mumps/rubella (mmr) vaccine and autism spectrum disorders. the paper was later fully-retracted due controversy surrounding the lead author, who had financial conflicts of interest and allegedly manipulated the study data. however, it generated worldwide concern over the safety of mmr and other types of vaccines, including diphtheria/tetanus/pertussis (dtp).\n",
      "in california by 2010, the growing trend for parents to opt out of having their children receive vaccines over the following decade coincided with the largest pertussis outbreak in more than 60 years. reduced vaccination frequency was also linked to a high-profile measles outbreak in 2014 that began at disneyland. the resulting california state legislation (senate bill 277), signed june 2015, made it much more difficult for parents to opt out of vaccinations for their children. the data set will allow you to explore individual public and private school vaccination rates of incoming kindergarten students for the 2000 to 2014 school years.\n",
      "content\n",
      "the data are records for every school with ten or more students reporting the number of incoming kindergarteners who provided either proof of immunization, personal beliefs exemption (pbe), or permanent medical exemption (pme). annual records for the 2000-2001 through 2014-2015 school years have been formatted and combined. common variables in these annual data sets included in the merged file are the number of students, school name, school county, the number of pbes, pmes, and number of students vaccinated for:\n",
      "diphtheria/tetanus/pertussis (dtp)\n",
      "polio\n",
      "measles/mumps/rubella (mmr)\n",
      "one additional file contains 5 years of county-level pertussis case numbers and rates. another additional data file contains the number of infant pertussis cases for infants under three months old for each county in california between 2014-2015.\n",
      "geographic data are available in a file based on scripted geocode calls using the ggmap r package to find latitude and longitude data using the school names and county names. not all calls returned a valid coordinate, so additional indicator variables in this file indicate the quality of the match. the isschool indicator variable is 1 if the geocode search meta data included \"school\" and the countymatch indicator is 1 if the latitude and longitude coordinates are contained within the appropriate county in ca.\n",
      "references:\n",
      "retracted lancet research article\n",
      "report on 2010 pertussis outbreak\n",
      "acknowledgements\n",
      "individual data files and detailed annual reports for every school year in this data set are provided by the california department of public health (cdph). individual schools and licensed child care facilities are required to report immunization information to cdph every year to maintain compliance with the california health and safety code. additional details as well as child care and 7th grade data files can be found on the cdph website: https://www.cdph.ca.gov/programs/immunize/pages/immunizationlevels.aspx\n",
      "county level case data were pulled from the following report: https://archive.cdph.ca.gov/programs/immunize/documents/pertussis_report_1-7-2015.pdf\n",
      "infant pertussis data were reported to cdph as of 2/10/2016. additional pertussis reports can be found here: https://www.cdph.ca.gov/programs/immunize/pages/pertussissummaryreports.aspx\n",
      "inspiration\n",
      "while the disneyland measles outbreak received much media attention, pertussis outbreaks in california present great health risks to infants and the elderly. can you predict which counties and schools are at greatest risk for outbreaks and/or quantify the association between vaccination rates and the number infant pertussis cases?\n",
      "context:\n",
      "youtube has introduced automatic generation of subtitles based on speech recognition of uploaded video. this dataset provides collection of subtitles donald trump's uploaded speeches. it serves as database for an introduction to algorithmic analysis of spoken language.\n",
      "content:\n",
      "mr donald trump speeches dataset consists of 836 subtitles (sets of words) retrieved from youtube playlists: \"donald trump speeches & events\", \"donald trump speeches & press conference\", \"president donald trump weekly address 2017\", \"president donald trump's first 100 days | nbc news\", \"donald trump rally speech events press conference rallies playlist\".\n",
      "this dataset consists of a single csv file mrtrumpspeeches.csv. the columns are: 'id', 'playlist', 'upload_date', 'title', 'view_count', 'average_rating', 'like_count', 'dislike_count', 'subtitles', which are delimited with tilde character '~'.\n",
      "text data in columns 'subtitles' is not sentence based, there are not commas or dots. it is only stream of words being translated from speech into text by googlevoice (more here https://googleblog.blogspot.com.au/2009/11/automatic-captions-in-youtube.html).\n",
      "acknowledgements:\n",
      "the data was downloaded using youtube-dl package.\n",
      "inspiration:\n",
      "i'm interested in psychological profiles of people speaking based on language used. (for example see https://medium.com/@tschnoebelen/trump-does-not-talk-like-a-woman-breaking-news-gender-continues-to-be-complicated-and-confusing-4c0d28b41d7)\n",
      "context\n",
      "this dataset includes locations of nyc subway stops, performance data for all transit modes, and turnstile data.\n",
      "content\n",
      "the stop and performance data were last update june 2017. the turnstile data is for every week in january-july 2016.\n",
      "acknowledgements\n",
      "all credit to the metropolitan transportation authority of the state of new york and nyc open data. data downloaded from the mta's http://datamine.mta.info/\n",
      "inspiration\n",
      "this dataset is intended as an accompaniment to the new new york city taxi trip duration challenge. i'm wondering if transit intensity or quality near where a taxi ride starts (or ends) affects how long a taxi ride will last.\n",
      "context\n",
      "the idea is to measure recommended route distance and duration(average based on historical data) between two co-ordinates using google's distance matrix api.\n",
      "content\n",
      "google's distance and duration data appended (as fetched from the api based on the co-ordinate given) to the kaggle's dataset for \"new york city taxi trip duration\" challenge.\n",
      "additionally, great-circle distance between two co-ordinates are also given.\n",
      "acknowledgements\n",
      "the data was retrieved on 29th of july, 2017 from the google's distance matrix api based on kaggle's dataset given for \"new york city taxi trip duration\" challenge.\n",
      "great circle distance calculated between two co-ordinates using \"geopy\".\n",
      "context\n",
      "have you taken a flight in the u.s. in the past 15 years? if so, then you are a part of monthly data that the u.s. department of transportation's transtats service makes available on various metrics for 15 u.s. airlines and 30 major u.s airports. their website unfortunately does not include a method for easily downloading and sharing files. furthermore, the source is built in asp.net, so extracting the data is rather cumbersome. to allow easier community access to this rich source of information, i scraped the metrics for every airline / airport combination and stored them in separate csv files.\n",
      "occasionally, an airline doesn't serve a certain airport, or it didn't serve it for the entire duration that the data collection period covers*. in those cases, the data either doesn't exist or is typically too sparse to be of much use. as such, i've only uploaded complete files for airports that an airline served for the entire uninterrupted duration of the collection period. for these files, there should be 174 time series points for one or more of the nine columns below. i recommend any of the files for american, delta, or united airlines for outstanding examples of complete and robust airline data.\n",
      "* no data for atlas air exists, and virgin america commenced service in 2007, so no folders for either airline are included.\n",
      "content\n",
      "there are 13 airlines that have at least one complete dataset. each airline's folder includes csv file(s) for each airport that are complete as defined by the above criteria. i've double-checked the files, but if you find one that violates the criteria, please point it out. the file names have the format \"airline-airport.csv\", where both airline and airport are iata codes. for a full listing of the airlines and airports that the codes correspond to, check out the airline_codes.csv or airport_codes.csv files that are included, or perform a lookup here. note that the data in each airport file represents metrics for flights that originated at the airport.\n",
      "among the 13 airlines in data.zip, there are a total of 161 individual datasets. there are also two special folders included - airlines_all_airports.csv and airports_all_airlines.csv. the first contains datasets for each airline aggregated over all airports, while the second contains datasets for each airport aggregated over all airlines. to preview a sample dataset, check out all_airlines_all_airports.csv, which contains industry-wide data.\n",
      "each file includes the following metrics for each month from october 2002 to march 2017:\n",
      "date (yyyy-mm-dd): all dates are set to the first of the month. the day value is just a placeholder and has no significance.\n",
      "asm_domestic: available seat-miles in thousands (000s). number of domestic flights * number of seats on each flight\n",
      "asm_international*: available seat-miles in thousands (000s). number of international flights * number of seats on each flight\n",
      "flights_domestic\n",
      "flights_international*\n",
      "passengers_domestic\n",
      "passengers_international*\n",
      "rpm_domestic: revenue passenger-miles in thousands (000s). number of domestic flights * number of paying passengers\n",
      "rpm_international*: revenue passenger-miles in thousands (000s). number of international flights * number of paying passengers\n",
      "* frequently contains missing values\n",
      "acknowledgements\n",
      "thanks to the u.s. department of transportation for collecting this data every month and making it publicly available to us all.\n",
      "source: https://www.transtats.bts.gov/data_elements.aspx\n",
      "inspiration\n",
      "the airline / airport datasets are perfect for practicing and/or testing time series forecasting with classic statistical models such as autoregressive integrated moving average (arima), or modern deep learning techniques such as long short-term memory (lstm) networks. the datasets typically show evidence of trends, seasonality, and noise, so modeling and accurate forecasting can be challenging, but still more tractable than time series problems possessing more stochastic elements, e.g. stocks, currencies, commodities, etc. the source releases new data each month, so feel free to check your models' performances against new data as it comes out. i will update the files here every 3 to 6 months depending on how things go.\n",
      "a future plan is to build a sqlite database so a vast array of queries can be run against the data. the data in it its current time series format is not conducive for this, so coming up with a workable structure for the tables is the first step towards this goal. if you have any suggestions for how i can improve the data presentation, or anything that you would like me to add, please let me know. looking forward to seeing the questions that we can answer together!\n",
      "context\n",
      "we are doing fintech data hakathon in tokyo everyweek. let's predict stock price in tokyo stock exchange.\n",
      "毎週水曜日東京・渋谷で開催している、team ai \"fintech data hackathon\"の題材として、 身近なユニクロ(ファーストリテイリング)の株価予測モデルをオープンイノベーションで構築します。 https://www.meetup.com/machine-learning-meetup-by-team-ai/events/242154425/\n",
      "content\n",
      "training; 5 year daily stock price info of fastretailing(uniqlo). you should predict \"close\" price. test: 1 week daily stock price\n",
      "acknowledgements\n",
      "thanks to open market data http://k-db.com/\n",
      "inspiration\n",
      "let's build basic stock prediction model together! 公開されたモデルを実際の取引に使う場合は十分注意ください。弊社側やコミュニティメンバー側では損失の責任は持てません。\n",
      "content\n",
      "every line in the dataset represents the stats of a match (the deck of two player, some players stats, the result of the match, game_type, ...) every line is in .json format the data collection starts on 2017 - 07 - 12 (yyyy/mm/dd) the data set contain 400k matches\n",
      "context\n",
      "bus breakdown and delays you can find the road where the traffic was heavy for the new york city taxi trip duration playground.\n",
      "content\n",
      "the bus breakdown and delay system collects information from school bus vendors operating out in the field in real time. bus staff that encounter delays during the route are instructed to radio the dispatcher at the bus vendor’s central office. the bus vendor staff are then instructed to log into the bus breakdown and delay system to record the event and notify opt. opt customer service agents use this system to inform parents who call with questions regarding bus service. the bus breakdown and delay system is publicly accessible and contains real time updates. all information in the system is entered by school bus vendor staff.\n",
      "you can find data for years 2015 to 2017.\n",
      "context\n",
      "the data includes 2d human pose estimates of parkinson's patients performing a variety of tasks (e.g. communication, drinking from a cup, leg agility). pose estimates were produced using convolutional pose machines (cpm, https://arxiv.org/abs/1602.00134).\n",
      "the goal of this project was to use features derived from videos of parkinson's assessment to predict the severity of parkinsonism and dyskinesia based on clinical rating scales.\n",
      "content\n",
      "data was acquired as part of a study to measure the minimally clinically important difference in parkinson's rating scales. participants received a two hour infusion of levodopa followed by up to two hours of observation. during this time, they were assessed at regular intervals and assessments were video recorded for post-hoc ratings by neurologists. there were between 120-130 videos per task.\n",
      "the data includes all movement trajectories (extracted frame-by-frame) from the videos of parkinson's assessments using cpm, as well as confidence values produced by cpm. ground truth ratings of parkinsonism and dyskinesia severity are included using the udysrs, updrs, and capsit rating scales.\n",
      "camera shake has been removed from trajectories (see paper for more details). no other preprocessing has been performed. files are saved in json format. for information on how to deal with files, see data_import_demo.ipynb or view online at https://github.com/limi44/parkinson-s-pose-estimation-dataset.\n",
      "acknowledgements\n",
      "we would like to acknowledge the staff and patients at toronto western hospital for their time and assistance in this study.\n",
      "for the papers accompanying these results and more details on data collection and processing, please see:\n",
      "[1] m.h. li, t.a. mestre, s.h. fox, b. taati, vision-based assessment of parkinsonism and levodopa-induced dyskinesia with deep learning pose estimation, arxiv:1707.09416 [cs]. (2017). http://arxiv.org/abs/1707.09416.\n",
      "[2] m.h. li, t.a. mestre, s.h. fox, b. taati, automated vision-based analysis of levodopa-induced dyskinesia with deep learning, in: 2017 39th annual international conference of the ieee engineering in medicine and biology society (embc), jeju island, south korea, 2017.\n",
      "[3] t.a. mestre, i. beaulieu-boire, c.c. aquino, n. phielipp, y.y. poon, j.p. lui, j. so, s.h. fox, what is a clinically important change in the unified dyskinesia rating scale in parkinson’s disease?, parkinsonism & related disorders. 21 (2015) 1349–1354. doi:10.1016/j.parkreldis.2015.09.044.\n",
      "inspiration\n",
      "in our study, we aimed to evaluate the readiness of off-the-shelf human pose estimation and deep learning for clinical applications in parkinson's disease. we hope that others may find this dataset useful for furthering progress in technology-based monitoring of neurological disorders.\n",
      "banner\n",
      "photo by jesse orrico on unsplash.\n",
      "content\n",
      "the london boroughs profiles data about demography, diversity, labour market, economy, community safety, housing, environment, transport, children, health and governance\n",
      "acknowledgements\n",
      "thanks for taking your time to look at this data and thanks for any suggestions.\n",
      "inspiration\n",
      "i am new to data analysis and i would like some suggestions on how to analyse this dataset and possibly create a visualasation, or predictive analysis.\n",
      "context\n",
      "to explore more on regression algorithm\n",
      "content\n",
      "each record in the database describes a boston suburb or town. the data was drawn from the boston standard metropolitan statistical area (smsa) in 1970. the attributes are deﬁned as follows (taken from the uci machine learning repository1): crim: per capita crime rate by town 2. zn: proportion of residential land zoned for lots over 25,000 sq.ft. 3. indus: proportion of non-retail business acres per town 4. chas: charles river dummy variable (= 1 if tract bounds river; 0 otherwise) 5. nox: nitric oxides concentration (parts per 10 million) 1https://archive.ics.uci.edu/ml/datasets/housing 123 20.2. load the dataset 124 6. rm: average number of rooms per dwelling 7. age: proportion of owner-occupied units built prior to 1940 8. dis: weighted distances to ﬁve boston employment centers 9. rad: index of accessibility to radial highways 10. tax: full-value property-tax rate per $10,000 11. ptratio: pupil-teacher ratio by town 12. b: 1000(bk−0.63)2 where bk is the proportion of blacks by town 13. lstat: % lower status of the population 14. medv: median value of owner-occupied homes in $1000s we can see that the input attributes have a mixture of units.\n",
      "acknowledgements\n",
      "thanks to dr.jason\n",
      "context\n",
      "this dataset looks at the demographics of world of warcraft players–gender, age, sexuality, etc.–and how they play the game–role, race, class–to see if there is an association between any of them. specifically, i was interested in how gender and sexuality affects the gender of the character they play, but there are many other things to look at. this data was gathered through a google forms survey, which was then posted on reddit, tumblr, twitter, and my wow guild's discord server.\n",
      "content\n",
      "there are 14 columns, 100 rows (not including the titles). 12 of those columns were gathered from a google forms survey, and the last two were added by hand. they are:\n",
      "timestamp: useless. just when the survey was completed.\n",
      "gender: the gender of the player.\n",
      "sexuality: the sexuality of the player.\n",
      "age: age of the player.\n",
      "country: country the player lives in.\n",
      "main: the gender of character the player mains\n",
      "faction: the faction the player mains.\n",
      "server: the server(s) the player mains.\n",
      "role: the role(s) the player mains (dps, healer, tank, or any combination therein).\n",
      "class: the class(es) the player mains. this was a question where the respondent check any number of boxes, so there are many different ways it could be answered. hard to analyze.\n",
      "race: the race(s) the player mains. same as class.\n",
      "max: the number of 110s (max level) the player has. only numerical variable in the dataset.\n",
      "attracted: the gender the player is attracted to.\n",
      "type: the \"type\" of person the player is. combines gender and sexuality (\"gay woman\", \"bi male\", etc.)\n",
      "acknowledgements\n",
      "this dataset belongs to me. i created the survey and compiled the data. however, i would like to thank stormwind-keep on tumblr and earth2gem on twitter for helping me get the survey out to a broader audience.\n",
      "inspiration\n",
      "i already ran a bunch of my own analyses using r, but i could not find a good way to analyze the class and race variables. if anyone can figure that one out, please do.\n",
      "context:\n",
      "digimon, short for “digital monsters”, is a franchise which revolves around a core mechanic of capturing, caring for and training monsters and then engaging in combat with them. it’s similar to pokémon.\n",
      "this dataset contains information on digimon from “digimon digimon story: cyber sleuth”, released for playstation vita in 2015 and playstation 4 in 2016.\n",
      "content:\n",
      "this database contains three files: a list of all the digimon that can be captured or fought in cyber sleuth, all the moves which digimon can perform, and all the support skills. (support skills are a passive, stackable, team-wide buff. each species of digimon is associated with a single support skill.)\n",
      "acknowledgements:\n",
      "this dataset was created by mark korsak and is used here with permission. you can find an interactive version of this database here. http://digidb.io/\n",
      "inspiration:\n",
      "this dataset will help you theorycraft the ultimate team as well as ask interesting questions.\n",
      "which set of moves will get the best ratio of attack power to sp spent?\n",
      "which team of 3 digimon have the highest attack? defense?\n",
      "what’s the tradeoff between hp and sp?\n",
      "are some types over- or under-represented?\n",
      "both the moves and support skills have short text descriptions. can an nlp analysis reveal underlying clusters of moves?\n",
      "are different types and attributes evenly represented across stages?\n",
      "context:\n",
      "pronouncing dictionaries contain a set of words as they appear in written texts as well as their pronunciations. they are often used by researchers who are working on speech technology applications.\n",
      "content:\n",
      "cmudict (the carnegie mellon pronouncing dictionary) is a free pronouncing dictionary of english, suitable for uses in speech technology. it was created and is maintained by the speech group in the school of computer science at carnegie mellon university. the version available here was current as of august 8, 2017.\n",
      "the pronunciations in this dictionary are annotated in arpabet. more information on aprabet can be found here and here. in this transcription system, each speech sound is represented with a unique one or two letter code, with a space between each speech sound. vowels are followed by a 1 if they receive the primary stress in a word, and a 0 if they do not.\n",
      "acknowledgements\n",
      "the carnegie mellon pronouncing dictionary, in its current and previous versions is copyright (c) 1993-2014 by carnegie mellon university. use of this dictionary for any research or commercial purpose is completely unrestricted. if you make use of or redistribute this material, please acknowledge its origin in your descriptions.\n",
      "for more information on the terms under which this dataset is distributed, see the license file.\n",
      "inspiration\n",
      "can you create an automatic mapping from english orthography (the way a word is spelled) to a word’s pronunciation? how well does this work on out-of-domain words?\n",
      "some words in the dictionary have multiple pronunciations. can you predict which words are more likely to have more than one pronunciation? does the length of the word have an effect? its frequency? the sounds in it?\n",
      "context:\n",
      "buzzfeed had previously reported on flights of spy planes operated by the fbi and the department of homeland security (dhs), and reasoned that it should be possible to train a machine learning algorthim to identify other aircraft performing similar surveillance, based on characteristics of the aircraft and their flight patterns. you can read the story here, and additional analysis and code by peter aldhous can be found here.\n",
      "content:\n",
      "buzzfeed news obtained more than four months of aircraft transponder detections from the plane tracking website flightradar24, covering august 17 to december 31, 2015 utc, containing all data displayed on the site within a bounding box encompassing the continental united states, alaska, hawaii, and puerto rico.\n",
      "flightradar24 receives data from its network of ground-based receivers, supplemented by a feed from ground radars provided by the federal aviation administration (faa) with a five-minute delay.\n",
      "after parsing from the raw files supplied by flightradar24, the data included the following fields, for each transponder detection:\n",
      "adshex unique identifier for each aircraft, corresponding to its \"mode-s\" code, in hexademical format.\n",
      "flight_id unique identifier for each \"flight segment,\" in hexadecimal format. a flight segment is a continuous series of transponder detections for one aircraft. there may be more than one segment per flight, if a plane disappears from flightradar24's coverage for a period --- for example when flying over rural areas with sparse receiver coverage. while being tracked by fightradar24, planes were typically detected several times per minute.\n",
      "latitude, longitude geographic location in digital degrees.\n",
      "altitude altitude in feet.\n",
      "speed ground speed in knots.\n",
      "squawk four-digit code transmitted by the transponder.\n",
      "type aircraft manufacter and model, if identified.\n",
      "timestamp full utc timestamp.\n",
      "track compass bearing in degrees, with 0 corresponding to north.\n",
      "we also calculated:\n",
      "steer change in compass bearing from the previous transponder detection for that aircraft; negative values indicate a turn to the left, positive values a turn to the right.\n",
      "feature engineering\n",
      "first we filtered the data to remove planes registered abroad, based on their adshex code, common commercial airliners, based on their type, and aircraft with fewer than 500 transponder detections.\n",
      "then we took a random sample of 500 aircraft and calculated the following for each one:\n",
      "duration of each flight segment recorded by flightradar24, in minutes.\n",
      "boxes area of a rectangular bounding box drawn around each flight segment, in square kilometers.\n",
      "finally, we calculated the following variables for each of the aircraft in the larger filtered dataset:\n",
      "duration1,duration2,duration3,duration4,duration5 proportion of flight segment durations for each plane falling into each of five quantiles calculated from duration for the sample of 500 planes. the proportions for each aircraft must add up to 1; if the durations of flight segments for a plane closely matched those for a typical plane from the sample, these numbers would all approximate to 0.2; a plane that mostly flew very long flights would have large decimal fraction for duration5.\n",
      "boxes1,boxes2,boxes3,boxes4,boxes5 proportion of bounding box areas for each plane falling into each of five quantiles calculated from boxes for the sample of 500 planes.\n",
      "speed1,speed2,speed3,speed4,speed5 proportion of speed values recorded for the aircraft falling into each of five quantiles recorded for speed for the sample of 500 planes.\n",
      "altitude1,altitude2,altitude3,altitude4,altitude5 proportion of altitude values recorded for the aircraft falling into each of five quantiles recorded for altitude for the sample of 500 planes.\n",
      "steer1,steer2,steer3,steer4,steer5,steer6,steer7,steer8 proportion of steer values for each aircraft falling into bins set manually, after observing the distribution for the sample of 500 planes, using the breaks: -180, -25, -10, -1, 0, 1, 22, 45, 180.\n",
      "flights total number of flight segments for each plane.\n",
      "squawk_1 squawk code used most commonly by the aircraft.\n",
      "observations total number of transponder detections for each plane.\n",
      "type aircraft manufacter and model, if identified, else unknown.\n",
      "the resulting data for 19,799 aircraft are in the file planes_features.csv.\n",
      "acknowledgements:\n",
      "this dataset was created by peter aldhous from raw flightradar24 data, as well as faa data.\n",
      "inspiration:\n",
      "peter used a random forest classifier--would another approach be better? worse?\n",
      "compare your list of candidates to his here.\n",
      "this data is from 2015--can you grab up to date data from ads-b exchange and find any new candidate planes?\n",
      "context\n",
      "the open data 500, funded by the john s. and james l. knight foundation (http://www.knightfoundation.org/) and conducted by the govlab, is the first comprehensive study of u.s. companies that use open government data to generate new business and develop new products and services.\n",
      "study goals\n",
      "provide a basis for assessing the economic value of government open data\n",
      "encourage the development of new open data companies\n",
      "foster a dialogue between government and business on how government data can be made more useful\n",
      "the govlab's approach\n",
      "the open data 500 study is conducted by the govlab at new york university with funding from the john s. and james l. knight foundation. the govlab works to improve people’s lives by changing how we govern, using technology-enabled solutions and a collaborative, networked approach. as part of its mission, the govlab studies how institutions can publish the data they collect as open data so that businesses, organizations, and citizens can analyze and use this information.\n",
      "company identification\n",
      "the open data 500 team has compiled our list of companies through (1) outreach campaigns, (2) advice from experts and professional organizations, and (3) additional research.\n",
      "outreach campaign\n",
      "mass email to over 3,000 contacts in the govlab network\n",
      "mass email to over 2,000 contacts opendatanow.com\n",
      "blog posts on thegovlab.org and opendatanow.com\n",
      "social media recommendations\n",
      "media coverage of the open data 500\n",
      "attending presentations and conferences\n",
      "expert advice\n",
      "recommendations from government and non-governmental organizations\n",
      "guidance and feedback from open data 500 advisors\n",
      "research\n",
      "companies identified for the book, open data now\n",
      "companies using datasets from data.gov\n",
      "directory of open data companies developed by deloitte\n",
      "online open data userbase created by socrata\n",
      "general research from publicly available sources\n",
      "what the study is not\n",
      "the open data 500 is not a rating or ranking of companies. it covers companies of different sizes and categories, using various kinds of data.\n",
      "the open data 500 is not a competition, but an attempt to give a broad, inclusive view of the field.\n",
      "the open data 500 study also does not provide a random sample for definitive statistical analysis. since this is the first thorough scan of companies in the field, it is not yet possible to determine the exact landscape of open data companies.\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes over 2 gb of stop data from texas, covering all of 2010 onwards. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes over 1 gb of stop data from florida. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes stop data from az, co, ct, ia, ma, md, mi and mo. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context\n",
      "dataset contains essential financial fundamental indicators for 2389 companies included into nasdaq index extracted from https://finintelligence.com service. dataset contains 10 indicators from income, cash flow and assets statements.\n",
      "content\n",
      "dataset contains essential financial fundamental indicators for 2389 companies included into nasdaq index extracted from https://finintelligence.com service. dataset contains 10 indicators from income, cash flow and assets statements. some indicators are automatically calculated, for example:\n",
      "companies often don’t report q4 indicators. from this reasons q4 amounts may be calculated as annual amount - (q1 + q2 + q3) amounts.\n",
      "companies report some indicators ambiguously. for example companies may report revenue as sales of services, sales of goods and in many other ways. from that reason revenue and some other indicators were automatically mapped from variety of reported source indicators.\n",
      "all indicators in datasets are provided for calendar quarters for years of 2014, 2015 and 2016. periods are rounded to calendar quarters, for example feb 1 - may 1 period is included as q1 into dataset.\n",
      "each record contains period, company name, stock tickers (some companies have several tickers), indicator name and amount. data is originally extracted from companies sec filings.\n",
      "acknowledgements\n",
      "data is prepared and provided by https://finintelligence.com service. service provides intelligent access to variety of corporate financial data extracted from companies documents, press releases and news. service is absolutely free, no payment required. visit us today!\n",
      "happy data hacking!\n",
      "context\n",
      "data downloaded from the empres global animal disease information system.\n",
      "content\n",
      "data shows the when, where and what of animal disease outbreaks from the last 2 years, including african swine fever, foot and mouth disease and bird-flu. numbers of cases, deaths, etc are also included.\n",
      "acknowledgements\n",
      "this data is from the food and agriculture organization of the united nations. the empres-i system can be access here read more about the details of the system here\n",
      "context\n",
      "the cat dataset includes over 9,000 cat images. for each image, there are annotations of the head of cat with nine points, two for eyes, one for mouth, and six for ears.\n",
      "content\n",
      "the annotation data are stored in a file with the name of the corresponding image plus .\"cat\" at the end. there is one annotation file for each cat image. for each annotation file, the annotation data are stored in the following sequence:\n",
      "number of points (default is 9)\n",
      "left eye\n",
      "right eye\n",
      "mouth\n",
      "left ear-1\n",
      "left ear-2\n",
      "left ear-3\n",
      "right ear-1\n",
      "right ear-2 -right ear-3\n",
      "acknowledgements\n",
      "weiwei zhang, jian sun, and xiaoou tang, cat head detection - how to effectively exploit shape and texture features, proc. of european conf. computer vision, vol. 4, pp.802-816, 2008.\n",
      "dataset originally found on the internet archive at https://archive.org/details/cat_dataset\n",
      "\n",
      "* this dataset is dedicated to dan becker, a huge cat lover. *\n",
      "history\n",
      "i have made the database of photos sorted by countries and pattern types. screenshots were performed only on official websites.\n",
      "content\n",
      "the main dataset (decor.zip) is 485 color images (150x150x3) of traditional decor patterns and the file with labels decor.csv. photo files are in the .png format and the labels are integers and values.\n",
      "the file decorcolorimages.h5 consists of preprocessing images of this set: image tensors and targets (labels).\n",
      "acknowledgements\n",
      "i have published the data for absolutely free usage by any site visitor. but this database contains the names of famous traditional decor styles, so it can not be used for commercial purposes.\n",
      "usage\n",
      "classification, image recognition or generation, colorizing, etc. in a case of a small number of images are useful exercises. there are three kinds of classification here: by country, by pattern, by types of image (pattern itself or product).\n",
      "improvement\n",
      "it's possible to find lots of ways for improving this set and the machine learning algorithms applying to it because of many traditional patterns in the world.\n",
      "context\n",
      "in 2007, i wrote my masters thesis on indoor location determination using wi-fi received signal strength indicator (rssi). as a part of my research, i gathered 120k rssi samples from 4 access points on floor 1 and 2 of the building of my faculty. i wrote a custom software to do this (no reliable open source software for this at the time) and sampling was a long and painful process. with my limited ml learning skills at the time, i came up with a simple algorithm that was able to find the location of a wi-fi device in that building with 85% accuracy. i imagine this is a solved problem now and state of the art indoor positioning systems are way more accurate today. yet, i decided to find and publish this data set because it's small and simple enough for practice and at the same time, it has some peculiarity for advanced data science and ml fun.\n",
      "content\n",
      "the samples were taken in a two-level building. each row in the dataset is a single rssi sample from one of the 4 access points. access points are identified by one of the letters a, b, c, or d. physical coordinates of the location where each sample was taken is identified by x,y, z coordinates with z being the floor (1 or 2). at each coordinate, multiple samples are taken, where sample number is identified by a field name sequence. the reason for taking multiple samples is that signal strength fluctuates due to things like scattering and reflection, specially in buildings with moving objects and people. so, to have reliable measurements, one needs 10s of samples from each access points to make for the variability.\n",
      "having- said this, each row of the sample set has the format: ap ,signal, sequence ,x,y,z where:\n",
      "ap: access point identifier. one of a, b, c or d\n",
      "signal: signal strength from access point ap. note: rssi values in the table are negated. that is, smaller values in this cell mean stronger signal reception from the access point.\n",
      "sequence: the sequence of sample from this particular access point at this particular coordinate\n",
      "x,y,z:: coordinates where this sample was taken\n",
      "note: do not assume all locations have the same number of samples from all access points. for example, at location (x=1,y=1,z=1) we might have a samples from a, b samples from b, c samples from c and d samples from d and a != b != c!= d. why? that's because in some areas we might have poor reception (or complete lack thereof) from an access point and so we end up with fewer or no samples.\n",
      "here's the floor plan of the building where sampling took place\n",
      "these are the locations of the access points:\n",
      "{\"a\": (23, 17, 2), \"b\": (23, 41, 2), \"c\" : (1, 15, 2), \"d\": (1, 41, 2)}\n",
      "you can also find the location of access points on the map.\n",
      "by poking at this data and comparing it with the floor plan, you'll learn interesting things about wi-fi radio signals (in 2.4 ghz frequency) and how they behave in indoors. one challenge is to come up with a ml model that given rssi from the 4 access points finds the (x,y,z) coordinates of the location.\n",
      "context\n",
      "the purpose of this dataset is to support the toxic comment classification competition.\n",
      "the goal is to help jigsaw create a model detecting language toxicity levels.\n",
      "building of the this dataset is available on my github.\n",
      "content\n",
      "this dataset contains a lot of bad words.\n",
      "acknowledgements\n",
      "carnegie mellon university\n",
      "user2592414 on stackexchange\n",
      "inspiration\n",
      "use this for good.\n",
      "public health data\n",
      "this is the public dataset made available at https://health.data.ny.gov/health/hospital-inpatient-discharges-sparcs-de-identified/82xm-y6g8 by the dept of health of new york state. the following description can be found at that page:\n",
      "the statewide planning and research cooperative system (sparcs) inpatient de-identified file contains discharge level detail on patient characteristics, diagnoses, treatments, services, and charges. this data file contains basic record level detail for the discharge. the de-identified data file does not contain data that is protected health information (phi) under hipaa. the health information is not individually identifiable; all data elements considered identifiable have been redacted. for example, the direct identifiers regarding a date have the day and month portion of the date removed.\n",
      "it would be nice to ...\n",
      "... for example, be able to predict length of stay in the hospital using the parameters likely to be available when teh patient is admitted.\n",
      "this series of datasets represent trade signals on continuous derivates contracts.\n",
      "each dataset is composed out of lines. every line represents a single event and contains the features at the point of the event firing.\n",
      "the goal is to build a model that can filter in real-time the generated signals and improve the overall performance of the trade robot.\n",
      "context\n",
      "i am fond of reading romances, so i find practice text mining with novels extremely amusing. i also think this is a good way to approach literary works in quantitative light and gain some new insights.\n",
      "content\n",
      "the dataset includes a plain text file and a microsoft word file of the book gone with the wind by margaret mitchell. there are some problems with utf-8 so more cleaning is needed.\n",
      "acknowledgements\n",
      "i converted this dataset from full pdf file of the book created by don lainson dlainson@sympatico.ca, sponsored by project gutenberg of australia ebooks. http://campbellmgold.com/archive_ebooks/gone_with_the_wind_mitchell.pdf\n",
      "inspiration\n",
      "i have started on some sentiment analysis based on chapter number, and based on characters. what is the general air of this classic? which chapter is the most depressing? which words are most associated with scarlett? rhett butler? what are the sentiments regarding the war? and the abolition?\n",
      "1. context\n",
      "capital punishment is one of the controversial human rights issues in the united states. while surfing the internet for an interesting dataset, i came across this database by texas department of criminal justice, which comprises of the offenders' last words before execution. some of the statements are:\n",
      "\"...young people, listen to your parents; always do what they tell you to do, go to school, learn from your mistakes. be careful before you sign anything with your name. never, despite what other people say...\" (ramiro hernandez, executed on april 9th, 2014)\n",
      "\"first and foremost i'd like to say, \"justice has never advanced by taking a life\" by coretta scott king. lastly, to my wife and to my kids, i love y'all forever and always. that's it.\" (taichin preyor, executed on july 27th, 2017)\n",
      "as i skimmed these lines, i decided to create this dataset.\n",
      "2. content\n",
      "this dataset includes information on criminals executed by texas department of criminal justice from 1982 to november 8th, 2017. in furman v georgia in 1972, the supreme court considered a group of consolidated cases, whereby it severely restricted the death penalty. however, like other states, texas adjusted its legislation to address the court's concern and once again allow for capital punishment in 1973. texas adopted execution by lethal injection in 1977 and in 1982, the starting year of this dataset, the first offender was executed by this method.\n",
      "the dataset consists of 545 observations with 21 variables. they are:\n",
      "- execution: the order of execution, numeric.\n",
      "- lastname: last name of the offender, character.\n",
      "- firstname: first name of the offender, character.\n",
      "- tdcjnumber: tdcj number of the offender, numeric.\n",
      "- age: age of the offender, numeric.\n",
      "- race: race of the offender, categorical : black, hispanic, white, other.\n",
      "- countyofconviction: county of conviction, character.\n",
      "- agewhenreceived: age of offender when received, numeric.\n",
      "- educationlevel: education level of offender, numeric.\n",
      "- native county: native county of offender, categorical : 0 = within texas, 1= outside texas.\n",
      "- previouscrime : whether the offender committed any crime before, categorical: 0= no, 1= yes.\n",
      "- codefendants: number of co-defendants, numeric.\n",
      "- numbervictim: number of victims, numeric.\n",
      "- whitevictim, hispanicvictim, blackvictim, victimotherrace. femalevictim, malevictim: number of victims with specified demographic features, numeric.\n",
      "- laststatement: last statement of offender, character.\n",
      "3. acknowledgement\n",
      "this dataset is derived from the database by texas department of criminal justice which can be found in this link: http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html . it can be seen that the original one has fewer than 10 variables and is embedded with some links to sub-datasets, so i manually inputted more variables based on those links.\n",
      "there are some complications with this dataset. firstly, the dataset was manually created so mistakes are inevitable, though i have tried my best to minimize them. secondly, the recording of offender information is not complete and consistent. for example, sometimes the education level of ged is interpreted as 11 years, at other times as 9 or 10 years. \"none\" and \"na\" are used interchangeably, making it hard to distinguish between 0 and na in the coded variable. the victim's information is often omitted, so i rely on the description of the crime for the names and pronouns to make a judgement of the number of victims and their gender. finally, the last statements are sometimes recorded in the first person and sometimes in the third, so the word choice might not be original. that being said, i find this dataset meaningful and worth sharing.\n",
      "4. inspiration\n",
      "what are the demographics of the death row inmates? what are the patterns of their last statements? what is the relationship between the two?\n",
      "context\n",
      "thousands of cryptocurrencies have sprung up in the past few years. can you predict which one will be the next btc?\n",
      "content\n",
      "the dataset contains daily opening, high, low, close, and trading volumes for over 1200 cryptocurrencies (excluding bitcoin).\n",
      "acknowledgements\n",
      "https://timescaledata.blob.core.windows.net/datasets/crypto_data.tar.gz\n",
      "inspiration\n",
      "speculative forces are always at work on cryptocurrency exchanges - but do they contain any statistically significant features?\n",
      "content\n",
      "the dataset was created using the rssi readings of an array of 13 ibeacons in the first floor of waldo library, western michigan university. data was collected using iphone 6s. the dataset contains two sub-datasets: a labeled dataset (1420 instances) and an unlabeled dataset (5191 instances). the recording was performed during the operational hours of the library. for the labeled dataset, the input data contains the location (label column), a timestamp, followed by rssi readings of 13 ibeacons. rssi measurements are negative values. bigger rssi values indicate closer proximity to a given ibeacon (e.g., rssi of -65 represent a closer distance to a given ibeacon compared to rssi of -85). for out-of-range ibeacons, the rssi is indicated by -200. the locations related to rssi readings are combined in one column consisting a letter for the column and a number for the row of the position. the following figure depicts the layout of the ibeacons as well as the arrange of locations.\n",
      "attribute information\n",
      "location: the location of receiving rssis from ibeacons b3001 to b3013; symbolic values showing the column and row of the location on the map (e.g., a01 stands for column a, row 1).\n",
      "date: datetime in the format of ‘d-m-yyyy hh:mm:ss’\n",
      "b3001 - b3013: rssi readings corresponding to the ibeacons; numeric, integers only.\n",
      "acknowledgements\n",
      "provider: mehdi mohammadi and ala al-fuqaha, {mehdi.mohammadi, ala-alfuqaha}@wmich.edu, department of computer science, western michigan university\n",
      "citation request:\n",
      "m. mohammadi, a. al-fuqaha, m. guizani, j. oh, “semi-supervised deep reinforcement learning in support of iot and smart city services,” ieee internet of things journal, vol. pp, no. 99, 2017.\n",
      "inspiration\n",
      "how unlabeled data can help for an improved learning system. how a gan model can synthesizes viable paths based on the little labeled data and larger set of unlabeled data.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 94,000 hotels) that was created by extracting data from booking.com, a leading travel portal.\n",
      "content\n",
      "this dataset has following fields:\n",
      "address\n",
      "city\n",
      "country\n",
      "crawl_date\n",
      "hotel_brand\n",
      "hotel_description\n",
      "hotel_facilities\n",
      "hotel_star_rating\n",
      "image_count\n",
      "latitude\n",
      "locality\n",
      "longitude\n",
      "pageurl\n",
      "property_id\n",
      "property_name\n",
      "property_type\n",
      "province\n",
      "qts\n",
      "room_count\n",
      "room_type\n",
      "similar_hotel\n",
      "site_review_count\n",
      "site_review_rating\n",
      "site_stay_review_rating\n",
      "sitename\n",
      "special_tag\n",
      "state\n",
      "uniq_id\n",
      "zone\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of the property ratings and property type can be performed.\n",
      "this dataset does not have a description yet.\n",
      "problem description:\n",
      "while gps enabled devices can guide you path on street or on roads. these devices are limited in their usage because these can’t help in walking thorough the enclosed spaces like rooms etc. and thus limitation for blind people to walk in enclosed space.¬ we come up with an idea to model first person walk in rooms to come out from the rooms or enclosed spaces like whenever any person sees a wall in front of him he moves to right or left and fine tune his direction and get out of the room and avoids wall collisions. so we have collected data to guide a person especially blind person to go out of room doors. the typical workflow consists of approach of computer vision in the following way: the data for first person walking simulator is gathered in a game environment where room’s maps are created and images are captured and labelled walk by forward, left and right classes. typically human walk is more complicated, but we in this stage only collected three type of action. in the aim to predict the direction to come out of the room.\n",
      "content:\n",
      "a human first recognize the view in front of him and then decides an action by taking step in forward, sideways, left , right or more complex movement. the same is done during data gathering stage i.e. we captured image first and labelled as forward if agent can move forward or right if he need to turn right to go out of the door/room.\n",
      "the whole community of deep learning is invited to help us in walking of disabled person.\n",
      "context\n",
      "pakistan has a large number of public and private universities offering degrees in multiple disciplines. there are 162 universities out of which 64 are in private sector and 98 are public sector/government universities recognized by the higher education commission of pakistan (hec).\n",
      "according to hec, pakistani universities are producing over half a million graduates per year, which include over more than 10,000 computer science/it graduates.\n",
      "from year 2001 to 2015 there is a mass increase in number of enrollment in universities. the recent statistics shows that in 2015, 1,298,600 students enrolled in different levels of degree, 869,378 in bachelors (16 years), 63,412 in bachelors (17 years), 219,280 in masters (16 years), 124,107 in m.phil/ms, 14,373 in ph.d, and 8,319 in p.g.d. however, in 2014 the number of doctoral degree awarded were 1,351 only.\n",
      "moreover, according to hec report, in 2014-2015 there are over 10,125 fulltime ph.d. faculty teaching in pakistan in all disciplines. computer science and related disciplines are widely taught in pakistan with over 90 universities offering this discipline with qualified faculty. according to our dataset, there are 504 phd faculty members in computer science in pakistan for 10,000 students. so we have a phd faculty member for every 20 students on average in computer science program.\n",
      "current student to phd professor ratio in pakistan is 130:1 (while india is going towards 10:1 in post-graduate and 25:1 in undergrad education).\n",
      "here is world's top 100 universities with student to staff ratio.\n",
      "content\n",
      "dataset: the dataset contains list of computer science/it professors from 89 different universities of pakistan.\n",
      "variables: the dataset contains serial no, teacher’s name, university currently teaching, department, province university located, designation, terminal degree, graduated from (university for professor), country of graduation, year, area of specialization/research interests, and some other information\n",
      "acknowledgements\n",
      "data has been collected from respective university websites. some of the universities did not mention about their faculty profiles or were unavailable (hence the limitation of this dataset). the statistics mentioned above are gathered by higher education commission of pakistan (hec) website and other web resources.\n",
      "inspiration\n",
      "here is what i like you to do:\n",
      "which area of interest/expertise is in abundance in pakistan and where we need more people?\n",
      "how many professors we have in data sciences, artificial intelligence, or machine learning?\n",
      "which country and university hosted majority of our teachers?\n",
      "which research areas were most common in pakistan?\n",
      "how does pakistan student to phd professor ratio compare against rest of the world, especially with usa, india and china?\n",
      "any visualization and patterns you can generate from this data\n",
      "let me know how i can improve this dataset and best of luck with your work\n",
      "this is a simplified version of the \"human activity recognition using smartphones data set \" that can be found in: (https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)\n",
      "the sole purpose of this simplification is to use it as a teaching tool at the introductory machine learning course of universidad de palermo (buenos aires, argentina). url of the competition: https://www.kaggle.com/t/5c27656d61ec4808bcbddd67ac1fdc5a\n",
      "there are no claimed rights of any kind, please refer to original dataset (link above) in order to get the full dataset.\n",
      "abstract: human activity recognition database built from the recordings of 30 subjects performing activities of daily living (adl) while carrying a waist-mounted smartphone with embedded inertial sensors.\n",
      "data set characteristics: multivariate, time-series\n",
      "source:\n",
      "jorge l. reyes-ortiz(1,2), davide anguita(1), alessandro ghio(1), luca oneto(1) and xavier parra(2) 1 - smartlab - non-linear complex systems laboratory diten - università degli studi di genova, genoa (i-16145), italy. 2 - cetpd - technical research centre for dependency care and autonomous living universitat politècnica de catalunya (barcelonatech). vilanova i la geltrú (08800), spain activityrecognition '@' smartlab.ws\n",
      "data set information:\n",
      "the experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. each person performed six activities (walking, walking_upstairs, walking_downstairs, sitting, standing, laying) wearing a smartphone (samsung galaxy s ii) on the waist. using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50hz. the experiments have been video-recorded to label the data manually. the obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n",
      "the sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). the sensor acceleration signal, which has gravitational and body motion components, was separated using a butterworth low-pass filter into body acceleration and gravity. the gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 hz cutoff frequency was used. from each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n",
      "check the readme.txt file for further details about this dataset.\n",
      "an updated version of this dataset can be found at (https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) it includes labels of postural transitions between activities and also the full raw inertial signals instead of the ones pre-processed into windows.\n",
      "context\n",
      "bart, short for \"bay area rapid transit\", is the transit system severing the san francisco bay area in california. bart operates six routes, 46 stations, and and 112 miles of track. it serves an average weekday ridership of 423,000 people, making it the fifth-busiest rapid transit system in the united states.\n",
      "this dataset contains daily information on bart ridership for a period covering all of 2016 and part of 2017. unlike some other rapid transit system datasets, this data includes movements between specific stations (there are just over 2000 station-to-station combinations).\n",
      "content\n",
      "this dataset is split in three files. station_info.csv includes generic information on individual stations. date-hour-soo-dest-2017.csv contains daily inter-station ridership for (part of) 2017. date-hour-soo-dest-2016.csv contains daily inter-station ridership for all of 2016.\n",
      "acknowledgements\n",
      "would like to thank the bart organization for recording the data and providing it to the public.\n",
      "https://www.bart.gov/about/reports/ridership\n",
      "inspiration\n",
      "which bart station is the busiest?\n",
      "what is the least popular bart route?\n",
      "when is the best time to go to sf if you want to find a seat?\n",
      "which day of the week is the busiest?\n",
      "how many people take the bart late at night?\n",
      "does the bart ever stop in a station without anyone going off or on?\n",
      "context\n",
      "we would like to have good open source speech recognition\n",
      "commercial companies try to solve a hard problem: map arbitrary, open-ended speech to text and identify meaning\n",
      "the easier problem should be: detect a predefined sequence of sounds and map it to a predefined action.\n",
      "lets tackle the simplest problem first: classifying single, short words (commands)\n",
      "audio training data is difficult to obtain.\n",
      "approaches\n",
      "the parent project (spoken verbs) created synthetic speech datasets using text-to-speech programs. the focus there is on single-syllable verbs (commands).\n",
      "the speech commands dataset (by pete warden, see the tensorflow speech recognition challenge) asked volunteers to pronounce a small set of words: (yes, no, up, down, left, right, on, off, stop, go, and 0-9).\n",
      "this data set provides synthetic counterparts to this real world dataset.\n",
      "open questions\n",
      "one can use these two datasets in various ways. here are some things i am interested in seeing answered:\n",
      "what is it in an audio sample that makes it \"sound similar\"? our ears can easily classify both synthetic and real speech, but for algorithms this is still hard. extending the real dataset with the synthetic data yields a larger training sample and more diversity.\n",
      "how well does an algorithm trained on one data set perform on the other? (transfer learning) if it works poorly, the algorithm probably has not found the key to audio similarity.\n",
      "are synthetic data sufficient for classifying real datasets? if this is the case, the implications are huge. you would not need to ask thousands of volunteers for hours of time. instead, you could easily create arbitrary synthetic datasets for your target words.\n",
      "a interesting challenge (idea for competition) would be to train on this data set and evaluate on the real dataset.\n",
      "synthetic data creation\n",
      "here i describe how the synthetic audio samples were created. code is available at https://github.com/johannesbuchner/spoken-command-recognition, in the \"tensorflow-speech-words\" folder.\n",
      "the list of words is in \"inputwords\". \"marvin\" was changed to \"marvel\", because \"marvin\" does not have a pronounciation coding yet.\n",
      "pronounciations were taken from the british english example pronciation dictionary (beep, http://svr-www.eng.cam.ac.uk/comp.speech/section1/lexical/beep.html ). the phonemes were translated for the next step with a translation table (see compile.py for details). this creates the file \"words\". there are multiple pronounciations and stresses for each word.\n",
      "a text-to-speech program (espeak) was used to pronounce these words (see generatetfspeech.sh for details). the pronounciation, stress, pitch, speed and speaker were varied. this gives >1000 clean examples for each word.\n",
      "noise samples were obtained. noise samples (airport babble car exhibition restaurant street subway train) come from aurora (https://www.ee.columbia.edu/~dpwe/sounds/noise/), and additional noise samples were synthetically created (ocean white brown pink). (see ../generatenoise.sh for details)\n",
      "noise and speech were mixed. the speech volume and offset were varied. the noise source, volume was also varied. see addnoise.py for details. addnoise2.py is the same, but with lower speech volume and higher noise volume. all audio files are one second (1s) long and are in wav format (16 bit, mono, 16000 hz).\n",
      "finally, the data was compressed into an archive and uploaded to kaggle.\n",
      "acknowledgements\n",
      "this work built upon\n",
      "pronounciation dictionary: beep: http://svr-www.eng.cam.ac.uk/comp.speech/section1/lexical/beep.html\n",
      "noise samples: aurora: https://www.ee.columbia.edu/~dpwe/sounds/noise/\n",
      "espeak: http://espeak.sourceforge.net/ and mbrola voices http://www.tcts.fpms.ac.be/synthesis/mbrola/mbrcopybin.html\n",
      "please provide appropriate citations to the above when using this work.\n",
      "to cite the resulting dataset, you can use:\n",
      "apa-style citation: \"buchner j. synthetic speech commands: a public dataset for single-word speech recognition, 2017. available from https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset/\".\n",
      "bibtex @article{speechcommands, title={synthetic speech commands: a public dataset for single-word speech recognition.}, author={buchner, johannes}, journal={dataset available from https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset/}, year={2017} }\n",
      "thanks to everyone trying to improve open source voice detection and speech recognition.\n",
      "links\n",
      "https://www.kaggle.com/jbuchner/spokenverbs\n",
      "https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
      "https://github.com/johannesbuchner/spoken-command-recognition/\n",
      "content\n",
      "emosim508 is the largest emoji similarity dataset that provides emoji similarity scores for 508 carefully selected emoji pairs. the most frequently co-occurring emoji pairs in a tweet corpus (that contains 147 million tweets) was used for creating the dataset and each emoji pair was annotated for its similarity using 10 human annotators. emosim508 dataset also consists of the emoji similarity scores generated from 8 different emoji embedding models proposed in \"a semantics-based measure of emoji similarity\" paper by wijeratne et al.\n",
      "acknowledgements\n",
      "emosim508 was developed by sanjaya wijeratne, lakshika balasuriya, amit sheth and derek doran. emosim508 is licensed under the creative commons attribution-noncommercial-sharealike 3.0 unported (cc by-nc-sa 3.0) license. please cite the following paper when using emosim508 dataset:\n",
      "sanjaya wijeratne, lakshika balasuriya, amit sheth, and derek doran. 2017. a semantics-based measure of emoji similarity. in proceedings of wi ’17, leipzig, germany, august 23-26, 2017, 8 pages. doi: 10.1145/3106426.3106490\n",
      "@inproceedings{dblp:conf/webi/wijeratnebsd17, author = {sanjaya wijeratne and lakshika balasuriya and amit sheth and derek doran}, title = {a semantics-based measure of emoji similarity}, booktitle = {proceedings of the international conference on web intelligence, leipzig, germany, august 23-26, 2017}, pages = {646--653}, year = {2017}, doi = {10.1145/3106426.3106490} }\n",
      "you can also find more information about the dataset on the project website.\n",
      "inspiration\n",
      "can you use these emoji similarity ratings to better group emoji in emoji keyboards?\n",
      "can you use emosim508 as a baseline dataset to test your emoji similarity approach's performance?\n",
      "can you use these emoji similarity ratings to suggest emoji domain names if they are already taken?\n",
      "context\n",
      "each file contains klines for 1 month period with 1 minute intervals. file name formating looks like mm-yyyy-smb1smb2 (e.g. 11-2017-xrpbtc).\n",
      "this data set contains now only xrp/btc and eth/usdt symbol pair now, but it will be expand soon.\n",
      "features\n",
      "open time -> timestamp (milliseconds)\n",
      "open price -> float\n",
      "high price -> float\n",
      "low price -> float\n",
      "close price -> float\n",
      "volume -> float\n",
      "quote asset volume -> float\n",
      "close time -> timestamp (milliseconds)\n",
      "number of trades -> int\n",
      "taker buy base asset volume -> float\n",
      "taker buy quote asset volume -> float\n",
      "acknowledgements\n",
      "this dataset was collected from binance exchange | worlds largest crypto exchange\n",
      "inspiration\n",
      "this data set could inspire you on most efficient trading algorithms.\n",
      "context\n",
      "the public plans dataset compiles publicly-available information on major u.s. public pension plans from their annual reporting. this includes state-level plans as well as some large plans from localities such as new york city and chicago.\n",
      "content\n",
      "the public plans dataset has data compiled from the actuarial reports and cafrs (comprehensive annual financial reports) for major plans. it includes balance sheet (assets/liabilities), income (e.g., investment income), and cash flow (e.g., benefit payments) information. in addition, there are items such as number of beneficiaries receiving payment, the number of active participants, the number vested, etc.\n",
      "the current set covers fiscal years 2001 - 2016, which usually run mid-year to mid-year (this can differ by plan).\n",
      "acknowledgements\n",
      "data from public plans data: http://publicplansdata.org/public-plans-database/\n",
      "the public plans data project comes out of a partnership between the center for retirement research at boston college (crr) and the center for state and local government excellence (slge). the national association of state retirement administrators (nasra), which has been collecting and sharing public plan data since 2001, supports the partnership by providing review and assistance on the development of data models, validation of data, and development and administration of surveys. more here: http://publicplansdata.org/about/our-research/\n",
      "cover photo by ashutosh nandeshwar on unsplash\n",
      "inspiration\n",
      "the center for retirement research often produces research briefs based on this data, which can be found here: http://publicplansdata.org/research/issue-briefs/?category=crr\n",
      "questions i have been pursuing using this data:\n",
      "what has been driving decreasing funded ratios in u.s. pensions?\n",
      "which pension plans are sustainable? which are in trouble?\n",
      "is there a link between pension fundedness and investment strategy?\n",
      "it may be useful to link these data sets to information on various state/local revenue amounts, in order to determine sustainability of pension costs.\n",
      "context\n",
      "the hc corpora was a great resource that contains natural language text from various newspapers, social media posts and blog pages in multiple languages. this is a cleaned version of the raw data from newspaper subset of the hc corpus.\n",
      "originally, this subset was created for a language identification task for similar languages\n",
      "content\n",
      "the columns of each row in the .tsv file are:\n",
      "langauge: language of the text.\n",
      "source: newspaper from which the text is from.\n",
      "date: date of the article that contains the text.\n",
      "text: sentence/paragraph from the newspaper\n",
      "the corpus contains 16,806,041 sentences/paragraphs in 67 languages:\n",
      "afrikaans\n",
      "albanian\n",
      "amharic\n",
      "arabic\n",
      "armenian\n",
      "azerbaijan\n",
      "bengali\n",
      "bosnian\n",
      "catalan\n",
      "chinese (simplified)\n",
      "chinese (traditional)\n",
      "croatian\n",
      "welsh\n",
      "czech\n",
      "german\n",
      "danish\n",
      "danish\n",
      "english\n",
      "spanish\n",
      "spanish (south america)\n",
      "finnish\n",
      "french\n",
      "georgian\n",
      "galician\n",
      "greek\n",
      "hebrew\n",
      "hindi\n",
      "hungarian\n",
      "icelandic\n",
      "indonesian\n",
      "italian\n",
      "japanese\n",
      "khmer\n",
      "kannada\n",
      "korean\n",
      "kazakh\n",
      "lithuanian\n",
      "latvian\n",
      "macedonian\n",
      "malayalam\n",
      "mongolian\n",
      "malay\n",
      "nepali\n",
      "dutch\n",
      "norwegian (bokmal)\n",
      "punjabi\n",
      "farsi\n",
      "polish\n",
      "portuguese (brazil)\n",
      "portuguese (eu)\n",
      "romanian\n",
      "russian\n",
      "serbian\n",
      "sinhalese\n",
      "slovak\n",
      "slovenian\n",
      "swahili\n",
      "swedish\n",
      "tamil\n",
      "telugu\n",
      "tagalog\n",
      "thai\n",
      "turkish\n",
      "ukranian\n",
      "urdu\n",
      "uzbek\n",
      "vietnamese\n",
      "languages in hc corpora but not in this (yet):\n",
      "estonian\n",
      "greenlandic\n",
      "gujarati\n",
      "acknowledge\n",
      "all credits goes to hans christensen, the creator of hc corpora.\n",
      "dataset image is from philip strong.\n",
      "inspire\n",
      "use this dataset to:\n",
      "create a language identifier / detector\n",
      "exploratory corpus linguistics (it’s one capstone project from coursera’s data science specialization )\n",
      "context\n",
      "this dataset includes quantitative and categorical features from online reviews from 21 hotels located in las vegas strip, extracted from tripadvisor. all the 504 reviews were collected between january and august of 2015.\n",
      "content\n",
      "the dataset contains 504 records and 20 tuned features, 24 per hotel (two per each month, randomly selected), regarding the year of 2015. the csv contains a header, with the names of the columns corresponding to the features.\n",
      "acknowledgements\n",
      "lichman, m. (2013). uci machine learning repository http://archive.ics.uci.edu/ml. irvine, ca: university of california, school of information and computer science\n",
      "downloaded form uci machine learning repository\n",
      "inspiration\n",
      "do machine learning algorithms take into account what happens in vegas stays in vegas?\n",
      "context\n",
      "shark tank is a great show based on an interesting concept wherein entrepreneurs and founders pitch their businesses in front of seasoned investors (aka sharks) who decide whether or not to invest in the businesses based on multiple parameters.\n",
      "the show has many versions in different regions, and this database is for the us version, featuring, among other guest sharks, mark cuban, robert herjavec, daymond john, kevin o'leary, barbara corcoran, and lori greiner.\n",
      "the investment decisions on the show are merely handshake deals which are followed up by a detailed due-diligence and subsequent final investment decisions. many of the deals taking place on the show do not go through.\n",
      "among many other points, some of the major decision vectors for the sharks to make a deal are:\n",
      "the relevance of the business to their fields of interest and exposure (daymond for fashion, lori for qvc, kevin for wines, etc.)\n",
      "the pitch quality (preparation, energy, etc. of the presenter)\n",
      "health of the business (financials, debts, etc.)\n",
      "valuation (the most important)\n",
      "since elements such as pitch quality, exact financials disclosed, and specifics of what communication happened between the sharks and the presenters can be considered to be copyrighted to the show, i picked up the publically available details of the pitches and the results (deal = yes or no) and the associated shark(s) from websites, consolidated and cleaned the data, and presented in this dataset.\n",
      "the idea is that a text vector based learning algorithm might be able to predict, given a description of a new pitch, how likely is the pitch to succeed in the shark tank, and even which shark might be more interested in the pitch.\n",
      "content\n",
      "the dataset contains following headers:\n",
      "season_epi_code - the data spans all 8 seasons of shark tank (us) and this code gives the season and the episode for indexing purposes. format = see (101 = 1st season 1st episode, 826 = 8th season 26th episode)\n",
      "pitched_business_identifier - a short name of the pitched business\n",
      "pitched_business_desc - brief description of the pitched business. combination of text from more than one source has been added here, and there might be repetition or a very small description.\n",
      "deal_status - status of whether the pitched business got a deal in the episode where at least one shark and the presenters agreed on a particular deal. format = (yes = 1, no = 0)\n",
      "deal_shark - which of the most common sharks agreed on the episode along with the presenters for a deal? format = either single shark's initials or '+' separated values of more than one shark's initials\n",
      "initials used: bc - barbara corcoran dj - daymond john kol - kevin o'leary lg - lori greiner mc - mark cuban rh - robert herjavec\n",
      "note: while i have tried my best to collect, consolidate and clean the data, i do not make any claims of completeness or accuracy of data in the dataset. the user assumes the entire risk with respect to the use of this dataset.\n",
      "acknowledgements\n",
      "abc for producing such an entertaining, educational and well-managed show. photo by jakob owens on unsplash\n",
      "inspiration\n",
      "the idea is that a text vector based learning algorithm might be able to predict, given a description of a new pitch, how likely is the pitch to succeed in the shark tank, and even which shark might be more interested in the pitch.\n",
      "i have planned to cover the 5 most interesting solutions (eda as well as actual prediction models) in a series of blog posts on thinkpatcri.com\n",
      "the h15 release from the federal reserve provides daily interest rate information for a variety of core interest rates, such as t bills and the federal funds rate. for some rates, this dataset extends all the way back to 1954.\n",
      "the rates included are:\n",
      "time_period\n",
      "federal_funds\n",
      "1_month_nonfinancial_commercial_paper\n",
      "2_month_nonfinancial_commercial_paper\n",
      "3_month_nonfinancial_commercial_paper\n",
      "1_month_financial_commercial_paper\n",
      "2_month_financial_commercial_paper\n",
      "3_month_financial_commercial_paper\n",
      "prime_rate\n",
      "discount_rate\n",
      "4_week_treasury_bill\n",
      "3_month_treasury_bill\n",
      "6_month_treasury_bill\n",
      "1_year_treasury_bill\n",
      "1_month_treasury_constant_maturity\n",
      "3_month_treasury_constant_maturity\n",
      "6_month_treasury_constant_maturity\n",
      "1_year_treasury_constant_maturity\n",
      "2_year_treasury_constant_maturity\n",
      "3_year_treasury_constant_maturity\n",
      "5_year_treasury_constant_maturity\n",
      "7_year_treasury_constant_maturity\n",
      "10_year_treasury_constant_maturity\n",
      "20_year_treasury_constant_maturity\n",
      "30_year_treasury_constant_maturity\n",
      "5_year_inflation_indexed_treasury_constant_maturity\n",
      "7_year_inflation_indexed_treasury_constant_maturity\n",
      "10_year_inflation_indexed_treasury_constant_maturity\n",
      "20_year_inflation_indexed_treasury_constant_maturity\n",
      "30_year_inflation_indexed_treasury_constant_maturity\n",
      "inflation_indexed_long_term_average\n",
      "please see https://www.federalreserve.gov/releases/h15/ for notices, caveats, and newly released data.\n",
      "this dataset records the time, location, priority, and reason for calls to 911 in the city of baltimore.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the city of baltimore. they update the data daily; you can find the original version here.\n",
      "inspiration\n",
      "the study discussed in this atlantic article reviewing 911 calls in milwaukuee found that that incidents of police violence lead to large drops in the number of 911 calls. does this hold true for baltimore as well?\n",
      "context:\n",
      "the thai language is the primary language of thailand and a recognized minority language in cambodia. it has approximately twenty million native speakers, in addition to 44 million second language speakers. it is written in thai script (also called the thai alphabet) which is notable for being the first writing system to incorporate tonal markers. thai is written without spaces between words.\n",
      "content:\n",
      "the hse thai corpus is a corpus of modern texts written in thai language. the texts, containing in whole 50 million tokens, were collected from various thai websites (mostly news websites). to make it easier for non-thai-speakers to comprehend and use texts in the corpus the researchers decided to separate words in each sentence with spaces.\n",
      "the data for the corpus was collected by means of scrapy. to tokenize texts the pythai module was used. the text in this dataset is encoded in utf-8.\n",
      "the corpus can be searched using a web interface at this site.\n",
      "this dataset contains text from two sources: wikipedia and thaigov.go.th. the former is licensed under a standard wikipedia license, and the latter under an open government license for thailand, which can be viewed here (in thai).\n",
      "acknowledgements:\n",
      "the thai corpus was developed by the team of students of hse school of linguistics in moscow under the guidance of professor boris orekhov. the team consisted of grigory ignatyev, alexandra ershova, anna kuznetsova, tatyana shalganova, daniil kolomeytsev and nikolai mikulin. the consulting help on thai language was provided by nadezhda motina. natalia filippova, elizaveta kuzmenko, tatyana gavrilova, elena krotova, elmira mustakimova, olga sozinova, aleksandra martynova, maria sheyanova, marina kustova and julia badryzlova also contributed to the project.\n",
      "inspiration:\n",
      "in this corpus, unlike in most written thai, words have been separated for you with spaces. can you remove spaces and write an algorithm to identify word boundaries?\n",
      "context:\n",
      "the home mortgage disclosure act (hmda) requires many financial institutions to maintain, report, and publicly disclose information about mortgages.\n",
      "content:\n",
      "this dataset covers all mortgage decisions made in 2015 for the state of new york. data for additional states and years can be accessed here.\n",
      "acknowledgements:\n",
      "this dataset was compiled by the consumer finance protection board.\n",
      "inspiration:\n",
      "where are mortgages most likely to be approved?\n",
      "can you predict mortgage decisions based on the criteria provided here?\n",
      "context:\n",
      "the people with significant control (psc) snapshot is a data snapshot containing the full list of psc's provided to companies house. the prime minister first put corporate transparency on the international agenda when he chaired the g8 summit in lough erne and secured commitment to action, the commitment to enhance corporate transparency in the uk was reaffirmed at london’s international anti-corruption summit in may 2016. since then the eu and g20 countries have also agreed to act. the uk is the first country in the g20 to create a public register of this kind.\n",
      "the uk has high standards of business behaviour and corporate governance. the overwhelming majority of uk companies contribute productively to the uk economy, abide by the law and make a valuable contribution to society. but there are exceptions. some of the features of the company structure which make it good for business also make it attractive to criminals. companies can be misused to facilitate a range of criminal activities - from money laundering to tax evasion, corruption to terrorist financing. sometimes those individuals running companies will not conduct themselves in accordance with the high standards we expect in the uk, posing a risk to other companies and consumers alike.\n",
      "information about the ownership and control of uk corporate entities will bring benefits for law enforcement, business, civil society and citizens. by making this information publicly available, free of charge, the government is setting a standard that we are persuading other countries to follow.\n",
      "content:\n",
      "a person of significant control is someone that holds more than 25% of shares or voting rights in a company, has the right to appoint or remove the majority of the board of directors or otherwise exercises significant influence or control. this is a snapshot of data in zipped json form, as of aug 23 2017. daily updated snapshots and streaming api details can be found here. the people with significant control (psc) register includes information about the individuals who own or control companies including their name, month and year of birth, nationality, and details of their interest in the company. from 30 june 2016, uk companies (except listed companies) and limited liability partnerships (llps) need to declare this information when issuing their annual confirmation statement to companies house.\n",
      "acknowledgements:\n",
      "guidance here. the data is collected by uk government.\n",
      "inspiration:\n",
      "who owns the most businesses? in certain areas?\n",
      "any weird looking situations where ownership might be obscured?\n",
      "context\n",
      "this dataset includes the 2010-2014 \"facility-level\" emissions data, combined with geographical & industry-related data. it is based on the epa's toxic release inventory (tri) & greenhouse gas reporting inventory (ghg), the national system of nomenclature that is used to describe the industry-related emissions.\n",
      "although the epa publishes and maintains the tri & ghg report in various forms, the combination of the two is not readily available. hence this dataset.\n",
      "content\n",
      "the csv has 28 columnar variables defined as:\n",
      "uniqueid\n",
      "facility name\n",
      "rank tri '14\n",
      "rank ghg '14\n",
      "latitude\n",
      "longitude\n",
      "location address\n",
      "city\n",
      "state\n",
      "zip\n",
      "county\n",
      "fips code\n",
      "primary naics\n",
      "second primary naics\n",
      "third primary naics\n",
      "industry type\n",
      "parent companies 2014 (ghg)\n",
      "parent companies 2014 (tri)\n",
      "tri air emissions 14 (in pounds)\n",
      "tri air emissions 13 [and previous years]\n",
      "ghg direct emissions 14 (in metric tons)\n",
      "ghg direct emissions 13 [and previous years]\n",
      "ghg facility id\n",
      "second ghg facility id [and third, fourth, etc.]\n",
      "tri id\n",
      "second tri id [and third, fourth, etc.]\n",
      "frs id\n",
      "second frs id [and third, fourth, etc.]\n",
      "acknowledgements\n",
      "this dataset was made available by the center for public integrity.\n",
      "context\n",
      "i found this at https://www.reddit.com/r/datasets/comments/47a7wh/ufc_fights_and_fighter_data/\n",
      "all credit goes to reddit user geyges and sherdog.\n",
      "i do not own the data.\n",
      "content\n",
      "this data has multiple categorical variables from every ufc fight from ufc 1 in 1993 - 2/23/2016.\n",
      "acknowledgements\n",
      "reddit u/geyges sherdog ufc\n",
      "inspiration\n",
      "so much information can be gained from this relevant to understanding how the sport has evolved over the years.\n",
      "content\n",
      "dataset containing monetary contributions from the telecom industry, as well as presumed net neutrality position.\n",
      "the dataset contains: name, contribution amount, vote, political party, state, political position\n",
      "the voting section refers to the vote on s.j.res. 34 - taken place in march, 2017.\n",
      "context\n",
      "these datums represent a multi-agent system for the care of elderly people living at home on their own, with the aim to prolong their independence. the system was designed to provide a reliable, robust and flexible monitoring by sensing the user in the environment, reconstructing the position and posture to create the physical awareness of the user in the environment, reacting to critical situations, calling for help in the case of an emergency, and issuing warnings if unusual behavior was detected.\n",
      "content\n",
      "columns descriptions\n",
      "sequence name: a01, a02, a03, a04, a05, b01, b02, b03, b04, b05, ...,e05\n",
      "a-e represent a person (5 total)\n",
      "01, 02, 03, 04, 05 = tag numbers\n",
      "tag identifiers\n",
      "ankle_left = 010-000-024-033\n",
      "ankle_right = 010-000-030-096\n",
      "chest = 020-000-033-111\n",
      "belt = 020-000-032-221\n",
      "time stamp\n",
      "date\n",
      "format = dd.mm.yyyy hh:mm:ss:sss\n",
      "x coordinate of the tag\n",
      "y coordinate of the tag\n",
      "z coordinate of the tag\n",
      "activity\n",
      "walking, falling, 'lying down', lying, 'sitting down', sitting, 'standing up from lying', 'on all fours', 'sitting on the ground', 'standing up from sitting', 'standing up from sitting on the ground\n",
      "acknowledgements\n",
      "b. kaluza, v. mirchevska, e. dovgan, m. lustrek, m. gams, an agent-based approach to care in independent living, international joint conference on ambient intelligence (ami-10), malaga, spain, in press\n",
      "inspiration\n",
      "given these data, can you classify the persons activity from the tags they wore?\n",
      "context\n",
      "the problem is to predict user ratings for web pages (within a subject category). the html source of a web page is given. users looked at each web page and indicated on a 3 point scale (hot medium cold) 50-100 pages per domain.\n",
      "content\n",
      "this database contains html source of web pages plus the ratings of a single user on these web pages. web pages are on four separate subjects (bands- recording artists; goats; sheep; and biomedical).\n",
      "acknowledgement\n",
      "data originally from the uci ml repository. donated by:\n",
      "michael pazzani department of information and computer science, university of california, irvine irvine, ca 92697-3425 pazzani@ics.uci.edu\n",
      "concept based information access with google for personalized information retrieval\n",
      "fuel economy data are the result of vehicle testing done at the environmental protection agency's national vehicle and fuel emissions laboratory in ann arbor, michigan, and by vehicle manufacturers with oversight by epa.\n",
      "content\n",
      "please see the csvs labeled with 'fields' for descriptions of the data fields; there are too many to list here.\n",
      "acknowledgements\n",
      "this dataset was kindly provided by the us epa. you can find the original dataset, which is updated regularly, here.\n",
      "inspiration\n",
      "how has the rate of change of fuel economy changed over time?\n",
      "do simple clustering techniques on vehicles lead to the same groupings that are typically associated with manufacturers, such as putting porsche and bmw together in a luxury car group?\n",
      "context\n",
      "the us bureau of labor statistics monitors and collects day-to-day information about the market price of raw inputs and finished goods, and publishes regularized statistical assays of this data. the consumer price index and the producer price index are its two most famous products. the former tracks the aggregate dollar price of consumer goods in the united states (things like onions, shovels, and smartphones); the latter (this dataset) tracks the cost of raw inputs to the industries producing those goods (things like raw steel, bulk leather, and processed chemicals).\n",
      "the us federal government uses this dataset to track inflation. while in the short term the raw dollar value of producer inputs may be volatile, in the long term it will always go up due to inflation --- the slowly decreasing buying power of the us dollar.\n",
      "content\n",
      "this dataset consists of a packet of files, each one tracking regularized cost of inputs for certain industries. the data is tracked-month to month with an index out of 100.\n",
      "acknowledgements\n",
      "this data is published online by the us bureau of labor statistics.\n",
      "inspiration\n",
      "how does the producer price index compare against the consumer price index?\n",
      "what have the largest spikes in input costs been, historically? can you determine why they occurred?\n",
      "what is the overall price index trend amongst us producers?\n",
      "context:\n",
      "thor is a painstakingly cultivated database of historic aerial bombings from world war i through vietnam. thor has already proven useful in finding unexploded ordinance in southeast asia and improving air force combat tactics. our goal is to see where public discourse and innovation takes this data. each theater of warfare has a separate data file, in addition to a thor overview.\n",
      "content:\n",
      "4.8 million rows with 47 columns describing each run. see the data dictionary here.\n",
      "acknowledgements:\n",
      "thor is a dataset project initiated by lt col jenns robertson and continued in partnership with data.mil, an experimental project, created by the defense digital service in collaboration with the deputy chief management officer and data owners throughout the u.s. military.\n",
      "inspiration:\n",
      "which campaigns saw the heaviest bombings?\n",
      "which months saw the most runs?\n",
      "what were the most used aircraft?\n",
      "the federal reserve's h.10 statistical release provides data on exchange rates between the us dollar, 23 other currencies, and three benchmark indexes. the data extend back to 1971 for several of these.\n",
      "please note that the csv has six header rows. the first contains the\n",
      "acknowledgements\n",
      "this dataset was provided by the us federal reserve. if you need the current version, you can find their weekly updates here.\n",
      "inspiration\n",
      "venezuela is both unusually dependent on oil revenues and experiencing unusual degrees of political upheaval. can you determine which movements in their currency were driven by changes in the oil price and which were driven by political events?\n",
      "context\n",
      "the word2vec sample model redistributed by nltk is used to demonstrate how word embeddings can be used together with gensim.\n",
      "the detailed demonstration can be found on https://github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest\n",
      "acknowledgements\n",
      "credit goes to word2vec and gensim developers.\n",
      "context:\n",
      "the nyc department of health requires all dog owners to license their dogs. the resulting names data was released on github with a nice interactive d3 word cloud. additional data (including type and color) is available from wnyc here.\n",
      "content:\n",
      "this data covers dog names, and the counts of each name.\n",
      "cheltenham pa, crime data\n",
      "cheltenham is a home rule township bordering north philadelphia in montgomery county. it has a population of about 37,000 people. you can find out more about cheltenham on wikipedia.\n",
      "cheltenham's facebook groups. contains postings on crime and other events in the community.\n",
      "getting started\n",
      "reading data is a simple python script for getting started.\n",
      "if you prefer to use r, there is an example kernel here.\n",
      "proximity to philadelphia\n",
      "this township borders on philadelphia, which may or may not influence crime in the community. for philadelphia crime patterns, see the philadelphia crime dataset.\n",
      "reference\n",
      "data was obtained from socrata.com\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context:\n",
      "*annual health survey : mortality schedule *\n",
      "this unit level dataset contains the details of clinical, anthropometric & bio-chemical (cab) survey. to supplement the information provided by annual health survey (ahs),a biomarker component has been introduced in ahs to collect data on nutritional status, life style diseases like diabetes & hypertension and anemia in empowered action group (eag) states & assam. this component, namely clinical, anthropometric and bio-chemical (cab) survey, is conducted on a sub-sample of ahs in all eag states namely bihar, chhattisgarh, jharkhand, madhya pradesh, odisha, rajasthan, uttarakhand & uttar pradesh and assam.\n",
      "there are total of 1.89million observations and 53 variables in this dataset.\n",
      "survey:\n",
      "base line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)\n",
      "these nine states, which account for about 48 percent of the total population, 59 percent of births, 70 percent of infant deaths, 75 percent of under 5 deaths and 62 percent of maternal deaths in the country, are the high focus states in view of their relatively higher fertility and mortality.\n",
      "content:\n",
      "the files contains the below columns.\n",
      "variable names:\n",
      "state_code\n",
      "district_code\n",
      "rural_urban\n",
      "stratum\n",
      "psu_id\n",
      "ahs_house_unit\n",
      "house_hold_no\n",
      "date_survey\n",
      "test_salt_iodine\n",
      "record_code_iodine\n",
      "record_code_iodine_reason\n",
      "sl_no\n",
      "sex\n",
      "usual_residance\n",
      "usual_residance_reason\n",
      "identification_code\n",
      "age_code\n",
      "age\n",
      "date_of_birth\n",
      "month_of_birth\n",
      "year_of_birth\n",
      "weight_measured\n",
      "weight_in_kg\n",
      "length_height_measured\n",
      "length_height_code\n",
      "length_height_cm\n",
      "haemoglobin_test\n",
      "haemoglobin\n",
      "haemoglobin_level\n",
      "bp_systolic\n",
      "bp_systolic_2_reading\n",
      "bp_diastolic\n",
      "bp_diastolic_2reading\n",
      "pulse_rate\n",
      "pulse_rate_2_reading\n",
      "diabetes_test\n",
      "fasting_blood_glucose\n",
      "fasting_blood_glucose_mg_dl\n",
      "marital_status\n",
      "gauna_perfor_not_perfor\n",
      "duration_pregnanacy\n",
      "first_breast_feeding\n",
      "is_cur_breast_feeding\n",
      "day_or_month_for_breast_feeding_code\n",
      "day_or_month_for_breast_feeding\n",
      "water_month\n",
      "ani_milk_month\n",
      "semisolid_month_or_day\n",
      "solid_month\n",
      "vegetables_month_or_day\n",
      "illness_type\n",
      "illness_duration\n",
      "treatment_type\n",
      "file content: mortality_data_dictionary.xlsx : this data dictionary excel work book has the detailed information about each and every column and codes used in the data.\n",
      "acknowledgements\n",
      "department of health and family welfare, govt. of india has published this data in open govt data platform india portal under govt. open data license - india.\n",
      "context\n",
      "having such a task as predicting the travel time of taxis, it can be insightful to have a deeper look at the underlying street network of the city. network analysis can enable us to get insights for why certain taxi trips take longer than others given some basic network properties. examples for the analysis can be: calculate the shortest path, measure the influence of specific streets on the robustness of the network or find out which streets are key points in the network when it comes to traffic flow.\n",
      "content\n",
      "this dataset contains one large graph for the street network of new york city in graphml format and a subgraph for the area of manhattan for fast testing of your analysis. each graph was created with the awesome python package https://github.com/gboeing/osmnx which is not available on kaggle. the graphs nodes attributes are taken from osm and contain information to which other nodes they are connected, how long the connection is, which speed limit it has etc.\n",
      "acknowledgements\n",
      "https://github.com/gboeing/osmnx\n",
      "inspiration\n",
      "explore the new york street network, gain a deeper understanding for network analysis and craft some useful features for the taxi trip prediction competition!\n",
      "context\n",
      "this data was extracted from the 1994 census bureau database by ronny kohavi and barry becker (data mining and visualization, silicon graphics). a set of reasonably clean records was extracted using the following conditions: ((aage>16) && (agi>100) && (afnlwgt>1) && (hrswk>0)). the prediction task is to determine whether a person makes over $50k a year.\n",
      "in order to make the job better we used artificial intelligence to automatically modify the columns.\n",
      "content\n",
      "this dataset contains the initial dataset columns as well as the new ones obtained by feeding the original us census dataset to predicsis.ai in order to automatically : 1. discretise continuous variables into relevant intervals. 2. group values of categorical variables together in order to reduce the modality of the variables.\n",
      "acknowledgements\n",
      "https://archive.ics.uci.edu/ml/datasets/census+income\n",
      "https://www.kaggle.com/uciml/adult-census-income\n",
      "https://predicsis.ai\n",
      "inspiration\n",
      "we want to see by how much auto ml/ai improves the data scientist work quality.\n",
      "this dataset contains estimates of the socioeconomic status (ses) position of each of 149 countries covering the period 1880-2010. measures of ses, which are in decades, allow for a 130 year time-series analysis of the changing position of countries in the global status hierarchy. ses scores are the average of each country’s income and education ranking and are reported as percentile rankings ranging from 1-99. as such, they can be interpreted similarly to other percentile rankings, such has high school standardized test scores. if country a has an ses score of 55, for example, it indicates that 55 percent of the countries in this dataset have a lower average income and education ranking than country a. iso alpha and numeric country codes are included to allow users to merge these data with other variables, such as those found in the world bank’s world development indicators database and the united nations common database.\n",
      "see here for a working example of how the data might be used to better understand how the world came to look the way it does, at least in terms of status position of countries.\n",
      "variable descriptions:\n",
      "unid: iso numeric country code (used by the united nations)\n",
      "wbid: iso alpha country code (used by the world bank)\n",
      "ses: country socioeconomic status score (percentile) based on gdp per capita and educational attainment (n=174)\n",
      "country: short country name\n",
      "year: survey year\n",
      "gdppc: gdp per capita: single time-series (imputed)\n",
      "yrseduc: completed years of education in the adult (15+) population\n",
      "region5: five category regional coding schema\n",
      "regionun: united nations regional coding schema\n",
      "data sources:\n",
      "the dataset was compiled by shawn dorius (sdorius@iastate.edu) from a large number of data sources, listed below. gdp per capita:\n",
      "maddison, angus. 2004. 'the world economy: historical statistics'. organization for economic co-operation and development: paris. gdp & gdp per capita data in (1990 geary-khamis dollars, ppps of currencies and average prices of commodities). maddison data collected from: http://www.ggdc.net/maddison/historical_statistics/horizontal-file_02-2010.xls.\n",
      "world development indicators database years of education 1. morrisson and murtin.2009. 'the century of education'. journal of human capital(3)1:1-42. data downloaded from http://www.fabricemurtin.com/ 2. cohen, daniel & marcelo cohen. 2007. 'growth and human capital: good data, good results' journal of economic growth 12(1):51-76. data downloaded from http://soto.iae-csic.org/data.htm\n",
      "barro, robert and jong-wha lee, 2013, \"a new data set of educational attainment in the world, 1950-2010.\" journal of development economics, vol 104, pp.184-198. data downloaded from http://www.barrolee.com/\n",
      "maddison, angus. 2004. 'the world economy: historical statistics'. organization for economic co-operation and development: paris. 13.\n",
      "united nations population division. 2009.\n",
      "context\n",
      "this dataset was downloaded from inep, a department from the brazilian education ministry. it contains data from the applicants for the 2015 national high school exam.\n",
      "content\n",
      "inside this dataset there are not only the exam results, but the social and economic context of the applicants.\n",
      "acknowledgements\n",
      "the original dataset is provided by inep (http://portal.inep.gov.br/microdados). i removed some information from original files to fit the file size into the kaggle constraints.\n",
      "inspiration\n",
      "the objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results.\n",
      "context\n",
      "all companies in the sec edgar database. companies are listed with company name and their unique cik key. data-set had 663000 companies listed, which includes all companies in the edgar database.\n",
      "content\n",
      "will update later.\n",
      "acknowledgements\n",
      "will update later.\n",
      "inspiration\n",
      "will update later.\n",
      "this dataset contains 28 million recommendation and click/no click pairs from users of the sowiport library. from the abstract of the pre-print discussing the dataset:\n",
      "stereotype and most-popular recommendations are widely neglected in the research-paper recommender-system and digital-library community. in other domains such as movie recommendations and hotel search, however, these recommendation approaches have proven their effectiveness. we were interested to find out how stereotype and most-popular recommendations would perform in the scenario of a digital library. therefore, we implemented the two approaches in the recommender system of gesis’ digital library sowiport, in cooperation with the recommendations-as-aservice provider mr. dlib. we measured the effectiveness of most-popular and stereotype recommendations with click-through rate (ctr) based on 28 million delivered recommendations. most-popular recommendations achieved a ctr of 0.11%, and stereotype recommendations achieved a ctr of 0.124%. compared to a “random recommendations” baseline (ctr 0.12%), and a content-based filtering baseline (ctr 0.145%), the results are discouraging. however, for reasons explained in the paper, we concluded that more research is necessary about the effectiveness of stereotype and most-popular recommendations in digital libraries.\n",
      "this dataset was kindly made available by the authors of \"stereotype and most-popular recommendations in the digital library sowiport\" under the cc-by 3.0 license. you can find additional information at http://mr-dlib.org/.\n",
      "the game\n",
      "\"who dies?\" is a simple physics puzzle available for android. randomly, a world full of stones, monsters, coil springs, slingshots and other objects is created. the user has to guess, which monster will get hit by a stone or falls of the platform when gravity is turned on. he gets point for every right guess, the more points he collects, the more complex the worlds will get.\n",
      "the development\n",
      "for development, phaser was used, the game map is a 35x20 grid. each tile in this grid can contain different blockers or objects.\n",
      "the data\n",
      "every time a user is playing the game, the position of all objects is recorded as well as the selection the user has made and the final set of monsters who died. the dataset consists of 5 columns: datetime is a timestamp of when a user played the game. complexity is a parameter that measures the difficulty of the game (1 = easy, 100 = hard). map is a json array containing 3 arrays: the first array contains immobile foreground objects described by a \"type\" property including x and y coordinates. the following list gives an overview about the the most commonly used types:\n",
      "the second array contains immobile background objects that don't interact with the game objects and are therefore not relevant. the third array contains movable foreground objects. these could be: monsters (\"guys\"), balls (\"smallball\", \"ball\" and \"bigball\" with or without an initial rotation to the left or right), spring (catapults boxes and balls up in the air but not monsters), box, chain, seesaw, spin, switch and switchwall (if a ball touches a switch, all switchwalls with the same color as the switch change their visibility and become transparent to foreground objects or vice versa). column \"monsters_selected\" contains all the monsters a player thought will get hit by a ball (ordered by the selection time) \"monsters_hit\" contains all monsters that were actually killed by balls or fell of the platform (ordered by time)\n",
      "the goal\n",
      "there are several interesting outcomes, for example:\n",
      "creating a ml algorithm that is able to correctly predict the outcome of the game (which monster will die)\n",
      "creating a ml algorithm that is able to correctly predict which monsters a user will most likely pick\n",
      "creating a ml algorithm that is able to create new (better?) game worlds\n",
      "context:\n",
      "signed languages have many unique ways to encode meaning. some of these ways include using different handshapes, motions, which direction the palm and wrist are facing, whether one hand or two is used, and facial expressions. this dataset compares which different sign languages use which of these grammatical building blocks.\n",
      "content:\n",
      "this database contains information on the parameters used by 87 signed languages, taken from various academic sources and compiled by hand.\n",
      "acknowledgements:\n",
      "this dataset was collected by rachael tatman during the process of linguistic research and is released to the public domain. the database can be cited by reference to this paper:\n",
      "tatman, r. (2015). the sign language analyses (slay) database⋆ (vol. 33). university of washington working papers in linguistics. https://depts.washington.edu/uwwpl/vol33/2-tatman-slay.pdf\n",
      "inspiration:\n",
      "one analysis of this data is presented can be found in this paper, , but there are plenty of additional questions that could be asked. some examples: - does a language’s geographic location factor into what parameters it uses? - does the year that a grammatical analysis was published have an effect on how many parameters it proposes for a language?\n",
      "you may also be interested in:\n",
      "atlas of pidgin and creole language structures\n",
      "world language family map\n",
      "world atlas of language structures: information on the linguistic structures in 2,679 languages\n",
      "context:\n",
      "brazilian literature is the literature written in the portuguese language by brazilians or in brazil, including works written prior to the country’s independence in 1822. throughout its early years, literature from brazil followed the literary trends of portugal, whereas gradually shifting to a different and authentic writing style in the course of the 19th and 20th centuries, in the search for truly brazilian themes and use of the portuguese language.\n",
      "content:\n",
      "this dataset contains over 3.7 million words of brazilian literature written between 1840 and 1908. there are 81 distinct works in this corpus, written by adolfo caminha, aluisio azevedo, bernardo guimaraes, joaquim manuel de macedo, jose de alencar, machado de assis and manuel antonio de almeida.\n",
      "inspiration:\n",
      "can you automatically identify topics/themes in each of these works?\n",
      "can you automatically identify the author of a specific text? (you might want to split each author’s works into a test set and training set.)\n",
      "context:\n",
      "crime in growing cities such as austin changes with the population. this data covers individual crimes reported in austin, primarily 2014-2015.\n",
      "content:\n",
      "159k rows of data on type of crime reported, location by various attributes (lat/lon, council district, census tract) and time are included. clearance status by austin pd is also recorded where available.\n",
      "acknowledgements:\n",
      "data was prepared from a txt file accessed via google cloud bigquery public datasets. image by tobias zils.\n",
      "inspiration:\n",
      "are there any clear seasonal or hourly trends in certain crimes? which crimes are most often cleared by austin pd, and which remain open? how long do clearances take?\n",
      "context\n",
      "on february 22, 2017 nasa announces the discovery of the trappist-1 solar system, which contains 7 earth sized exoplanets orbiting close to a dim star. the planets' orbits are situated in the 'goldilocks zone', making them prime candidates for extraterrestrial life.\n",
      "content\n",
      "the data published here was pulled from nasa and caltech's exoplanet archive: http://exoplanetarchive.ipac.caltech.edu/index.html\n",
      "column contents are explained further: http://exoplanetarchive.ipac.caltech.edu/docs/api_exoplanet_columns.html\n",
      "acknowledgements\n",
      "use of the nasa exoplanet archive, which is operated by the california institute of technology, under contract with the national aeronautics and space administration under the exoplanet exploration program\n",
      "inspiration\n",
      "a playground to work with data from an exciting discovery. a lot of questions remained to be answered, pending future studies.\n",
      "context\n",
      "the us bureau of labor statistics monitors and collects day-to-day information about the market price of raw inputs and finished goods, and publishes regularized statistical assays of this data. the consumer price index and the producer price index are its two most famous products. the former tracks the aggregate dollar price of consumer goods in the united states (things like onions, shovels, and smartphones); the latter (this dataset) tracks the cost of raw inputs to the industries producing those goods (things like raw steel, bulk leather, and processed chemicals).\n",
      "the us federal government uses this dataset to track inflation. while in the short term the raw dollar value of producer inputs may be volatile, in the long term it will always go up due to inflation --- the slowly decreasing buying power of the us dollar.\n",
      "content\n",
      "this dataset consists of a packet of files, each one tracking regularized cost of inputs for certain industries. the data is tracked-month to month with an index out of 100.\n",
      "acknowledgements\n",
      "this data is published online by the us bureau of labor statistics.\n",
      "inspiration\n",
      "how does the producer price index compare against the consumer price index?\n",
      "what have the largest spikes in input costs been, historically? can you determine why they occurred?\n",
      "what is the overall price index trend amongst us producers?\n",
      "open beauty facts - the free cosmetic products database.\n",
      "open your cosmetics and know what you use on your body\n",
      "be part of our collaborative, free and open database of cosmetics products from around the world!\n",
      "a cosmetics products database\n",
      "open beauty facts is a database of cosmetics products with ingredients, allergens, indicia and all the tidbits of information we can find on product labels.\n",
      "made by everyone - crowdsourcing\n",
      "open beauty facts is a non-profit association of volunteers.\n",
      "many contributors like you have added 2 000+ products using our android, iphone or windows phone app or their camera to scan barcodes and upload pictures of products and their labels.\n",
      "for everyone - open data\n",
      "data about cosmetics is of public interest and has to be open. the complete database is published as open data and can be reused by anyone and for any use. check-out the cool reuses or make your own!\n",
      "you can grow the database in your country easily using the mobile apps:\n",
      "android version: https://play.google.com/store/apps/details?id=org.openbeautyfacts.scanner\n",
      "iphone/ipad version: https://itunes.apple.com/us/app/open-beauty-facts/id1122926380\n",
      "you can also browse the web version at: https://world.openbeautyfacts.org\n",
      "content\n",
      "the correlates of war (cow) project has utilized a classification of wars that is based upon the status of territorial entities, in particular focusing on those that are classified as members of the interstate system. wars have been categorized by whether they primarily take place between/among states, between/among a state and a non-state actor, and within states.\n",
      "within the cow war typology, an interstate, intrastate, or extrastate war must meet same definitional requirements of all wars in that the war must involve sustained combat, involving organized armed forces, resulting in a minimum of 1,000 battle-related combatant fatalities within a twelve month period. for a state to be considered a war participant, the minimum requirement is that it has to either commit 1,000 troops to the war or suffer 100 battle-related deaths.\n",
      "when correlates of war scholars j. david singer and melvin small first extended their study of war to include intrastate wars in resort to arms, they established the requisite condition that for a conflict to be a war, it must involve armed forces capable of “effective resistance” on both sides. they then developed two alternative criteria for defining effective resistance: “both sides had to be initially organized for violent conflict and prepared to resist the attacks of their antagonists, or the weaker side, although initially unprepared, is able to inflict upon the stronger opponents at least five percent of the number of fatalities it sustains.” the effective resistance criteria were specifically utilized to differentiate wars from massacres, one-sided state killings, or general riots by unorganized individuals.\n",
      "acknowledgements\n",
      "the dataset was created by meredith reid sarkees, american university, and professor frank wayman, university of michigan-dearborn, and published by the correlates of war project.\n",
      "context\n",
      "measuring the relative location of u.s. supreme court justices on an ideological continuum allows us to better understand the politics of the high court. such measures are an important building blocking of statistical models of the supreme court, the separation of powers system, and the judicial hierarchy.\n",
      "content\n",
      "the \"martin-quinn\" judicial ideology scores are estimated for every justice serving from the october 1937 term to the present. the measures are estimated using a dynamic item response theory model, allowing judicial ideology to trend smoothly through time. since the scores are estimated from a probability model, they can be used to form other quantities of interest, such as locating the pivotal \"median\" justice, as all well the location of each case in the policy space.\n",
      "acknowledgements\n",
      "the martin-quinn scores were developed by andrew d. martin of the university of michigan and kevin m. quinn of the uc berkeley school of law and supported by a national science foundation grant. the scores are based on the 2016 release of the supreme court database.\n",
      "content\n",
      "this dataset includes a record for each israeli neighborhood in east jerusalem and settlement in the west bank recognized by israeli authorities; therefore, israeli outposts, constructed without government authorization, and nahal settlements, established by israel defense forces and thus regarded as military bases, are excluded. each row includes the settlement name in english and hebrew, year established, regional council, location in latitude/longitude coordinates and relative to the west bank barrier, and population estimates for the past fifteen years.\n",
      "acknowledgements\n",
      "the settlement population statistics were collected by the israel central bureau of statistics.\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one datafile for each state in the u.s. west region.\n",
      "states included in this dataset:\n",
      "alaska - ak.csv\n",
      "arizona - az.csv\n",
      "california - ca.csv\n",
      "colorado - co.csv\n",
      "hawaii - hi.csv\n",
      "idaho - id.csv\n",
      "montana - mt.csv\n",
      "new mexico - nm.csv\n",
      "nevada - nv.csv\n",
      "oregon - or.csv\n",
      "utah - ut.csv\n",
      "washington - wa.csv\n",
      "wyoming - wy.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets for crime or weather\n",
      "context\n",
      "everypolitician is a project with the goal of providing data about every politician in the world. they collect open data on as many politicians as they can find and these datasets are just a small sample of the data available at http://www.everypolitician.org.\n",
      "content\n",
      "each country has their own governmental structure and everypolitician provides data for as many countries as possible. at the time of publishing, there was information on politicians from 233 countries. i chose to publish json files for these 10 countries:\n",
      "australia\n",
      "brazil\n",
      "china\n",
      "france\n",
      "india\n",
      "nigeria\n",
      "russia\n",
      "south_africa\n",
      "uk\n",
      "us\n",
      "these json files follow the popolo format\n",
      "acknowledgements\n",
      "these data were collected from http://everypolitician.org/. their website has more data than i have published here - this is a small sample.\n",
      "content\n",
      "track all-time top performances (top 1000) for olympics distances and the half marathon.\n",
      "acknowledgements\n",
      "the data was scraped from http://www.alltime-athletics.com/index.html\n",
      "context\n",
      "i saw the pokemon dataset from alberto barradas a few weeks ago and i started tinkering around with it to build my own dataset. my first step was building up some python classes for pokemon, then for trainers. for the pokemon class i implemented the statistic calculation as defined here and battle mechanics as defined here. my goal was to battle all possible combinations of all pokemon trainers, but that quickly proved infeasible. as it turns out, there are over 200,000 trillion such combinations, so a simulation like that would take some major computing resources that i do not have. so instead, i thought i'd try battles that actually might exist by scraping bulbapedia for trainers and what pokemon they have. it was a little hacky and it's not a fully exhaustive list (e.g. some trainer classes like special characters are recorded differently, so i skipped them), but it's still a pretty long list of trainers. i used the name from bulbapedia to look up the pokemon in alberto barrada's pokemon dataset for base stats, then used the stat determination calculation from bulbapedia to fill in the pokemon's stats. i used tinydb to store the result. see the code here\n",
      "content\n",
      "the data is structured in a sqlite file with two tables, trainers and pokemon. trainers has a trainerid and trainername; pokemon has trainerid (foreign key) place (position in trainer's party), pokename (name of the pokemon) and the following stats/types from the original pokemon with stats dataset: \"name\", \"level\", \"type1\", \"type2\", \"hp\", \"maxhp\", \"attack\", \"defense\", \"spatk\", \"spdef\", and \"speed\", most of which should be self explanatory. \"hp\" is the current hp of the pokemon, \"maxhp\" is the hp stat of the pokemon.\n",
      "acknowledgements\n",
      "alberto barradas - pokemon data set; bulbapedia - trainer data and stat mechanics; azure heights pokemon lab - battle mechanics (not used in this data, but still helpful in general); tinydb - relatively quick, zero-config, json file based nosql database;\n",
      "inspiration\n",
      "my goal is to simulate all (or at least some of) these trainers battling each other and record the result. that would mean a dataset of pokemon battles that could be used to answer all sorts of trivial pokemon questions. stay tuned, now that i have the dataset and my model working, it shouldn't be too long!\n",
      "context\n",
      "every presidency starts off with the inaugural address. this defines the course for the next 4 years. how do the length, word usage, lexical diversity change from president to president?\n",
      "content\n",
      "the data was scraped from http://www.bartleby.com/124/ using r's rvest web scraping library. the procedure can be found here the data set is in the .csv format. the columns are : name ,inaugural address , date and text.\n",
      "acknowledgements\n",
      "i would like to thank http://www.bartleby.com for making their data available for free.\n",
      "inspiration\n",
      "i saw a documentary by vox on presidential inaugural speeches. they conducted a study on the common characteristics amongst speeches by influential presidents. through a data driven approach, we can find several interesting insights which help correlate a president's influence and his inaugural speech. what word patterns do influential presidents use often? how does speech length vary?\n",
      "context\n",
      "i wanted to study player stats for australian rules football (afl), using machine learning to identify who the key players are, predict performance and results. my aim is to create the ultimate tipping predictor or maybe a fantasy league tool. i couldn't find a dataset anywhere so i created my own gcd project and am sharing the database for anybody to explore.\n",
      "content\n",
      "every key stat from kicks to clangers to bounces. every player, game by game.\n",
      "that is:\n",
      "over 45,000 rows of data\n",
      "1,048 individual players\n",
      "seasons 2012 to 2016\n",
      "22 key player stats per game\n",
      "match result, winning margin & location\n",
      "all in the one csv file\n",
      "if you have any further suggestions please comment below. i plan to add weather data from a separate source in a future version.\n",
      "acknowledgements\n",
      "data is taken with thanks from afltables.com and www.footywire.com\n",
      "inspiration\n",
      "i want to see key insights into the players' performance that nobody has realised before. with tipping contests in the afl as popular as any other sport, surely this is just waiting for data science to take over and pick winners like never before!!\n",
      "source\n",
      "the data was collected and organized by https://data.stadt-zuerich.ch/ specifically under the link https://data.stadt-zuerich.ch/dataset/vbz-fahrzeiten-ogd\n",
      "data\n",
      "the data table is a variance analysis of the times certain trams and busses should have departed and when they actually departed.\n",
      "interesting questions / challenges\n",
      "what is the fastest way between klusplatz and oerlikon at different times of day?\n",
      "what is the most punctual tram stop in zurich?\n",
      "context\n",
      "this dataset is generated via merging \"san francisco fire department calls\" and \"san francisco elevation data\". fire calls-for-service includes all fire units responses to calls. each record includes the call number, neighborhood, location, unit type, call type, and all relevant time intervals are also included.\n",
      "content\n",
      "call type: type of call the incident falls into.\n",
      "call final disposition: disposition of the call (code). for example th2: transport to hospital - code 2, fir: resolved by fire department\n",
      "unit type: unit type\n",
      "received dttm: date and time of call is received at the 911 dispatch center.\n",
      "response dttm: date and time this unit acknowledges the dispatch and records that the unit is en route to the location of the call.\n",
      "on scene dttm: date and time the unit records arriving to the location of the incident\n",
      "call type group: call types are divided into four main groups: fire, alarm, potential life threatening and non life threatening.\n",
      "neighborhood district: neighborhood district associated with this address, boundaries available here\n",
      "location: latitude and longitude of address obfuscated either to the midblock, intersection or call box\n",
      "elevation:elevation in meters\n",
      "acknowledgements\n",
      "san francisco fire department calls are downloaded from sfopen webpage.\n",
      "san francisco dem (digital elevation models) file is obtained from national centers for environmental information web page*\n",
      "*carignan, k.s., l.a. taylor, b.w. eakins, r.j. caldwell, d.z. friday, p.r. grothe, and e. lim, 2011. digital elevation models of central california and san francisco bay: procedures, data sources and analysis, noaa technical memorandum nesdis ngdc-52, u.s. dept. of commerce, boulder, co, 49 pp.\n",
      "inspiration\n",
      "do fire fighters only fight fire? there is a wide range of calls directed to fd, what is the leading cause?\n",
      "how often do firefighters actually fight a fire on a given day/week?\n",
      "how fast do they respond to calls? does the elevation lag the response?\n",
      "are there special times/months where they receive more or less calls?\n",
      "is there a relationship between the elevation and the rate or type of calls?\n",
      "context\n",
      "this content was scraped for a previous project in 2014. i thought this community might find it useful.\n",
      "it was originally used as part of an english learning application, which automatically tailored exercises that were optimized to accelerate language acquisition. unfortunately, the app was not commercially viable.\n",
      "content\n",
      "each record contains the following variables\n",
      "body: the article body text.\n",
      "title: the article header.\n",
      "last_crawl_date: the date that this article was crawled.\n",
      "url: the original url of the article.\n",
      "due to upload size limits, i've had to remove many of the articles. but the original is well over the 500mb upload limit.\n",
      "the file may contain duplicate or low-value records. it also contains broken tags and characters. the corpus should be cleaned before use.\n",
      "inspiration\n",
      "use nlp and classification to figure out which web site an article came from.\n",
      "context\n",
      "the awesome datasets graph is a neo4j graph database which catalogs and classifies datasets and data sources as scraped from the awesome public datasets github list.\n",
      "content\n",
      "we started with a simple list of links on the awesome public datasets page. we now have a semantic graph database with 10 labels, five relationship types, nine property keys, and more than 400 nodes. all within 1mb of database footprint. all database operations are query driven using the powerful and flexible cypher graph query language.\n",
      "the download includes csv files which were created as an interim step after scraping and wrangling the source. the download also includes a working neo4j graph database. login: neo4j | password: demo.\n",
      "acknowledgements\n",
      "data scraped from awesome public datasets page. prepared for the book data science solutions.\n",
      "inspiration\n",
      "while we have done basic data wrangling and preparation, how can this graph prove useful for your data science workflow? can we record our data science project decisions taken across workflow stages and how the data catalog (datasources, datasets, tools) use cases help in these decisions by achieving data science solutions strategies?\n",
      "the dataset contains the 2017 assembly elections results for 5 indian states; manipur (mr), goa (ga), uttar pradesh (up), uttarakhand (ut) and punjab (pb).\n",
      "the data was scraped from election commission of india website: eci\n",
      "the scrapper used for this data collection is here\n",
      "data fields\n",
      "the datasets contains 6 files; one for each state and the last one is the aggregated data for all 5 states. each data file has the following 5 fields: state, constituency, candidate, party, votes\n",
      "context\n",
      "i scraped all of the currently available urban dictionary pages (611) on 3/26/17\n",
      "content\n",
      "word - the slang term added to urban dictionary\n",
      "definition - the definition of said term\n",
      "author - the user account who contributed the term\n",
      "tags - a list of the hashtags used\n",
      "up - upvotes\n",
      "down - downvotes\n",
      "date - the date the term was added to urban dictionary\n",
      "acknowledgements\n",
      "i would like to thank my good friend neil for giving the idea to scrape these terms.\n",
      "context\n",
      "id\n",
      "title\n",
      "nearest_five_percent\n",
      "tagline\n",
      "cached_collected_pledges_count\n",
      "igg_image_url\n",
      "compressed_image_url\n",
      "balance\n",
      "currency_code\n",
      "amt_time_left\n",
      "url\n",
      "category_url\n",
      "category_name\n",
      "category_slug\n",
      "card_type\n",
      "collected_percentage\n",
      "partner_name\n",
      "in_forever_funding\n",
      "friend_contributors\n",
      "friend_team_members\n",
      "source_url\n",
      "from the datasets forum part of kaggle\n",
      "nobody posted this and i thought it might be cool\n",
      "context\n",
      "a local vermont/new hampshire real estate firm is looking into modeling closed prices for houses. this dataset contains features of houses in three towns in vermont, which make up a sizable chunk of the real estate firm's business.\n",
      "content\n",
      "mls is the real estate information platform that is publicly available. features were exported from an mls web platform. features include # of baths, # of bedrooms, and # of acres. there are also categorical features, such as town and address.\n",
      "hint: natural language processing techniques that identify and leverage the road that a house is on may improve prediction accuracy.\n",
      "acknowledgements\n",
      "thank you to ah.\n",
      "goal\n",
      "there is a train, validate, and, test. can you show a cross validated result that beats 10.0% error in closed price? you can use any measure to train your model - rmse, rmsle, etc.; however, the accuracy metric is simply mean percent error!\n",
      "please note: these houses can be uniquely identified on the mls website, which does also have photos of the houses. computer vision techniques that retrieve information from photos on the data are of interest to the company, but are not encouraged for this simple dataset, which serves as a jumping off point for future endeavors as it contains data that is already compiled and understood by the firm.\n",
      "the web of know-how: human instructions dataset\n",
      "overview\n",
      "this dataset has been produced as part of the the web of know-how project\n",
      "to cite this dataset use: paolo pareti, benoit testu, ryutaro ichise, ewan klein and adam barker. integrating know-how into the linked data cloud. knowledge engineering and knowledge management, volume 8876 of lecture notes in computer science, pages 385-396. springer international publishing (2014) (pdf) (bibtex)\n",
      "quickstart: if you want to experiment with the most high-quality data before downloading all the datasets, download the file *9of11_knowhow_wikihow*, and optionally files instruction set entities, process - inputs, process - outputs, process - step links and wikihow categories hierarchy.\n",
      "data representation based on the prohow vocabulary\n",
      "data extracted from existing web resources is linked to the original resources using the open annotation specification\n",
      "data concerning the manual evaluation of this dataset is available here\n",
      "data also available from datahub\n",
      "available datasets\n",
      "instruction datasets:\n",
      "datasets *1of11_knowhow_wikihow* to *9of11_knowhow_wikihow* contain instructions from the wikihow website. instructions are allocated in the datasets in order of popularity. this means that the most popular and high-quality instructions are found in 9of11_knowhow_wikihow, while the least popular ones are in dataset 1of11_knowhow_wikihow. these instructions are also classified according to the hierarchy found in wikihow categories hierarchy. wikihow instructions are community generated. as a result of this, each task is described by at most one set of instructions, which is usually of high quality thanks to the collective community contributions.\n",
      "datasets *10of11_knowhow_snapguide* to *11of11_knowhow_snapguide* contain instructions from the snapguide website. these instructions are not sorted by their popularity, but they are classified in one of the snapguide categories. snapguide instructions are created by single users. as a result of this there might be multiple sets of instructions to achieve the same task; however these instructions on average contain more noise as they are not peer-reviewed.\n",
      "links datasets:\n",
      "the process - inputs datasets contain detailed information about the inputs of the sets of instructions, including links to dbpedia resources\n",
      "the process - outputs datasets contains detailed information about the outputs of the sets of instructions, including links to dbpedia resources\n",
      "the process - step links datasets contains links between different sets of instructions\n",
      "other datasets:\n",
      "the wikihow categories hierarchy dataset contains information on how the various wikihow categories are hierarchically structured, and how they relate to the snapguide categories.\n",
      "the instruction set entities dataset lists all the top-level entities in a sets of instructions. in other words, all the entities which correspond to the title of a set of instructions.\n",
      "the wikihow community links dataset lists the links manually created by the wikihow community of users that interlink entities belonging to different sets of instructions.\n",
      "data model\n",
      "the following figure is a simple example of how the prohow vocabulary is used in the datasets. instructions in the dataset can have more complex structures, for example instructions could have multiple methods, steps could have further sub-steps, and complex requirements could be decomposed into sub-requirements.\n",
      "sparql queries\n",
      "sample sparql queries are available here.\n",
      "sample sparql endpoint\n",
      "a sample sparql endpoint is available at dydra. you can use this uri for federated queries. to get you started, here are some sample queries. note: this endpoint exposes only a subset of the dataset, more specifically files:\n",
      "*8of11_knowhow_wikihow*\n",
      "*9of11_knowhow_wikihow*\n",
      "instruction set entities\n",
      "process - inputs\n",
      "process - outputs\n",
      "process - step links\n",
      "wikihow categories hierarchy\n",
      "multilingual version of the data\n",
      "a multilingual version of this data can be found on kaggle. this multilingual dataset contains over 800k instructions in 16 languages.\n",
      "a graphical visualisation tool and live demo\n",
      "the howlinks tool can be used to visualise this dataset and its links to dbpedia in a web browser.\n",
      "a live demo of this tool (although only serving a subset of this dataset) is available here.\n",
      "statistics\n",
      "23,033,490: number of triples.\n",
      "2,610,223: number of labelled rdf nodes.\n",
      "215,959: number of instructions. 77% from wikihow (datasets *1of11_knowhow_wikihow* to *9of11_knowhow_wikihow*) and 23% from snapguide (datasets *10of11_knowhow_snapguide* to *11of11_knowhow_snapguide*).\n",
      "255,101: number of process inputs linked to 8,453 distinct dbpedia concepts (dataset process - inputs)\n",
      "4,467: number of process outputs linked to 3,439 distinct dbpedia concepts (dataset process - outputs)\n",
      "193,701: number of step links between 114,166 different sets of instructions (dataset process - step links)\n",
      "this dataset is partially based on original instructions from wikihow and snapguide accessed on the 16th of july 2014. doi: http://dx.doi.org/10.7488/ds/1394\n",
      "for any queries and requests contact: paolo pareti\n",
      "context\n",
      "collection of 24000+ paper meta data.\n",
      "content\n",
      "this data contains all paper related to ml, cl, ner, ai and cv field publish between 1992 to 2017.\n",
      "acknowledgements\n",
      "arxiv is open source library for research papers. thanks to arxiv for spreading knowledge.\n",
      "inspiration\n",
      "to know what research is going on in the computer science all around the world.\n",
      "context\n",
      "on qxczv.pw, a 4chan styled board, an anon posted the whole bible in king james version. i chose to scrape it and format it into a bible data set.\n",
      "content\n",
      "data is in csv in the format: citation, book, chapter, verse, text. for example: citation: genesis 1:1 book: genesis chapter: 1 verse: 1 text: \"in the beginning god created the heaven and the earth. \"\n",
      "acknowledgements\n",
      "i'd like to thank qxczv.pw, andrew palmer, jessica butterfield, gary handwerk, brian wurtz, and the whole lake washington high school. papa bless.\n",
      "inspiration\n",
      "i am unsure what data can be analysis from this data set but am thinking graphing distributions of words or running natural language processing on this could be interesting. send me a pm if you have any ideas.\n",
      "context\n",
      "as part of a capstone project, we wanted to compare what social media users are talking about to what's going on in the world to see if and how social media users care about news events. we scraped data from twitter, reddit, reliable news sources, and google trending topics.\n",
      "content\n",
      "this data set includes nine tables: twitter, news, google trending topics, and six popular subreddits (news, worldnews, upliftingnews, sports, politics, television).\n",
      "twitter: trending topic, date trending, sentiment analysis scores, most common word associated with the trend, most common pairs of words associated with the trend.\n",
      "news: headlines (collected from bbc news, usa today, and the washington post), date the article was posted.\n",
      "google trending topics: trending topic, date trending.\n",
      "subreddits: post title, time, date, score (upvotes - downvotes), number of comments.\n",
      "acknowledgements\n",
      "this data was collected as part of a semester project in the capstone in social network analytics at virginia tech, spring 2017, taught by siddharth krishnan. the data was collected over a period of eight days in april 2017.\n",
      "inspiration\n",
      "what do social media users care about, and in what ways do they care? what may they not know about? what types of trends appear most on each social media platform? are people who get the majority of their news from social media able to get an accurate and comprehensive idea of what is going on? how can algorithms such as twitter’s trending topics algorithm influence and shape what users talk about, read, and react to?\n",
      "context\n",
      "in early 2016, the washington post wrote that the justice department is \"resuming a controversial practice that allows local police departments to funnel a large portion of assets seized from citizens into their own coffers under federal law.\n",
      "the \"equitable sharing program\" gives police the option of prosecuting some asset forfeiture cases under federal instead of state law, particularly in instances where local law enforcement officers have a relationship with federal authorities as part of a joint task force. federal forfeiture policies are more permissive than many state policies, allowing police to keep up to 80 percent of assets they seize.\" (link to the full article can be found here).\n",
      "this is the raw data from the department of justice’s equitable sharing agreement and certification forms that was released by the u.s. department of justice asset forfeiture and money laundering section.\n",
      "content\n",
      "spending_master.csv is the main spending dataset that contains 58 variables.\n",
      "notes.csv lists the descriptions for all variables.\n",
      "acknowledgements\n",
      "the original dataset can be found here. the data was originally obtained from a freedom of information act request fulfilled in december 2014.\n",
      "inspiration\n",
      "which agency/sector/item received the most amount of funds from the justice department?\n",
      "how many agencies received non-cash assets from the federal government through equitable sharing?\n",
      "are there any trends in the total equitable sharing fund across agencies?\n",
      "context\n",
      "in new york over 10,000 parole eligible prisoners are denied release every year, and while the consequences of these decisions are costly (at $60,000 annually to incarcerate one individual, and more to incarcerate older individuals with illnesses), the process of how these determinations are made is unclear. advocates for parole reform argue that parole commissioners too often base their decisions on \"the nature of the crime\" for which the individual was convicted, rather than on that individual's accomplishments and growth while serving a sentence in prison.\n",
      "the parole hearing data project is part of a broader body of work that can be found on the museum of the american prison's website.\n",
      "content\n",
      "dataset includes sex, race / ethnicity, housing or interview facility, parole board interview type, and interview decision among other factors. scraping is up-to-date as of july 2016.\n",
      "descriptions provided by the department of corrections and community service (doccs) can be found here.\n",
      "acknowledgements\n",
      "data was collected and managed by nikki zeichner, rebecca ackerman, and john krauss. original dataset, including a scraper to gather the latest updates, can be found here.\n",
      "inspiration\n",
      "does the housing or interview facility play a role in the parole decision? are any of these facilities particularly likely to influence a positive decision?\n",
      "do interview decisions vary based on race / ethnicity? sex?\n",
      "what is the typical time between entry and release?\n",
      "content\n",
      "this dataset reflects the daily volume of speed violations that have been recorded by each camera installed in the city of chicago as part of the automated speed enforcement program. the data reflects violations that occurred from july 1, 2014 until december 31, 2016. the reported violations were collected by the camera and radar system and reviewed by two separate city contractors. this dataset contains all violations regardless of whether a citation was issued.\n",
      "acknowledgements\n",
      "the speed camera data was collected and published by the chicago police department on the city of chicago data portal website.\n",
      "inspiration\n",
      "what neighborhood has the highest density of speed cameras? do speed cameras capture more violations on weekdays or weekends? which camera has captured the most violations? has the number of speed violations recorded decreased over time?\n",
      "context\n",
      "the european union is a unique economic and political union between twenty-eight countries that together cover much of the continent. it was created in the aftermath of the second world war to foster economic cooperation and thus avoid conflict. the result was the european economic community (eec), established in 1958, with belgium, germany, france, italy, luxembourg and the netherlands as its members. what began as an economic union has evolved into an organization spanning policy areas, from climate, environment and health to external relations and security, justice and migration. the 1993 name change from the european economic community (eec) to the european union (eu) reflected this.\n",
      "the european union has delivered more than half a century of peace, stability and prosperity, helped raise living standards and launched a single european currency: the euro. in 2012, the eu was awarded the nobel peace prize for advancing the causes of peace, reconciliation, democracy and human rights in europe. the single market is the eu's main economic engine, enabling most goods, services, money and people to move freely.\n",
      "content\n",
      "the european union covers over 4 million square kilometers and has 508 million inhabitants — the world’s third largest population after china and india. this dataset includes information on each eu member state, candidate state, or european free trade agreement (efta) signatory state.\n",
      "acknowledgements\n",
      "the membership, population, and economic data was published by the european commission's eurostat. gross domestic product and gdp per capita in us dollars was provided by the world bank.\n",
      "inspiration\n",
      "how has the european union grown in the past fifty years? what is the largest country by population or surface area? which country has the largest economy by gross domestic product? how many different languages are spoken across all the member states?\n",
      "i use a gps watch to measure and angle while kitefoiling. i uploaded my gps watch data to analyze whether or not i'm getting faster and pointing higher (when kitefoil racing, you want to increase boat speed while pointing as close to the wind as possible).\n",
      "this analysis will also be useful to other kitefoilers. and hopefully it's a nice example that allows others to analyze gps watch data for their chosen sport.\n",
      "data from data.gov.in about the coal production in diffrent sectors india\n",
      "context\n",
      "this is a sample of the training data used in the numerai machine learning competition. https://numer.ai/about\n",
      "content\n",
      "the data is cleaned, regularized and encrypted global equity data. the first 21 columns (feature1 - feature21) are features, and target is the binary class you’re trying to predict.\n",
      "goal\n",
      "we want to see what the kaggle community will produce with this dataset using kernels.\n",
      "center for international earth science information network ( ciesin )/columbia university. 2013. urban-rural population and land area estimates version 2. palisades, ny: nasa socioeconomic data and applications center ( sedac ).\n",
      "i used the what.cd api to obtain information on all torrents tagged \"hip.hop\" (75,719 releases as of october 22 2016). the result is a sqlite database with two tables:\n",
      "torrents\n",
      "this table contains information about all 75,719 releases in the database. it has the following fields:\n",
      "groupname (text): release title\n",
      "totalsnatched (integer): number of times the release has been downloaded.\n",
      "artist (text): artist / group name.\n",
      "groupyear (integer): release year.\n",
      "releasetype (text): release type (e.g., album, single, mixtape)\n",
      "groupid (integer): unique release identifier from what.cd. used to ensure no releases are duplicates.\n",
      "id (integer): unique identifier (essentially an index).\n",
      "tags\n",
      "this table contains the tags associated with each of the releases in the torrents table. because being tagged 'hip.hop' is requisite for inclusion in the database, this tag is not listed.\n",
      "index (integer): index.\n",
      "id (integer): release identifier (can be matched with id field in the torrents table).\n",
      "tag (text): tag.\n",
      "i love hip hop, so this database has become my go-to data for satisfying curiosities i might have.\n",
      "context\n",
      "svhn is a real-world image dataset.\n",
      "fragments of this dataset were preprocessed:\n",
      "fields of photos that do not contain digits were cut off;\n",
      "the photos were formatted to the standard 32x32 size;\n",
      "three color channels were converted into one channel (grayscaled);\n",
      "each of the resulting images was represented as an array of numbers;\n",
      "the data were converted into .csv files.\n",
      "content\n",
      "64000 32x32 greyscaled images of number photos with 1-5 digits (represented as arrays).\n",
      "11 categories of labels (ten \"digit\" categories and one \"empty character\" category).\n",
      "information about file names in the original dataset.\n",
      "acknowledgements\n",
      "the original data contains a notice \"for non-commercial use only\".\n",
      "inspiration\n",
      "image recognition and classification is a huge part of machine learning practice. in addition, this data is based on real photos.\n",
      "context\n",
      "this is a collection of all the works of charles dickens that are available through project gutenberg.\n",
      "this dataset is subject to the project gutenberg license.\n",
      "it is very possible that i have missed some of his works. please add them and update the \"last updated\" date below if you get the chance. otherwise, if you leave a comment on this dataset, i will try and do so myself.\n",
      "i did some very rough deduplication of his works. if i missed anything, please call it out with a comment and i will rectify the situation.\n",
      "content\n",
      "last updated: october 13, 2017\n",
      "to be read at dusk - 924-0.txt\n",
      "the seven poor travellers - pg1392.txt\n",
      "the pickwick papers - 580-0.txt\n",
      "a message from the sea - pg1407.txt\n",
      "the old curiosity shop - 700-0.txt\n",
      "pictures from italy - 650-0.txt\n",
      "the magic fishbone a holiday romance from the pen of miss alice rainbird, aged 7 - pg23344.txt\n",
      "a tale of two cities a story of the french revolution - 98-0.txt\n",
      "the life and adventures of nicholas nickleby - 967-0.txt\n",
      "little dorrit - 963-0.txt\n",
      "the uncommercial traveller - 914-0.txt\n",
      "oliver twist - pg730.txt\n",
      "three ghost stories - 1289-0.txt\n",
      "the chimes - 653-0.txt\n",
      "mugby junction - 27924-0.txt\n",
      "great expectations - 1400-0.txt\n",
      "the battle of life - pg676.txt\n",
      "david copperfield - 766-0.txt\n",
      "bleak house - pg1023.txt\n",
      "sketches by boz illustrative of everyday life and every-day people - 882-0.txt\n",
      "the haunted man and the ghost's bargain - 644-0.txt\n",
      "a child's history of england - pg699.txt\n",
      "american notes for general circulation - 675-0.txt\n",
      "hunted down [1860] - 807-0.txt\n",
      "hard times - 786-0.txt\n",
      "the mystery of edwin drood - 564-0.txt\n",
      "dickens' stories about children every child can read - pg32241.txt\n",
      "the cricket on the hearth a fairy tale of home - 678-0.txt\n",
      "our mutual friend - 883-0.txt\n",
      "a christmas carol - pg19337.txt\n",
      "barnaby rudge - 917-0.txt\n",
      "some christmas stories - 1467-0.txt\n",
      "acknowledgements\n",
      "content taken from project gutenberg\n",
      "image taken from wikimedia commons\n",
      "inspiration\n",
      "making this data available for any kind of textual analysis. i intend for this to be part of a series.\n",
      "context\n",
      "ppt is the biggest bbs in taiwan, it contained lots of news and discussion in different boards. ptt stock board is really popular because many of the users would give their opinions on trends of the market. most of them tried to predict the trend of the index(^twii). it would be interesting to know if the users activities or texts was correlated with index price or trend.\n",
      "content\n",
      "there are 3 csv files.\n",
      "\"ptt_stock_p3000_p3718.csv\"\n",
      "it contains about 2 years of the stock discussion topic name, topic_url, author id, number of people liked it or not(push type).\n",
      "\"daychat_push_60d_1006.csv\"\n",
      "it contains 60 days of instant intraday trading chats texts (2017/7/17~2017/10/06)\n",
      "\"twii_20151001_20171006.csv\"\n",
      "daily ohlc , volume, up or down, %changes and volatility level of ^twii historical prices between 2015/10/01 and 2017/10/06\n",
      "acknowledgements\n",
      "all of the text data could be found on: https://www.ptt.cc/bbs/stock/index.html.\n",
      "the historical price data of taiwan stock market index (^twii) was from yahoo finance.\n",
      "inspiration\n",
      "there are few nlp data presented in mandarin. it would be a new challenge to interpret texts not written in english.\n",
      "since the text data were all about stock or index going up or down, so it would be interesting to know which id predicts accurate or misleading the others about the market trend.\n",
      "is volatility level correlated with intraday trading chat amounts or topic amounts of the day?\n",
      "can you tell which is the most popular stock they are observing, and the trend is going up or down?\n",
      "car_sales data set contains all the information from manufacturer, type, brand, category, price etc.\n",
      "context\n",
      "i have desk job. a very interesting desk job but nevertheless a desk job. therefore i started doing sports a while ago and now i can't stop anymore. like every other geek i need a gadet for every hobby i have and in this case it was a gps-sport-smartwatch: the vivoactive and the vivoactive hr by garmin. they also offer a data analysis center and as a data scientist i of course had to export the data and employ some analysis that go beyond bar charts. this dataset is basically the bulk-downloaded-not-cleaned-dataset from the mentioned data center. for those interested: there is a nice github project for bulk downloading from garmin connect: https://github.com/kjkjava/garmin-connect-export. the weather data has to be included by hand.\n",
      "content\n",
      "you can find 155 samples, each one representing one sport activity, mainly from the black forest. there are a lot of useless colmuns, which either contain no data or the same value for every sample. you will have to identify these columns in any case and remove them. the zip includes the gpx tracks of all activities and can be used as well. the two devices use different gps sensors and are from my feeling of different precision and reliability (untested). additionally, the hr device had a lot connectivity problems since summer 2017. the devices lost the signal during a numerous amount of runs and therefore the distance value is not always correct. usually the start and end timestamps are correct (except for one case) and the gpx files might help to figure out which track i was using. with a single exception all start and ending points are the same in all tracks. this means i started and ended recording at the same cross-roads, not exactly the same position. if you decide to open the gpx files in a gis, you should be able to repair the affected datasets. i did this using qgis. you will find one instance with two activities on a single day. this is actually the same activity, where i went to the peak of rosskopf in the black forest. because my brain was undersupplied after runnning up there i ended the activity instead of pausing it, that's why i ended up with two activities, that have to be merged together.\n",
      "soft data\n",
      "for the dataset, there is also some soft data which might be helpful:\n",
      "i commute 130 km to work since june 2016, usually on mon, tue, wed. possibly, i spent less time on sports since then on these days.\n",
      "i wrote my master thesis from sep / 2015 until mar 2016. maybe i was doing more sports during the thesis (except for the last two weeks?)\n",
      "since i commute, i would say i do sports less frequently, but on longer distances and over higher elevation gains\n",
      "i bought new shoes in august 2016, which are way more comfortable\n",
      "acknowledgements\n",
      "the bulk downloading script for garmin connect was really helpful: https://github.com/kjkjava/garmin-connect-export. without this tool i most likely would not have created this dataset.\n",
      "inspiration\n",
      "i am personally very interested if my running performance is dependent on specific weather conditions and eventually predictable. another interesting thing would be to see how other people rate the performance based on the given data. i use this dataset also during my teaching in a python and a statistics class at university. i decided to upload the data and some of my teaching notebooks to kaggle (in the near future) in oder to give my students access to external comments on my kernels, givem them the opportunity to upload their solutions to a bigger community and eventually scan your kernels on the data.\n",
      "this dataset contains enrollment numbers for every course offered at harvard during fall term 2015.\n",
      "the data\n",
      "the course enrollment data contains the following fields:\n",
      "course: the course name (consists of the department/program abbreviation and a course number/letter; the abbreviation and the number/letter are separated by a space)\n",
      "department: the abbreviation for this course's department\n",
      "courseid: a unique identifier for the course\n",
      "classnbr: another unique identifier for the course?\n",
      "totalenrollment: total number of students enrolled, from every school\n",
      "gsas: number of students enrolled from the graduate school of arts and sciences\n",
      "hcol: number of students enrolled from harvard college (undergraduate)\n",
      "nondgr: number of non-degree-seeking students enrolled\n",
      "vus: number of students enrolled from the visiting undergraduate students program\n",
      "xreg: number of students from other universities who are cross-registered in the course\n",
      "note that there is also a row whose course value is totals: and whose department, courseid, and classnbr values are empty. this row lists the total number of students from each school (gsas, hcol, etc) in all of the courses.\n",
      "for more info on what each of the courses is, check out the harvard course catalog.\n",
      "acknowledgments\n",
      "all of the data in this dataset comes from the harvard open data dataverse. the specific citation is:\n",
      "mehta, neel, 2016, \"course enrollment stats\", doi:10.7910/dvn/9mwtyo, harvard dataverse, v1 [unf:6:pa8a+2yr3ngt9i9xghweig==]\n",
      "context\n",
      "the datasets contain all the data for the number of cs ap a exam taken in each state from 1998 to 2013, and detailed data on pass rates, race, and gender from 2006-2013. the data was complied from the data available at http://research.collegeboard.org/programs/ap/data. this data was originally gathered by the csta board, but barb ericson of georgia tech keeps adding to it each year.\n",
      "content\n",
      "historical.csv contains data for the number of cs ap a exam taken in each state from 1998 to 2013:\n",
      "state: us states\n",
      "1998-2013\n",
      "pop: population\n",
      "pass_06_13.csv contains exam pass rates, race and gender data from 2006 to 2013 for selected states.\n",
      "pass_12_13.csv contains exam pass rates, race and gender information for every state for 2012 and 2013.\n",
      "acknowledgements\n",
      "the original datasets can be found here and here.\n",
      "inspiration\n",
      "using the datasets, can you examine the temporal trends in the exam pass rates by race, gender, and geographical location?\n",
      "context\n",
      "this is official open data from the ministry of internal affairs of the russian federation on missing and wanted people, identified and unindentified corpses. original data available here source.\n",
      "content\n",
      "file meta.csv - contain information about data source and contact information of original owners in russian.\n",
      "file structure-20140727.csv - describe datastructure in russian. main things that you need to know about data columns are here:\n",
      "\"subject\" - the official name of the subject of statistical reporting. that's russian regions, note that crimean federal district and city of sevastopol are included.\n",
      "\"point fpsr\" - item of the federal statistical work plan. you don't need to know this.\n",
      "\"name of the statistical factor\" - this one speaks for itself. available factors:\n",
      "-- identified persons from among those who were wanted, including those who disappeared from the bodies of inquiry, investigation, court.\n",
      "-- total cases on the identification of citizens on unidentified corpses that were on the register.\n",
      "-- total wanted persons, including those who disappeared from the bodies of inquiry, investigation, court.\n",
      "-- identified persons from among the wanted persons, including those missing.\n",
      "-- total wanted persons.\n",
      "-- number (balance) of unreturned missing persons in relation to 2011 (%)\n",
      "-- number (balance) of unresolved criminals against 2011 (%)\n",
      "-- total discontinued cases in connection with the identification of the person\n",
      "-- total wanted persons, including those missing\n",
      "-- identified persons from the number of wanted persons\n",
      "\"importance of the statistical factor\" - value of correspondent statistical factor.\n",
      "files data-%y%m%d-structure-20140727.csv contain actual data. names of the files contain release date. data aggregated by quarters of each year, for example data-20150127-structure-20140727.csv - data for whole 2014 year data-20150627-structure-20140727.csv - data for q1 and q2 of 2015\n",
      "file translate.csv is used to simplify translation from russian to english. see usage in the kernel.\n",
      "acknowledgements\n",
      "thanks to newspaper komsomolskaya pravda for bringing up the issue of missing kids in russia.\n",
      "thanks to liza alert - volunteer search and rescue squad for efforts in rescue of missing people in russia.\n",
      "photo by alessio lin on unsplash\n",
      "inspiration\n",
      "missing people, especially kids, is a serious problem. however there is not much detailed information about it. russian officials provide overall information without detalisation of victim's age. as a result many speculations appear in media on this topic:\n",
      "last year about 200,000 reports of missing people were filed with police in russia.\n",
      "45,000 kids lost every year\n",
      "more than 15,000 kids lost every year\n",
      "radio interview - starting from minute 7:55 main point: \"more than 15k kids lost completely, i.e. was not ever found\"\n",
      "some insights to official data can be found here interview, year 2012: \"annually in russia about 20 thousand minors disappear, in 90% of cases the police find children\".\n",
      "still there is no information about kids in recent years. if you have any reliable sources, please share.\n",
      "context\n",
      "i was searching far and wide, but i could not find any dataset that had a collection of all the pokemon images. so i decided to write a python script. 15 minutes later i had this.\n",
      "content\n",
      "a collection of 801 transparent png pokemon images of size 215x215\n",
      "acknowledgements\n",
      "use at will\n",
      "inspiration\n",
      "\"i want to be the very best, like no one ever was...\"\n",
      "context\n",
      "this dataset was downloaded from inep, a department from the brazilian education ministry. it contains data from the applicants for the 2016 national high school exam.\n",
      "content\n",
      "inside this dataset there are not only the exam results, but the social and economic context of the applicants.\n",
      "acknowledgements\n",
      "the original dataset is provided by inep (http://portal.inep.gov.br/microdados).\n",
      "inspiration\n",
      "the objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results.\n",
      "context\n",
      "data on parliamentary agendas and actions taken by the verkhovna rada (верховна рада), the ukrainian parliament, in 2014 through 2017.\n",
      "content\n",
      "for the period of 27th nov 2014 through 17th oct 2017:\n",
      "list of deputies\n",
      "list of parliamentary fractions\n",
      "session days\n",
      "daily agenda results, including:\n",
      "total voting result\n",
      "individual deputy votes\n",
      "speech authors and timings, no full text\n",
      "registration performed as the session day starts\n",
      "acknowledgements\n",
      "sourced from: http://data.rada.gov.ua/open/data/ppz-skl8, on the ukrainian government open data portal. thanks to everyone who made this open data possible.\n",
      "photo by illia cherednychenko on unsplash.\n",
      "inspiration\n",
      "how are the ukranian parliamentary factions structured?\n",
      "does parliamentary activity in this dataset reflect the ongoing political events in the country?\n",
      "context\n",
      "in lithuania, lithuanian parliament has sittings several times per week, during each sitting member of parliament vote for proposed questions. some questions are related to law amendments other questions could be something like \"should we do a break before taking more questions\". in one they there can be several sittings, usually one in the morning and another in the evening.\n",
      "all members of parliament (mps) form parliamentary groups, there is a group having majority of ms's and other smaller groups.\n",
      "in this dataset you will find all votes made by all mps since 1997 up until 2017. so that is a lot of data and basically most of the political history of independent lithuania.\n",
      "content\n",
      "each row in votes.csv file represents a vote mode by single mp on a single question. also for each vote there is a number of meta data provided:\n",
      "voting_id - unique voting id, can be a negative number. you can reconstruct url to the voting page using this template: http://www.lrs.lt/sip/portal.show?p_r=15275&p_k=1&p_a=sale_bals&p_bals_id={voting_id}, replace {voting_id} with a voting id.\n",
      "voter_id - unique mp id.\n",
      "time - date and time when a vote was cast.\n",
      "group - abbreviated name of a parliamentary group.\n",
      "voter - full name of an mp.\n",
      "question - question text, usually question sounds like \"do we accept this proposal or not\", unfortunately title and texts of proposed documents are not included in this dataset.\n",
      "sitting_type - one of:\n",
      "rytinis - in the morning\n",
      "vakarinis - in the evening\n",
      "neeilinis - additional sitting\n",
      "nenumatytas - not planned\n",
      "vote - vote value, one of:\n",
      "1 - aye\n",
      "-0.5 - abstain\n",
      "-1 - against\n",
      "n_eligible_voters - total number of eligible to vote mps.\n",
      "n_eligible_voters - number of mps who voted in a voting.\n",
      "acknowledgements\n",
      "all the data where scraped from lithuanian parliament web site. code of web scraper can be found here:\n",
      "https://github.com/sirex/databot-bots/blob/master/bots/lrs/balsavimai.py\n",
      "inspiration\n",
      "one of the most interesting questions i would like to get is a way to automatically categorize all the voting by topic. usually there are several major topics involving multiple documents and many voting sessions, some times these topics can last for years. unfortunately there is no such field, with a topic, in order to discover a topic one would need to analyze content of documents and probably sitting transcripts to find a topic. but in order to do this, data of documents and sitting transcripts will be needed, i will provide this data some time later.\n",
      "other interesting things to look into is how different parliamentary groups or people relates to one another by their votes.\n",
      "«datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\n",
      "context\n",
      "en aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.\n",
      "hem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. és important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.\n",
      "content\n",
      "en aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.\n",
      "pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.\n",
      "pel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. els camps especialment destacats són:\n",
      "• nom: nom empresa o cryptomoneda;\n",
      "• preu: valor en euros d’una acció o una cryptomoneda;\n",
      "• volum: en euros/volum 24 hores,acumulat de les transaccions diàries en milions d’euros\n",
      "• estat: estat final en tancament en alta o baixa del dia.\n",
      "• var. per cent: variació en el moment del tancament amb tant per cent respecte el dia anterior\n",
      "• var. en euros: variació en el moment del tancament amb euros respect el dia anterior.\n",
      "• capitalització: valor de l’empra respecte les seves accions.\n",
      "• per: la ràtio preu-benefici\n",
      "• rent./div: rendibilitat de l’acció respecte el valor inicial de la acció.\n",
      "acknowledgements\n",
      "en aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:\n",
      "http://www.eleconomista.es\n",
      "https://coinmarketcap.com\n",
      "per aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. pel que fa a la terminologia financera podem veure vocabulari en renta4banco.\n",
      "[https://www.r4.com/que-necesitas/formacion/diccionario]\n",
      "inspiration\n",
      "hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:\n",
      "https://arxiv.org/pdf/1410.1231v1.pdf\n",
      "en aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.\n",
      "la comunitat podrà respondre, entre altres preguntes, a:\n",
      "està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'espanya?\n",
      "els efectes o agents externs afecten per igual a les accions o cryptomonedes?\n",
      "hi ha relacions cause efecte entre les acciones i cryptomonedes?\n",
      "project repository\n",
      "https://github.com/acostasg/scraping\n",
      "datasets\n",
      "els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:\n",
      "https://www.kaggle.com/acostasg/stock-index/\n",
      "https://www.kaggle.com/acostasg/crypto-currencies\n",
      "per una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. de forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció.\n",
      "the economic statistics branch of the united nations statistics division (unsd) maintains and annually updates the national accounts official country data database. this work is carried out in accordance with the recommendation of the statistical commission at its first session that the statistics division of the united nations should publish regularly the most recent available data on national accounts for as many countries and areas as possible. the database contains detailed official national accounts statistics in national currencies as provided by the national statistical offices.\n",
      "data are available for most of the countries or areas of the world and form a valuable source of information on their economies. the database contains data as far back as 1946, up to the year t-1, with data for most countries available from the 1970s. the database covers not only national accounts main aggregates such as gross domestic product, national income, saving, value added by industry and household and government consumption expenditure and its relationships; but also detailed statistics for institutional sectors (including the rest of the world), comprising the production account, the generation of income account, the allocation of primary income account, the secondary distribution of income account, the use of disposable income account, the capital account and the financial account, if they are compiled by countries.\n",
      "the statistics for each country or area are presented according to the uniform table headings and classifications as recommended in the united nations system of national accounts 1993 (1993 sna). a summary of the 1993 sna conceptual framework, classifications and definitions are included in the yearly publication “national accounts statistics, main aggregates and detailed tables”.\n",
      "acknowledgements\n",
      "this dataset was kindly published by the united nation on the undata site. you can find the original dataset here.\n",
      "license\n",
      "per the undata terms of use: all data and metadata provided on undata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that undata is cited as the reference.\n",
      "this is a mirror of the data from original source of the dslcc at http://ttg.uni-saarland.de/resources/dslcc/\n",
      "dsl corpus collection (dslcc).\n",
      "the dslcc is a multilingual collection of short excerpts of journalistic texts. it has been used as the main data set for the dsl shared tasks organized within the scope of the workshop on nlp for similar languages, varieties and dialects (vardial). for more information, please check the dsl shared task reports (links below) or the website of past editions of vardial workshop: vardial 2017 at eacl, vardial 2016 at coling, lt4vardial 2015 at ranlp, and vardial 2014 at coling.\n",
      "so far, five versions of the dslcc have been released. languages included in each version of the dslcc grouped by similarity are the table below. click on the respective version to download the dataset.\n",
      "citing the dataset\n",
      "if you used the dataset we kindly ask you to refer to the corpus description paper where you can also find more information about the dslcc:\n",
      "liling tan, marcos zampieri, nikola ljubešić, jörg tiedemann (2014) merging comparable data sources for the discrimination of similar languages: the dsl corpus collection. proceedings of the 7th workshop on building and using comparable corpora (bucc). pp. 6-10. reykjavik, iceland. pdf bib\n",
      "the dsl reports\n",
      "for the results obtained by the participants of the four editions of the dsl shared task, please see the shared task reports below. in 2017, the dsl shared task was part of the vardial evaluation campaign.\n",
      "2017 - marcos zampieri, shervin malmasi, nikola ljubešić, preslav nakov, ahmed ali, jörg tiedemann, yves scherrer, noëmi aepli (2017) findings of the vardial evaluation campaign 2017. proceedings of the fourth workshop on nlp for similar languages, varieties and dialects (vardial). pp. 1-15. valencia, spain. pdf bib\n",
      "2016 - shervin malmasi, marcos zampieri, nikola ljubešić, preslav nakov, ahmed ali, jörg tiedemann (2016) discriminating between similar languages and arabic dialect identification: a report on the third dsl shared task. proceedings of the third workshop on nlp for similar languages, varieties and dialects (vardial). pp. 1-14. osaka, japan. pdf bib\n",
      "2015 - marcos zampieri, liling tan, nikola ljubešić, jörg tiedemann, preslav nakov (2015) overview of the dsl shared task 2015. proceedings of the joint workshop on language technology for closely related languages, varieties and dialects (lt4vardial). pp. 1-9. hissar, bulgaria. pdf bib\n",
      "2014 - marcos zampieri, liling tan, nikola ljubešić, jörg tiedemann (2014) a report on the dsl shared task 2014. proceedings of the 1st workshop on applying nlp tools to similar languages, varieties and dialects (vardial). pp. 58-67. dublin, ireland. pdf bib\n",
      "additional datasets\n",
      "the following datasets have been used in other shared tasks organized within the scope of the vardial workshop.\n",
      "arabic dialect identification (adi): a dataset containing four arabic dialects: egyptian, gulf, levantine, north african, and msa.\n",
      "german dialect identification (gdi): the archimob corpus containing swiss german dialects from basel, bern, lucerne, and zurich.\n",
      "cross-lingual parsing (clp): datasets for parsing similar languages: croatian - slovenian, slovak - czech, norwegian - danish and swedish.\n",
      "acknowledgements\n",
      "credits of the datasets goes to the original data creators and the vardial workshop organizers.\n",
      "credits of the banner image goest to g. crescoli on unsplash\n",
      "on the internet of the 1980's everything was stored in ascii text files. during these early days, many literary works were manually typed up and shared widely. textfiles.com is a website by jason scott dedicated to collecting and preserving text files from this internet of the past. this dataset is a small subset of his total collection - focussing exclusively on english literary works.\n",
      "each book is stored in its own ascii text file and all are in english. how similar are the writing styles of so-called classic authors? can you train a model to determine if a work is fictional or not? what words or phrases are the most popular in these books?\n",
      "the data represented 50 ids from formspring.me that were crawled in summer 2010.\n",
      "for each id, the profile information and each post (question and answer) was extracted.\n",
      "each post was loaded into amazon's mechanical turk and labeled by three workers for cyberbullying content.\n",
      "the data contains the following profile fields: bio - profile biography created by owner of the id date - the date the id was crawled location - location provided by the owner of the id userid - the actual id itself\n",
      "the data contains the following information on each post text - the question and answer (separated by a\n",
      ") asker - the id of the person asking the question (blank if anonymous)\n",
      "3 occurrences of labeldata:\n",
      "    answer - yes or no as to whether the post contains cyberbullying\n",
      "    cyberbullyingwork - word(s) or phrase(s) identified by the mechanical turk worker as the reason it was tagged as cyberbullying (n/a or blank if no cyberbullying detected)\n",
      "    severity - cyberbullying severity from 0 (no bullying) to 10 \n",
      "    other - other comments from the mechanical turk worker\n",
      "    worktime - time needed to label the post (in seconds)\n",
      "    worker - mechanical turk worker id\n",
      "information on how this data was used is available at:\n",
      "reynolds, k, a. kontostathis and l. edwards. 2011. using machine learning to detect cyberbullying. in proceedings of the 2011 10th international conference on machine learning and applications workshops (icmla 2011). december 2011. honolulu, hi.\n",
      "this material is based upon work supported by the national science foundation under grant no. 0916152.   any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the national science foundation.\n",
      "i am currently working on detecting cyberbullying among teenagers in social media. in both physical and cyber worlds, bullying has been recognized as a serious national health issue among adolescents. . the potential of sentiment analysis can help identify victims who pose high risk to themselves or others, and to enhance the scientific understanding of bullying overall.\n",
      "tumblr gif (tgif) dataset\n",
      "the tumblr gif (tgif) dataset contains 100k animated gifs and 120k sentences describing visual content of the animated gifs. the animated gifs have been collected from tumblr, from randomly selected posts published between may and june of 2015. we provide the urls of animated gifs in this release. the sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. we provide one sentence per animated gif for the training and validation splits, and three sentences per gif for the test split. the dataset shall be used to evaluate animated gif/video description techniques.\n",
      "if you end up using the dataset, we ask you to cite the following paper: preprint\n",
      "yuncheng li, yale song, liangliang cao, joel tetreault, larry goldberg, alejandro jaimes, jiebo luo. \"tgif: a new dataset and benchmark on animated gif description\", cvpr 2016\n",
      "if you have any question regarding the dataset, please contact:\n",
      "yuncheng li\n",
      "license\n",
      "this dataset is provided to be used for approved non-commercial research purposes. no personally identifying information is available in this dataset.\n",
      "data\n",
      "contains urls to download animated gif files, sentence descriptions, train/test splits, baseline results and evaluation scripts.\n",
      "tgif-v1.0.tsv\n",
      "animated gif urls and descriptions. each row contains a url and a sentence, tab-separated.\n",
      "examples:\n",
      "https://38.media.tumblr.com/9f6c25cc350f12aa74a7dc386a5c4985/tumblr_mevmyaktdf1rgvhr8o1_500.gif a man is glaring, and someone with sunglasses appears.\n",
      "https://38.media.tumblr.com/9ead028ef62004ef6ac2b92e52edd210/tumblr_nok4eeontv1s2yegdo1_400.gif a cat tries to catch a mouse on a tablet\n",
      "https://38.media.tumblr.com/9f43dc410be85b1159d1f42663d811d7/tumblr_mllh01j96x1s9npefo1_250.gif a man dressed in red is dancing.\n",
      "https://38.media.tumblr.com/9f659499c8754e40cf3f7ac21d08dae6/tumblr_nqlr0rn8ox1r2r0koo1_400.gif an animal comes close to another in the jungle\n",
      "https://38.media.tumblr.com/9ed1c99afa7d71411884101cb054f35f/tumblr_mvtuwlhske1qbnleeo1_500.gif a man in a hat adjusts his tie and makes a weird face.\n",
      "https://38.media.tumblr.com/9e437d26769cb2ac4217df14dbb20034/tumblr_npw7v7w07c1tmj047o1_250.gif someone puts a cat on wrapping paper then wraps it up and puts on a bow\n",
      "https://38.media.tumblr.com/9e4ab65c0e7d4bb8aa6b5be854b83794/tumblr_mdlv9v6he91qanrf2o1_r11_500.gif a brunette woman is looking at the man\n",
      "https://38.media.tumblr.com/9ecd3483028290171dcb5e920ff4e3bb/tumblr_nkcmeflavj1u26rdio1_500.gif a man on a bicycle is jumping over a fence.\n",
      "https://38.media.tumblr.com/9f83754d20ce882224ae3392a8372ee8/tumblr_mkwd0y8poo1qlnbq8o1_400.gif a group of men are standing and staring in the same direction.\n",
      "https://38.media.tumblr.com/9e6fcb37722bf01996209bdf76708559/tumblr_np9xo74ugd1ux4g5vo1_250.gif a boy is happy parking and see another boy\n",
      "splits\n",
      "contains train/test splits used in our cvpr 2016 paper. we include one sentence per gif for training split, three sentence per gif for test split.\n",
      "acknowledgement\n",
      "we thank the flickr vision team, including gerry pesavento, huy nguyen and others for their support and help in collecting descriptions via crowdsourcing.\n",
      "notes\n",
      "last edit: april 5, 2016\n",
      "context\n",
      "the summoner ids, which are unique to each player, were collected and sorted by ranked tier around october of last year. the game data was collected from only gold-ranked summoner ids.\n",
      "i originally collected this data to identify which champions top quinn mains tend to play aside from quinn. the strongest correlation i found was that top quinn players tend to also play graves in the top lane. i recently revisited this project to put together a simple recommender system with newer data, and that system can be found here.\n",
      "i am sharing the 2016 data here because riot's api seems to only provide a summoner's current rank, i.e. there is no rank history. this 2016 data could be useful for anyone interested in seeing how summoners have evolved over time. to get you started on working with champion data, i also added the 2016 game data i collected from gold-ranked summoners when i was investigating top quinn players.\n",
      "content\n",
      "all data was collected through riot games' publicly available api.\n",
      "summids2016 - 480421 summoner ids sorted by tier as of late october 2016.\n",
      "goldsummdata2016 - game data from 131552 gold-ranked summoners. for each summoner, all champions that were played in the 2016 season at the time of collection are presented and sorted by role. the roles are those provided by riot's api. the columns are separated by commas, while the champions in each role are separated by spaces.\n",
      "champid2name - maps champion ids to champion names.\n",
      "acknowledgements\n",
      "this data is only available because riot provides a publicly accessible api. thanks riot! the banner image is also the property of riot games.\n",
      "inspiration\n",
      "given that the unique aspect of this data set is the rank of each summoner in 2016, it would be interesting to see how many summoners improved their performance from 2016 to 2017. perhaps you can identify an underlying trend that can explain why some summoner's went up/down in rank, e.g. top quinn players may have increased in rank due to the buffs to lethality.\n",
      "because league of legends changes with each patch, it would also be interesting to see how someone can leverage year-old data to make recommendations that are still relevant.\n",
      "it's stack overflow's 2016 dataset\n",
      "context\n",
      "pokemon has been around for the majority of my life. i obsessed over it as a child and enjoy seeing the success it carries still today. i figure that i can give the pokemon company a nod by applying my passion for data science to their datasets.\n",
      "content\n",
      "this is compiled data from two websites that i will acknowledge soon. as for the data, the primary variables are id, name, attack, defense, health, and cp. other variables include tankiness, potentialdamage, basicdps, attackbasicdps (damage per second with strongest basic attack*attack), chargedps, oneshotpotential (damage per second with strongest charge attack*attack), and various rankings in each category as well as growth rates for each ranking (this variable only makes sense when sorted by each variable's ranking).\n",
      "acknowledgements\n",
      "i have to pay tribute to two websites:\n",
      "serebii.net and pokemongodb.net // these two pages allowed me to find the data that helped best explore the pokemon go universe. thank you very much.\n",
      "inspiration\n",
      "the data makes it pretty clear what plays into a pokemon's cp. but, i am curious to know what hidden gems you might find when going through this data. for example, is dragonite really the powerhouse we think it is?\n",
      "the data given here pertains to tuberculosis spread across countries from 2007 to 2014. aim of this exercise is to understand how well we can utilize or what insights we can derive from disease spread data collected by international organizations like who.\n",
      "data: global health observatory data repository (who) : http://apps.who.int/gho/data/view.main.57020mp?lang=en\n",
      "this data has been used in the past to create country wise distribution pattern.\n",
      "startups from angellist in the bay area\n",
      "for my data mining lab where we had to execute algorithms like apriori, it was very difficult to get a small data set with only a few transactions. it was infeasible to run the algorithm with datasets containing over 10000 transactions. this dataset contains 11 items : jam, maggi, sugar, coffee, cheese, tea, bournvita, cornflakes, bread, biscuit and milk.\n",
      "the data was scraped from www.reddit.com on 1/20/17 using the query string: https://www.reddit.com/search?q=inauguration\n",
      "attributes in order (left to right):\n",
      "title (string)\n",
      "post (string)\n",
      "post date (datetime)\n",
      "metadata (#points, #comments, author)- needs additional parsing (string)\n",
      "comments (string)\n",
      "post location (string)\n",
      "latlon (post location)\n",
      "latlon (comments location)\n",
      "if you are interested in having help with the analytics email me: amalinow1973@gmail.com\n",
      "content\n",
      "ef- gdp\n",
      "column descriptions\n",
      "country\n",
      "ecological footprint where record = \"efconstotgha\" and year = 2013 | total ecological footprint\n",
      "ecological footprint where record = \"efconstotgha\" and year = 2009 | total ecological footprint\n",
      "2013 gdp (total; based on value of 2010 us dollar)\n",
      "2009 gdp (total; based on value of 2010 us dollar)\n",
      "difference of ef2013 - ef2009\n",
      "difference gdp2013 - gdp2009\n",
      "gdpdelta_p - efdelta_p\n",
      "percent change in ef from 2009 to 2013: ((ef2013 - ef2009) / ef2009) * 100\n",
      "percent change in gdp from 2009 to 2013: ((gdp2013 - gdp2009)/ gdp2009) * 100\n",
      "gdpdelta_p - efdelta_p\n",
      "ordinal ranking of ddelta_p (range 0-153)\n",
      "ordinal ranking of efdelta_p (range 0-153)\n",
      "ordinal ranking of gdpdelta_p (range 0-153)\n",
      "\"decoupled flag\" 1 if (gdpdelta >= 0) & (efdelta <= 0) else 0\n",
      "gdp normalized to min-max scale 25-2500 (necessary for scaling size of markers properly in scatterplot).\n",
      "nfa 2017\n",
      "column descriptions\n",
      "country\n",
      "year\n",
      "country code\n",
      "record-type\n",
      "the ecological footprint of cropland demand.\n",
      "the ecological footprint of grazing land demand.\n",
      "the ecological footprint of forest land demand.\n",
      "the ecological footprint of fishing ground demand.\n",
      "the ecological footprint of built-up land demand.\n",
      "the ecological footprint of carbon demand.\n",
      "the total ecological footprint of demand (sum of all land types)\n",
      "data quality score\n",
      "view my kernel to see a sample of data\n",
      "context\n",
      "minneapolis air quality survey results\n",
      "content\n",
      "contained in the file are minneapolis air quality survey results obtained between november 2013 and august 2014. the data set was obtained from http://opendata.minneapolismn.gov.\n",
      "inspiration\n",
      "visualizing air pollutants quantities over the city of minneapolis may provide evidence for the source of certain air pollutants.\n",
      "this dataset contains estimates of the socioeconomic status (ses) position of each of 149 countries covering the period 1880-2010. measures of ses, which are in decades, allow for a 130 year time-series analysis of the changing position of countries in the global status hierarchy. ses scores are the average of each country’s income and education ranking and are reported as percentile rankings ranging from 1-99. as such, they can be interpreted similarly to other percentile rankings, such has high school standardized test scores. if country a has an ses score of 55, for example, it indicates that 55 percent of the world’s people live in a country with a lower average income and education ranking than country a. iso alpha and numeric country codes are included to allow users to merge these data with other variables, such as those found in the world bank’s world development indicators database and the united nations common database.\n",
      "see here for a working example of how the data might be used to better understand how the world came to look the way it does, at least in terms of status position of countries.\n",
      "variable descriptions: unid: iso numeric country code (used by the united nations) wbid: iso alpha country code (used by the world bank) ses: socioeconomic status score (percentile) based on gdp per capita and educational attainment (n=174) country: short country name year: survey year ses: socioeconomic status score (1-99) for each of 174 countries gdppc: gdp per capita: single time-series (imputed) yrseduc: completed years of education in the adult (15+) population popshare: total population shares\n",
      "data sources: the dataset was compiled by shawn dorius (sdorius@iastate.edu) from a large number of data sources, listed below. gdp per capita: 1. maddison, angus. 2004. 'the world economy: historical statistics'. organization for economic co-operation and development: paris. maddison population data in 000s; gdp & gdp per capita data in (1990 geary-khamis dollars, ppps of currencies and average prices of commodities). maddison data collected from: http://www.ggdc.net/maddison/historical_statistics/horizontal-file_02-2010.xls. 2. world development indicators database years of education 1. morrisson and murtin.2009. 'the century of education'. journal of human capital(3)1:1-42. data downloaded from http://www.fabricemurtin.com/ 2. cohen, daniel & marcelo cohen. 2007. 'growth and human capital: good data, good results' journal of economic growth 12(1):51-76. data downloaded from http://soto.iae-csic.org/data.htm 3. barro, robert and jong-wha lee, 2013, \"a new data set of educational attainment in the world, 1950-2010.\" journal of development economics, vol 104, pp.184-198. data downloaded from http://www.barrolee.com/ total population 1. maddison, angus. 2004. 'the world economy: historical statistics'. organization for economic co-operation and development: paris. 13.\n",
      "2. united nations population division. 2009.\n",
      "context\n",
      "beginning in 2009, the frequency of earthquakes in the u.s. state of oklahoma rapidly increased from an average of fewer than two 3.0+ magnitude earthquakes per year since 1978 to hundreds per year in 2014, 2015, and 2016. thousands of earthquakes have occurred in oklahoma and surrounding areas in southern kansas and north texas since 2009. scientific studies attribute the rise in earthquakes to the disposal of wastewater produced during oil extraction that has been injected deeply into the ground. (wikipedia)\n",
      "injection wells are utilized to dispose of fluid created as a byproduct of oil and gas production activities. likewise, hydraulic fracturing, ie \"fracking\", produces large byproducts of water. this byproduct is then injected deep back into the earth via disposal/injection wells.\n",
      "content\n",
      "this dataset contains two data files. one detailing \"active\" saltwater injection wells in oklahoma, as of september 2017. the second file lists earthquakes in the oklahoma region (oklahoma and surrounding states) since 1977.\n",
      "acknowledgements\n",
      "data was gathered from oklahoma corporation commission and the united states geological survey.\n",
      "inspiration\n",
      "is there a correlation between earthquakes and injection well activity?\n",
      "can the data be used as a predictor of general proximity and/or time of future earthquakes ?\n",
      "i wrote an article a while back about how as tom cruise gets older his love interests stay the same age. while cruise is by no means exceptional in this respect, his age gap seemingly mirrors and confirms the larger critique of hollywood’s bias against older actresses (as gestured towards by the brilliant work of the folks at time and the pudding).\n",
      "i am not a numbers person by any stretch and have no experience handling data let alone analyzing and visualizing it. but i did make this hilariously crude google doc and that's got to count for something. if you're curious about methodology, it's specified at the tail end of the article.\n",
      "context\n",
      "this data set contains the acquired time series from 16 chemical sensors exposed to gas mixtures at varying concentration levels. in particular, we generated two gas mixtures: ethylene and methane in air, and ethylene and co in air. each measurement was constructed by the continuous acquisition of the 16-sensor array signals for a duration of about 12 hours without interruption.\n",
      "the data set was collected in a gas delivery platform facility at the chemosignals laboratory in the biocircuits institute, university of california san diego. the measurement system platform provides versatility for obtaining the desired concentrations of the chemical substances of interest with high accuracy and in a highly reproducible manner.\n",
      "the sensor array included 16 chemical sensors (figaro inc., us) of 4 different types: tgs-2600, tgs-2602, tgs-2610, tgs-2620 (4 units of each type). the sensors were integrated with customized signal conditioning and control electronics. the operating voltage of the sensors, which controls the sensorsâ€™ operating temperature, was kept constant at 5 v for the whole duration of the experiments. the sensorsâ€™ conductivities were acquired continuously at a sampling frequency of 100 hz. the sensor array was placed in a 60 ml measurement chamber, where the gas sample was injected at a constant flow of 300 ml/min.\n",
      "each measurement was constructed by the continuous acquisition of the 16-sensor array signals while concentration levels changed randomly. for each measurement (each gas mixture), the signals were acquired continuously for about 12 hours without interruption.\n",
      "the concentration transitions were set at random times (in the interval 80-120s) and to random concentration levels. the data set was constructed such that all possible transitions are present: increasing, decreasing, or setting to zero the concentration of one volatile while the concentration of the other volatile is kept constant (either at a fixed or at zero concentration level). at the beginning, ending, and approximately every 10,000 s, we inserted additional predefined concentration patterns with pure gas mixtures.\n",
      "the concentration ranges for ethylene, methane, and co were selected such that the induced magnitudes of the sensor responses were similar. moreover, for gas mixtures, lower concentration levels were favored. therefore, the multivariate response of the sensors to the presented set of stimuli is challenging since none of the configurations (single gas or mixture presentation) can be easily identified from the magnitude of sensorsâ€™ responses. in particular ethylene concentration ranges from 0-20 ppm; 0-600 ppm for co; and 0-300 ppm for methane.\n",
      "the primary purpose of making this data set freely accessible on-line is to provide extensive and continuous time series acquired from chemical sensors to the sensor and artificial intelligence research communities to develop and test strategies to solve a wide variety of tasks. in particular, the data set may be useful to develop algorithms for continuous monitoring or improve response time of sensory systems. also, the repetition of the same type of sensors in the array will allow further investigation on sensor variability (reproducibility of sensors of the same kind). other interesting topics may include sensor failure (to what extent system predictions degrade when sensors start failing) or calibration transfer (whether the model for one sensor can be extended to other sensors).\n",
      "more information on the generated data set can be found in fonollosa et al. 'reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring'; sensors and actuators b, 2015.\n",
      "the data set can be used exclusively for research purposes. commercial purposes are fully excluded.\n",
      "content\n",
      "the data is presented in two different files: each file contains the data from one mixture. the file ethylene_co.txt contains the recordings from the sensors when exposed to mixtures of ethylene and co in air. the file ethylene_methane.txt contains the acquired time series induced by the mixture of methane and ethylene in air.\n",
      "the structure of the files is the same: data is distributed in 19 columns. first column represents time (in seconds), second column represents methane (or co) concentration set point (in ppm), third column details ethylene concentration set point (in ppm), and the following 16 columns show the recordings of the sensor array.\n",
      "files include a header (one line) with the information of each column:\n",
      "time (seconds), methane conc (ppm), ethylene conc (ppm), sensor readings (16 channels)\n",
      "the order of the sensors in the files is as follows: tgs2602; tgs2602; tgs2600; tgs2600; tgs2610; tgs2610; tgs2620; tgs2620; tgs2602; tgs2602; tgs2600; tgs2600; tgs2610; tgs2610; tgs2620; tgs2620\n",
      "sensors' readings can be converted to kohms by 40.000/s_i, where s_i is the value provided in the text files.\n",
      "acknowledgements\n",
      "this dataset is republished as-is from the uci ml data repository, available here. the attribution thereof is:\n",
      "fonollosa et al. 'reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring'; sensors and actuators b, 2015\n",
      "creators:\n",
      "jordi fonollosa (fonollosa '@'ucsd.edu)\n",
      "biocircutis institute\n",
      "university of california san diego\n",
      "san diego, california, usa\n",
      "donors of the dataset:\n",
      "jordi fonollosa (fonollosa '@'ucsd.edu)\n",
      "ramon huerta (rhuerta '@' ucsd.edu)\n",
      "harvard tuition data since 1985, for both the undergraduate college and the graduate and professional schools.\n",
      "the data\n",
      "this dataset consists of two files: tuition_graduate.csv and undergraduate_package.csv, which contain the tuition and fees data for the graduate schools and undergraduate college, respectively.\n",
      "tuition_graduate.csv contains the following fields:\n",
      "academic.year: the academic year, between 1985 and 2017\n",
      "school: the name of the graduate or professional school; one of gsas, business (mba), design, divinity, education, government, law, medical/dental, public health (1-year mph)\n",
      "cost: the cost of tuition at a given school in a given year\n",
      "undergraduate_package.csv contains the following fields:\n",
      "academic.year: the academic year, between 1985 and 2017\n",
      "component: the component of undergraduate fees; one of tuition,*health services fee*,student services fee,*room*,board,*total*\n",
      "cost: the cost of the component; or, if the component is total, the sum of the costs of the other components in that year\n",
      "acknowledgements\n",
      "all of the data in this dataset comes from the harvard open data dataverse. specific citations are as follows:\n",
      "for the graduate tuition data:\n",
      "harvard financial aid office, 2015, \"harvard graduate school tuition\", doi:10.7910/dvn/lv0ysq, harvard dataverse, v1\n",
      "for the undergraduate tuition and fees data:\n",
      "harvard financial aid, 2015, \"harvard college tuition\", doi:10.7910/dvn/mss2be, harvard dataverse, v1 [unf:6:fyxnny+kbtglx+dzewzefg==]\n",
      "about this data\n",
      "this is a list of over 2,000 vacation rental properties in palm springs, ca provided by datafiniti's property database.\n",
      "the dataset includes property name, # beds, # bathrooms, price, and more. note that each property will have an entry for each price found for it and so a single property may have multiple entries.\n",
      "what you can do with this data\n",
      "a similar dataset was used to determine the most and least expensive cities for short-term vacation rentals in the us. e.g.:\n",
      "what are the least and most expensive one-bed rentals?\n",
      "what are the least and most expensive two-bed rentals?\n",
      "what are the least and most expensive three-bed rentals?\n",
      "what is the median price for short-term rental properties?\n",
      "what is the variation in rental prices?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "context\n",
      "this dataset consists of public records requests made by the washington post to police departments that oversee security at each nfl stadium.\n",
      "content\n",
      "twenty-nine of the 31 jurisdictions provided at least partial data, though reporting methods differed from agency to agency; cleveland and new orleans did not submit data. certain data were omitted if found to be incomplete or unreliable. among those jurisdictions sending partial arrest figures for home games between 2011 and 2015 were buffalo, miami and oakland. st. louis provided only year-by-year arrest data, rather than game-by-game numbers. detroit, minneapolis and atlanta did not provide data for arrests that took place in stadium parking lots.\n",
      "the main dataset includes fields such as the day of the week, which teams were playing on which home field, and the score of the game, between 2011 and 2015.\n",
      "inspiration\n",
      "which stadiums had the most arrests? the least?\n",
      "are arrests more likely when the home team lost a game? does the score correlate with number of arrests? (for example, if the game ended in a narrow loss for the home team, does this correlate with more arrests?)\n",
      "are there any stadiums with consistent arrest rates, regardless of how the game ended?\n",
      "acknowledgements\n",
      "data was collected and reported by kent babb and steven rich of the washington post, and the original dataset can be found here.\n",
      "this dataset contains results of the final round of nips 2017 adversarial learning competition.\n",
      "content\n",
      "matrices with intermediate results\n",
      "following matrices with intermediate results are provided:\n",
      "accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense\n",
      "error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense\n",
      "hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense\n",
      "in each of these matrices, rows correspond to defenses, columns correspond to attack. also first row and column are headers with kaggle team ids (or baseline id).\n",
      "scores and run time statistics of submissions\n",
      "following files contain scores and run time stats of the submissions:\n",
      "non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks\n",
      "targeted_attack_results.csv - scores and run time statistics of all targeted attacks\n",
      "defense_results.csv - scores and run time statistics of all defenses\n",
      "each row of these files correspond to one submission. columns have following meaning:\n",
      "kaggleteamid - either kaggle team id or id of the baseline.\n",
      "teamname - human readable team name\n",
      "score - raw score of the submission\n",
      "normalizedscore - normalized (to be between 0 and 1) score of the submission\n",
      "minevaltime - minimum evaluation time of 100 images\n",
      "maxevaltime - maximum evaluation time of 100 images\n",
      "medianevaltime - median evaluation time of 100 images\n",
      "meanevaltime - average evaluation time of 100 images\n",
      "notes about the data\n",
      "due to team mergers, team name in these files might be different from the leaderboard.\n",
      "not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).\n",
      "about\n",
      "this selection of images are controls selected from a screen to find novel anti-infectives using the roundworm c.elegans . the animals were exposed to the pathogen enterococcus faecalis and either untreated or treated with ampicillin, a known antibiotic against the pathogen. the untreated (negative control) worms display predominantly the \"dead\" phenotype: worms appear rod-like in shape and slightly uneven in texture. the treated (ampicillin, positive control) worms display predominantly the \"live\" phenotype: worms appear curved in shape and smooth in texture. for more information, please see moy et al. (acs chem biol, 2009) [http://dx.doi.org/10.1021/cb900084v]\n",
      "images\n",
      "one image per channel (channel 1 = brightfield; channel 2 = gfp) was acquired at mgh on a discovery-1 automated microscope (molecular devices). original image size is 696 x 520 pixels. images are available in 16-bit tif.\n",
      "ground truth\n",
      "the 384 images are from a plate of positive and negative controls. the images are named using this format: <plate>_<wellrow>_<wellcolumn>_<wavelength>_<fileid>.tif columns 1-12 are positive controls treated with ampicillin. columns 13-24 are untreated negative controls.\n",
      "we also provide human-corrected binary images of foreground/background segmentation. to address the problem of correctly segmenting individual worms also when they overlap or cluster, we provide one binary foreground/background segmentation ground truth image for each worm:\n",
      "acknowledgements\n",
      "the data have been reposted from the original data taken from the broad institute. please acknowledge the original source if this is used in other works. the original data can be found and downloaded here: https://data.broadinstitute.org/bbbc/bbbc010/\n",
      "these images were originally acquired for a screen in fred ausubel's lab at mgh. please contact aconery at molbio.mgh.harvard.edu for more information.\n",
      "original publication: http://dx.doi.org/10.1038/nmeth.1984\n",
      "inspiration\n",
      "this dataset contains commits with detailed information about changed files from about 12 years of the linux kernel master branch. it contains about 600.000 (filtered) commits and this breaks down to about 1.4 million file change records.\n",
      "each row represents a changed file in a specific commit, with annotated deletions and additions to that file, as well as the filename and the subject of the commit. i also included anonymized information about the author of each changed file aswell as the time of commit and the timezone of the author.\n",
      "the columns in detail:\n",
      "author_timestamp: unix timestamp of when the commit happened\n",
      "commit_hash: sha-1 hash of the commit\n",
      "commit_utc_offset_hours: extraced utc offset in hours from commit time\n",
      "filename: the filename that was changed in the commit\n",
      "n_additions: number of added lines\n",
      "n_deletions: number of deleted lines\n",
      "subject: subject of commit\n",
      "author_id: anonymized author id.\n",
      "i'm sure with this dataset nice visualizations can be created, let's see what we can come up with!\n",
      "for everybody interested how the dataset was created, i've setup a github repo that contains all the required steps to reproduce it here.\n",
      "if you have any questions, feel free to contact me via pm or discussions here.\n",
      "context\n",
      "compas (correctional offender management profiling for alternative sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendant’s likelihood of reoffending (recidivism). it has been shown that the algorithm is biased in favor of white defendants, and against black inmates, based on a 2 year follow up study (i.e who actually committed crimes or violent crimes after 2 years). the pattern of mistakes, as measured by precision/sensitivity is notable.\n",
      "quoting from propublica: \"\n",
      "black defendants were often predicted to be at a higher risk of recidivism than they actually were. our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent). white defendants were often predicted to be less risky than they were. our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders (48 percent vs. 28 percent). the analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.\n",
      "black defendants were also twice as likely as white defendants to be misclassified as being a higher risk of violent recidivism. and white violent recidivists were 63 percent more likely to have been misclassified as a low risk of violent recidivism, compared with black violent recidivists.\n",
      "the violent recidivism analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 77 percent more likely to be assigned higher risk scores than white defendants. \"\n",
      "content\n",
      "data contains variables used by the compas algorithm in scoring defendants, along with their outcomes within 2 years of the decision, for over 10,000 criminal defendants in broward county, florida. 3 subsets of the data are provided, including a subset of only violent recividism (as opposed to, e.g. being reincarcerated for non violent offenses such as vagrancy or marijuana).\n",
      "indepth analysis by propublica can be found in their data methodology article.\n",
      "acknowledgements\n",
      "data & original analysis gathered by propublica. original data methodology article: https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
      "original article: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
      "original data from propublica: https://github.com/propublica/compas-analysis\n",
      "additional \"simple\" subset provided by fairml, based on the propublica data:\n",
      "http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html\n",
      "inspiration\n",
      "ideas:\n",
      "feature importance when predicting the compass score itself, or recividism/crime risks.\n",
      "reweighting data to compensate for bias, e.g. subsetting for the violent offenders, or adjusting better for base risk.\n",
      "feature selection based on \"legal usage\"/fairness (e.g. exclude race and see how well your model works. it worked for me).\n",
      "context\n",
      "the data was obtained from the oecd website's productivity statistics section on 4 may 2017.\n",
      "productivity = output per units of input.\n",
      "to be expanded on.\n",
      "content\n",
      "to be filled in.\n",
      "acknowledgements\n",
      "the data in this dataset is the property of the organisation for economic co-operation and development (the “oecd”).\n",
      "the oecd makes data (the “data”) available for use and consultation by the public. as stated in section i(a) above, data may be subject to restrictions beyond the scope of these terms and conditions, either because specific terms apply to those data or because third parties may have ownership interests. it is the user’s responsibility to verify either in the metadata or source information whether the data is fully or partially owned by third parties and/or whether additional restrictions may apply, and to contact the owner of the data before incorporating it in your work in order to secure the necessary permissions. the oecd in no way represents or warrants that it owns or controls all rights in all data, and the oecd will not be liable to any user for any claims brought against the user by third parties in connection with the use of any data.\n",
      "permitted use except where additional restrictions apply as stated above, you can extract from, download, copy, adapt, print, distribute, share and embed data for any purpose, even for commercial use. you must give appropriate credit to the oecd by using the citation associated with the relevant data, or, if no specific citation is available, you must cite the source information using the following format: oecd (year), (dataset name),(data source) doi or url (accessed on (date)). when sharing or licensing work created using the data, you agree to include the same acknowledgment requirement in any sub-licenses that you grant, along with the requirement that any further sub-licensees do the same.\n",
      "inspiration: research questions\n",
      "why is productivity growth slowing down in many advanced and emerging economies?\n",
      "context\n",
      "in an effort to database all results from the olympic track & field events, this dataset is scraped from https://olympic.org/athletics.\n",
      "content\n",
      "all column headers are provided below.\n",
      "inspiration\n",
      "as a former (and hopefully future) runner, i'm inspired by the idea that data can help us better understand the progression of athletes over time at one of the largest global stages for the events, the olympics!\n",
      "code\n",
      "scraper: https://github.com/jayrav13/olympics-athletics\n",
      "context\n",
      "this is home value data for the hot nashville market.\n",
      "content\n",
      "there are 56,000+ rows altogether. however, i'm missing home detail data for about half. so if anyone wants to track that down then go for it! i'll be looking in the mean time. enjoy.\n",
      "will add the python file that retrieved this data once i clean it up.\n",
      "shameless plug:\n",
      "visit this link for my latest project, a sql magic function for ipython notebook.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "we do have a dataset with given loans and its arrears rate which allow a supervised machine learning.\n",
      "content\n",
      "this is a dataset of given loans with its default rate\n",
      "the dataset collected from an online survey questionnaire includes behavioral, psychographic, geographic and demographic information about armenian pubs.\n",
      "the data has been intended for an independent project organized by the students of the american university of armenia solely for educational purposes.\n",
      "this data is unique as the pubs sector in armenia has not been reasearched so far.\n",
      "use fivethirtyeight's march madness (men's basketball) forecasts to make the perfect bracket.\n",
      "round-by-round probability for each team. be sure to account for upsets!\n",
      "huge thanks to fivethirtyeight for allowing public access to this data.\n",
      "who can make the perfect bracket?\n",
      "opendata aig brazil\n",
      "sobre o projeto\n",
      "download dos dados\n",
      "ocorrências aeronáuticas\n",
      "aeronaves envolvidas\n",
      "fatores contribuintes\n",
      "recomendações de segurança\n",
      "notas técnicas\n",
      "os textos dentro das colunas estão denotados por aspas duplas (\"\").\n",
      "as colunas das tabelas estão separadas por til (~).\n",
      "as tabelas contém cabeçalhos que identificam suas colunas.\n",
      "em cada tabela existe uma coluna contendo a informação sobre a data de extração dos dados.\n",
      "outras informações \"for dummies\"\n",
      "os relatórios finais podem ser consultados no site do cenipa - relatórios.\n",
      "as recomendações de segurança podem ser consultadas no site do cenipa - recomendações.\n",
      "artigos científicos sobre o tema podem ser encontrados / publicados na revista conexão sipaer.\n",
      "outros recursos\n",
      "outras bases de dados para consultas:\n",
      "ntsb\n",
      "bea\n",
      "risco da fauna\n",
      "raio laser\n",
      "risco baloeiro\n",
      "aeródromos brasileiros\n",
      "aerovias brasileiras\n",
      "dicas para melhor aproveitamento dos recursos\n",
      "antes de fazer o download dos dados, leia com calma todo o texto desta página. este recurso irá guiá-lo(a) para um adequado entendimento sobre os relacionamentos entre os conjuntos de dados disponíveis (ocorrencia, aeronave envolvida, fator_contribuinte e recomendações de segurança).\n",
      "para aprofundar-se no tema, visite o site do cenipa e confira as legislações que norteiam a investigação e prevenção de acidentes aeronáuticos no brasil.\n",
      "conheça o manual de investigação do sipaer. nos anexos deste documento você encontrará uma tabela de domínios (taxonomia) para algumas das variáveis disponíveis nos conjuntos de dados.\n",
      "devido ao dinamismo dos trabalhos de investigação e preocupação do cenipa com a agilidade na disponibilização dos dados, os conjuntos de dados estarão sujeitos a modificações sempre que forem atualizados. portanto, sempre que possível, utilize a \"data de extração\" dos conjuntos de dados para justificar/referenciar os seus estudos e análises.\n",
      "saiba como trabalhar com dados no formato csv. clique aqui para aprender\n",
      "dúvidas\n",
      "se persistirem dúvidas, por gentileza me enviem uma issue (relatar problema). clique aqui para relatar um problema\n",
      "context\n",
      "the following is the results of an online survey conducted by boilingsteam.com among the linux gamers' community (n=560, sharing only here answers where respondents explicitly agreed to have their answers made public, i.e. total n size was higher) in end of q1 2016, to better understand their hardware, usage habits and reactions to several of valve's steam initiatives. most of the answers are coming from members of the r/linux_gaming and r/linux subreddits, so you need to take in account that this may not be representative of your typical linux user.\n",
      "content\n",
      "there are many variables in this data set, with both numerical, free text and categorical answers. every line corresponds to an individual response. note that answers are anonymous. the first row is the coding you can use for your analysis (that should save a bit of time), the second row is the actual question asked (you can erase it), and the data starts from the third row.\n",
      "questions cover some of the following attributes (there are much more in the actual datasheet):\n",
      "demographics / geography\n",
      "family situation\n",
      "os used for work and at home\n",
      "linux usage experience\n",
      "linux gaming experience\n",
      "type of gamer (hardcore or not)\n",
      "playing exclusively on linux or not\n",
      "time spent playing per week\n",
      "budget spent on linux games per month\n",
      "games played recently\n",
      "games bought recently\n",
      "hardware gpu for gaming\n",
      "hardware gpu model\n",
      "general hardware at home using linux\n",
      "usage of resellers (steam, gog, humblebundle)\n",
      "satisfaction of different resellers\n",
      "awareness of steam machines\n",
      "awareness of steam controller, steam link\n",
      "intent of purchase of steam machines\n",
      "intent of building steam machine diy\n",
      "steamos and opinion towards it\n",
      "general feeling towards future of linux\n",
      "stance about drm\n",
      "stance about wine\n",
      "wine usage and satisfaction\n",
      "and much more...\n",
      "acknowledgements\n",
      "the questionnaire was designed by ekianjo at boilingsteam.com. if you have suggestions for improvements of future surveys of the same kind, please reach us on kaggle or on our contact page: http://boilingsteam.com/about-boiling-steam/\n",
      "past research\n",
      "you can see some analysis done a previous iteration of this survey (previous data can not be made public however) - this may serve as a good benchmark to measure changes: http://boilingsteam.com/the-three-kinds-of-linux-gamers/\n",
      "inspiration\n",
      "feel free to play with the data, and share what insights you may find. we are big proponents of making data free in general for transparency purposes, so if your analysis can help generate a better understanding of who are linux gamers, this would be a great outcome.\n",
      "context\n",
      "this 2009 version represents the 13th iteration of the recs program. first conducted in 1978, the residential energy consumption survey is a national sample survey that collects energy-related data for housing units occupied as a primary residence and the households that live in them. data were collected from 12,083 households selected at random using a complex multistage, area-probability sample design. the sample represents 113.6 million u.s. households, the census bureau's statistical estimate for all occupied housing units in 2009 derived from their american community survey (acs)\n",
      "the csv data file is accompanied by a corresponding \"layout file\", which contains descriptive labels and formats for each data variable. the \"variable and response codebook\" file contains descriptive labels for variables, descriptions of the response codes, and indicators for the variables used in each end-use model.\n",
      "context\n",
      "since downloading the lichess app on july 10, 2016 (and as of march 16, 2017) i have played 2237 games of chess on my phone. lichess allows one to play chess against other players or computers with a variety of time formats and rule variations. it is my main phone addiction and occasional cause of procrastination. upon discovering kaggle, i decided to explore the site by uploading the data of these games and playing around with them.\n",
      "content\n",
      "most of the data is contained in 20170316_lichess_games.csv, accompanied by two separate files for white and black featuring move times (in tenths of seconds) for each game. the main files includes data on:\n",
      "unique identifiers (including game id, url, date/time created, date/time of last move)\n",
      "game parameters (including rated or not, rule variant, clock settings)\n",
      "game results (including total playing time, total turns, last player to move, winning color, method of winning [checkmate/resigns/etc])\n",
      "game play content (including move list, opening names)\n",
      "player specific information (including player id, elo rating [before game], elo rating change [if rated], total playing time, and min/max move times)\n",
      "acknowledgements\n",
      "all data was downloaded via the lichess api.\n",
      "inspiration\n",
      "besides the personal interests of seeing what variables are most predictive of my winning or what areas in which i have the most opportunity to improve, it may be interesting to explore how to create a chess coach for individual players.\n",
      "context\n",
      "using devices such as jawbone up, nike fuelband, and fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. these type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. one thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. our goal here will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. they were asked to perform barbell lifts correctly and incorrectly in 5 different ways. more information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the weight lifting exercise dataset).\n",
      "content\n",
      "the dataset contains about 160 predictors (most of which are not required) and classifiers column is 'classe' and the exercise pattern is classified into 5 types- a, b, c, d, e\n",
      "acknowledgements\n",
      "velloso, e.; bulling, a.; gellersen, h.; ugulino, w.; fuks, h. qualitative activity recognition of weight lifting exercises. proceedings of 4th international conference in cooperation with sigchi (augmented human '13) . stuttgart, germany: acm sigchi, 2013.\n",
      "read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz4dpxkfugx\n",
      "inspiration\n",
      "what better ways of cleaning up the data? which model will fit it best and how to go about handling it in r\n",
      "context\n",
      "this data comes from the prosper p2p lending platform. i came across the data during the udacity data analyst nanodegree. all credit goes to prosper and udacity. a link to the data is here: https://s3.amazonaws.com/udacity-hosted-downloads/ud651/prosperloandata.csv. a variable dictionary can be found here: https://docs.google.com/spreadsheets/d/1gdyi_l4uvirltec6wri5nbammkgmlqbk-yx3z0xdeti/edit#gid=0.\n",
      "content\n",
      "the data contains a wide variety of information concerning loans on the prosper p2p lending platform. more information can be found in the variable dictionary linked to above.\n",
      "acknowledgements\n",
      "once again, all credit for this data goes to prosper and to udacity. i would be happy to remove it if either party has any qualms with it being shared here.\n",
      "inspiration\n",
      "there are a wide variety of questions to be investigated with this fantastic dataset.\n",
      "content\n",
      "fivethirtyeight’s world cup forecasting model used espn’s soccer power index (spi) — a system that combines game and player ratings to estimate a team’s overall skill level — to calculate odds of each country’s performance during the two stages of the world cup. the probabilities, based on 10,000 simulations, were updated at the end of each match and aggregated into one file with the prediction date and time.\n",
      "acknowledgements\n",
      "the 2014 prediction data was featured in the fivethirtyeight article it's brazil's world cup to lose and the interactive infographic 2014 world cup predictions. the world cup match scores were scraped from the fifa archive.\n",
      "context\n",
      "huffpost pollster (originally pollster.com) aims to report the results of every public poll that claims to provide a representative sample of the population or electorate. we have included polls of varying methodology, including automated or recorded voice telephone polls and online surveys using non-probability internet samples.\n",
      "as of 2010, we require all polls from a new organization (or one new to us) to meet all of the minimal disclosure requirements of the national council on public polling. we have always excluded polls that fail to disclose survey dates, sample size, and sponsorship; however, we may now choose in our editorial discretion to exclude polls or pollsters that do not provide sufficient methodological information for us or our readers to determine their quality.\n",
      "content\n",
      "this dataset lists results from public opinion surveys regarding the president's job approval since 2008.\n",
      "acknowledgements\n",
      "the presidential approval ratings were provided by the organization listed and aggregated by huffpost pollster.\n",
      "inspiration\n",
      "how does public approval of the president change over time? how do current events and presidential responses impact approval ratings? can you predict how trump's approval rating will change over the course of his presidency?\n",
      "context\n",
      "college presidents across the nation recognized a need to track how student-athletes are doing academically prior to graduation. starting in 2003, colleges and universities in ncaa division i — the largest and highest profile athletics programs — implemented a comprehensive academic reform package designed to improve the academic success and graduation of all student-athletes. the centerpiece of the academic reform package was the development of a real-time academic measurement for sports teams, known as the academic progress rate (apr).\n",
      "the apr includes student-athlete eligibility, retention and graduation as factors in a formula that yields a single number, providing a much clearer picture of the current academic culture on each division i sports team in the country. since its inception, the apr has become an important measure of student-athlete academic success. for high apr scores, the ncaa recognizes member institutions for ensuring that student-athletes succeed in the classroom. if, however, low apr scores are earned consistently, member institutions can be subjected to penalties including scholarship reductions and the loss of eligibility to compete in championships.\n",
      "content\n",
      "this study was created, by the national collegiate athletic association (ncaa), to provide public access to team-level apr scores, eligibility rates, retention rates, and athlete counts on division i athletic programs starting with the 2003-2004 season through the 2013-2014 season\n",
      "inspiration\n",
      "which sport or school has the highest academic score? which schools' scores have increased or decreased significantly in the past decade? are men's or women's team academic performance better? what about public and private colleges?\n",
      "dataset source: https://github.com/metmuseum/openaccess\n",
      "the metropolitan museum of art presents over 5,000 years of art from around the world for everyone to experience and enjoy. the museum lives in three iconic sites in new york city—the met fifth avenue, the met breuer, and the met cloisters. millions of people also take part in the met experience online.\n",
      "since it was founded in 1870, the met has always aspired to be more than a treasury of rare and beautiful objects. every day, art comes alive in the museum's galleries and through its exhibitions and events, revealing both new ideas and unexpected connections across time and across cultures.\n",
      "the metropolitan museum of art provides select datasets of information on more than 420,000 artworks in its collection for unrestricted commercial and noncommercial use. to the extent possible under law, the metropolitan museum of art has waived all copyright and related or neighboring rights to this dataset using creative commons zero. this work is published from: the united states of america. you can also find the text of the cc zero deed in the file license in this repository. these select datasets are now available for use in any media without permission or fee; they also include identifying data for artworks under copyright. the datasets support the search, use, and interaction with the museum’s collection.\n",
      "at this time, the datasets are available in csv format, encoded in utf-8. while utf-8 is the standard for multilingual character encodings, it is not correctly interpreted by excel on a mac. users of excel on a mac can convert the utf-8 to utf-16 so the file can be imported correctly.\n",
      "additional usage guidelines\n",
      "images not included\n",
      "images are not included and are not part of the dataset. companion artworks listed in the dataset covered by the policy are identified in the collection section of the museum’s website with the creative commons zero (cc0) icon.\n",
      "for more details on how to use images of artworks in the metropolitan museum of art’s collection, please visit our open access page.\n",
      "documentation in progress\n",
      "this data is provided “as is” and you use this data at your own risk. the metropolitan museum of art makes no representations or warranties of any kind. documentation of the museum’s collection is an ongoing process and parts of the datasets are incomplete.\n",
      "we plan to update the datasets with new and revised information on a regular basis. you are advised to regularly update your copy of the datasets to ensure you are using the best available information.\n",
      "pull requests\n",
      "because these datasets are generated from our internal database, we do not accept pull requests. if you have identified errors or have extra information to share, please email us at openaccess@metmuseum.org and we will forward to the appropriate department for review.\n",
      "attribution\n",
      "please consider attributing or citing the metropolitan museum of art's cc0 select datasets, especially with respect to research or publication. attribution supports efforts to release other datasets in the future. it also reduces the amount of \"orphaned data,\" helping to retain source links.\n",
      "do not misrepresent the dataset\n",
      "do not mislead others or misrepresent the datasets or their source. you must not use the metropolitan museum of art’s trademarks or otherwise claim or imply that the museum or any other third party endorses you or your use of the dataset.\n",
      "whenever you transform, translate or otherwise modify the dataset, you must make it clear that the resulting information has been modified. if you enrich or otherwise modify the dataset, consider publishing the derived dataset without reuse restrictions.\n",
      "the writers of these guidelines thank the the museum of modern art, tate, cooper-hewitt, and europeana.\n",
      "context\n",
      "while searching for alpha-numeric handwritten dataset i came across hasyv2 dataset but it contains lot of other classes too. so i filtered the dataset which now contains only alpha-numeric handwritten data.\n",
      "content\n",
      "the dataset contains 2 numpy files and 1 csv file:\n",
      "alphanum-hasy-data-x.npy: contains images data-set with size (4658, 32, 32)\n",
      "alphanum-hasy-data-y.npy : contains corresponding labels data-set with size (4658,)\n",
      "symbols.csv : contains mapping between symbol_id and its symbol (i.e digit/char)\n",
      "more information at alphanum-hasyv2 github repository.\n",
      "acknowledgements\n",
      "hasyv2: https://arxiv.org/abs/1701.08380\n",
      "context\n",
      "abraham lincoln's election produced southern secession, war, and abolition. this dataset was used to study connections between news and slave prices for the period 1856-1861. by august 1861, slave prices had declined by roughly one-third from their 1860 peak. that decline was similar for all age and sex cohorts and thus did not reflect expected emancipation without compensation. the decision to secede reflected beliefs that the north would not invade and that emancipation without compensation was unlikely. both were encouraged by lincoln's conciliatory tone before the attack on fort sumter, and subsequently dashed by lincoln's willingness to wage all-out war.\n",
      "calomiris, charles w., and jonathan pritchett. 2016. \"betting on secession: quantifying political events surrounding slavery and the civil war.\" american economic review, 106(1): 1-23.\n",
      "content\n",
      "data description: by jonathan pritchett these data were collected from the office of the orleans parish civil clerk of court. the sample includes all slave sales recorded by the register of conveyance from october 1856 to august 1861. the construction of the dataset is similar to that employed previously by fogel and engerman (1976). the unit of observation is the individual with the exception of children who were bundled with their mothers. fields are defined as follows:\n",
      "id number: unique observation number.\n",
      "conveyance: number of conveyance volume.\n",
      "page: page number of transaction.\n",
      "researcher: initials of research assistant who transcribed transaction.\n",
      "notary first name: first name of public notary who recorded transaction.\n",
      "notary last name: last name of public notary.\n",
      "sales date: sales date of transaction (mm/dd/yyyy format). this is not the date the sale was recorded in conveyance office.\n",
      "sellers first name: seller’s first name.\n",
      "sellers last name: seller’s last name.\n",
      "sellers county of origin: seller’s county (or city) of origin.\n",
      "sellers state of origin: seller’s state of origin.\n",
      "representing seller: name of agent representing seller if seller is not present at time of sale (normally blank).\n",
      "relationship to seller: agent’s relationship to seller.\n",
      "buyers first name: buyer’s first name.\n",
      "buyers last name: buyer’s last name.\n",
      "buyers county of origin: buyer’s county (or city) of origin.\n",
      "buyers state of origin: buyer’s state of origin.\n",
      "representing buyer: name of agent representing buyer if buyer is not present at time of sale (normally blank).\n",
      "relationship to buyer: agent’s relationship to buyer.\n",
      "slave name: slave’s first name; rarely last name also listed.\n",
      "sex: male (m) or female (f). gender often inferred from sale record, such as negro vs. negress, mulatto vs. mulattress, etc.\n",
      "age: age in years.\n",
      "color: description of slave’s skin color, but may also indicate ancestry (such as mulatto).\n",
      "occupation: slave’s occupation – often blank.\n",
      "family relationship: describes family relationship of slaves (if any) sold in groups\n",
      "name child 1: name of child 1 (sold with mother). blank when slaves are listed separately and/or with separate prices.\n",
      "sex child 1: gender of child 1 (sold with mother).\n",
      "age child 1: age in years of child 1 (sold with mother). decimal equivalents for age in months.\n",
      "name child 2: name of child 2 (sold with mother). blank when slaves are listed separately and/or with separate prices.\n",
      "sex child 2: gender of child 2 (sold with mother).\n",
      "age child 2: age in years of child 2 (sold with mother). decimal equivalents for age in months.\n",
      "name child 3: name of child 3 (sold with mother). blank when slaves are listed separately and/or with separate prices.\n",
      "sex child 3: gender of child 3 (sold with mother).\n",
      "age child 3: age in years of child 3 (sold with mother). decimal equivalents for age in months.\n",
      "name child 4: name of child 4 (sold with mother). blank when slaves are listed separately and/or with separate prices.\n",
      "sex child 4: gender of child 4 (sold with mother).\n",
      "age child 4: age in years of child 4 (sold with mother). decimal equivalents for age in months.\n",
      "name child 5: name of child 5 (sold with mother). blank when slaves are listed separately and/or with separate prices\n",
      "sex child 5: gender of child 5 (sold with mother).\n",
      "age child 5: age in years of child 5 (sold with mother). decimal equivalents for age in months.\n",
      "name child 6: name of child 6 (sold with mother). blank when slaves are listed separately and/or with separate prices\n",
      "sex child 6: gender of child 6 (sold with mother).\n",
      "age child 6: age in years of child 6 (sold with mother). decimal equivalents for age in months.\n",
      "name child 7: name of child 7 (sold with mother). blank when slaves are listed separately and/or with separate prices.\n",
      "sex child 7: gender of child 7 (sold with mother).\n",
      "age child 7: age in years of child 7 (sold with mother). decimal equivalents for age in months.\n",
      "name child 8: name of child 8 (sold with mother). blank when slaves are listed separately and/or with separate prices.\n",
      "sex child 8: gender of child 8 (sold with mother).\n",
      "age child 8: age in years of child 8 (sold with mother). decimal equivalents for age in months.\n",
      "guaranteed: yes or no. most conveyance records omit this information (missing value).\n",
      "notes on guarantee: description of guarantee/flaw; reason why slave doesn’t have full guarantee.\n",
      "number of total slaves: total number of slaves listed in transaction.\n",
      "number of adult slaves: total number of “principal” slaves – corresponds to the number of separate entries for each transaction. recall that children (especially those under 10 years) were bundled with mothers.\n",
      "number of child slaves: total number of children listed in transaction.\n",
      "number of prices: total number of prices listed in transaction.\n",
      "price: price associated with slaves. for group sales with a single price, only one price is listed for first slave, and for other slaves, entry is blank.\n",
      "payment method: cash or credit. sometimes “cash and credit” for credit sales with down payment. also slave exchange, typical result of redhibition claim.\n",
      "payment flag: description of payment schedule for credit sales.\n",
      "dummy credit: 0 or 1 indicator for credit sales.\n",
      "down payment: number value of cash down payment for credit sales.\n",
      "mthcred: maximum length of credit (in months).\n",
      "interest rate: annual interest rate charged on credit sales.\n",
      "discount rate: calculated monthly discount rate.\n",
      "predicted rate: predicted monthly interest rate for credit sales without explicit interest rates.\n",
      "calculations: intermediate calculation – please ignore.\n",
      "ratio: intermediate calculation – please ignore.\n",
      "presentvalue: present value calculation for credit sales; blank for cash sales.\n",
      "dummy omission: 0 or 1 indicator for omitting observation.\n",
      "reason for omission: reason for excluding observation from working sample.\n",
      "comments: description of unusual characteristics for observation.\n",
      "dummy estate sale: 0 or 1 indicator for estate sale.\n",
      "acknowledgements\n",
      "calomiris, charles w., and jonathan pritchett. 2016. \"betting on secession: quantifying political events surrounding slavery and the civil war.\" american economic review, 106(1): 1-23. doi: 10.1257/aer.20131483\n",
      "this dataset was converted from xlsx to csv\n",
      "inspiration\n",
      "as a principal port, new orleans played a major role during the antebellum era in the atlantic slave trade. the authors of \"betting on secession: quantifying political events surrounding slavery and the civil war.\" did a fantastic job of putting this dataset together to learn more about the country's connections between slave trade and the american civil war.\n",
      "context\n",
      "jester is a joke recommender system developed at uc berkeley to study social information filtering. users of the system are presented a joke and then they rate them. this dataset is a collection of those ratings.\n",
      "http://eigentaste.berkeley.edu/\n",
      "eigentaste: a constant time collaborative filtering algorithm. ken goldberg, theresa roeder, dhruv gupta, and chris perkins. information retrieval, 4(2), 133-151. july 2001.\n",
      "content\n",
      "notes from the source:\n",
      "each row is a user (row 1 = user #1)\n",
      "each column is a joke (column 1 = joke #1)\n",
      "ratings are given as real values from -10.00 to +10.00\n",
      "99 corresponds to a null rating\n",
      "as of may 2009, the jokes 7, 8, 13, 15, 16, 17, 18, 19 are the \"gauge set\" (as discussed in the eigentaste paper)\n",
      "acknowledgements\n",
      "thanks go to dr. ken golberg's group for putting this super cool data together and for permission to share it with the kaggle community!\n",
      "eigentaste: a constant time collaborative filtering algorithm. ken goldberg, theresa roeder, dhruv gupta, and chris perkins. information retrieval, 4(2), 133-151. july 2001.\n",
      "the original data file was converted to a csv format before uploading.\n",
      "the original list of jokes was converted from dat to tsv.\n",
      "original files can be found here: http://eigentaste.berkeley.edu/dataset/\n",
      "inspiration\n",
      "take a look at the eigentaste paper to learn more about how the data was used. see if you can recreate the study or glean some new insight!\n",
      "context\n",
      "recognizing lexical inference is an essential component in natural language understanding. in question answering, for instance, identifying that broadcast and air are synonymous enables answering the question \"when was 'friends' first aired?\" given the text \"'friends' was first broadcast in 1994\". semantic relations such as synonymy (tall, high) and hypernymy (cat, pet) are used to infer the meaning of one term from another, in order to overcome lexical variability. this inference should typically be performed within a given context, considering both the term meanings in context and the specific semantic relation that holds between the terms.\n",
      "content\n",
      "this dataset provides annotations for fine-grained lexical inferences in-context. the dataset consists of 3,750 term pairs, each given within a context sentence, built upon a subset of terms from ppdb. each term pair is annotated to the semantic relation that holds between the terms in the given contexts.\n",
      "files:\n",
      "full_dataset.csv - the full dataset is provided, as well as the train-test-validation split.\n",
      "train.csv, test.csv, validation.csv - a split of the dataset to 70% train, 25% test, and 5% validation sets. each of the sets contains different term-pairs, to avoid overfitting for the most common relation of a term-pair in the training set.\n",
      "file structure: comma-separated file\n",
      "fields:\n",
      "x: the first term\n",
      "y: the second term\n",
      "context_x: the sentence in which x appears (highlighted by x)\n",
      "context_y: the sentence in which y appears (highlighted by y)\n",
      "semantic_relation: the (directional) semantic relation that holds between x and y: equivalence, forward_entailment, reverse_entailment, alternation, other-related and independence.\n",
      "confidence: the relation annotation confidence (percentage of annotators that selected this relation), in a scale of 0-1\n",
      "acknowledgements\n",
      "if you use this dataset, please cite the following paper:\n",
      "adding context to semantic data-driven paraphrasing.\n",
      "vered shwartz and ido dagan. *sem 2016.\n",
      "inspiration\n",
      "i hope that this dataset will motivate the development of context-sensitive lexical inference methods, which have been relatively overlooked, although they are crucial for applications.\n",
      "this data set was collected by a survey conducted by google forms for a bangladeshi university in order to examine their current academic situation and also to improve on them. this survey was part of the institutional quality assurance program, initiated by university grant commission, bangladesh and funded by world bank.\n",
      "to meet the globalization challenges raising higher education quality to the world standard is essential. bangladesh govt. has taken initiatives to develop the quality of tertiary education. govt. plans to prepare university graduates in such way that they can successfully compete in the context of international knowledge society.\n",
      "accordingly, the ministry of education, with the assistance of the world bank, has undertaken a higher education quality enhancement project (heqep). the project aims at improving the quality of teaching-learning and research capabilities of the tertiary education institutions through encouraging both innovation and accountability and by enhancing the technical and institutional capacity of the higher education sector.\n",
      "the university grants commission of bangladesh is the implementing agency of the project. a heqep unit has been established in ugc for implementation, management, monitoring and evaluation of the activities.\n",
      "the data set contains 500 rows and they are timestamped showing the exact time of data collection process. the survey was conducted on undergraduate and postgraduate level students of a bangladeshi private university. among the various columns of data, the gpa columns contains important linear data with strong correlation. these data can be used to predict other gpa columns. so that we can predict a student's gpa in advance and can take necessary steps to improve his or her score.\n",
      "some acronyms that might help understand the data set: s.s.c- secondary school certificate ( 10th class public exam) h.s.c- higher secondary school certificate ( 12th class public exam) area of evaluation - there are 1 to 5 points. where 5 being the best and 1 being the worst.\n",
      "my heartfelt acknowledgement goes to the students who helped me sharing their data and time to make this survey a success. without their help this could not be possible for me.\n",
      "context\n",
      "these datasets are extractions provided by frackingdata.org of the sql server 2012 backup file obtained on a monthly basis from fracfocus.org's \"fracfocus data download\" web page. as fracfocus.org's sql server 2012 backup file is inconvenient to ingest for most citizen-scientists or data analysts, frackingdata.org ingests the database and outputs both csv and sqlite files more readily suitable for analysis. the files in question, available herein, are also available at fracfocus.org's \"fracfocus data\" web page.\n",
      "content\n",
      "fracking well chemical disclosures, the \"registry\" files, hierarchy as follows:\n",
      "registryupload, this is the header file.\n",
      "registrypurpose, this is an intermediate file between the header and the ingredients.\n",
      "registryingredients, this is the detail file of the chemical ingredients used in each well.\n",
      "chemical health effects and toxicities by chemical abstract society registry number (casrn).\n",
      "acknowledgements\n",
      "frackingdata.org for the ingestion and conversion of the fracfocus registry database.\n",
      "fracfocus.org for the collection of the fracking well chemical disclosures.\n",
      "scorecard chemical health effects for the compilation of the various chemicals' health effects and toxicities.\n",
      "inspiration\n",
      "start by associating each fracking well chemical disclosure to its chemical health effects using the casnumber or casrn, as appropriate.\n",
      "acb-database\n",
      "the aim of this project is to collect information about all the games of the acb spanish basketball league from 1994 to 2016. a sqlite database has been used for this purpose. the code can be found in https://github.com/jgonzalezferrer/acb-database-scraping.\n",
      "why from 1994? in 1994, the acb changed to the current modern league format, which consists of a regular season and a playoff competition between the best 8.\n",
      "content\n",
      "this dataset includes statistics about the games, teams, players and coaches. it is divided in the following tables:\n",
      "game: basic information about the game such as the venue, the attendance, the kickoff, the involved teams and the final score.\n",
      "participant: a participant is a player, coach or referee that participates in a game. a participant is associated to a game, an actor and a team. each row contains information about different stats such as number of points, assists or rebounds.\n",
      "actor: an actor represents a player or a coach. it contains personal information about them, such as the height, position or birthday. with this table we can track the different teams that a player has been into.\n",
      "team: this class represents a team.\n",
      "teamname: the name of a team can change between seasons (and even within the same season).\n",
      "in summation, this database contains the stats from games such as http://www.acb.com/fichas/lacb61295.php\n",
      "context\n",
      "the ability to monitor species in their natural habitat is useful when determining how the species respond to changes in environment. this may be particularly important when the species being studied are endangered and a population decline will trigger management action. the eastern ground parrot (pezoporous wallicus wallicus) is found only in australia, where it is officially listed as a rare and threatened species. furthermore, it is difficult to directly observe, as it lives in dense vegetation and rarely flies. as such, the chance of successful monitoring depends on locating the vocalisation or calls. one of the techniques that has been commonly used to monitor ground parrots (gps) in their natural habitat is for experienced ecologists to listen to the calls and manually locate and plot the perceived distribution of the birds. more recently, surveys have also used techniques based on audio signal processing and pattern recognition, where records of acoustic signals are analysed and specific vocalisations are detected. such detection can be used for the purpose of recognition and estimation of the number of gps in the habitat.\n",
      "to facilitate research on bird call detection using pattern recognition techniques, we provide here a dataset of gp vocalisation. the dataset contains 4 audio sequences of different lengths sampled at 16khz using song meter sm2 devices. the dataset was recorded at dusk and dawn times on different days and at four different locations in the barren grounds nature reserve, a protected nature park located in the southern highlands region of new south wales, australia. the sequences include overlapping gp calls and sounds from sources other than gp (e.g. wind, spurious noise, and vocalisations of other species). all the gp calls in the dataset are manually annotated and used as the ground truth for evaluation of detection algorithms.\n",
      "content\n",
      "this is the dataset used for ground parrot call detection project. the folder data includes 1) 4 audio sequences formatted in wav files and named sq1, sq2, sq3, and sq4. 2) training. this folder contains the training data and includes 2 sub-folders: groundparrot (ground parrot calls) and others (sounds other than ground parrot calls). each folder contains 25 wav files and each file is 1 second length. 3) groundtruth. this folder includes 4 files. each file contains the ground-truth of the ground parrot calls of one of the 4 sequences, e.g. sq1_gt.txt is the ground-truth of the sequence sq1.wav. the ground-truth files are written in the following format, - the first line contains the number of ground parrot calls. - each following line describes a call with the start and end time stamp. the time stamps are represented in hours:minutes:seconds\n",
      "acknowledgements\n",
      "this work was supported by funding from the office of environment and heritage, nsw australia.\n",
      "context:\n",
      "the eurovision song contest, which originated in 1956, is present on youtube through uploads of songs performed in the contest. any user can freely comment on these songs. this dataset is made of up a collection of comments made on four youtube videos of eurovision entries by belgium. the comments are in a number of languages.\n",
      "content:\n",
      "the youtube online forums associated with the eurovision song contest have a large number of users from varied linguistic backgrounds who, because of their interests in song performance, are particularly attentive to language-related issues, such as the accent of the performers and the choice of language of the songs. commentaries are made by forum participants from disparate locations on a variety of topics, one of the most prominent being language, including language features and perceptions of language use.\n",
      "acknowledgements:\n",
      "this dataset was collected by dejan ivković for the purpose of linguistic research. if you made use of this data, please cite the following article:\n",
      "ivković, d. (2013). the eurovision song contest on youtube: a corpus-based analysis of language attitudes. language@internet, 10, article 1. (urn:nbn:de:0009-7-35977)\n",
      "inspiration:\n",
      "this dataset contains multiple languages. can you identify and the language of each comment?\n",
      "can you automatically find positive and negative comments about different country’s songs?\n",
      "are some commenters more positive or more negative than others?\n",
      "project description\n",
      "the paradis corpus consists of naturalistic language samples from 25 children learning english as a second language (english language learners or learners of english as an additional language). transcription is in english orthography only; phonetic transcription was not included in this research. any real names of people or places in the transcripts have been replaced with pseudonyms. the participants are identified with four letter codes.\n",
      "content\n",
      "the data in this corpus was collected in 2002 in edmonton, canada. children were video-­‐taped in conversation with a student research assistant in their homes for approximately 45 minutes. during this time, the research assistant had a list of “interview” questions to ask. if the child introduced his or her own topics and the conversation moved forward, the questions were not asked. this dataset only includes data from the first stage of data collection, in 2002. the full longituinal corpus may be found on the childes website, here: http://childes.talkbank.org/access/biling/paradis.html\n",
      "these data are in .cha files, which are intended for use with the program clan (http://alpha.talkbank.org/clan/). however, you may also treat these files as raw text files, with one speech snippet per line. lines starting with @ are metadata.\n",
      "file format information:\n",
      "*exp: experimenter speaking\n",
      "*chi: child speaking\n",
      "%[some text]: these lines contain non-linguistic information\n",
      "biographical data\n",
      "participants in this study were children from newcomer (immigrant and refugee) families to canada. the children started to learn english as a second language (l2) after their first language (l1) had been established, at 4 years 11 months on average. in the table below, “aoa” refers to the “age of arrival” of the child when the family immigrated. the number “1” indicates children who were canadian born. the column “aoe” refers to the age of onset of english acquisition. all ages are in months. each child’ s l1 and gender is also listed in the table below.\n",
      "for more information about the participants and procedures in this research, see the following:\n",
      "paradis, j. (2005). grammatical morphology in children learning english as a second language: implications of similarities with specific language impairment. language, speech and hearing services in the schools, 36, 172-187. golberg, h., paradis, j. & crago, m. (2008). lexical acquisition over time in minority l1 children learning english as a l2. applied psycholinguistics, 29, 1-25.\n",
      "inspiration:\n",
      "does children’s first language affect what english words they use? how many words?\n",
      "do some children pause (marked as (.) or (..)) more often than others?\n",
      "do children at different ages interrupt/overlap their speech more often? (marked by <> around text.)\n",
      "does a children’s age of first exposure to english affect how often then say “um”? (transcribed as“&-um”.)\n",
      "related datasets:\n",
      "when do children learn words?\n",
      "diagnosing specing language impairment in children\n",
      "context:\n",
      "an independence day is an annual event commemorating the anniversary of a nation's independence or statehood, usually after ceasing to be a group or part of another nation or state; more rarely after the end of a military occupation. most countries observe their respective independence days as national holidays.\n",
      "content:\n",
      "this dataset is a collection of information about 184 independence celebrations in countries around the world.\n",
      "acknowledgements:\n",
      "this dataset was taken from the wikipedia article “list of national independence days” on july 17, 2017.\n",
      "inspiration:\n",
      "are there seasonal clusters of independence day celebrations?\n",
      "how popular is it for an independence celebration’s name to include the name of the country?\n",
      "can you create an interactive visualization that shows when/where independence days are celebrated?\n",
      "context/background\n",
      "discourse acts are the different types of things you can do in a conversation, like agreeing, disagreeing or elaborating. this dataset contains annotations of the discourse acts of different twitter comments. the discourse acts labeled here are “coarse” in the sense that they’re labelled broadly (for the whole reddit comment) rather than for individual sentences or phrases, not in the sense of being vulgar. the discourse act of each post has been annotated by multiple annotators.\n",
      "content\n",
      "a large corpus of discourse annotations and relations on ~10k forum threads. please refer to the following paper for an in depth analysis and explanation of the data: characterizing online discussion using coarse discourse sequences (icwsm '17).\n",
      "explanation of fields\n",
      "thread fields\n",
      "url - reddit url of the thread\n",
      "title - title of the thread, as written by the first poster\n",
      "is_self_post - true if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)\n",
      "subreddit - the subreddit of the thread\n",
      "posts - a list of all posts in the thread\n",
      "post fields\n",
      "id - post id, reddit id of the current post\n",
      "in_reply_to - parent id, reddit id of the parent post, or the post that the current post is in reply to\n",
      "post_depth - the number of replies the current post is from the initial post\n",
      "is_first_post - true if the current post is the initial post\n",
      "annotations - a list of all annotations made to this post (see below)\n",
      "majority_type - the majority annotated type, if there is a majority type between the annotators, when considering only the main_type field\n",
      "majority_link - the majority annotated link, if there is a majority link between the annotators\n",
      "annotation fields\n",
      "annotator - an unique id for the annotator\n",
      "main_type - the main discourse act that describes this post\n",
      "secondary_type - if a post contains more than one discourse act in sequence, this is the second discourse act in the post\n",
      "link_to_post - the post that this post is linked to\n",
      "data sampling and pre-processing\n",
      "selecting reddit threads\n",
      "this data was randomly sampled from the full reddit dataset starting from its inception to the end of may 2016, which is made available publicly as a dump on google bigquery. this dataset was subsampled from the larger dataset and does not include posts with fewer than two comments, not in english, which contain pornographic material or from subreddits focused on trading. further, the number of replies to a single thread was limited to 40.\n",
      "annotation\n",
      "three annotators were assigned to each thread and were instructed to annotate each comment in the thread with its discourse act (main_type) as well as the relation of each comment to a prior comment (link_to_post), if it existed. annotators were instructed to consider the content at the comment level as opposed to sentence or paragraph level to make the task simpler.\n",
      "authors\n",
      "amy x. zhang, mit csail, cambridge, ma, usa. axz@mit.edu\n",
      "ka wong, google, mountain view, ca, usa. kawong@google.com\n",
      "bryan culbertson, calthorpe analytics, berkeley, ca, usa. bryan.culbertson@gmail.com\n",
      "praveen paritosh, google, mountain view, ca, usa. pkp@google.com\n",
      "citation guidelines\n",
      "if you are using this data towards a research publication, please cite the following paper.\n",
      "amy x. zhang, bryan culbertson, praveen paritosh. characterizing online discussion using coarse discourse sequences. in proceedings of the international aaai conference on weblogs and social media (icwsm '17). montreal, canada. 2017.\n",
      "bibtex: @inproceedings{coarsediscourse, title={characterizing online discussion using coarse discourse sequences}, author={zhang, amy x. and culbertson, bryan and paritosh, praveen}, booktitle={proceedings of the 11th international aaai conference on weblogs and social media}, series={icwsm '17}, year={2017}, location = {montreal, canada} }\n",
      "license\n",
      "cc-by\n",
      "inspiration\n",
      "can you visualize which discourse acts are used to in replies to each kind of discourse act?\n",
      "are threads more likely to be made up of a single type of discourse act or multiple discourse acts?\n",
      "are certain discourse acts more closely associated with specific subreddits?\n",
      "context:\n",
      "the armed conflict location and event data project is designed for disaggregated conflict analysis and crisis mapping. this dataset codes the dates and locations of all reported political violence and protest events in dozens of developing countries in africa. political violence and protest includes events that occur within civil wars and periods of instability, public protest and regime breakdown. the project covers all african countries from 1997 to the present.\n",
      "content:\n",
      "these data contain information on:\n",
      "dates and locations of conflict events;\n",
      "specific types of events including battles, civilian killings, riots, protests and recruitment activities;\n",
      "events by a range of actors, including rebels, governments, militias, armed groups, protesters and civilians;\n",
      "changes in territorial control; and\n",
      "reported fatalities.\n",
      "event data are derived from a variety of sources including reports from developing countries and local media, humanitarian agencies, and research publications. please review the codebook and user guide for additional information: the codebook is for coders and users of acled, whereas the brief guide for users reviews important information for downloading, reviewing and using acled data. a specific user guide for development and humanitarian practitioners is also available, as is a guide to our sourcing materials.\n",
      "acknowledgements:\n",
      "acled is directed by prof. clionadh raleigh (university of sussex). it is operated by senior research manager andrea carboni (university of sussex) for africa and hillary tanoff for south and south-east asia. the data collection involves several research analysts, including charles vannice, james moody, daniel wigmore-shepherd, andrea carboni, matt batten-carew, margaux pinaud, roudabeh kishi, helen morris, braden fuller, daniel moody and others. please cite:\n",
      "raleigh, clionadh, andrew linke, håvard hegre and joakim karlsen. 2010. introducing acled-armed conflict location and event data. journal of peace research 47(5) 651-660.\n",
      "inspiration:\n",
      "do conflicts in one region predict future flare-ups? how do the individual actors interact across time? do some sources report more often on certain actors?\n",
      "context\n",
      "this data set is the aggregate of 1,559 kcbs competitions from july 2013 through december 2016. the kansas city barbeque society (kcbs) is \"world's largest organization of barbeque and grilling enthusiasts with over 20,000 members worldwide.\" the data set was constructed by scraping the kcbs events page\n",
      "a standard competition\n",
      "at a standard kcbs bbq competition, 30 certified barbeque judges (cbjs) blindly judge the bbq served by 36 teams. judges are broken up into tables of 6. there are four categories of meat: chicken, pork ribs, pork and brisket. each judge receives six samples representing six different teams.\n",
      "scoring\n",
      "samples are scored across three characteristics: appearance, taste and tenderness. scores range from 0 to 9, with a 9 being perfect, a 6 corresponding to average, and a 0 given as part of a dq or other official sanction. a team's score within a category is calculated by a weighted sum of the six judges' scores. the kcbs scoring weights were last changed in july 2013 which aligns with the start date of this data set. the maximum score in a category is 180 (9 points * 3 characteristics * 6 judges). the maximum overall score in a standard 4-category competition is 720 (180 points * 4 categories). not all competitions in this data set are standard 4-category; see the is_standard feature.\n",
      "content\n",
      "competition data features:\n",
      "this csv file contains 1,559 rows of competition data. scoring and placing resultsare stored separately across five category-specific files. the scoring results are linked to their competitions by a foreign key. there are many results for each competition in this file.\n",
      "contest_key - the primary key used to link rows of results to a competition\n",
      "date - the date of the competition\n",
      "title - the name of the competition\n",
      "location_str - the full string of the location ie new palestine, in\n",
      "city - the city extracted from location_str\n",
      "state - the state abbreviation extracted from location_str\n",
      "state_full - the full name of the state extrapolated from state\n",
      "prize - the total prize money awarded (i believe it's total, and not just 1st place)\n",
      "cbj_percentage - the percentage of judges that are certified barbeque judges (cbjs)\n",
      "is_championsip - a boolean indicating if the competition is a \"state championship\" (note: each state has more than one per year)\n",
      "is_standard - a boolean indicating if the competition consists of and only of the four categories: chicken, pork ribs, pork and brisket. some competitions include extra categories like dessert and these are considered non-standard (note: overall scores in a non-standard competition may be greater that 4 * 180)\n",
      "results data features:\n",
      "there are five separate tables of results; one for each of the standard categories. the features are the same across each of these tables.\n",
      "contest_key - the foreign key linking a result row to its competition\n",
      "place - the place earned by a competing team (1st 2nd 3rd as 1, 2, 3)\n",
      "score - the total score (0-180 in a chicken, ribs, pork and brisket, 0-720 in overall-- assuming a standard competition)\n",
      "team_name - the name of the team (usually clever, and good for a word cloud!)\n",
      "acknowledgements\n",
      "i'd like to thank the kcbs for providing fun opportunities to taste great bbq, and recording event data on the website in an fairly accessible and light-weight manner. in the future i'd love to experiment with anonymized judge scoring data. a judge can view their scoring history and how it compared sample-by-sample with other judges' scores at the table.\n",
      "inspiration\n",
      "i've been a cbj for 4 years!\n",
      "context:\n",
      "a trademark is a brand name. a trademark or service mark includes any word, name, symbol, device, or any combination, used or intended to be used to identify and distinguish the goods/services of one seller or provider from those of others, and to indicate the source of the goods/services.\n",
      "content:\n",
      "the trademark case files dataset contains detailed information on 8.6 million trademark applications filed with or registrations issued by the uspto between january 1870 and january 2017. it is derived from the uspto main database for administering trademarks and includes data on mark characteristics, prosecution events, ownership, classification, third-party oppositions, and renewal history.\n",
      "this dataset is a partial version of the full dataset, made up of only the case files and owner information. for the full dataset and additional information, please see the uspto website.\n",
      "inspiration:\n",
      "which owner has filed for multiple trademarks with the longest break in between?\n",
      "who is the most prolific trademarker?\n",
      "how has the volume of trademarks changed since 1870?\n",
      "which us city has produced the most trademark owners?\n",
      "context\n",
      "workforce diversity is an increasingly salient issue, but it can be difficult to easily check how a specific company is performing. this dataset was created by fortune to show what was discoverable by someone considering employment with one of the fortune 500 firms and curious about their commitment to diversity and inclusion could find.\n",
      "content\n",
      "this dataset contains the name of each firm, its rank in the 2017 fortune 500, a link to its diversity and inclusion page or equal opportunity statement, and whether the company releases full, partial, or no data about the gender, race, and ethnicity of its employees. additional detail is included where it was available. as there are over 200 fields in this dataset; please consult the data dictionary for details about specific features.\n",
      "acknowledgements\n",
      "this dataset was assembled by fortune.com data reporter grace donnelly. the details of her data preparation process can be found here.\n",
      "inspiration\n",
      "are the companies that release the most information more or less diverse than their peers? are there any particular industries that stand out?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes 2gb of stop data from washington state. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one data file for each of these countries in asia and oceania.\n",
      "united arab emirates - arab_emirates.csv\n",
      "australia - australia.csv\n",
      "china - china.csv\n",
      "iceland - iceland.csv\n",
      "israel - israel.csv\n",
      "japan - japan.csv\n",
      "kazakhstan - kazakhstan.csv\n",
      "kuwait - kuwait.csv\n",
      "new caldonia - new_caldonia.csv\n",
      "new zealand - new_zealand.csv\n",
      "qatar - qatar.csv\n",
      "saudi arabia - saudiarabia.csv\n",
      "singapore - singapore.csv\n",
      "south korea - south_korea.csv\n",
      "taiwan - taiwan.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip.\n",
      "context\n",
      "coming soon\n",
      "content\n",
      "coming soon\n",
      "acknowledgements\n",
      "special thanks to http://www.histdata.com/download-free-forex-data/\n",
      "inspiration\n",
      "実際の取引にこの情報を使うときは十分ご注意ください。弊社およびコミュニティメンバーは損失の責任を取ることができません。\n",
      "context\n",
      "coming soon.\n",
      "content\n",
      "historical pm2.5 data in shanghai.\n",
      "acknowledgements\n",
      "special thanks to uci https://archive.ics.uci.edu/ml/datasets/pm2.5+data+of+five+chinese+cities\n",
      "inspiration\n",
      "we need care more about environment. let's use data science for social good.\n",
      "the results of kaggle's box office prediction league.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "the rajya sabha has published the questions and answers that were discussed in each session on their webiste. but one has to search by question/session/ministry wise to get the detail. there were no direct way to get all the quesions and answers in structured way(i.e, csv) to do an analysis and find insights. so i've written a web scrapper in r to scrape the data from the rajya sabha website and created csv files for each year.\n",
      "content\n",
      "this dataset helps one to understand what was being discussed in parliament (rajya sabha) of india. there are over 88000+ questions and answers that were discussed in rajya sabha from 2009 till date (sep'2017).\n",
      "variables detail:\n",
      "id - unique identifier\n",
      "answer_date - answer date\n",
      "ministry - ministry name\n",
      "question_type - type of question (starred or unstarred)*\n",
      "question_no - question number. (this question no. is unique per session)\n",
      "question_by - minister who has raised the question.\n",
      "question_title - discussion title\n",
      "question_description - detailed question.\n",
      "answer - detailed answer to the above question.\n",
      "*starred questions : these are questions to which answers are desired to be given orally on the floor of the house during the question hour. these are distinguished in the printed lists by asterisks. 15 such questions are listed each day.\n",
      "unstarred questions: these are questions to which written answers are given by ministers which are deemed to have been laid on the table of the house at the end of the question hour. upto 160 such questions are listed each day in a separate list.\n",
      "source\n",
      "acknowledgements\n",
      "thanks to rajya sabha for making the question and answers searchable.\n",
      "inspiration\n",
      "the below are some of the questions can be answered from this dataset.\n",
      "which state or district names mentioned most in question/answer?\n",
      "sentiment analysis\n",
      "no. of questions ministry wise\n",
      "no. of questions/answers per day. typically it should be 175 per day. how many days were less productive?\n",
      "who has raised more question? etc.\n",
      "context\n",
      "as a participant to nyc taxi trip duration, i'm providing additional data, to help extracting many new usefull features.\n",
      "to do so i'm using a high performance routing engine designed to run on openstreetmap data.\n",
      "having the whole blind test data, i decided also to share a small amount concerning erroneous samples (less than 0.15%), so competitors can focus matching real world data and to not try to fit randomness.\n",
      "note: the steps files are big so i split them into two parts. part 1, part2\n",
      "content\n",
      "description of different tables used.\n",
      "train/test augmented\n",
      "id: record id\n",
      "distance: route distance (m)\n",
      "duration: osrm trip duration (s)\n",
      "motorway, trunk, primary, secondary, tertiary, unclassified, residential:\n",
      "the proportion spent on different kind of roads (% of total distance)\n",
      "ntrafficsignals: the number of traffic signals.\n",
      "ncrossing: the number of pedestrian crossing.\n",
      "nstop: the number of stop signs.\n",
      "nintersection: the number of intersections, if you are osrm user, intersection have different meaning than the one used in osrm.\n",
      "*intersection can be crossroad, but not a highway exit...\n",
      "srccounty, dstcounty: pickup/dropoff county.\n",
      "na: not in nyc\n",
      "1: brooklyn\n",
      "2: queens\n",
      "3: staten island\n",
      "4: manhattan\n",
      "5: bronx\n",
      "train/test steps\n",
      "for each trip we saved all the ways (route portion).\n",
      "id: train/test id.\n",
      "wayid: way id, you can check the way using www.openstreetmap.org/way/wayid\n",
      "portion: the proportion of the total distance\n",
      "polylines table\n",
      "it contains encoded nodes (lon/lat coordinates), of the used ways.\n",
      "wayid: the way identification.\n",
      "polyline: encoded polylines.\n",
      "train/test bugs (0.15% of total data)\n",
      "id: same as original data.\n",
      "bug: kind of the bug (0=none)\n",
      "trip duration higher than 1 day;\n",
      "drop off on the day after pickup, and trip duration higher than 6h;\n",
      "drop off time at 00:00:00 and vendor_id eq 2.\n",
      "trip_duration: taxi trip duration\n",
      "credits\n",
      "real-time routing with openstreetmap data (http://project-osrm.org/)\n",
      "osm (http://www.openstreetmap.org)\n",
      "banner (photo by nicolai berntsen on unsplash)\n",
      "context\n",
      "the dataset here is the csv (comma separated value) formatted data of 1000+ indian companies' historical stock data which are listed on nse web scrapped using python. this data helps the community to dive into algorithmic trading using the ml techniques and can be used for any task. hope this will be of great use for everyone.\n",
      "content\n",
      "this dataset(.zip) is a collection of numerous csv formatted files that are in format of ['date','open','high','low','close','adj close','volume']. i've acquired this data using the yahoo finance v7 server using the python requests and a bit of pre-processing.\n",
      "maruti_data.csv is the sample data of maruti stock data from 2003-07 to till data (updated on 18-feb-2018) .\n",
      "companies_dict.d is the python pickle dictionary variable to get company name from the symbol or name if the csv file. you can load this using the pickle library and get the actual company symbol to legal name. e.g.python code\n",
      "symbol2name = pickle.load(open('company_symbol_name_dict.d','rb')) print(symbol2name['maruti']) #will give you maruti_suzuki_india_ltd\n",
      "acknowledgements\n",
      "i would like to thank this githubrepo for making the python file this script of mine is based on.\n",
      "inspiration\n",
      "i would love to see many people like me to get their hands dirty with this data and use it effectively to correlate the inter relationships among the companies.\n",
      "data are available for each junction to junction link on the major road network (motorways and a roads). data are also available for the sample of points on the minor road network (b, c and unclassified roads) that are counted each year, and these counts are used to produce estimates of traffic growth on minor roads.\n",
      "the data are produced for every year, and are in three formats: a) the raw manual count data collected by trained enumerators; b) annual average daily flows (aadfs) for count points on major roads and minor roads; and c) traffic figures for major roads only. explanatory notes (metadata) are available for each dataset, and in one combined note.\n",
      "a description of how annual road traffic estimates are produced is available at https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/270083/contents-page.pdf\n",
      "this dataset was kindly released by the british department of transportation. you can find the original dataset here.\n",
      "this dataset contains an annual summary of the assets and liabilities from the bank's founding in 1696 through 2014.\n",
      "content\n",
      "the csv is a condensed version of the original spreadsheet. some notes, disclaimers, and a small portion of the data have been discarded to enable the format conversion.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the bank of england. you can find the original dataset here.\n",
      "inspiration\n",
      "can you back out key moments in history from this dataset using time series analysis?\n",
      "context\n",
      "\"tate is an institution that houses the united kingdom's national collection of british art, and international modern and contemporary art. it is a network of four art museums: tate britain, london (until 2000 known as the tate gallery, founded 1897), tate liverpool (founded 1988), tate st ives, cornwall (founded 1993) and tate modern, london (founded 2000), with a complementary website, tate online (created 1998). tate is not a government institution, but its main sponsor is the uk department for culture, media and sport.\n",
      "\"the name 'tate' is used also as the operating name for the corporate body, which was established by the museums and galleries act 1992 as 'the board of trustees of the tate gallery'.\n",
      "\"the gallery was founded in 1897, as the national gallery of british art. when its role was changed to include the national collection of modern art as well as the national collection of british art, in 1932, it was renamed the tate gallery after sugar magnate henry tate of tate & lyle, who had laid the foundations for the collection. the tate gallery was housed in the current building occupied by tate britain, which is situated in millbank, london. in 2000, the tate gallery transformed itself into the current-day tate, or the tate modern, which consists of a federation of four museums: tate britain, which displays the collection of british art from 1500 to the present day; tate modern, which is also in london, houses the tate's collection of british and international modern and contemporary art from 1900 to the present day. tate liverpool has the same purpose as tate modern but on a smaller scale, and tate st ives displays modern and contemporary art by artists who have connections with the area. all four museums share the tate collection. one of the tate's most publicised art events is the awarding of the annual turner prize, which takes place at tate britain.\"\n",
      "-- tate. (n.d.). in wikipedia. retrieved august 18, 2017, from https://en.wikipedia.org/wiki/plagiarism. text reproduced here under a cc-by-sa 3.0 license.\n",
      "content\n",
      "this dataset contains the metadata for around 70,000 artworks that tate owns or jointly owns with the national galleries of scotland as part of artist rooms. metadata for around 3,500 associated artists is also included.\n",
      "the metadata here is released under the creative commons public domain cc0 licence. images are not included and are not part of the dataset.\n",
      "this dataset contains the following information for each artwork:\n",
      "id\n",
      "accession_number\n",
      "artist\n",
      "artistrole\n",
      "artistid\n",
      "title\n",
      "datetext\n",
      "medium\n",
      "creditline\n",
      "year\n",
      "acquisitionyear\n",
      "dimensions\n",
      "width\n",
      "height\n",
      "depth\n",
      "units\n",
      "inscription\n",
      "thumbnailcopyright\n",
      "thumbnailur\n",
      "url\n",
      "you may also like\n",
      "museum of modern art collection: title, artist, date, and medium of every artwork in the moma collection\n",
      "the metropolitan museum of art open access: explore information on more than 420,000 historic artworks\n",
      "context:\n",
      "prop. e was a measure on the november 8, 2016 san francisco ballot regarding responsibility for maintaining street trees and surrounding sidewalks. voters were asked if the city should amend the city charter to transfer responsibility from property owners to the city for maintaining trees on sidewalks adjacent to their property, as well for repairing sidewalks damaged by the trees. prop e passed with almost 80% of the voters’ support. as part of this proposition, a sf tree census was conducted by sf public works.\n",
      "content:\n",
      "18 columns of data includes address (including lat/longs), caretaker details, legal details, size of plot, time of planting, surrounding site context, and species.\n",
      "acknowledgements:\n",
      "this data was collected by sf public works. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too.\n",
      "inspiration:\n",
      "sf is notorious for it’s microclimates--can you identify zones from particular trees that thrive there?\n",
      "combine this data with the nyc tree census--which species are most common? least?\n",
      "context\n",
      "meetup.com is a website for people organizing and attending regular or semi-regular events (\"meet-ups\"). the relationships amongst users—who goes to what meetups—are a social network, ideal for graph-based analysis.\n",
      "this dataset was generated for a talk titled principles of network analysis with networkx, embedded online here (or with notebooks, etc. on github). it forms the basis for a series of tutorials i presented on at pynash and pytennessee. in them, we work through the basics of graph theory and how to use networkx, a popular open-source python package. we then apply this knowledge to extract insights about the social fabric of tennessee meetup groups.\n",
      "content\n",
      "graph data\n",
      "member-to-group-edges.csv: edge list for constructing a member-to-group bipartite graph. weights represent number of events attended in each group.\n",
      "group-edges.csv: edge list for constructing a group-to-group graph. weights represent shared members between groups.\n",
      "member-edges.csv: edge list for constructing a member-to-member graph. weights represent shared group membership.\n",
      "rsvps.csv: raw member-to-event attendance data, which was aggregated to form member-to-group-edges.csv.\n",
      "metadata\n",
      "meta-groups.csv: information for each group, including name and category. group_id can serve as index.\n",
      "meta-members.csv: information for each member, including name and location. member_id can serve as index.\n",
      "meta-events.csv: information for each event, including name and time. event_id can serve as index.\n",
      "acknowledgements\n",
      "i'd like to acknowledge the folks at meetup.com, who have made their database publicly available via a convenient rest api. even newbies like myself can access and enjoy!\n",
      "context\n",
      "the data was collected by social science research institutes:\n",
      "1973 bis 1993: institut für jugendforschung gmbh (ijf), münchen; 1997: gfm-getas/wba gmbh, hamburg 2001 bis 2015: forsa gesellschaft für sozialforschung und statistische analysen mbh, dortmund und berlin.\n",
      "long term studies regarding the alcohol and (illegal) drug consumption habits of 12 to 25 year old german teens were taken. the motivations and influences in drug consumption were studied. the aim was to develop preventional means and ways of communication with drug consuming teenagers.\n",
      "content\n",
      "young germans between 12 and 25 years were asked about their drug consumptions in the last 12 months. the survey was taken from the 70's until today. different datasets were provided and for some years features are missing.\n",
      "acknowledgements\n",
      "the data is provided by german bundeszentrale für gesundheitliche aufklärung (ministry of health education) and can be accessed at http://www.gbe-bund.de.\n",
      "photo by stas svechnikov on unsplash.\n",
      "inspiration\n",
      "while growing up in the late 2000's in germany i had the impression that teenagers smoke less cigarettes and drink less alcohol than they used to in the 90's. instead, they consumed more cannabis. i wonder if i was right ...\n",
      "context\n",
      "this dataset summarizes valuable data about recent total solar power generation and total electricity demand in a specific european country like italy. data are time series with hourly resolution, and values represent average of real-time power (generated and used) per market time unit (*). data correspond to years 2015 and 2016. they are useful to analyze, for instance, the variation of solar generation with time in the four seasons of the year, the change of electricity demand depending on the day of the week, or in summer/winter holidays. use of historical weather data could help to visualize the variation of solar power generation with climate conditions, extremely useful excercise for solar power generation forecasting. studies of load forecasting could be also conducted by making use of the present dataset.\n",
      "(*) detailed data descriptions\n",
      "content\n",
      "the two files include time series data of solar generation and total electricity consumption in italy during the years 2015 and 2016, with hourly resolution. csv files are structured in three columns: 1. date and time 2. load 3. solar generation the time is expressed in coordinated universal time (utc), and the format of date and time is \"%y-%m-%dt%h%m%sz\". solar generation and load are floating point numbers, which represent power expressed in mw (mega watts) units. solar generation is the total solar power generated in italy in 2015 and 2016, calculated by adding the generation in the different italian bidding zones (6 geographical regions: nord, centro nord, centro sud, sud, sardegna and sicilia, and 4 poles: brindisi, foggia, priolo and rossano). load represents the total demand of power in the same periods. note: the 2015 file presents a few missing data.\n",
      "acknowledgements\n",
      "data has been extracted from \"open power system data. 2017. data package time series. version 2017-07-09 (https://data.open-power-system-data.org/time_series/2017-07-09).\"\n",
      "the primary data source is entso-e transparency, the central data platform of the european transmission system operators (https://transparency.entsoe.eu).\n",
      "inspiration\n",
      "do you think we could improve the day-ahead load forecasting? navigate for instance the entso-e transparency website, it shows up-to-date comparisons between day ahead total load forecast and actual total load, by bidding zones or countries. as you will see, sometimes the differences may be significant.\n",
      "important: the entso-e platform is a great repository of energy data. measured data and forecasts are provided to the platform by the primary data owners (see terms and conditions at https://transparency.entsoe.eu).\n",
      "context\n",
      "this dataset comes from research by semeion, research center of sciences of communication. the original aim of the research was to correctly classify the type of surface defects in stainless steel plates, with six types of possible defects (plus \"other\"). the input vector was made up of 27 indicators that approximately [describe] the geometric shape of the defect and its outline. according to the research paper, semeion was commissioned by the centro sviluppo materiali (italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as input vectors or the types of the 6 classes of defects.\n",
      "content\n",
      "there are 34 fields. the first 27 fields describe some kind of steel plate faults seen in images. unfortunately, there is no other information that i know of to describe these columns.\n",
      "x_minimum\n",
      "x_maximum\n",
      "y_minimum\n",
      "y_maximum\n",
      "pixels_areas\n",
      "x_perimeter\n",
      "y_perimeter\n",
      "sum_of_luminosity\n",
      "minimum_of_luminosity\n",
      "maximum_of_luminosity\n",
      "length_of_conveyer\n",
      "typeofsteel_a300\n",
      "typeofsteel_a400\n",
      "steel_plate_thickness\n",
      "edges_index\n",
      "empty_index\n",
      "square_index\n",
      "outside_x_index\n",
      "edges_x_index\n",
      "edges_y_index\n",
      "outside_global_index\n",
      "logofareas\n",
      "log_x_index\n",
      "log_y_index\n",
      "orientation_index\n",
      "luminosity_index\n",
      "sigmoidofareas\n",
      "the last seven columns are one hot encoded classes, i.e. if the plate fault is classified as \"stains\" there will be a 1 in that column and 0's in the other columns. if you are unfamiliar with one hot encoding, just know that the last seven columns are your class labels.\n",
      "pastry\n",
      "z_scratch\n",
      "k_scatch\n",
      "stains\n",
      "dirtiness\n",
      "bumps\n",
      "other_faults\n",
      "acknowledgements\n",
      "metanet: the theory of independent judges (pdf download available). available from: https://www.researchgate.net/publication/13731626_metanet_the_theory_of_independent_judges [accessed sep 6, 2017].\n",
      "dataset provided by semeion, research center of sciences of communication, via sersale 117, 00128, rome, italy. www.semeion.it\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "this set of contract awards includes data on commitments against contracts that were reviewed by the bank before they were awarded (prior-reviewed bank-funded contracts) under ida/ibrd investment projects and related trust funds. this dataset does not list all contracts awarded by the bank, and should be viewed only as a guide to determine the distribution of major contract commitments among the bank's member countries. \"supplier country\" represents place of supplier registration, which may or not be the supplier's actual country of origin. information does not include awards to subcontractors nor account for cofinancing. the procurement policy and services group does not guarantee the data included in this publication and accepts no responsibility whatsoever for any consequences of its use. the world bank complies with all sanctions applicable to world bank transactions.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the world bank. you can find the original dataset here.\n",
      "inspiration\n",
      "how do the contract awards compare to each nations's voting rights? are there any unexpected, consistent preferences?\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 33344 hotels) that was created by extracting data from goibibo.com, a leading travel site from india.\n",
      "content\n",
      "this dataset has following fields:\n",
      "address\n",
      "area - the sub-city region that this hotel is located in, geographically.\n",
      "city\n",
      "country - always india.\n",
      "crawl_date\n",
      "guest_recommendation - how many guests that stayed here have recommended this hotels to others on the site.\n",
      "hotel_brand - the chain that owns this hotel, if this hotel is part of a chain.\n",
      "hotel_category\n",
      "hotel_description - a hotel description, as provided by the lister.\n",
      "hotel_facilities -\n",
      "hotel_star_rating - the out-of-five star rating of this hotel.\n",
      "image_count - the number of images provided with the listing.\n",
      "latitude\n",
      "locality\n",
      "longitude\n",
      "pageurl\n",
      "point_of_interest - nearby locations of interest.\n",
      "property_name\n",
      "property_type - the type of property. usually a hotel.\n",
      "province\n",
      "qts - crawl timestamp.\n",
      "query_time_stamp - copy of qts.\n",
      "review_count_by_category - reviews for the hotel, broken across several different categories.\n",
      "room_area\n",
      "room_count\n",
      "room_facilities\n",
      "room_type\n",
      "similar_hotel\n",
      "site_review_count - the number of reviews for this hotel left on the site by users.\n",
      "site_review_rating - the overall rating for this hotel by users.\n",
      "site_stay_review_rating\n",
      "sitename - always goibibo.com\n",
      "state\n",
      "uniq_id\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "try exploring some of the amenity categories. what do you see?\n",
      "try applying some natural language processing algorithms to the hotel descriptions. what are the some common words and phrases? how do they relate to the amenities the hotel offers?\n",
      "what can you discover by drilling down further into hotels in different regions?\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 16,000 books) that was created by extracting data from paytm.com, a leading ecommerce store in india.\n",
      "content\n",
      "this dataset has following fields:\n",
      "amtsave\n",
      "brand\n",
      "breadcrumbs\n",
      "country\n",
      "desc\n",
      "discount\n",
      "domain\n",
      "gallery\n",
      "image\n",
      "insertedon\n",
      "list_price\n",
      "model\n",
      "name\n",
      "other_sellers\n",
      "payment_methods_supported\n",
      "productcode\n",
      "selling_price\n",
      "specifications\n",
      "type\n",
      "uniq_id\n",
      "url\n",
      "weight\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analyses of pricing, discount, specifications and authors can be performed.\n",
      "context:\n",
      "this dataset reflects the daily volume of violations created by the city of chicago red light program for each camera. the data reflects violations that occurred from july 1, 2014 until present, minus the most recent 14 days. this data may change due to occasional time lags between the capturing of a potential violation and the processing and determination of a violation. the most recent 14 days are not shown due to revised data being submitted to the city of chicago during this period. the reported violations are those that have been collected by the camera system and reviewed by two separate city contractors. in some instances, due to the inability the registered owner of the offending vehicle, the violation may not be issued as a citation. however, this dataset contains all violations regardless of whether a citation was actually issued, which provides an accurate view into the red light program. because of occasional time lags between the capturing of a potential violation and the processing and determination of a violation, as well as the occasional revision of the determination of a violation, this data may change.\n",
      "content:\n",
      "more information on the red light program can be found here.\n",
      "data covers july 1, 2014 to sept 7, 2017. rows include:\n",
      "intersection: intersection of the location of the red light enforcement camera(s). there may be more than one camera at each intersection.\n",
      "camera id: a unique id for each physical camera at an intersection, which may contain more than one camera.\n",
      "address: the address of the physical camera (camera id). the address may be the same for all cameras or different, based on the physical installation of each camera.\n",
      "violation date: the date of when the violations occurred. note: the citation may be issued on a different date.\n",
      "violations: number of violations for each camera on a particular day.\n",
      "x coordinate: the x coordinate, measured in feet, of the location of the camera. geocoded using illinois state plane east (esri:102671).\n",
      "y coordinate: the y coordinate, measured in feet, of the location of the camera. geocoded using illinois state plane east (esri:102671).\n",
      "latitude: the latitude of the physical location of the camera(s) based on the address column. geocoded using the wgs84.\n",
      "longitutde: the longitude of the physical location of the camera(s) based on the address column. geocoded using the wgs84.\n",
      "location: the coordinates of the camera(s) based on the latitude and longitude columns. geocoded using the wgs84.\n",
      "acknowledgements:\n",
      "dataset compiled by city of chicago here.\n",
      "inspiration:\n",
      "which intersections have the most violations?\n",
      "when do most violations occur?\n",
      "context\n",
      "the punkt.zip file contains pre-trained punkt sentence tokenizer (kiss and strunk, 2006) models that detect sentence boundaries. these models are used by nltk.sent_tokenize to split a string into a list of sentences.\n",
      "a brief tutorial on sentence and word segmentation (aka tokenization) can be found in chapter 3.8 of the nltk book.\n",
      "the punkt.zip file contents:\n",
      "readme: contains the information of how and what the models are trained on.\n",
      "py3/: contains the below pickled files as above for python3\n",
      "czech.pickle\n",
      "danish.pickle\n",
      "dutch.pickle\n",
      "english.pickle\n",
      "estonian.pickle\n",
      "finnish.pickle\n",
      "french.pickle\n",
      "german.pickle\n",
      "greek.pickle\n",
      "italian.pickle\n",
      "norwegian.pickle\n",
      "polish.pickle\n",
      "portuguese.pickle\n",
      "slovene.pickle\n",
      "spanish.pickle\n",
      "swedish.pickle\n",
      "turkish.pickle\n",
      "citations\n",
      "kiss, tibor and strunk, jan (2006): unsupervised multilingual sentence boundary detection.\n",
      "computational linguistics 32: 485-525.\n",
      "context\n",
      "did you know that claims can be filed against tsa? sometimes us terminal security agency (tsa) makes mistakes. people can get hurt and property can be damaged, lost, or stolen. claims are generally filed against tsa for personal injuries and lost or damaged property during screenings and they keep records of every claim!\n",
      "content\n",
      "the dataset includes claims filed between 2002 through 2015.\n",
      "claim number\n",
      "date received\n",
      "incident date\n",
      "airport code\n",
      "airport name\n",
      "airline name\n",
      "claim type\n",
      "claim site\n",
      "item\n",
      "claim amount\n",
      "status\n",
      "close amount\n",
      "disposition\n",
      "acknowledgements\n",
      "file modifications: - excel format to tsv - commas to semicolons - tsv to csv\n",
      "original data can be found here: https://www.dhs.gov/tsa-claims-data\n",
      "inspiration\n",
      "i took a quick look at these data and discovered that the most claims are filed against tsa at john f. kennedy international. i also discovered four claims against wrongful death!\n",
      "context\n",
      "this is a dataset of every registered elevator in new york city. it was generated by the nyc department of buildings in september 2015 in response to a journalistic freedom of information law request, and contains information on elevator type, status, and function provided in the city's bisweb interface.\n",
      "content\n",
      "the addresses, locations, and statuses of elevators in new york city.\n",
      "acknowledgements\n",
      "this data is republished as-is from its public source on github. that data in turn came from the nyc department of buildings.\n",
      "inspiration\n",
      "where are the elevators, how many of them are functional, and what floors do they go to?\n",
      "resnet-152\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "the mnist dataset is one of the best known image classification problems out there, and a veritable classic of the field of machine learning. this dataset is more challenging version of the same root problem: classifying letters from images. this is a multiclass classification dataset of glyphs of english letters a - j.\n",
      "this dataset is used extensively in the udacity deep learning course, and is available in the tensorflow github repo (under examples). i'm not aware of any license governing the use of this data, so i'm posting it here so that the community can use it with kaggle kernels.\n",
      "content\n",
      "notmnist _large.zip is a large but dirty version of the dataset with 529,119 images, and notmnist_small.zip is a small hand-cleaned version of the dataset, with 18726 images. the dataset was assembled by yaroslav bulatov, and can be obtained on his blog. according to this blog entry there is about a 6.5% label error rate on the large uncleaned dataset, and a 0.5% label error rate on the small hand-cleaned dataset.\n",
      "the two files each containing 28x28 grayscale images of letters a - j, organized into directories by letter. notmnist_large.zip contains 529,119 images and notmnist_small.zip contains 18726 images.\n",
      "acknowledgements\n",
      "thanks to yaroslav bulatov for putting together the dataset.\n",
      "context\n",
      "kaggle dataset becomes a popular growing place to share datasets. almost every day there will be new datasets uploaded. i am curious to explore what can be extracted from the information of each dataset.\n",
      "content\n",
      "this dataset consists 2885 datasets information in 15 columns:\n",
      "title\n",
      "subtitle\n",
      "owner\n",
      "vote\n",
      "last update\n",
      "tags\n",
      "datatype\n",
      "size\n",
      "license\n",
      "views\n",
      "downloads\n",
      "kernels\n",
      "topics\n",
      "url\n",
      "description\n",
      "acknowledgements\n",
      "all data were taken from kaggle website. collected on 21 feb 2018\n",
      "inspiration\n",
      "with this dataset, we may try to predict the upcoming datasets uploaded, including its topics, number of votes, number of downloads, etc. data visualization involving clustering may be performed also.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "this is the food 101 dataset, also available from https://www.vision.ee.ethz.ch/datasets_extra/food-101/\n",
      "it contains images of food, organized by type of food. it was used in the paper \"food-101 – mining discriminative components with random forests\" by lukas bossard, matthieu guillaumin and luc van gool. it's a good (large dataset) for testing computer vision techniques.\n",
      "acknowledgements\n",
      "the food-101 data set consists of images from foodspotting [1] which are not property of the federal institute of technology zurich (ethz). any use beyond scientific fair use must be negociated with the respective picture owners according to the foodspotting terms of use [2].\n",
      "[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/\n",
      "dataset\n",
      "this dataset contains chat messages from dota 2 — video game by valve, one of the most popular esport discipline. the dataset was used to train roflan bot. it contains chats of almost 1m matches from public matchmaking (when players are selected by the game server at random with about the same skill level).\n",
      "caution and disclaimer\n",
      "important, please read. this dataset is completely not safe for work.\n",
      "in dota 2 the players communicate with each other in a very specific way. for instance, you may found a lot of abbreviations and game-specific terms. for dota 2 player it is typical to blame teammates and opponents for failure in the game. unfortunately, many messages may contain coarse insults, humiliation of another player's family, expressions of racism and other awful things. we provide the messages \"as is\" without any filters and censorship and we are not responsible for offensive content inside the data.\n",
      "our goal is to give researchers an opportunity to explore players community by diving into a real dialogs. we want to draw an attention to the problem of outstanding toxicity of the most dota 2 players, we consider this behaviour of players unhealthy.\n",
      "usage of the dataset\n",
      "see rough explanations on how do we learn our roflan bot intended to mirror typical player's chat behaviour. you can apply your own language models on this dataset and make alternative chat bot or just compare performance of learning.\n",
      "look at this arxiv paper with analysis of esport spectators' chats. you can apply similar analysis to game participants chats.\n",
      "context\n",
      "tatoeba is a crowd-sourced dataset made up of example sentences and their translations.\n",
      "this dump uploaded on kaggle is downloaded some time in oct/nov 2017. the latest dumps can be downloaded from https://tatoeba.org/eng/downloads\n",
      "acknowledgements\n",
      "credits goes to the maintainers and the crowd on https://tatoeba.org\n",
      "credits of the banner image goes to patrick tomasso\n",
      "license\n",
      "the official license is cc by-sa 2.0\n",
      "transport and logistics case study data set (cargo 2000)\n",
      "cargo 2000 is an initiative of iata, the international air transport association (cargo 2000 has been re-branded as cargo iq in 2016). it aims at delivering a new quality management system for the air cargo industry. cargo 2000 allows for unprecedented transparency in the supply chain. stakeholders involved in the transport process can share agreed cargo 2000 messages, comprising transport planning, replanning and service completion events. cargo 2000 is based on the following key principles: (1) every shipment gets a plan (called a route map) describing predefined monitoring events. (2) every service used during shipment is assigned a predefined milestone with a planned time of achievement. (3) stakeholders receive alerts when a milestone has failed and notifications upon milestone completion, which include the effective time the milestone has been achieved.\n",
      "content\n",
      "the case study data comprises tracking and tracing events from a forwarding company’s cargo 2000 system for a period of five months. from those cargo 2000 messages, we reconstructed execution traces of 3,942 actual business process instances, comprising 7,932 transport legs and 56,082 service invocations. each execution trace includes planned and effective durations (in minutes) for each of the services of the business process (introduced in section ii), as well as airport codes for the dep (“departure”) and rcf (“arrival”) services. due to the fact that handling of transport documents along the business process differs based on whether the documents are paper-based or electronic, we focus on the flow of physical goods, as our data set did not allow us to discern the different document types.\n",
      "the reconstruction process involved data sanitation and anonymization. we filtered overlapping and incomplete cargo 2000 messages, removed canceled transports (i.e., deleted route maps), sanitized for exceptions from the c2k system (such as events occurring before route map creation) and homogenized the way information was represented in different message types. finally, due to confidentiality reasons, message fields which might exhibit business critical or customer-related data (such as airway bill numbers, flight numbers and airport codes) have been eliminated or masked.\n",
      "each of the transport legs involves the following physical transport services:\n",
      "• rcs: check in freight at departure airline. shipment is checked in and a receipt is produced at departure airport.\n",
      "• dep: confirm goods on board. aircraft has departed with shipment on board.\n",
      "• rcf: accept freight at arrival airline. shipment is checked in according to the documents and stored at arrival warehouse.\n",
      "• dlv: deliver freight. receipt of shipment was signed at destination airport.\n",
      "acknowledgements\n",
      "a. metzger, p. leitner, d. ivanovic, e. schmieders, r. franklin, m. carro, s. dustdar, and k. pohl, “ comparing and combining predictive business process monitoring techniques,” ieee trans. on systems man cybernetics: systems, 2015.\n",
      "a. metzger, r. franklin, and y. engel, “ predictive monitoring of heterogeneous service-oriented business networks: the transport and logistics case,” in service research and innovation institute global conference (srii 2012), ser. conference publishing service (cps), r. badinelli, f. bodendorf, s. towers, s. singhal, and m. gupta, eds. ieee computer society, 2012.\n",
      "z. feldmann, f. fournier, r. franklin, and a. metzger, “industry article: proactive event processing in action: a case study on the proactive management of transport processes,” in proceedings of the seventh acm international conference on distributed event-based systems, debs 2013, arlington, texas, usa, s. chakravarthy, s. urban, p. pietzuch, e. rundensteiner, and s. dietrich, eds. acm, 2013.\n",
      "context\n",
      "the ramen rater is a product review website for the hardcore ramen enthusiast (or \"ramenphile\"), with over 2500 reviews to date. this dataset is an export of \"the big list\" (of reviews), converted to a csv format.\n",
      "content\n",
      "each record in the dataset is a single ramen product review. review numbers are contiguous: more recently reviewed ramen varieties have higher numbers. brand, variety (the product name), country, and style (cup? bowl? tray?) are pretty self-explanatory. stars indicate the ramen quality, as assessed by the reviewer, on a 5-point scale; this is the most important column in the dataset!\n",
      "note that this dataset does not include the text of the reviews themselves. for that, you should browse through https://www.theramenrater.com/ instead!\n",
      "acknowledgements\n",
      "this dataset is republished as-is from the original big list on https://www.theramenrater.com/.\n",
      "inspiration\n",
      "what ingredients or flavors are most commonly advertised on ramen package labels?\n",
      "how do ramen ratings compare against ratings for other food products (like, say, wine)?\n",
      "how is ramen manufacturing internationally distributed?\n",
      "context\n",
      "bob has started his own mobile company. he wants to give tough fight to big companies like apple,samsung etc.\n",
      "he does not know how to estimate price of mobiles his company creates. in this competitive mobile phone market you cannot simply assume things. to solve this problem he collects sales data of mobile phones of various companies.\n",
      "bob wants to find out some relation between features of a mobile phone(eg:- ram,internal memory etc) and its selling price. but he is not so good at machine learning. so he needs your help to solve this problem.\n",
      "in this problem you do not have to predict actual price but a price range indicating how high the price is\n",
      "context\n",
      "cannabis strains\n",
      "content\n",
      "strain name: given name of strain\n",
      "type of strain: indica, sativa, hybrid\n",
      "rating: user ratings averaged\n",
      "effects: different effects optained\n",
      "taste: taste of smoke\n",
      "description: backround, etc\n",
      "acknowledgements\n",
      "leafly.com\n",
      "inspiration\n",
      "marijuana may get a bad rep in the media as far as the decriminalization debate goes, but its health benefits can no longer go unnoticed. with various studies linking long-term marijuana use to positive, health-related effects, there are more than just a few reasons to smoke some weed every day.\n",
      "a study done by the boston medical center and the boston university of medicine, examined 589 drug users—more than 8 out of 10 of whom were pot smokers. it determined that “weed aficionados” were no more likely to visit the doctor than non-drug users. if an increased risk of contracting ailments is what’s preventing you from smoking more weed, it looks like you’re in the clear!\n",
      "one of the greatest medicinal benefits of marijuana is its pain relieving qualities, which make it especially effective for treating chronic pain. from menstruation cramps to nerve pain, as little as three puffs of bud a day can help provide the same relief as synthetic painkillers. marijuana relieves pain by “changing the way the nerves function,” says mark ware, md and assistant professor of anesthesia and family medicine at mcgill university.\n",
      "studies have found that patients suffering from arthritis could benefit from marijuana use. this is because naturally occurring chemicals in cannabis work to activate pathways in the body that help fight off joint inflammation.\n",
      "context\n",
      "data includes one month informatation of top 23 users in kernel rank. recording data in each day takes my 20 minutes so i only collect 1 month information of users.\n",
      "content\n",
      "there are 21 features(columns). date: date of the record. from 20.11.2017 to 17.12.2017. (day.month.year), name: name of the users, kernel_gold: number of gold medal that is won at kernel ranking, kernel_silver: number of silver medal that is won at kernel ranking,\n",
      "kernel_bronze: number of bronze medal that is won at kernel ranking,\n",
      "kernel_points: number of kernel points in kernel ranking, followers: number of followers of users, following: number of following, run_activity: number of daily run activity, comment_activity: number of daily comment activity, datasets: number of published datasets, kernel_rank: number of kernel rank, kernel_level: kernel level like kernel master or kernel expert, discussion_level: discussion level like discussion master or discussion expert, kernel_number: number of published kernel,\n",
      "discussion_number: number of discussion discussion_rank: discussion rank\n",
      "discussion_gold: number of gold medal that is won at discussion ranking, discussion_silver: number of silver medal that is won at discussion ranking,\n",
      "discussion_bronze: number of bronze medal that is won at discussion ranking,\n",
      "competition_rank: competition rank\n",
      "acknowledgements\n",
      "i take records at almost 22:00 according to istanbul clock therefore due to kaggle update time there can be inconsistency between discussion number and daily comment_activity.\n",
      "inspiration\n",
      "i collect this data because of my curiosity. however, 1 month information is not enough for detailed analysis. therefore, maybe more detailed public information of users can be published by kaggle team.\n",
      "context\n",
      "the global dataset of oil and natural gas production, prices, exports, and net exports.\n",
      "content\n",
      "oil production and prices data are for 1932-2014 (2014 data are incomplete); gas production and prices are for 1955-2014; export and net export data are for 1986-2013. country codes have been modified from earlier versions to conform to correlates of war (cow) and quality of government (qog) standards\n",
      "acknowledgements\n",
      "ross, michael; mahdavi, paasha, 2015, \"oil and gas data, 1932-2014\", doi:10.7910/dvn/ztpw0y, harvard dataverse\n",
      "inspiration\n",
      "how has the price varied from 1900s to 2000s?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this is an aggregate of the data i studied for my thesis titled, \"data mining in presidential debates and speeches: how campaign rhetoric shaped voter opinion in the 2016 u.s. presidential race\". the goal of my thesis was to use nlp techniques to understand how donald trump’s rhetoric impacted the opinions of various voter groups throughout his campaign. here is a summary of my findings:\n",
      "trump’s words were typically more common in an american english corpus and more extreme on both ends of the sentiment spectrum\n",
      "trump not only used rhetorical devices for persuasion but also adeptly coupled these devices with the right talking points based on the composition of his audience\n",
      "precise execution of the above strategy garnered him an unexpectedly large number of votes from the white female and hispanic demographics\n",
      "i hope that others can use this dataset to answer questions of their own about the 2016 presidential campaign.\n",
      "content\n",
      "collection of data from the 2016 u.s. presidential election campaign containing:\n",
      "transcripts of the three presidential debates, divided into separate trump and clinton text files\n",
      "transcripts of trump's 64 speeches delivered after the rnc and clinton's 35 speeches delivered after the dnc\n",
      "transcripts of select speeches delivered by candidates during the primary campaigns\n",
      "usc dornsife/la times presidential election poll, with daily breakdown by voter groups\n",
      "five thirty eight election poll, containing daily data from numerous pollsters\n",
      "acknowledgements\n",
      "debate and speech texts scraped from the american presidency project website.\n",
      "context\n",
      "machine learning with r by brett lantz is a book that provides an introduction to machine learning using r. as far as i can tell, packt publishing does not make its datasets available online unless you buy the book and create a user account which can be a problem if you are checking the book out from the library or borrowing the book from a friend. all of these datasets are in the public domain but simply needed some cleaning up and recoding to match the format in the book.\n",
      "content\n",
      "columns - age: age of primary beneficiary - sex: insurance contractor gender, female, male - bmi: body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9 - children: number of children covered by health insurance / number of dependents - smoker: smoking - region: the beneficiary's residential area in the us, northeast, southeast, southwest, northwest. - charges: individual medical costs billed by health insurance\n",
      "acknowledgements\n",
      "the dataset is available on github here.\n",
      "inspiration\n",
      "can you accurately predict insurance costs?\n",
      "english word vectors with sub-word information\n",
      "about fasttext\n",
      "fasttext is a library for efficient learning of word representations and sentence classification. one of the key features of fasttext word representation is its ability to produce vectors for any words, even made-up ones. indeed, fasttext word vectors are built from vectors of substrings of characters contained in it. this allows you to build vectors even for misspelled words or concatenation of words.\n",
      "about the vectors\n",
      "these pre-trained vectors contain 1 million word vectors learned with subword information on wikipedia 2017, the umbc webbase corpus and the statmt.org news dataset. in total, it contains 16b tokens.\n",
      "the first line of the file contains the number of words in the vocabulary and the size of the vectors. each line contains a word followed by its vectors, like in the default fasttext text format. each value is space separated. words are ordered by descending frequency.\n",
      "acknowledgements\n",
      "these word vectors are distributed under the creative commons attribution-share-alike license 3.0.\n",
      "p. bojanowski*, e. grave*, a. joulin, t. mikolov, enriching word vectors with subword information\n",
      "a. joulin, e. grave, p. bojanowski, t. mikolov, bag of tricks for efficient text classification\n",
      "a. joulin, e. grave, p. bojanowski, m. douze, h. jégou, t. mikolov, fasttext.zip: compressing text classification models\n",
      "\n",
      "(* these authors contributed equally.)\n",
      "about this data\n",
      "this is a list of over 2,000 reviews for beer, liquor, and wine sold online provided by datafiniti's product database. the dataset includes text and title of the review, the name and manufacturer of the product, reviewer metadata, and more.\n",
      "what you can do with this data\n",
      "you can use this data to discover insights into how consumers review alcoholic beverages. e.g.:\n",
      "which brands have the best reviews?\n",
      "does white wine or red wine get better reviews?\n",
      "what words are most commonly associated with each beverage type?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "about this data\n",
      "this is a list of over 71,045 reviews from 1,000 different products provided by datafiniti's product database. the dataset includes the text and title of the review, the name and manufacturer of the product, reviewer metadata, and more.\n",
      "what you can do with this data\n",
      "you can use this data to assess how writing quality impacts positive and negative online product reviews. e.g.:\n",
      "do reviewers use punctuation correctly?\n",
      "does the number of spelling errors differ by rating?\n",
      "what is the distribution of star ratings across products?\n",
      "how does review length differ by rating?\n",
      "how long is the typical review?\n",
      "what is the frequency of words with spelling errors by rating?\n",
      "what is the number of reviews that don’t end sentences with punctuation?\n",
      "what is the proportion of reviews with spelling errors?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "this first-of-its-kind citizen science project is a collection of photos submitted by a group of dedicated volunteers from locations across the united states during the august 21, 2017 total solar eclipse.\n",
      "the bigquery tables include metadata for the photos, links to the photos, and astronomical measurements extracted from the photos.\n",
      "acknowledgements\n",
      "this dataset was kindly prepared by google, uc berkeley, and thousands of volunteers. please see https://eclipsemega.movie/ for more information.\n",
      "inspiration\n",
      "can you map out the locations of the contributors to the project? how many of them were outside the path of totality?\n",
      "resnet-50\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "resnet-101\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "dataset includes check-in, tip and tag data of restaurant venues in nyc collected from foursquare from 24 october 2011 to 20 february 2012. it contains 3,112 users, 3,298 venues with 27,149 check-ins and 10,377 tips.\n",
      "content\n",
      "ny_restauraunts_checkins.csv {originally dataset_ubicomp2013_checkins.txt} has two columns. each line represents a check-in event. the first column is user id, while the second column is venue id.\n",
      "ny_restauraunts_tips.csv {originally dataset_ubicomp2013_tips.txt} has three columns. each line represents a tip/comment a user left on a venue. the first and second columns are user id and venue id, repsectively. the third column is tip text.\n",
      "ny_restauraunts_tags.csv {originally dataset_ubicomp2013_tags.txt} has two columns. each line represents the tags users added to a venue. the first column is venue id while the second column is tag set of the corresponding venues. empty tag sets may exist for a venue if no user has ever added a tag to it.\n",
      "acknowledgements\n",
      "columns headers, details and file formats added manually.\n",
      "source: scraped from foursquare and downloaded from: https://sites.google.com/site/yangdingqi/home/foursquare-dataset\n",
      "dingqi yang, daqing zhang, zhiyong yu and zhiwen yu, fine-grained preference-aware location search leveraging crowdsourced digital footprints from lbsns. in proceeding of the 2013 acm international joint conference on pervasive and ubiquitous computing (ubicomp 2013), september 8-12, 2013, in zurich, switzerland.\n",
      "dingqi yang, daqing zhang, zhiyong yu and zhu wang, a sentiment-enhanced personalized location recommendation system. in proceeding of the 24th acm conference on hypertext and social media (ht 2013), 1-3 may, 2013, paris, france. dingqi yang, daqing zhang, zhiyong yu, zhiwen yu, djamal zeghlache.\n",
      "sesame: mining user digital footprints for fine-grained preference-aware social media search. acm trans. on internet technology, (toit), 14(4), 28, 2014.\n",
      "original readme included (note that columns were added).\n",
      "inspiration\n",
      "interesting questions:\n",
      "linkage to additional data.\n",
      "sentiment analysis.\n",
      "recommender systems, prediction of checkins to related venues or tags.\n",
      "use for augmenting other datasets with geospatial or geotemporal data (for that period).\n",
      "context\n",
      "this is the dataset that is used in the telstra network disruptions competition that ran between nov 2015 and feb 2016 this competition provided a very nice and small dataset that allows many aspects of predictive modelling:\n",
      "relational data between different entities of the disruption data\n",
      "clean dataset that provides consistent and reliable feedback\n",
      "ideal for practices for many parts of the predictive modelling pipelin: feature engineering, cross-validation, stacking, etc\n",
      "magic feature! see forum thread for more details :)\n",
      "this dataset is re-uploaded since the original competition did not feature kernels, and it is made available here give people a chance to practice their data science/predictive modelling skill with this nice little dataset\n",
      "content\n",
      "the goal of the problem is to predict telstra network's fault severity at a time at a particular location based on the log data available. each row in the main dataset (train.csv, test.csv) represents a location and a time point. they are identified by the \"id\" column, which is the key \"id\" used in other data files.\n",
      "fault severity has 3 categories: 0,1,2 (0 meaning no fault, 1 meaning only a few, and 2 meaning many).\n",
      "different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv, severity_type.csv.\n",
      "note: “severity_type” is a feature extracted from the log files (in severity_type.csv). often this is a severity type of a warning message coming from the log. \"severity_type\" is categorical. it does not have an ordering. “fault_severity” is a measurement of actual reported faults from users of the network and is the target variable (in train.csv).\n",
      "acknowledgements\n",
      "this dataset is made available entirely for educational use only, it is shared by telstra and kaggle for the original competition, and is subjected to their permission of usage.\n",
      "inspiration\n",
      "how far up can you get in the post-deadline lb? :)\n",
      "context\n",
      "we have a few soccer datasets are already uploaded to kaggle platform. however, after playing with some hypothesis, we found that we need to have updates more often to complete started framework.\n",
      "main difference between well-known european soccer database are:\n",
      "added not only odds per match, but under over, asian handicaps.\n",
      "more seasons, leagues and matches;\n",
      "up to date data and regular updates of db\n",
      "since, that is first release of dataset, we will fix all bugs to have good training history.\n",
      "content\n",
      "data dictionary is described fully here - http://www.football-data.co.uk/notes.txt\n",
      "acknowledgements\n",
      "thanks to\n",
      "football-data.co.uk for collected results, fixtures and odds\n",
      "phoebet.com - for technical consultancies and helping to generate the dataset\n",
      "inspiration\n",
      "is it possible to get statistical advantage in long game?\n",
      "when better to use different financial strategies?\n",
      "what is more promising ordinar's or accumulator systems?\n",
      "ps. we are not encourage to play bets or have bonuses from advertisements! we just focused to share and open knowledge to community about this domain as particular case of arbitrage\n",
      "context\n",
      "the plants database provides standardized information about the vascular plants, mosses, liverworts, hornworts, and lichens of the u.s. and its territories. it includes names, plant symbols, checklists, distributional data, species abstracts, characteristics, images, plant links, references, and crop information, and automated tools.\n",
      "this particular dataset is the crop nutrient database.\n",
      "content\n",
      "these are the fields included in the dataset. i'll be honest, i have no idea what some of them mean:\n",
      "crop\n",
      "scientificname\n",
      "symbol\n",
      "nucontavailable\n",
      "plantpartharvested\n",
      "cropcategory\n",
      "yieldunit\n",
      "avyieldunitweight(lb)\n",
      "avmoisture%\n",
      "avn%(dry)\n",
      "avp%(dry)\n",
      "avk%(dry)\n",
      "yieldunitweight(lb)_set\n",
      "yieldunitweight(lb)_bau\n",
      "yieldunitweight(lb)_joh\n",
      "yieldunitweight(lb)_roberts\n",
      "yieldunitweight(lb)_weep\n",
      "yieldunitweight(lb)_men\n",
      "yieldunitweight(lb)_guy\n",
      "yieldunitweight(lb)_mc\n",
      "yieldunitweight(lb)_mah\n",
      "yieldunitweight(lb)_sha\n",
      "yieldunitweight(lb)_sch\n",
      "yieldunitweight(lb)_atu\n",
      "yieldunitweight(lb)_zim\n",
      "yieldunitweight(lb)_scu\n",
      "yieldunitweight(lb)_john\n",
      "yieldunitweight(lb)_arc\n",
      "drymatter%_m-ff\n",
      "drymatter%_nas\n",
      "drymatter%_f&l\n",
      "drymatter%_f&n\n",
      "drymatter%_alb\n",
      "drymatter%_est1\n",
      "drymatter%_est2\n",
      "drymatter%_est3\n",
      "drymatter%_est4\n",
      "drymatter%_est5\n",
      "drymatter%_est6\n",
      "drymatter%_m&r\n",
      "drymatter%_m&l\n",
      "drymatter%_sun\n",
      "drymatter%_gro\n",
      "drymatter%_agh8-9\n",
      "drymatter%_agh8-12\n",
      "drymatter%_b788\n",
      "avdrymatter%\n",
      "n%(dry)_nas\n",
      "n%(dry)_f&l\n",
      "n%(dry)_f&n\n",
      "n%(dry)_swa\n",
      "n%(dry)_chapko\n",
      "n%(dry)_hill\n",
      "n%(dry)_bru\n",
      "n%(dry)_agh8-9\n",
      "n%(dry)_agh8-12\n",
      "n%(dry)_b788\n",
      "n%(dry)_m&l\n",
      "n%(dry)_m-ff\n",
      "n%(dry)_m&r\n",
      "n%(dry)_foster\n",
      "n%(dry)_rob1\n",
      "n%(dry)_rob2\n",
      "n%(dry)_coa\n",
      "n%(dry)_and\n",
      "n%(dry)_gol1\n",
      "n%(dry)_gol2\n",
      "n%(dry)_wol\n",
      "n%(dry)_pete\n",
      "n%(dry)_col\n",
      "n%(dry)_alb\n",
      "n%(dry)_arc\n",
      "n%(dry)_bis\n",
      "n%(dry)_gar\n",
      "n%(dry)_heg\n",
      "n%(dry)_flo\n",
      "n%(dry)_feil\n",
      "n%(dry)_bre\n",
      "n%(dry)_burns\n",
      "n%(dry)_coc\n",
      "p%(dry)_m-ff\n",
      "p%(dry)_nas\n",
      "p%(dry)_f&l\n",
      "p%(dry)_f&n\n",
      "p%(dry)_agh8-9\n",
      "p%(dry)_agh8-12\n",
      "p%(dry)_b788\n",
      "p%(dry)_m&l\n",
      "p%(dry)_l&v\n",
      "p%(dry)_foster\n",
      "p%(dry)_rob1\n",
      "p%(dry)_rob2\n",
      "p%(dry)_coa\n",
      "p%(dry)_and\n",
      "p%(dry)_gol1\n",
      "p%(dry)_gol2\n",
      "p%(dry)_sims\n",
      "p%(dry)_wol\n",
      "p%(dry)_pete\n",
      "p%(dry)_col\n",
      "p%(dry)_alb\n",
      "p%(dry)_arc\n",
      "p%(dry)_swa\n",
      "p%(dry)_rei\n",
      "k%(dry)_m-ff\n",
      "k%(dry)_nas\n",
      "k%(dry)_f&l\n",
      "k%(dry)_f&n\n",
      "k%(dry)_agh8-9\n",
      "k%(dry)_agh8-12\n",
      "k%(dry)_b788\n",
      "k%(dry)_foster\n",
      "k%(dry)_rob1\n",
      "k%(dry)_rob2\n",
      "k%(dry)_coa\n",
      "k%(dry)_and\n",
      "k%(dry)_gol1\n",
      "k%(dry)_gol2\n",
      "k%(dry)_sims\n",
      "k%(dry)_wol\n",
      "k%(dry)_pete\n",
      "k%(dry)_col\n",
      "k%(dry)_alb\n",
      "k%(dry)_arc\n",
      "k%(dry)_swa\n",
      "k%(dry)_rei\n",
      "moisture%_m&r\n",
      "moisture%_m&l\n",
      "moisture%_sun\n",
      "moisture%_gro\n",
      "gwater/100g_agh8-9\n",
      "gwater/100g_agh8-12\n",
      "gwater/100g_b788\n",
      "protein%(dry)_nas\n",
      "protein%(dry)_f&l\n",
      "protein%(dry)_f&n\n",
      "protein%(dry)_swa\n",
      "protein%(dry)_chapko\n",
      "protein%(dry)_hill\n",
      "protein%(dry)_bru\n",
      "protein%(dry)_bis\n",
      "protein%(dry)_gar\n",
      "protein%(dry)_heg\n",
      "protein%(dry)_flo\n",
      "protein%(dry)_feil\n",
      "protein%(dry)_bre\n",
      "protein%(dry)_burns\n",
      "gprotein/100g(wet)_agh8-9\n",
      "gprotein/100g(wet)_agh8-12\n",
      "gprotein/100g(wet)_b788\n",
      "protein%(wet)_m&l\n",
      "n%(wet)_m-ff\n",
      "p%(wet)_m-ff\n",
      "gp/100g(wet)_agh8-9\n",
      "gp/100g(wet)_agh8-12\n",
      "gp/100g(wet)_b788\n",
      "p%(wet)_m&l\n",
      "k%(wet)_m-ff\n",
      "gk/100g(wet)_agh8-9\n",
      "gk/100g(wet)_agh8-12\n",
      "gk/100g(wet)_b788\n",
      "this dataset records police responses to 911 calls in the city of seattle.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the city of seattle. they update the data daily; you can find the original version here.\n",
      "inspiration\n",
      "the study discussed in this atlantic article reviewing 911 calls in milwaukuee found that that incidents of police violence lead to large drops in the number of 911 calls. does this hold true for seattle as well? this dataset technically only contains the responses to 911 calls rather than the calls themselves, but it should be feasible to use the responses as a decent proxy for calls.\n",
      "context\n",
      "are you tired of hearing your elders talk about how much cheaper things were back in their day? would you like to one-up them by talking about how much cheaper goods were a thousand years before they were born? of course you would!\n",
      "you might also have an interest in an unusually comprehensive set of historic prices that cover long time spans. english port wine prices, for example, stretch from 1209 through 1869.\n",
      "content\n",
      "each commodity contains prices in the local currency and standardized silver units that allow for broader comparisons.\n",
      "please note that this dataset has been consolidated into a single file from the original thousand or so csvs, so the format is slightly different.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by robert allen and richard unger. you can find the original dataset here.\n",
      "inspiration\n",
      "can you identify goods or locations that remained largely unaffected by the industrial revolution?\n",
      "if you like\n",
      "if you enjoyed this dataset, you might also like the millennium of macroeconomic data dataset.\n",
      "context:\n",
      "\"lynching\" historically includes not only southern lynching but frontier lynching and vigilantism nationwide and many labor-related incidents. persons of any race or ethnicity and either gender may have been either perpetrators or victims of lynching. the lynchings in this dataset follow an naacp definition for including an incident in the inventory of lynchings:\n",
      "there must be evidence that someone was killed;\n",
      "the killing must have occurred illegally;\n",
      "three or more persons must have taken part in the killing; and\n",
      "the killers must have claimed to be serving justice or tradition.\n",
      "content:\n",
      "the original data came from the naacp lynching records at tuskegee institute, tuskegee, alabama. stewart tolnay and e.m. beck examined these records for name and event duplications and other errors with funding from a national science foundation grant and made their findings available to project hal in 1998. project hal is inactive now, but it’s original purpose was to build a data set for researchers to use and to add to.\n",
      "the dataset contains the following information for each of the 2806 reported lynchings:\n",
      "state: state where the lynching took place\n",
      "year: year of the lynching\n",
      "mo: month\n",
      "day: day\n",
      "victim: name of the victim\n",
      "county: county where the lynching occurred (keep in mind that county names have changed & boundaries redrawn)\n",
      "race: race of the victim\n",
      "sex: sex of the victim\n",
      "mob: information on the mob\n",
      "offense: victim’s alleged offense\n",
      "note: note (if any)\n",
      "2nd name: name of the 2nd victim (if any)\n",
      "3rd name: name of the 3rd victim (if any)\n",
      "comments: comments (if any)\n",
      "source: source of the information (if any)\n",
      "acknowledgements:\n",
      "this dataset was compiled by dr. elizabeth hines and dr. eliza steelwater. if you use this dataset in your work, please include the following citation:\n",
      "hines, e., & steelwater, e. (2006). project hal: historical american lynching data collection project. university of north carolina, http://people.uncw.edu/hinese/hal/hal%20web%20page.htm\n",
      "you may also like:\n",
      "bryan stevenson’s equal justice initiative, eji posts lynching information and stories and is currently quite active: https://www.eji.org/\n",
      "first person narratives of the american south: personal accounts of southern life between 1860 and 1920\n",
      "inspiration:\n",
      "can you use the county-level data in this dataset to create a map of lynchings in the us?\n",
      "what demographic qualities were most associated with lynching victims?\n",
      "how did patterns of lynching change over time?\n",
      "context:\n",
      "there are lots of really cool datasets getting added to kaggle every day, and as part of my job i want to help people find them. i’ve been tweeting about datasets on my personal twitter accounts @rctatman and also releasing a weekly newsletter of interesting datasets.\n",
      "i wanted to know which method was more effective at getting the word out about new datasets: twitter or the newsletter?\n",
      "content:\n",
      "this dataset contains two .csv files. one has information on the impact of tweets with links to datasets, while the other has information on the impact of the newsletter.\n",
      "twitter:\n",
      "the twitter .csv has the following information:\n",
      "month: the month of the tweet (1-12)\n",
      "day: the day of the tweet (1-31)\n",
      "hour: the hour of the tweet (1-24)\n",
      "impressions: the number of impressions the tweet got\n",
      "engagement: the number of total engagements\n",
      "clicks: the number of url clicks\n",
      "fridata newsletter:\n",
      "the fridata .csv has the following information:\n",
      "date: the date the newsletter was sent out\n",
      "month: the month the newsletter was sent out (1-12)\n",
      "day: the day the newsletter was sent out (1-31)\n",
      "# of dataset links: how many links were in the newsletter\n",
      "recipients: how many people received the email with the newsletter\n",
      "total opens: how many times the newsletter was opened\n",
      "unique opens: how many individuals opened the newsletter\n",
      "total clicks: the total number of clicks on the newsletter\n",
      "unique clicks: (unsure; provided by tinyletter)\n",
      "notes: notes on the newsletter\n",
      "acknowledgements:\n",
      "this dataset was collected by the uploader, rachael tatman. it is released here under a cc-by-sa license.\n",
      "inspiration:\n",
      "which format receives more views?\n",
      "which format receives more clicks?\n",
      "which receives more clicks/view?\n",
      "what’s the best time of day to send a tweet?\n",
      "context:\n",
      "in the wcs investigation, an average of 24 native speakers of each of 110 unwritten languages were asked\n",
      "to name each of 330 munsell chips, shown in a constant, random order,\n",
      "exposed to a palette of these chips and asked to to pick out the best example(s) (\"foci\") of the major terms elicited in the naming task.\n",
      "see the original wcs instructions to fieldworkers for further information on the data elicitation method. the files in this archive display the results of that investigation.\n",
      "content:\n",
      "demographics on speakers, color chip coordinates, terms used, and the wcs and the munsell coordinates, as well as the ciel*a*b* coordinates. further details and resources at source.\n",
      "acknowledgements:\n",
      "this material is based upon work supported by the national science foundation under grant no. 0130420. richard cook1, paul kay2, and terry regier3\n",
      "university of california at berkeley\n",
      "international computer science institute, berkeley, ca\n",
      "university of chicago\n",
      "in any published work based on these data, please cite these archives. url: http://www.icsi.berkeley.edu/wcs/data.html\n",
      "context:\n",
      "london's fire and rescue service is the busiest in england and one of the largest firefighting and rescue organisations in the world. in the aftermath of the grenfell tower fire, it is critical that firefighting resources are accurately and appropriately deployed.\n",
      "content:\n",
      "this data covers jan 01-april 30 2017, consisting of 32 columns containing information on time, type, and address of call, as well the home station, stay duration, and arrival time of attending pumps.\n",
      "acknowledgements:\n",
      "this dataset was compiled by the city of london. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too.\n",
      "inspiration:\n",
      "which boroughs have the shortest average call response? longest?\n",
      "which boroughs have the greatest volume of calls?\n",
      "context\n",
      "data taken from the marine management organisation on all uk vessels landing in ports, or foreign vessels landing in uk ports. this dataset contains data on the catch, also the weight and value (£) of the catch.\n",
      "content\n",
      "year year of landing, 2008-2015\n",
      "month calendar months, 1-12\n",
      "port of landing name of port\n",
      "port nationality nationality of the port of landing\n",
      "vessel nationality nationality of the vessel\n",
      "length group length of the vessel, 10m or under/over 10m\n",
      "gear category gear carried by the vessel\n",
      "species code three letter code of the catch\n",
      "species name name of the catch\n",
      "species as shown in publication name of the catch with fewer subcategories\n",
      "species group catch species\n",
      "live weight (tonnes) weight of live catch\n",
      "landed weight (tonnes) landed weight of catch\n",
      "value (£) the value of the catch in gbp, most likely without any inflation adjustment\n",
      "points to note\n",
      "data on port nationality and vessel nationality for 2008 were supplied as 3 letter codes, not all of which matched standard country codes. i've tried to clean these up as best i can to match with standard country codes but ones that couldn't be matched have been left as-is.\n",
      "species code and species name were not supplied for 2008. you may be able to infer some from the 2009-2015 data.\n",
      "inspiration\n",
      "which uk ports see the greatest activity?\n",
      "which foreign ports see the greatest number of uk vessels?\n",
      "has the price-paid per tonne of goods varied during the data collection period?\n",
      "has there been any major changes in activity for particular species?\n",
      "acknowledgements\n",
      "data taken from the office of national statistics and is part of the uk sea fisheries annual statistics. data are available under a open government licence v3.0.\n",
      "context\n",
      "nasa has something like 400 different facilities across the united states! this dataset is a collection of those facilities and their locations.\n",
      "content\n",
      "center: name of the \"center\", a collection facilities\n",
      "center search status: public or...?\n",
      "facility: name of the facility\n",
      "facilityurl\n",
      "occupied\n",
      "status\n",
      "url link\n",
      "record date\n",
      "last update\n",
      "country\n",
      "location\n",
      "city\n",
      "state\n",
      "zipcode\n",
      "acknowledgements\n",
      "this dataset was downloaded from https://data.nasa.gov/management-operations/nasa-facilities/gvk9-iz74. the original file was modified to remove contact information for each facility.\n",
      "context\n",
      "extra-vehicular activities are activities done by an astronaut or cosmonaut outside a spacecraft beyond the earth's appreciable atmosphere. i like to just call it space walking :) it's unclear if this is a complete record of spacewalks. so keep that in mind.\n",
      "content\n",
      "eva #\n",
      "country\n",
      "crew: crew members, separated with |\n",
      "vehicle: space craft, space ship, space station, etc. if multiple vehicles, they are separated with |\n",
      "date\n",
      "duration\n",
      "purpose: description of the eva. some of these have internal commas and are enclosed with double quotes (\")\n",
      "acknowledgements\n",
      "these data were collected from here\n",
      "the original csv was modified slightly to remove extra spaces\n",
      "beef. lamb. veal. we might not all eat them, but they are the meats whose grades the us department of agriculture has seen fit to publish. this dataset contains records on meat production and quality as far back as 1930.\n",
      "after meat and poultry are inspected for wholesomeness, producers and processors may request that they have products graded for quality by a licensed federal grader. the usda's agricultural marketing service (http://www.ams.usda.gov) is the agency responsible for grading meat and poultry. those who request grading must pay for the service. grading for quality means the evaluation of traits related to tenderness, juiciness, and flavor of meat; and, for poultry, a normal shape that is fully fleshed and meaty and free of defects.\n",
      "usda grades are based on nationally uniform federal standards of quality. no matter where or when a consumer purchases graded meat or poultry, it must have met the same grade criteria. the grade is stamped on the carcass or side of beef and is usually not visible on retail cuts. however, retail packages of beef, as well as poultry, will show the u.s. grade mark if they have been officially graded.\n",
      "to better understand the available fields: - all fields labeled 'pounds' are really in units of indicate millions of pounds. - you can find a helpful explanation of what the different grades mean here.\n",
      "acknowledgements\n",
      "this data was kindly released by the us department of agriculture. you can find their most recent meat updates here.\n",
      "inspiration\n",
      "this is a good dataset for anyone looking to do basic data cleanup. i've converted it into a properly formed csv, but there are still numerous missing values, footnoted fields, and exceptions.\n",
      "this is a good candidate for regression analysis, especially in conjunction with other datasets. can you identify correlates for the amount of beef produced? validate how well cattle futures predict annual yields?\n",
      "2015 was a banner year for beef production. what happened?\n",
      "the objective of the brfss is to collect uniform, state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases, injuries, and preventable infectious diseases in the adult population. factors assessed by the brfss include tobacco use, health care coverage, hiv/aids knowledge or prevention, physical activity, and fruit and vegetable consumption. data are collected from a random sample of adults (one per household) through a telephone survey.\n",
      "the behavioral risk factor surveillance system (brfss) is the nation's premier system of health-related telephone surveys that collect state data about u.s. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. established in 1984 with 15 states, brfss now collects data in all 50 states as well as the district of columbia and three u.s. territories. brfss completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\n",
      "content\n",
      "each year contains a few hundred columns. please see one of the annual code books for complete details.\n",
      "these csv files were converted from a sas data format using pandas; there may be some data artifacts as a result.\n",
      "if you like this data, you might also enjoy the 2011-2015 batch. please note that those years use a different format.\n",
      "acknowledgements\n",
      "this dataset was released by the cdc. you can find the original dataset, manuals, and additional years of data here.\n",
      "context\n",
      "firearms sold in the united states must be licensed by the us department of justice bureau of alcohol, tobacco, firearms and explosives. this dataset is a record of every firearm license which was still current as of july 2017.\n",
      "content\n",
      "this dataset contains the names, license types, expiration dates, and locations of all federal firearms license (ffl) holders in the united states. the possible license types are:\n",
      "01 dealer in firearms other than destructive devices (includes gunsmiths)\n",
      "02 pawnbroker in firearms other than destructive devices\n",
      "03 collector of curios and relics\n",
      "06 manufacturer of ammunition for firearms\n",
      "07 manufacturer of firearms other than destructive devices\n",
      "08 importer of firearms other than destructive devices\n",
      "09 dealer in destructive devices\n",
      "10 manufacturer of destructive devices\n",
      "11 importer of destructive devices\n",
      "acknowledgements\n",
      "this data is published online in a tab-separated format by the department of justice bureau of alcohol, tobacco, firearms, and explosives. it has been lightly retouched into a csv file before publication here.\n",
      "inspiration\n",
      "can you geocode this data to determine where licensed gun shops are distributed?\n",
      "what is the distribution of gun licenses across different types?\n",
      "the united states code (\"code\") contains the general and permanent laws of the united states, arranged into 54 broad titles according to subject matter. the organization of the code was originally established by congress in 1926 with the enactment of the act of june 30, 1926, chapter 712. since then, 27 of the titles, referred to as positive law titles, have been restated and enacted into law by congress as titles of the code. the remaining titles, referred to as non-positive law titles, are made up of sections from many acts of congress that were either included in the original code or subsequently added by the editors of the code, i.e., the office of the law revision counsel, and its predecessors in the house of representatives. positive law titles are identified by an asterisk on the search & browse page. for an explanation of the meaning of positive law, see the positive law codification page.\n",
      "each title of the code is subdivided into a combination of smaller units such as subtitles, chapters, subchapters, parts, subparts, and sections, not necessarily in that order. sections are often subdivided into a combination of smaller units such as subsections, paragraphs, subparagraphs, clauses, subclauses, and items. in the case of a positive law title, the units are determined by congress in the laws that enact and later amend the title. in the case of a non-positive law title, the organization of the title since 1926 has been determined by the editors of the code and has generally followed the organization of the underlying acts 1 as much as possible. for example, chapter 7 of title 42 sets out the titles, parts, and sections of the social security act as corresponding subchapters, parts, and sections of the chapter.\n",
      "in addition to the sections themselves, the code includes statutory provisions set out as statutory notes, the constitution, several sets of federal court rules, and certain presidential documents, such as executive orders, determinations, notices, and proclamations, that implement or relate to statutory provisions in the code. the code does not include treaties, agency regulations, state or district of columbia laws, or most acts that are temporary or special, such as those that appropriate money for specific years or that apply to only a limited number of people or a specific place. for an explanation of the process of determining which new acts are included in the code, see the about classification page.\n",
      "the code also contains editorially created source credits, notes, and tables that provide information about the source of code sections, their arrangement, the references they contain, and their history.\n",
      "the law contained in the code is the product of over 200 years of legislating. drafting styles have changed over the years, and the resulting differences in laws are reflected in the code. similarly, code editorial styles and policies have evolved over the 80-plus years since the code was first adopted. as a result, not all acts have been handled in a consistent manner in the code over time. this guide explains the editorial styles and policies currently used to produce the code, but the reader should be aware that some things may have been done differently in the past. however, despite the evolution of style over the years, the accuracy of the information presented in the code has always been, and will always remain, a top priority.\n",
      "content\n",
      "this dataset is a snapshot of the xml version of the united states code. it is not a suitable for any form of legal work and is intended for research purposes only.\n",
      "the data are stored in a large json dictionary, indexed by the title of the code.\n",
      "acknowledgements\n",
      "this dataset was released by the united states government publishing office. you can find the original dataset here.\n",
      "context:\n",
      "dataset is a list of film and television permits received from the mayor's office of media and entertainment in response to a series of foil requests in 2015. the permits stretch from october 2011 through september 2015.\n",
      "content:\n",
      "projecttitle: the title of the film/television project.\n",
      "eventname: a shorthand name for the specific shoot/event being permitted, e.g. sunsetpark-010815.\n",
      "eventtype: one of the following: scouting permit, rigging permit, shooting permit, film shoot / production, dcas prep/shoot/wrap permit, grid request, or red carpet premiere. according to the mome, shooting permit and film shoot / production are interchangeable.\n",
      "eventstartdate and eventenddate: the start and end date and time of the permit.\n",
      "location: one or more locations covered by the permit.\n",
      "boro: what borough the listed locations are in.\n",
      "projectid: an internal identifier.\n",
      "categoryname: film or television.\n",
      "subcategoryname: includes values such as pilot, student film, variety, reality, etc. this probably isn't a reliable classification for tv shows: it's chosen by the permit applicant on the online form and is not vetted by the mayor's office. it also includes overlapping subcategories. for example, a show could be both a morning show and a talk show but would have to choose one or the other.\n",
      "companyname: the supplied production company name, which can be useful in connecting a working title to an actual film/show.\n",
      "the project title is sometimes a variation on the actual title (e.g. mozart in the jungle s1 or the wolf of wall street reshoots) or a working title (e.g. untitled female buddy cop movie instead of the heat, st james place instead of bridge of spies).\n",
      "in some cases, the locations listed actually span multiple boroughs, and the boro field only represents the primary borough, or the borough of the first listed location. in some cases, the boro field is blank.\n",
      "a given shooting permit can have any number of locations listed for a single day. according to the guidelines, the locations are supposed to be listed in the order they're used on that day. most locations are either an address or a range of blocks in the format of street 1 between street 2 and street 3.\n",
      "permits are generally required when asserting the exclusive use of city property, like a sidewalk, a street, or a park. a shooting permit on a street doesn't necessarily mean there is exterior shooting on the street. it may just mean, for example, that something is being shot indoors and the crew needs special parking privileges for trucks. see \"when a permit is required\".\n",
      "shooting on department of citywide administrative services (dcas) property, like in a city courthouse, involves an additional permitting process.\n",
      "shooting on mta property or on state/federal property is subject to a different permitting process.\n",
      "a shooting permit is typically, but not always, for a single day or a single overnight period.\n",
      "acknowledgements:\n",
      "data was foil’d by wnyc data journalism team and hosted originally on github here. check out these great related resources:\n",
      "general mome permit info\n",
      "the made in ny location library\n",
      "dcas managed public buildings\n",
      "metrocosm's nyc film permits map\n",
      "2015 bcg report on media and entertainment in nyc\n",
      "inspiration:\n",
      "where do most films occur in the city?\n",
      "when is the most common filming time?\n",
      "who films the most in the city?\n",
      "context\n",
      "the democracy fund voter study group is using a unique longitudinal data set that most recently surveyed 8,000 adults (age 18+) in december 2016 via yougov. participants were identified from a pool of respondents who participated in a similar survey in december 2011, as well as a second pre-election interview in 2012, and a third interview following the 2012 presidential election. for these 8,000 respondents, we have measures of their political attitudes, values, and affinities in 2011 as well as self-reports of their turnout and vote choice in november 2012.\n",
      "content\n",
      "the voter (views of the electorate research) survey was conducted by the survey firm yougov. in total, 8,000 adults (age 18+) with internet access took the survey online between november 29 and december 29, 2016. the estimated margin of error is plus or minus 2.2 percent. yougov also supplied measures of primary voting behavior from the end of the primary period (july 2016), when these respondents had been contacted as part of a different survey project.\n",
      "these respondents were originally interviewed by yougov in 2011 to 2012 as part of the 2012 cooperative campaign analysis project (ccap). in that survey, 45,000 respondents were first interviewed in december 2011 and were interviewed a second time in one of the 45 weekly surveys between january 1 and november 8, 2012. after the november election, 35,408 respondents were interviewed a third time. we invited 11,168 panelists from the 2012 ccap. of those invited, 8,637 (77 percent) completed the 2016 voter survey.\n",
      "the 2012 ccap was constructed using yougov’s sample matching procedure. a stratified sample is drawn from yougov’s panel, which consists of people who have agreed to take occasional surveys. the strata are defined by the combination of age, gender, race, and education, and each stratum is sampled in proportion to its size in the u.s. population. then, each element of this sample is matched to a synthetic sampling frame that is constructed from the u.s. census bureau’s american community survey, the current population survey voting and registration supplement, and other databases. the matching procedure finds the observation in the sample from yougov’s panel that most closely matches each observation in the synthetic sampling frame on a set of demographic characteristics. the resulting sample is then weighted by a set of demographic and non-demographic variables.\n",
      "information on variables can be found in the \"guide to the 2016 voter survey\" and included in the dataset.\n",
      "acknowledgements\n",
      "the democracy fund, a charitable foundation committed to the protection and enhancement of democratic values, found the rise of these movements and candidates worthy of analysis. in may 2016, the democracy fund chose to begin a rigorous project that would supply hard data to test proposed theories to explain these phenomenon. working with henry olsen of the ethics and public policy center and john sides of george washington university, the democracy fund assembled a diverse group of scholars and analysts representing political viewpoints from all angles.\n",
      "additional information on participants can be found here.\n",
      "data was originally published here.\n",
      "to reference the voter survey, please use this protocol: democracy fund voter study group. views of the electorate research survey, december 2016. [computer file] release 1: august 28, 2017. washington dc: democracy fund voter study group [producer] https://www.voterstudygroup.org/.\n",
      "inspiration\n",
      "the democracy fund voter study group has made available a series of insights on the dataset--read them for further data inspiration:\n",
      "executive summary\n",
      "political divisions in 2016 and beyond\n",
      "race, religion, and immigration in 2016\n",
      "the story of trump's appeal\n",
      "the five types of trump voters\n",
      "methodology\n",
      "read the latest news and updates from (and about) the democracy fund voter study group.\n",
      "context\n",
      "the 2016 statewide general election results for arizona.\n",
      "arizona's 15 counties are required by statute to publish tabulated general election results by precinct. this file represents a standardized and aggregated version of all 15 files. please note that while the file is mostly standardized, many of the attributes are relatable accross counties via a fuzzy match (the keyword \"congress\" etc...).\n",
      "content\n",
      "county: abbreviation of arizona's 15 counties\n",
      "ap: apache\n",
      "ch: cochise\n",
      "cn: coconino\n",
      "gi: gila\n",
      "gm: graham\n",
      "gn: greenlee\n",
      "lp: la paz\n",
      "mc: maricopa\n",
      "mo: mohave\n",
      "na: navajo\n",
      "pm: pima\n",
      "pn: pinal\n",
      "sc: santa cruz\n",
      "ya: yavapai\n",
      "yu: yuma\n",
      "precinctid: precinct identification number designated by the counties. county shorthand has been added.\n",
      "precinctname: precinct name designated by the counties. this is directly passed from the tabulation files.\n",
      "contestid: contest identification number designated by the counties. this is directly passed from the tabulation files and may not be standardized across counties.\n",
      "contesttitle: title of race as designated by counties. this is directly passed from the tabulation files and may not be standardized across counties.\n",
      "candidateid: candidate identification number designated by the counties. this is directly passed form the tabulation files and may not be standardized across counties.\n",
      "candidatename: name of candidate as desingated by the counties. this is directly passed form the tabulation files and may not be standardized across counties.\n",
      "totalvotes: vote total aggregated from the attributes \"pollvotes, earlyvotes, provisionals, latepoll, lateearly\".\n",
      "pollvotes: total votes tabulated at a designated precinct location on election day.\n",
      "earlyvotes: total votes tabulated by the counties during the 29 day early voting period.\n",
      "provisionals: total votes tabulated at a designated precinct location that were deemed acceptable provisional ballots. 12: lateearly: total votes tabulated by the counties of early votes that were dropped off at designated polling locations rather than received in the mail. (note: only a few counties separated this number from earlyvote in their tabulation files).\n",
      "registered: the number of registered voters at the time of the election in each designated precinct.\n",
      "undervote: the number of ballots that did note cast the allowed number of votes for any given race. (example: voters are allowed to \"vote for 2\" in the arizona house of representatives race, in this case these ballots were either left blank or only voted for 1)\n",
      "contesttotal: total votes cast in a given contest.\n",
      "candidateparty: party of candidate in a given contest.\n",
      "rep: republican\n",
      "dem: democrat\n",
      "np: no party\n",
      "grn: green\n",
      "lbt: libertarian\n",
      "totalturnout: total turnout for a designated precinct.\n",
      "edturnout: total turnout for a designated precinct on election day.\n",
      "earlyturnout: total turnout for a designated precinct during the 29 day early voting period. (note, this number will include early ballots dropped off at the designated polling location.)\n",
      "final note: there are certain records in the file that are not part of any contest. they are normally designated by a contest id that begins with a \"999\" these are records that the tabulators append to every file to provide background on each of the designated precincts.\n",
      "context\n",
      "this dataset is meant to complement the austin bikesharing dataset.\n",
      "content\n",
      "contains the:\n",
      "date (yyyy-mm-dd)\n",
      "temphighf (high temperature, in fahrenheit)\n",
      "tempavgf (average temperature, in fahrenheit)\n",
      "templowf (low temperature, in fahrenheit)\n",
      "dewpointhighf (high dew point, in fahrenheit)\n",
      "dewpointavgf (average dew point, in fahrenheit)\n",
      "dewpointlowf (low dew point, in fahrenheit)\n",
      "humidityhighpercent (high humidity, as a percentage)\n",
      "humidityavgpercent (average humidity, as a percentage)\n",
      "humiditylowpercent (low humidity, as a percentage)\n",
      "sealevelpressurehighinches (high sea level pressure, in inches)\n",
      "sealevelpressureavginches (average sea level pressure, in inches)\n",
      "sealevelpressurelowinches (low sea level pressure, in inches)\n",
      "visibilityhighmiles (high visibility, in miles)\n",
      "visibilityavgmiles (average visibility, in miles)\n",
      "visibilitylowmiles (low visibility, in miles)\n",
      "windhighmph (high wind speed, in miles per hour)\n",
      "windavgmph (average wind speed, in miles per hour)\n",
      "windgustmph (highest wind speed gust, in miles per hour)\n",
      "precipitationsuminches (total precipitation, in inches) ('t' if trace)\n",
      "events (adverse weather events. ' ' if none)\n",
      "this dataset contains data for every date from 2013-12-21 to 2017-07-31.\n",
      "acknowledgements\n",
      "this dataset was obtained from weatherunderground.com, at the austin katt station.\n",
      "inspiration\n",
      "can we use this dataset to explain some of the variation in the austin bikesharing dataset?\n",
      "south african stock price predictions\n",
      "welcome to sa stock market data :)\n",
      "the dataset contains information for the largest 34 companies in south africa by market cap as well as data for the sa40 futures.\n",
      "the price data (p.csv) is in the format: close, open, high, low, vol, change\n",
      "the financials data (f.csv) is in the format: revenue, cost of sales, gross profit, net profit, issue of shares, share repurchase, non-current assets, current assets,\n",
      "non-current liabilities, current liabilities, net cash inflow/outflow from operating activities\n",
      "interest rate data (r.csv) has a single column: rates\n",
      "symbol reference:\n",
      "btij = british american tobacco plc\n",
      "bilj = bhp billiton plc\n",
      "bgaj = barclays africa group ltd\n",
      "cfrj = compagnie financiere richemont sa drc\n",
      "ccoj = capital & counties properties plc\n",
      "aglj = anglo american plc\n",
      "mtnj = mtn group ltd\n",
      "npnjn = naspers ltd\n",
      "solj = sasol ltd\n",
      "sbkj = standard bank group ltd\n",
      "vodj = vodacom group ltd\n",
      "kioj = kumba iron ore ltd\n",
      "fsrj = firstrand ltd\n",
      "omlj = old mutual plc\n",
      "slmj = sanlam ltd\n",
      "shpj = shoprite holdings ltd\n",
      "remj = remgro ltd\n",
      "nedj = nedbank group ltd\n",
      "apnj = aspen pharmacare holdings ltd\n",
      "bvtj = the bidvest group ltd\n",
      "angj = anglogold ashanti ltd\n",
      "impj = impala platinum holdings ltd\n",
      "whlj = woolworths holdings ltd\n",
      "tbsj = tiger brands ltd\n",
      "exxj = exxaro resources ltd\n",
      "rmhj = rmb holdings ltd\n",
      "ituj = intu properties plc\n",
      "grtj = growthpoint properties ltd\n",
      "mndj = mondi ltd\n",
      "snhj = steinhoff international holdings ltd\n",
      "inpj = investec plc\n",
      "lhcj = life healthcare\n",
      "reij = reinet\n",
      "dsyj = discovery holdings ltd\n",
      "iplj = imperial holdings ltd\n",
      "arij = african rainbow minerals ltd\n",
      "sa = ftse/jse top 40 futures\n",
      "if you are interested in how i put everything together to build a single model you can look at the following github link, please note this contains a lot of copy-pasta and needs to be simplified with some loops and functions before i put it on kaggle: https://github.com/nlabbert/sa-stock-market\n",
      "this is the shot log for nba in 16-17 regular season, grouped by teams. inside the dataset includes the shot type, shot distance, shot angle, shot player, shot time, team name, etc. this data is scraped down from https://www.mysportsfeeds.com/\n",
      "i came across these two great articles which discusses about the shooting rationality of individual nba players: https://www.kaggle.com/selfishgene/kobe-bryant-shot-selection/psychology-of-a-professional-athlete https://www.kaggle.com/drgilermo/irrational-shot-selection the basic idea is that if an nba player makes one shot, the next shot he takes tends to be more difficult and further away from the basket. so i was wondering, will this be the case for a team? if a team makes one shot, will they take a further and more difficult shot for the next one? individuals tend to be less rational after making one shot, but how about a team as a group? will the decision making from a group compensate for the irrationality of individuals?\n",
      "i wrote a blog about my analysis here: https://haowang204.wordpress.com/2017/06/03/shooting-rationality-of-nba-teams/\n",
      "the python code is uploaded to: https://github.com/wh0801/nba-shooting-rationality-2016-17-regular-season let me know your thoughts!\n",
      "context\n",
      "we are working on a project relating to predicting and voting for academy award. with the primetime emmy awards coming up this week, i thought it would be interesting to see if i could integrated those. i couldn't find too many well organized datasets relating to those awards. i decided to spend the afternoon and build my own. we probably won't use this information this year, but it might be something we could use in the future.\n",
      "content\n",
      "i created a simple web parser in go, and parsed the data from the emmy awards website. the data is a representation of primetime emmy nominees from the first emmy awards (1949)... to the current ones that will air sunday september 17th, 2017. after this date, the winner will have to be updated. in work we've done with academy awards, we used movie title and name as the main structure for the data. i kind of felt this was a little inconsistent as certain awards focus on one or the other. with the emmy nominees, i made it more general with nominee and additional detail, i believe this will make the data more consistent and easier to manipulate.\n",
      "acknowledgements\n",
      "i based the structure of the data from the kaggle dataset of the academy awards . i would also like to acknowledge the academy of television arts & sciences for providing the data on their website.\n",
      "inspiration\n",
      "who won the most emmys for outstanding comedy series? i think it would be cool, if we could answer: who will win the emmy for outstanding comedy series in 2018? but, i think we more than just historical data.\n",
      "context\n",
      "*this dataset shows the migration to and from new zealand by country and citizenship from 1979 to 2016. *\n",
      "content\n",
      "the columns in this dataset are:\n",
      "measure: the signal type given in this row, one of: \"arrivals\", \"departures\", \"net\"\n",
      "country: country from where people arrived into to new zealand (for measure = \"arrivals\") or to where they left (for measure = \"departures\"). contains special values \"not stated\" and \"all countries\" (grand total)\n",
      "citizenship: citizenship of the migrants, one of: \"new zealand citizen\", \"australian citizen\", \"total all citizenships\"\n",
      "year: year of the measurement\n",
      "value: number of migrants\n",
      "permanent and long-term arrivals include overseas migrants who arrive in new zealand intending to stay for a period of 12 months or more (or permanently), plus new zealand residents returning after an absence of 12 months or more. permanent and long-term departures include new zealand residents departing for an intended period of 12 months or more (or permanently), plus overseas visitors departing new zealand after a stay of 12 months or more. for arrival series, the country of residence is the country where a person arriving in new zealand last lived for 12 months or more (country of last permanent residence). for departure series, the country of residence is the country where a person departing new zealand intends to live for the next 12 months or more (country of next permanent residence).\n",
      "acknowledgements\n",
      "curated data by figure.nz, original data from stats nz. dataset licensed under creative commons 4.0 - cc by 4.0.\n",
      "inspiration\n",
      "a good challenge would be to explain new zealand migration flows as a function of the economic performance of new zealand or other countries (combine with other datasets). the data could be possibly linked up with other data sources to predict general migration to/from countries based on external factors.\n",
      "context\n",
      "some countries have a very divergent gdp per capita between its regions. sometimes a given country's regions tend to converge over time, while in other cases the disparity between the poorer and the richer regions is kept over the decades. in this dataset we can examine the spanish case. which has been the evolution of the nominal gdp per capita by regions in spain since the year 2000 ? have the regions converged ? which is the spread between regions ? can we make a cluster analysis of the regions ?\n",
      "content\n",
      "we have a dataframe of the evolution of the nominal gdp per capita across the 19 spanish regions (autonomous communities & cities) since 2000 to 2016.\n",
      "** acknowledgements **\n",
      "the data used has been compiled by the ine (spanish institute of statistics). http://www.ine.es/\n",
      "context:\n",
      "paranormal romance is a subgenre of romance that combines fantasy and romance elements. notable examples are the twilight series and the southern vampire mysteries, which the t.v. show true blood was based on.\n",
      "content:\n",
      "this is a list of 4000 paranormal romance novel titles, scraped from the web by mark riedl. some longer titles have been truncated, and end with ellipses (...).\n",
      "inspiration:\n",
      "can you generate new titles using a markov chain text generator?\n",
      "some novel titles are truncated. can you generate the complete version?\n",
      "can you cluster novels into sub genres based on their titles?\n",
      "context:\n",
      "portuguese is a romance language that is the native language of over 215 million speakers worldwide. like spanish, english and french, it was the language of both its country of origin and also that country’s colonial possessions. this corpus contains examples of historical portuguese written between 1500 and 1936, both in portugal and brazil.\n",
      "content:\n",
      "the corpus contains complete portuguese manuscripts published from 1500 to 1936 divided into 5 sub-corpora per century (summarized in the table below). the part of speech (pos) of words in this corpus was tagged using treetagger. you can find more information on this corpus on the colonia homepage.\n",
      "century texts tokens 16th 13 399,245 17th 18 709,646 18th 14 425,624 19th 38 2,490,771 20th 17 1,132,696 total 100 5,157,982\n",
      "texts are balanced in terms of the variety, consisting of 48 european portuguese texts and 52 brazilian portuguese texts. you can find more information in the paper that describes the corpus. the complete inventory of texts is here and more detail regarding annotation can be found here.\n",
      "part of speech (pos) tags\n",
      "the works in this corpus have been automatically tagged for their part of speech (pos). the tagset used to annotate the corpus is presented in the table below. it contains not only the classic pos tags (e.g. v, det, n) but also a couple of compound tags, such as the combination of preposition plus determiner as (prep+det) or verb plus pronoun (v+p). the tool used to annotate the corpus was treetagger.\n",
      "category pos example adjective adj bonita adverb adv muita determiner det os cardinal card primeiro noun nom mesa pronoun p eles preposition prp de verb v fazer interjection i oh! commas virg , punctuation sent .\n",
      "studies report that treetagger achieves performance higher than 95% accuracy in attributing the correct pos tag and lemma of a token.\n",
      "acknowledgements:\n",
      "if you use this corpus in your work, please cite this paper:\n",
      "zampieri, m. and becker, m. (2013) colonia: corpus of historical portuguese. in: zsm studien, special volume on non-standard data sources in corpus-based research. volume 5. shaker.\n",
      "inspiration:\n",
      "what changes have occurred in portuguese over time? have words changed? syntactic structures? how grammatical agreement is expressed?\n",
      "can you create a classifier which can classify the era and unseen work is from?\n",
      "using the part of speech tags in this tagger, can you train a new tagger and run it over the brazilian portuguese literature corpus linked below?\n",
      "you may also like:\n",
      "a 3.7 million word literary corpus of brazilian portugese\n",
      "context:\n",
      "languages tend to be used differently in different places. one of the ways that language use varies is that different words are used in different areas. for example, in philadelphia you might hear someone refer to something they’ve forgotten the name for as a “jawn”, and in maine and the northeast, hedgehogs are sometimes called “quill-pigs”. this dataset contains information on many regionally-specific words of american english and where they are used..\n",
      "content:\n",
      "dareds is a dataset of words found in american english, created from the dictionary of american regional english (dare). dare collects information on dialect regions, which terms are used in them and the meaning of those terms. it is based on dialectal surveys from different rege in order to construct a dataset based on dare, the web version of dare was downloaded. it was then cleaned, removing both multiword expressions and very common words. dialect regions that didn’t correspond to a single state or set of cities (e.g. south) were mapped to the most populous cities within each region. for example, within the pacific northwest dialect region, the most populous cities (seattle, tacoma, portland, salem, eugene) were added to this dataset as subregions. the resulting dataset (dareds) consists of around 4.3k dialect terms from 99 u.s. dialect regions.\n",
      "acknowledgements:\n",
      "this dataset was compiled by afshin rahimi, trevor cohn and timothy baldwin. if you use this dataset, please cite both their paper and the dictionary of american regional english.\n",
      "rahimi, afshin, cohn, trevor and baldwin, timothy. \"a neural model for user geolocation and lexical dialectology.\" acl 2017 (2017): 209.\n",
      "cassidy, f. g. (1985). dictionary of american regional english. belknap press of harvard university press.\n",
      "inspiration:\n",
      "this dataset was originally composed to help validate the geographic origin of twitter data (if someone uses a word that’s only used in a small geographic area, then it’s more likely that they are from there). but there are lots of other interesting questions you can ask with this data!\n",
      "how many of these words can you find in twitter datasets like this one of celebrity tweets? how accurately can you guess the location of these celebrities given this dataset?\n",
      "can you train word vectors based on these words? do words tend to cluster by regional origin?\n",
      "many of the words in this dataset have become less popular over time. can you tell which ones are still popular, perhaps by using google’s n-gram viewer?\n",
      "context\n",
      "need to maintain a database of atp tour data for forecasting and predictions, etc.\n",
      "content\n",
      "retrieved data from online, but compiled file from 2012 to 2017 through 07/20/2017. note that there are errors in some 2012 dates, but i revised the initial dataset as most recent version. also, the w_odds and l_odds columns are averages of odds taken from several online gaming sites including betonline365 and others.\n",
      "most recent data is usually updated weeks prior to all major grand slam events.\n",
      "acknowledgements\n",
      "compiled all useful data from the following link: http://tennis-data.co.uk/alldata.php\n",
      "inspiration\n",
      "building dataset for match prediction\n",
      "context:\n",
      "the current employment statistics (ces) program provides estimates of employment, hours, and earnings information on a national basis and in considerable industry detail. the bureau of labor statistics collects payroll data each month from a sample of business and government establishments in all nonfarm activities.\n",
      "employment data include series for total employment, number of women employees, and number of production or nonsupervisory employees. estimates of average hourly earnings, average weekly hours, average weekly earnings, and average weekly overtime hours are produced for both all employees and for production or nonsupervisory employees. overtime hours are produced for manufacturing industries only.\n",
      "a sample of approximately 147,000 businesses and government agencies representing approximately 634,000 worksites throughout the united states is utilized for this monthly survey. the sample contains about 300,000 employer units.\n",
      "all employment, hours and earnings series are classified according to the 2012 north american industry classification system (naics). the industry code used in the survey corresponds to the naics code, except in those cases where multiple industries have been combined.\n",
      "content:\n",
      "please refer to ce.txt for a description of how to parse and use the unique identifiers.\n",
      "this dataset was collected on june 27th, 2017 and may not be up-to-date.\n",
      "summary of data available: for all employees, women employees, and production or nonsupervisory employees, ces publishes about 4,300 monthly series. the series for all employees cover more than 900 industries on both a seasonally adjusted and not seasonally adjusted basis.\n",
      "for private-sector industries, nearly 7,500 series are published each month for average weekly earnings, average hourly earnings, average weekly hours, and, in manufacturing, average weekly overtime hours. hours and earnings data for all employees are available for about 620 industries and for production or nonsupervisory employees about 550 industries.\n",
      "from the employment, hours, and earnings series, ces produces about 7,500 derivative series, such as indexes and real earnings series.\n",
      "most employment series begin in 1990, although some series, including industry supersectors, are available from 1939. supersectors include: mining and logging; construction; manufacturing; trade, transportation, and utilities; information; financial activities; professional and business services, education and health services; leisure and hospitality, other services, and government.\n",
      "frequency of observations: data series are monthly in most cases; quarterly averages are available for total employment, average weekly hours, and average overtime hours, seasonally adjusted (datatypes 19, 20, 25, 36, and 37).\n",
      "annual averages are available for all series that are not adjusted for seasonality (except for the 12-month diffusion index series).\n",
      "data characteristics: earnings are measured in dollars and are published to the nearest cent (two decimal places). average weekly and overtime hours are measured in hours and are published to the nearest tenth of an hour (one decimal place).\n",
      "employment is measured in thousands of workers and is stored with no decimal place for all supersectors and for both durable goods and nondurable goods in manufacturing. employment for all other industries are stored to one decimal place.\n",
      "special characteristics of the data are footnoted where necessary. for example; i indicates that the seasonally adjusted series is independently seasonally adjusted and not used in aggregating to higher summary industries. for all employees, higher summary series, such as total nonfarm, are aggregated up from the 3-digit naics level.\n",
      "each year with the release of january estimates in february, ces data are re-anchored to universe counts of nonfarm employment or benchmarks for the most recent march. for example, ces introduced march 2016 benchmark counts with the release of january 2017 first preliminary estimates in february 2017. on a not seasonally adjusted basis, all series are subject to revision back to the prior year’s benchmarked data (21 months of data), while seasonally adjusted estimates may be revised back 5 years (or more).\n",
      "references: bls handbook of methods, chapter 2, \"employment, hours, and earnings from\n",
      "the establishment survey\", https://www.bls.gov/opub/hom/pdf/homch2.pdf\n",
      "acknowledgements:\n",
      "this dataset was taken directly from the u.s. bureau of labor statistics website at http://www.bls.gov/data/ and converted to csv format. originally, there were 55 data files; one for each major sector of industry. those files have been combined into one one file. an additional column has been added to record the original file name from which each row came from.\n",
      "inspiration:\n",
      "the bureau of labor statistics has done a great job of providing this source of information for the public to explore. with this dataset you can explore which industries make the most money or or work the most. if you’re feeling frisky, you might stretch those data science skills and combine this dataset with the bls - consumer price index dataset to find out which cities pay the most for snacks per hours worked!\n",
      "context:\n",
      "article five of the united states constitution describes a process that allows for alteration of the federal constitution. so far, 27 amendments have been fully added to the federal constitution but there have been a lot of proposals that didn’t make it through. this dataset contains information about a whopping 11,797 constitutional amendments that have been proposed to congress from 1787 to 2014!\n",
      "content:\n",
      "this dataset consists of 11,797 rows with 17 fields and was compiled by nara volunteers that transcribed information from written records that were issued by congress. because a lot of the information comes from written records, the dataset may not be a complete record of all amendment proposals. proposals before 1973 were taken from various government publications and proposals after 1973 are publicly available on https://www.congress.gov.\n",
      "identifier: unique code generated by nara that serves as a unique identifier.\n",
      "source_code: code generated by nara that represents the government publication or website where the information was transcribed.\n",
      "source_citation: bibliographic citation of the government publication or website from which the information was transcribed.\n",
      "source_index_number: an index number listed within the source publication.\n",
      "title_or_description_from_source: title or description of the proposed amendment, transcribed from the source publication or website.\n",
      "date_approximation: when the date is estimated, the term includes “circa”, otherwise it is left blank.\n",
      "year: year that the amendment was proposed in congress (yyyy format).\n",
      "month: month that the amendment was proposed in congress (blank if unknown).\n",
      "day: day that the amendment was proposed in congress (blank if unknown).\n",
      "congress: the number for the congress in which the amendment was proposed.\n",
      "congressional_session: the number or designation of the session of congress in which the amendment was proposed.\n",
      "joint_resolution_chamber: the chamber of congress in which the amendment was proposed.\n",
      "joint_resolution_number: the number assigned to the proposed amendment by congress. (amendments are submitted as joint resolutions)\n",
      "sponsor_name: the name of the member of congress or group who proposed the amendment. (blank if unknown).\n",
      "sponsor_state_or_territory: the u.s. state or territory represented by the amendment’s sponsoring member of congress.\n",
      "committee_of_referral: the committee to which the amendment was referred for further consideration, following formal introduction to congress.\n",
      "last_modified: the timestamp of the most recent modification made on the data contained within the particular row.\n",
      "acknowledgements:\n",
      "the national archives and records administration created this dataset as part of the amending america initiative. to prepare for the 2016 \"amending america\" exhibition at the national archives museum in washington, d.c., nara volunteers and staff transcribed and edited over 11,000 entries representing proposed amendments to the u.s. constitution, as recorded by congress. http://www.archives.gov/amending-america\n",
      "inspiration:\n",
      "this is an interesting dataset because it contains a lot of history that isn’t necessarily reflected in the constitution. you could use it to glean insight into the political climates at different times in the history of the united states. for instance, what kind of amendments were being proposed during the civil war? who proposed the most amendments? historically, have there been more proposals during any particular time of year?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes over 2gb of stop data from california, covering all of 2013 onwards. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes stop data from ms, mt, nd, nh, nj, nv, or, ri, sd, tn, va, v, wi, and wy. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "properati is a competitive marketplace for latin american real estate that strives to assist consumers in purchasing and renting homes, while simultaneously providing location-specific property databases for the public.\n",
      "properati currently lists over 1.5 million properties through hubs in argentina, mexico, and brazil. listings include property name, type, price, listing date, surface area, coordinates, description, and photos. most of these premises are located in urban settings, with few straying into suburban or rural areas. innovators at properati understand that a house is usually the most important and costly purchase an individual makes in his or her lifetime. properati has developed tools, such as preciómetro, to help people make well-educated property investments.\n",
      "properati also creates data analyses based on over 1.5 million properties to better understand the real estate market. through these reports, properati uncovers social trends and urban processes that help characterize current and future listings. visit properati’s blog and website to join them in their advanced market knowledge.\n",
      "properati invites you to utilize their location based datasets from current and past listings in big query https://bigquery.cloud.google.com/dataset/properati-data-public:properties_ar https://bigquery.cloud.google.com/dataset/properati-data-public:properties_br https://bigquery.cloud.google.com/dataset/properati-data-public:properties_mx\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one datafile for each state in the u.s. northeast region.\n",
      "states included in this dataset:\n",
      "connecticut - ct.csv\n",
      "massachusetts - ma.csv\n",
      "maine - me.csv\n",
      "new hampshite - nh.csv\n",
      "new jersey - nj.csv\n",
      "new york - ny.csv\n",
      "pennsylvania - pa.csv\n",
      "rhode island - ri.csv\n",
      "vermont - vt.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets for crime or weather\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one datafile for each state in the u.s. south region (although some are arguably not in the south).\n",
      "states included in this dataset:\n",
      "alabama - al.csv\n",
      "arkansas - ar.csv\n",
      "washington d.c. - dc.csv\n",
      "delaware - de.csv\n",
      "florida - fl.csv\n",
      "georgia - ga.csv\n",
      "kentucky - ky.csv\n",
      "louisiana - la.csv\n",
      "maryland - md.csv\n",
      "mississippi - ms.csv\n",
      "north carolina - nc.csv\n",
      "oklahoma - ok.csv\n",
      "south carolina - sc.csv\n",
      "tennessee - tn.csv\n",
      "texas - tx.csv\n",
      "virginia - va.csv\n",
      "west virginia - wv.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets for crime or weather\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one datafile for each state in the u.s. midwest region (although some are arguably not in the midwest).\n",
      "states included in this dataset:\n",
      "iowa - ia.csv\n",
      "illinois - il.csv\n",
      "indiana - in.csv\n",
      "kansas - ks.csv\n",
      "michigan - mi.csv\n",
      "minnesota - mn.csv\n",
      "missouri - mo.csv\n",
      "north dakota - nd.csv\n",
      "nebraska - ne.csv\n",
      "ohio - oh.csv\n",
      "south dakota - sd.csv\n",
      "wisconsin -wi.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets for housing prices, crime, or weather!\n",
      "context\n",
      "more than 10 million people worldwide are living with parkinson's disease. improving machine learning model which identifies parkinson's disease will lead to helping patients with early dialogs and reduction of treatment cost.\n",
      "content\n",
      "handwriting database consists of 62 pwp(people with parkinson) and 15 healthy individuals. the data was collected in 2009.\n",
      "number of instances: 77, number of attributes: 7\n",
      "acknowledgements\n",
      "source: https://archive.ics.uci.edu/ml/datasets/parkinson+disease+spiral+drawings+using+digitized+graphics+tablet\n",
      "citation:\n",
      "1.isenkul, m.e.; sakar, b.e.; kursun, o. . 'improved spiral test using digitized graphics tablet for monitoring parkinson's disease.' the 2nd international conference on e-health and telemedicine (icehtm-2014), pp. 171-175, 2014.\n",
      "2.erdogdu sakar, b., isenkul, m., sakar, c.o., sertbas, a., gurgen, f., delil, s., apaydin, h., kursun, o., 'collection and analysis of a parkinson speech dataset with multiple types of sound recordings', ieee journal of biomedical and health informatics, vol. 17(4), pp. 828-834, 2013.\n",
      "individual informal consumer complaint data detailing complaints filed with the consumer help center beginning october 31, 2014. this data represents information selected by the consumer. the fcc does not verify the facts alleged in these complaints.\n",
      "this dataset contains everything you need to analyze the jerk companies who call during dinner: time, mode of communication, logged phone number (of the hassler), and what they were trying to do.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the fcc. you can find the original dataset here.\n",
      "the united state federal highway administration (fhwa) collects and updates information on the nation's bridges that are located on public roads, including both interstate and us highways, state and county roads, and publicly accessible bridges on federal land. this collection of information is known as the national bridge inventory (nbi), and it has been captured electronically since 1972. while parts of the data were first made available to the public in 1997, it wasn't until 2007 that the fwha decided to make all elements of the nbi database publicly available.\n",
      "this nbi data set contains 135 variables describing over 600,000 bridges. variables describe the location, structure, maintenance, usage, status, and other aspects of the bridges. an extremely detailed (124 page) pdf guide to the variables, codes, and other metadata on the nbi can be found here:\n",
      "https://www.fhwa.dot.gov/bridge/mtguide.pdf\n",
      "acknowledgements\n",
      "the department of transportation fhwa collects and provides the nbi data as authorized by statue 23, u.s.c. 151\n",
      "this data set was downloaded from the homeland infrastructure foundation here:\n",
      "https://hifld-dhs-gii.opendata.arcgis.com/datasets/94c41e96db0d4b85b9eb622923e0a0e8_0\n",
      "inspiration\n",
      "this data set contains variables describing the cost of bridge (item94) and roadway (item95) improvement in thousands of dollars. how many improvement projects could be completed for $20b?\n",
      "use of the nbi data also enables fhwa to satisfy its requirements under 23 u.s.c. 144, which mandate the inventory, classification, cost estimates for replacement or rehabilitation, and assignment of replacement or rehabilitation priorities for all highway bridges on all public roads. can you come up with better cost estimates and classifications? for example, in the absence of additional information, fhwa recommends using 10% of the bridge cost as a roadway improvement cost estimator.\n",
      "using latitude and longitude data and operational status (item41), can you find any bridges to nowhere?\n",
      "background\n",
      "on april 18, 2017, prime minister theresa may announced that she was seeking a general election to be held on june 8. the day after, the mps voted to dissolve parliament and a new election is confirmed. it would be interesting to do some analysis of past election results on the 650 constituencies in to find out:\n",
      "the most vulnerable seats from each party.\n",
      "the seats targeted by each party.\n",
      "can the brexit referendum result affect the outcome of this election in a particular region of the uk?\n",
      "how a swing of x% from party a to party b affect the seat distribution?\n",
      "among many other relevant questions.\n",
      "dataset contents\n",
      "this dataset consists of a mixture between two tables that were scrapped from wikipedia.\n",
      "1) 2015 general election results by constituency: https://en.wikipedia.org/wiki/results_of_the_united_kingdom_general_election,_2015_by_parliamentary_constituency\n",
      "2) 2016 referendum result by constituency: https://en.wikipedia.org/wiki/results_of_the_united_kingdom_european_union_membership_referendum,_2016#list_of_constituency_results this table used results estimated by chris hanretty in [1].\n",
      "the tables were joined and cleaned. some imputation errors were found and corrected. a couple of columns was created (or removed). the following set of columns was obtained:\n",
      "constituency: name of constituency\n",
      "region: region where the constituency is located\n",
      "con: conservative party votes\n",
      "lab: labour party votes\n",
      "ukip: ukip votes\n",
      "ld: liberal democrat votes\n",
      "snp: scottish national party votes\n",
      "grn: green party votes\n",
      "dup: democratic unionist party votes\n",
      "pc: plaid cymru votes\n",
      "sf: sinn fein votes\n",
      "uup: ulster unionist party votes\n",
      "sdlp: social democratic labour party votes\n",
      "alliance: alliance party votes\n",
      "ind: independent votes\n",
      "spk: speaker votes\n",
      "others: other parties votes\n",
      "validvotes: sum of votes received by all parties\n",
      "winningparty: party that holds the seat\n",
      "secondplace: party that finished in 2nd place\n",
      "winningvotes: number of votes received by the winning party\n",
      "secondplacevotes: number of votes received by the 2nd placed party\n",
      "winningpct: percentage of votes received by winner\n",
      "majority: winningvotes - secondplacevotes\n",
      "majoritypct: majority as a percentage of validvotes\n",
      "turnoutpct2015: turnout in the last general election\n",
      "remainpct: percentage of remain votes in 2016 referendum\n",
      "leavepct: percentage of leave votes in 2016 referendum\n",
      "leavemajority: leavepct - remainpct\n",
      "reference\n",
      "[1] https://medium.com/@chrishanretty/final-estimates-of-the-leave-vote-or-areal-interpolation-and-the-uks-referendum-on-eu-membership-5490b6cab878\n",
      "disclaimer\n",
      "cover picture is from chensiyuan and made availabe under a creative commons attribution-share alike 4.0 international, 3.0 unported, 2.5 generic, 2.0 generic and 1.0 generic license. https://commons.wikimedia.org/wiki/file:1_westminster_palace_panorama_2012_dusk.jpg\n",
      "context\n",
      "this is a ecg/ekg dataset from the data for development and evaluation of ecg-based apnea detectors\n",
      "content\n",
      "it contains 70 electrocardiography records:\n",
      "training set\n",
      "a01.dat to a20.dat\n",
      "b01.dat to b05.dat\n",
      "c01.data to c10.dat\n",
      "test set\n",
      "x01.dat to x25.dat\n",
      "the .dat files contain the digitized ecgs (16 bits per sample, least significant byte first in each pair, 100 samples per second, nominally 200 a/d units per millivolt).\n",
      "for each a.dat file in the training set there is .apn file that is annotation (by human experts) stating for each minute of recording if there was apnea or not\n",
      "acknowledgements\n",
      "this database is described in t penzel, gb moody, rg mark, al goldberger, jh peter. the apnea-ecg database. computers in cardiology 2000;27:255-258.\n",
      "goldberger al, amaral lan, glass l, hausdorff jm, ivanov pch, mark rg, mietus je, moody gb, peng c-k, stanley he. physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation 101(23):e215-e220 [circulation electronic pages; http://circ.ahajournals.org/content/101/23/e215.full]; 2000 (june 13).\n",
      "links\n",
      "https://physionet.org/physiobank/database/apnea-ecg/\n",
      "https://physionet.org/faq.shtml\n",
      "license\n",
      "odc public domain dedication and licence (pddl) https://opendatacommons.org/licenses/pddl/1.0/ according to physionet faq https://physionet.org/faq.shtml the redistribution of the data is allowed\n",
      "cars data has information about 3 brands/make of cars. namely us, japan, europe. target of the data set to find the brand of a car using the parameters such as horsepower, cubic inches, make year, etc.\n",
      "a decision tree can be used create a predictive data model to predict the car brand.\n",
      "context\n",
      "this dataset is conducive for various types of audio-video analysis.(just want to mention one thing here,the files in the data for video analysis i am not uploading courtesy size issues but for those interested can download here)\n",
      "content\n",
      "the vidtimit dataset is comprised of video and corresponding audio recordings of 35 people(though the original data contains the data for 43 people but some links were missing), reciting short sentences. it can be useful for research on topics such as automatic lip reading, multi-view face recognition, multi-modal speech recognition and person identification. the dataset was recorded in 3 sessions, with a mean delay of 7 days between session 1 and 2, and 6 days between session 2 and 3. the sentences were chosen from the test section of the timit corpus. there are 10 sentences per person. the first six sentences (sorted alpha-numerically by filename) are assigned to session 1. the next two sentences are assigned to session 2 with the remaining two to session 3.\n",
      "the first two sentences for all persons are the same, with the remaining eight generally different for each person.\n",
      "the corresponding audio is stored as a mono, 16 bit, 32 khz wav file.\n",
      "acknowledgements\n",
      "the vidtimit dataset is copyright © 2001 conrad sanderson. for more details refer here\n",
      "inspiration\n",
      "there are many reasons for uploading the data as fetching audio-video data free of cost (and even with cost) is relatively hard and this data can be used to build models like speaker_recognition,person verification and much more.\n",
      "hi, i have extracted the tweets related to oscar 2017.\n",
      "the timeframe is from feb 27th,2017 to march 2nd,2017.\n",
      "the number oftweets is 29498.\n",
      "the whole idea of extraction to know how people reacted in general about oscars and also after the best picture mix up.\n",
      "context\n",
      "i pulled data from the github repo for linux. https://github.com/torvalds/linux. this is looking at the source code committed by the public for the linux operating system.\n",
      "content\n",
      "the content contains 6 columns: - date: the date of the code commit - commits: the total number of commits on that day by that user - additions: the total additions made to the code. essentially added characters - deletions: the total deletions made to the code. - userid: the unique user who pushed the commit. - startofweek: i don't completely know.\n",
      "acknowledgements\n",
      "github and linux\n",
      "overwatch is a team-based multiplayer first-person shooter video game developed and published by blizzard entertainment.\n",
      "overwatch puts players into two teams of six, with each player selecting one of several pre-defined hero characters with unique movement, attributes, and abilities; these heroes are divided into four classes: offense, defense, tank and support. players on a team work together to secure and defend control points on a map and/or escort a payload across the map in a limited amount of time. players gain cosmetic rewards that do not affect gameplay, such as character skins and victory poses, as they continue to play in matches. the game was launched with casual play, while blizzard added competitive ranked play about a month after launch. additionally, blizzard has developed and added new characters, maps, and game modes post-release, while stating that all overwatch updates will remain free, with the only additional cost to players being microtransactions to earn additional cosmetic rewards. ( wikipedia - https://en.wikipedia.org/wiki/overwatch_(video_game) )\n",
      "league of legends matchid dataset v2.0\n",
      "as people who like data analysis , but young enough to still like gaming, we thought that league of legends would be a great game to analyze. due to competitive play some statistics and predictions were quite welcome. there are of course a lot of websites that offer that by them selves, but we think that league community needed an open dataset to work with, as there was none that offered some real volume of data. there came the idea for a bigger dataset which would offer other people to drive their projects without the struggle of long lasting process of parsing matches with riot api (which has a limit of 500 calls per 10 minutes...so yea)\n",
      "this is not the finished project, but more like a post along the way. the dataset only consists of one column and its basically useless by it self. the file consists of 223 715 match ids of ranked games . each column represents the matchid of a single match played in league, which can be than accessed with riot api the purpose is only to allow others like us, to continue the research with riot api with some pre gathered data and save them some precious time that way.\n",
      "the final dataset \"league of legends matchesdataset v1.0\" we will be posting, consists of 100 000 matches in json which will be directly suitable for data analysis.\n",
      "link to the dataset: wip\n",
      "we are also open sourcing the data gathering program (written in python)\n",
      "github link: github program\n",
      "this project has been posted by me (lan vukušič) as data scientist but the main credit goes to lead programmer matej urbas who is responsible for the data gathering in this project and without whom the project would not exist.\n",
      "we are happy to give the dataset out for free, to let the comunity use that dataset. we would love to see what people are going to create. we know that we are \"rookies\" in that field but would still like to contribute to evergrowing field of data science. so if there is really anything that should be changed in upcoming updates please feel free to message us and tell us your thoughts.\n",
      "contacts : leaguedataset@gmail.com\n",
      "best regards\n",
      "league of legends matchid dataset v1.0 and league of legends matchid dataset v2.0 aren't endorsed by riot games and doesn't reflect the views or opinions of riot games or anyone officially involved in producing or managing league of legends. league of legends and riot games are trademarks or registered trademarks of riot games, inc. league of legends © riot games, inc.\n",
      "context\n",
      "this data contains employee's terminated and hired data.\n",
      "content\n",
      "file contains 4 sheets- terminated, active, complete data and sheet4 with appendix each sheet contains country, gender hired on date, terminated date, action and grades\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "i thought it might be neat to do some simple analytics on the bachelor/the bachelorette contestant data.\n",
      "content\n",
      "contestant info from seasons 1, 2, 5, 9-21 of abc's the bachelor and 1-12 of the bachelorette -- very incomplete.\n",
      "acknowledgements\n",
      "i just compiled this from wikipedia. some data on the bachelor contestants comes from reddit user u/nicolee314.\n",
      "inspiration\n",
      "are there any predictors for success on reality dating shows? has the typical contestant changed over the years? are certain qualities under/over-represented in these contestants?\n",
      "upon reviewing the train data for the sberbank russian housing market competition, i noticed noise & errors. obviously, neither of these should be present in your training set, and as such, you should remove them. this is the updated train set with all noise & errors i found removed.\n",
      "data was removed when:\n",
      "full_sq-life_sq<0 full_sq-kitch_sq<0 life_sq-kitch_sq<0 floor-max_floor<0\n",
      "i simply deleted the row from the dataset, and did not really use anything special other than that.\n",
      "context\n",
      "for democracy to function, we need transparency. part of the transparency is given to us through government salaries and names of employees. since 1996, the white house has been required by congress to disclose a list of staff and their salaries.\n",
      "content\n",
      "the obama_staff_salaries covers salaries under the obama administration from 2009-2016 (by julianna langston). the white_house_2017_salaries was released by the white house as a 16-page pdf, detailing the salaries of trump administration's employees. this is a csv scraped from the pdf using tabula (by carl v. lewis).\n",
      "acknowledgements\n",
      "i would like to thank @julilangston and @carblewis at data.world for providing these datasets.\n",
      "inspiration\n",
      "how do wh staff salaries compare to trump staff salaries?\n",
      "how much did each administration spend on immigration advisor/assistant staff?\n",
      "who has more executive assistants?\n",
      "how did salaries under the obama's administration change from 2009-2016?\n",
      "about this data\n",
      "this is a list of 19,439 restaurants and similar businesses with menu items containing \"burrito\" or \"taco\" in their names as provided by datafiniti's business database.\n",
      "the dataset includes the category, cuisine, restaurant information, and more for a menu item. note that each row corresponds to a single menu item from the restaurant, and the entirety of each restaurant's menu is not listed. only burrito or taco items are listed.\n",
      "what you can do with this data\n",
      "you can use this data to discover which parts of the country offer the most for mexican food aficionados. e.g.:\n",
      "what is the ratio of burritos and tacos on restaurant menus from each city?\n",
      "what is the ratio of burritos and tacos on restaurant menus from cities with the most restaurants per capita (10,000 residents)?\n",
      "what is the ratio of cities with the most authentic mexican restaurants per capita (10,000 residents)?\n",
      "which cities have the most authentic mexican restaurants?\n",
      "which cities have the most mexican restaurants?\n",
      "which mexican restaurants have the most locations nationally?\n",
      "data schema\n",
      "a full schema for the data is available in our support documentation.\n",
      "about datafiniti\n",
      "datafiniti provides instant access to web data. we compile data from thousands of websites to create standardized databases of business, product, and property information. learn more.\n",
      "want more?\n",
      "you can get more data like this by joining datafiniti or requesting a demo.\n",
      "cdc began collecting childhood blood lead surveillance data in april 1995. the national surveillance system is composed of data from state and local health departments.\n",
      "states maintain their own child-specific databases so they can identify duplicate test results or sequential test results on individual children. these databases contain follow-up data on children with elevated blood lead levels including data on medical treatment, environmental investigations, and potential sources of lead exposure. states extract fields from their child-specific surveillance databases and transfer them to cdc for the national database.\n",
      "state child-specific databases contain follow-up data on children with elevated blood lead levels including data on medical treatment, environmental investigations, and potential sources of lead exposure. surveillance fields for cdc's national database are extracted from state child-specific databases and transferred to cdc.\n",
      "state surveillance systems are based on reports of blood lead tests from laboratories. ideally, laboratories report results of all blood lead tests, not just elevated values, to state health departments. states determine the reporting level for blood lead tests and decide which data elements should accompany the blood lead test result.\n",
      "these data were collected for program management purposes. the data have limitations, and we cannot compare across states or counties because data collection methods vary across grantees. data are not generalizable at the national, state, or local level.\n",
      "content\n",
      "the public libraries survey (pls) is conducted annually by the institute of museum and library services under the mandate in the museum and library services act of 2010. the data file includes all public libraries identified by state library administrative agencies in the 50 states and the district of columbia. the reporting unit for the survey is the administrative entity, defined as the agency that is legally established under local or state law to provide public library service to the population of a local jurisdiction. the fy 2014 pls collected state characteristics data, including the state total population estimate, number of central and branch libraries, and the total library visits and circulation transactions, and data on each public library, such as its name and location, population of legal service area, print and digital collections, full-time-equivalent staff, and operating revenue and expenditures.\n",
      "acknowledgements\n",
      "the u.s. census bureau is the data collection agent for imls public libraries survey.\n",
      "content\n",
      "the museum dataset is an evolving list of museums and related organizations in the united states. the data file includes basic information about each organization (name, address, phone, website, and revenue) plus the museum type or discipline. the discipline type is based on the national taxonomy of exempt entities, which the national center for charitable statistics and irs use to classify nonprofit organizations.\n",
      "non-museum organizations may be included. for example, a non-museum organization may be included in the data file because it has a museum-like name on its irs record for tax-exempt organizations. museum foundations may also be included.\n",
      "museums may be missing. for example, local municipal museums may be undercounted because original data sources used to create the compilation did not include them.\n",
      "museums may be listed multiple times. for example, one museum may be listed as both itself and its parent organization because it was listed differently in each original data sources. duplicate records are especially common for museums located within universities.\n",
      "information about museums may be outdated. the original scan and compilation of data sources occurred in 2014. scans are no longer being done to update the data sources or add new data sources to the compilation. information about museums may have changed since it was originally included in the file.\n",
      "acknowledgements\n",
      "the museum data was compiled from imls administrative records for discretionary grant recipients, irs records for tax-exempt organizations, and private foundation grant recipients.\n",
      "inspiration\n",
      "which city or state has the most museums per capita? how many zoos or aquariums exist in the united states? what museum or related organization had the highest revenue last year? how does the composition of museum types differ across the country?\n",
      "content\n",
      "this dataset includes presidential candidates endorsed by the top 100 american newspapers by circulation for every election since 1980.\n",
      "acknowledgements\n",
      "the historical candidate endorsements were compiled and documented by github user veltman, and the presidential election results were provided by the federal election commission.\n",
      "content\n",
      "the 2016 edition of freedom of the press, which provides analytical reports and numerical scores for 199 countries and territories, continues a process conducted by freedom house since 1980. each country and territory is given a total press freedom score from 0 (best) to 100 (worst) on the basis of 23 methodology questions divided into three subcategories. the total score determines the status designation of free, partly free, or not free. the scores and reports included in freedom of the press 2016 cover events that took place between january 1, 2015, and december 31, 2015.\n",
      "the level of press freedom in each country and territory is evaluated through 23 methodology questions divided into three broad categories: the legal environment, the political environment, and the economic environment. for each methodology question, a lower number of points is allotted for a more free situation, while a higher number of points is allotted for a less free environment. a country or territory’s final score (from 0 to 100) represents the total of the points allotted for each question. a total score of 0 to 30 results in a press freedom status of free; 31 to 60 results in a status of partly free; and 61 to 100 indicates a status of not free.\n",
      "the legal environment category encompasses an examination of both the laws and regulations that could influence media content, and the extent to which they are used in practice to enable or restrict the media’s ability to operate. we assess the positive impact of legal and constitutional guarantees for freedom of expression; the potentially negative aspects of security legislation, the penal code, and other statutes; penalties for libel and defamation; the existence of and ability to use freedom of information legislation; the independence of the judiciary and official regulatory bodies; registration requirements for both media outlets and journalists; and the ability of journalists’ organizations to operate freely.\n",
      "under the political environment category, we evaluate the degree of political influence in the content of news media. issues examined include the editorial independence of both state-owned and privately owned outlets; access to information and sources; official censorship and self-censorship; the vibrancy of the media and the diversity of news available within each country or territory; the ability of both foreign and local reporters to cover the news in person without obstacles or harassment; and reprisals against journalists or bloggers by the state or other actors, including arbitrary detention, violent assaults, and other forms of intimidation.\n",
      "our third category examines the economic environment for the media. this includes the structure of media ownership; transparency and concentration of ownership; the costs of establishing media as well as any impediments to news production and distribution; the selective withholding of advertising or subsidies by the state or other actors; the impact of corruption and bribery on content; and the extent to which the economic situation in a country or territory affects the development and sustainability of the media.\n",
      "the dataset contains information about the number of bicycles that used certain bicycle lanes in montreal in the year 2015.\n",
      "why?\n",
      "inspired by an interest in scraping and organizing data released by the federal government, this is an easy-to-navigate csv of all of the documents released by the obama white house, now found on obamawhitehouse.archives.gov.\n",
      "this includes the full content of each of the documents released (wherever possible), along with link references to each document.\n",
      "further, each of them is broken down by document type as well.\n",
      "context\n",
      "i thought kaggle could use more datasets for natural language processing projects, so what better way to provide some data than to use some of the most popular books of all time!\n",
      "content\n",
      "each file contains the full book from project gutenberg.\n",
      "acknowledgements\n",
      "i really want to thank the people who have created project gutenberg for making such a wide selection of books available! it will be great to see what projects we can create from their work!\n",
      "inspiration\n",
      "use one or more of these books for a text generation project.\n",
      "build an interesting visual using word vectors.\n",
      "compare the vocabulary used between authors.\n",
      "context\n",
      "this is a dataset for a larger project i have been working on. my idea is to analyze and compare real historical weather with weather folklore.\n",
      "content\n",
      "the csv file includes a hourly/daily summary for szeged, hungary area, between 2006 and 2016.\n",
      "data available in the hourly response:\n",
      "time\n",
      "summary\n",
      "preciptype\n",
      "temperature\n",
      "apparenttemperature\n",
      "humidity\n",
      "windspeed\n",
      "windbearing\n",
      "visibility\n",
      "loudcover\n",
      "pressure\n",
      "acknowledgements\n",
      "many thanks to darksky.net team for their awesome api.\n",
      "context\n",
      "linio.com is latin america’s leading ecommerce platform. it have recently released the 2016-17 technology price index comparing the cost of 14 popular electronic devices and brands, across 72 countries. linio conducted a pretty impressive research study for better understanding the global economic trends in the price of most popular electronic devices. from http://www.outsourcingportal.eu/en/linio-com-compares-the-cost-of-technology-products-in-72-countries: to conduct the research linio looked at the costs of all products in the study from several brick and mortar chain stores and smaller retailers in all major cities in each country. the study also took into account average costs from at least three reputable online outlets in each country. taxes and other associated purchasing costs, minus delivery, were also accounted for.\n",
      "content\n",
      "this dataset contains the cost of 14 different devices including smartphones, laptops, game consoles, tablets, smart devices, and other gadgets, across 72 different countries. ranking the countries on the average cost of all products researched. the dataset was downloaded from linio.com in december 2016.\n",
      "acknowledgements\n",
      "the dataset was taken from here: https://www.linio.com.mx/sp/technology-price-index-2016 without any modifications.\n",
      "inspiration\n",
      "is it possible to predict the cost of my favorite iphone based on 13 other devices?\n",
      "what device has the most unpredictable trend?\n",
      "what products have the highest variability in price and whether it was caused by the same countries?\n",
      "this dataset stems from alberto barradas' popular pokemon with stats dataset by listing the tiered pokemon in smogon 6v6.\n",
      "smogon 6v6 is one of the most popular formats for competitive pokemon. although it is not the \"official\" competitive format, there is still a significant number of people who play the format. there are a number of 'tiers' of pokemon in which people can play, the most popular being ou. this dataset seeks to display both a pokemon's stats and corresponding tier for easier competitive analysis.\n",
      "in addition to the addition of the 'tier' variable, there are several other changes i made to the set:\n",
      "classified mythical pokemon as 'legendary'\n",
      "changed the naming convention of mega evolutions and some form changes\n",
      "addition of 'mega' tier to signify mega evolutions\n",
      "note that this dataset includes only pokemon tiered from pu to ag. nfe and lc pokemon are not included unless they appear in smogon's list. list of which pokemon are in which tier was found here.\n",
      "thank you to alberto barradas for his comprehensive pokemon dataset.\n",
      "this listing includes four datasets which, when combined, thoroughly describe the existing airfields in the state of alaska. the challenge for this dataset, for those who care to attempt it, is to create an interactive graphical representation of this information that provides as much of this data on demand as possible.\n",
      "this dataset was pulled from the faa.gov and is current as of 30 march 2017.\n",
      "no research has yet been done on this dataset. creating a usable graphical representation of this data (i.e. a map that shows all airfields and provides detailed information when each airfield is selected) would prove a very useful planning tool for emergency response planning in the state of alaska. the intent of posting this dataset is to seek feedback and analysis along those lines.\n",
      "above all it is important that any code used to transform this dataset be reusable, since the faa regularly updates their information on these airfields. an ideal solution would allow these datasets to be fed in one end and spit out a beautiful, intuitive, user-friendly product at the other end.\n",
      "all the data files are provided in .csv format. the dictionary that contains the definitions for the various fields is provided in excel because it contains multiple spreadsheets (one to describe each .csv file). it is recommended that you download the excel file so you can refer to it in excel while you work on the .csv files.\n",
      "this data set contains google stock pricing from the year 2004 up to 2017 which includes opening,closing, high ,low and adjusted stock prices.\n",
      "the data set contains following columns: date open high low close volume ex-dividend split ratio adj. open\n",
      "adj. high\n",
      "adj. low adj. close\n",
      "adj. volume\n",
      "context\n",
      "the type allocation code (tac) is the initial eight-digit portion of the 15-digit imei and 16-digit imeisv codes used to uniquely identify wireless devices. (wikipedia)\n",
      "tac numbers can be used to identify devices connected to networks. complete tac databases are hard to find and cost a furtune, as maintaining them and keeping them up to date is labour intensive due to the large amount of devices being released every day.\n",
      "content\n",
      "the dataset contains information about the devices' tac number, manufacturer, model, aliases of the model, operating system, year of release and lte compatibility.\n",
      "some devices may have multiple tac numbers, as they are different subversions of the same hardware. this tac database is nowhere near complete, and some of the data provided here could be incorrect, as even manufacturers sometimes share inaccurate information about their own devices. lte capability is the worst offender in this case, as companies often release tac information with contradictory stats about lte compatibility.\n",
      "acknowledgements\n",
      "this is a merged and cleaned dataset based on free tac databases found on the internet, including:\n",
      "http://tacdb.osmocom.org/\n",
      "https://www.mulliner.org/tacdb/feed/\n",
      "inspiration\n",
      "this database is useful for anyone who works with telecommunication networks and wants to identify their users.\n",
      "content\n",
      "this is the combined raw crime data for 2012 through 2016. please note that it is missing a few weeks in both december 2015 and december 2016 (winter break).\n",
      "from aug 29, 2014 - mar 23, 2016\n",
      "content\n",
      "indicator: 36 distinct\n",
      "country: 12 distinct\n",
      "date: 259 distinct\n",
      "value: > 1,000\n",
      "acknowledgements\n",
      "original source https://data.humdata.org/dataset/ebola-cases-2014\n",
      "context\n",
      "game title\n",
      "game release date\n",
      "game current price\n",
      "problem(s)\n",
      "in the price column, if a game is on sale it shows both prices for example:\n",
      "$39.99$30.99\n",
      "possible kernels\n",
      "games per year\n",
      "average price per game per year\n",
      "most occurring names in games (wordcloud)\n",
      "date -> price\n",
      "highest price games, oldest, etc\n",
      "enjoy!\n",
      "context\n",
      "i've processed the data, freely available from the bulgarian ministry of interior affairs, and decided to provide it to see what interesting visualizations you might come up with. it would also be interesting to see a forecast of the same crime types for futures years based on the available data.\n",
      "content\n",
      "the dataset contains information about various types of criminal activity. it shows the percentage of resolved crimes between 2000 and 2014 in all 28 regions of bulgaria.\n",
      "acknowledgements\n",
      "data is taken from the reports found on the homepage of the bulgarian ministry of interior affairs.\n",
      "crowdflower was used in a recent study published in plos one on how narrative style affects the way scientific findings are cited and shared. refer to the article’s supplementary materials for more information.\n",
      "the dataset contains abstracts from peer-reviewed studies on climate change that were labeled using the crowdflower platform. abstracts were each assessed by multiple raters (n = 7) for their narrativity. narrativity includes whether the abstract appeals to the reader, has a narrative perspective, using sensory language, and other factors. the dataset also contains additional information about the studies including citation rate, journal identity, and number of authors.\n",
      "past research\n",
      "from the study abstract:\n",
      "peer-reviewed publications focusing on climate change are growing exponentially with the consequence that the uptake and influence of individual papers varies greatly. here, we derive metrics of narrativity from psychology and literary theory, and use these metrics to test the hypothesis that more narrative climate change writing is more likely to be influential, using citation frequency as a proxy for influence. from a sample of 732 scientific abstracts drawn from the climate change literature, we find that articles with more narrative abstracts are cited more often. this effect is closely associated with journal identity: higher-impact journals tend to feature more narrative articles, and these articles tend to be cited more often. these results suggest that writing in a more narrative style increases the uptake and influence of articles in climate literature, and perhaps in scientific literature more broadly.\n",
      "inspiration\n",
      "can you replicate the authors' findings? are abstracts with narrative qualities cited more often? what other variables are associated with narrativity in scientific abstracts about climate change? examine relationships between citation rate, abstract length, abstract authors, and more.\n",
      "acknowledgements\n",
      "this dataset is made available via crowdflower's \"data for everyone\" collection which hosts open data jobs that have come through the crowdsourced labeling platform.\n",
      "context\n",
      "this data set includes the office of the assessor-recorder’s secured property tax roll spanning from 2007 to 2015 (~1.6m). it includes all legally disclosable information, including location of property, value of property, the unique property identifier, and specific property characteristics. the data is used to accurately and fairly appraise all taxable property in the city and county of san francisco. the office of the assessor-recorder makes no representation or warranty that the information provided is accurate and/or has no errors or omissions.\n",
      "potential question(s) to get started with!\n",
      "can the effects of prop 13 been seen in the historic property tax rolls?\n",
      "fields\n",
      "there are 48 fields in this dataset.\n",
      "a full data dictionary can be found here.\n",
      "we have included the following commonly used geographic shapefiles:\n",
      "analysis neighborhoods\n",
      "supervisor districts as of april 2012\n",
      "acknowledgements\n",
      "data provided by the san francisco office of the assessor-recorder via the san francisco open data portal at https://data.sfgov.org/d/wv5m-vpq2 pddl 1.0 odc public domain dedication and licence (pddl)\n",
      "photo from flickr via rebecca morgan (cc by-nc-sa 2.0)\n",
      "context:\n",
      "the sf health department has developed an inspection report and scoring system. after conducting an inspection of the facility, the health inspector calculates a score based on the violations observed. violations can fall into:high risk category: records specific violations that directly relate to the transmission of food borne illnesses, the adulteration of food products and the contamination of food-contact surfaces.moderate risk category: records specific violations that are of a moderate risk to the public health and safety.low risk category: records violations that are low risk or have no immediate risk to the public health and safety.the score card that will be issued by the inspector is maintained at the food establishment and is available to the public in this dataset.\n",
      "potential question(s) to get started with!\n",
      "what are some predictors of health scores? what relevant outside data can you bring to bear on the question, including restaurant reviews, sentiment analysis, demographic data, etc?\n",
      "fields:\n",
      "san francisco's lives restaurant inspection data leverages the lives flattened schema (https://goo.gl/c3nnvr), which is based on lives version 2.0, cited on yelp's website (http://www.yelp.com/healthscores).\n",
      "please refer to https://goo.gl/c3nnvr for detailed data dictionary.\n",
      "further info on the food safety program can be found here.\n",
      "we have included the following commonly used geographic shapefiles:\n",
      "analysis neighborhoods\n",
      "supervisor districts as of april 2012\n",
      "acknowledgements:\n",
      "data provided by the san francisco health department via the san francisco open data portal at https://data.sfgov.org/d/pyih-qa8i license: pddl 1.0 odc public domain dedication and licence (pddl)\n",
      "photo via flickr rob hyndman attribution-noncommercial-sharealike 2.0 generic (cc by-nc-sa 2.0)\n",
      "this comma-separated text file contains the 27,723 alien bird records that form the core of the global avian invasions atlas (gavia) project. these records represent 971 species, introduced to 230 countries and administrative areas across all eight biogeographical realms, spanning the period 6000 bce – ad 2014. the data comprises taxonomic (species-level), spatial (geographic location, realm, land type) and temporal (dates of introduction and spread) components, as well as details relating to the introduction event (how and why the species was introduced, whether or not it is established). each line of data consists of an individual record concerning a specific alien bird species introduced to a specific location. the data derives from both published and unpublished sources, including atlases, country species lists, peer-reviewed articles, websites and via correspondence with in-country experts.\n",
      "acknowledgements\n",
      "dyer, ellie; redding, david; blackburn, tim (2016): data from: the global avian invasions atlas - a database of alien bird distributions worldwide. figshare.\n",
      "context\n",
      "this dataset takes the environment agency's risk of flooding from rivers and sea, and places english postcodes in their appropriate flood risk area, allowing you to look up flood risk from postcode.\n",
      "content\n",
      "risk of flooding from rivers and sea consists of geographical areas within england which are at risk of flooding from rivers and sea. each area is assigned a flood risk within a banding:\n",
      "high\n",
      "medium\n",
      "low\n",
      "very low\n",
      "none\n",
      "open flood risk by postcode takes postcodes as point locations (from open postcode geo) and places the postcode in the appropriate flood risk area. it is important to note that actual properties within a specific postcode may have a slightly different point location and therefore be in a different flood risk area. generally speaking the point location of a postcode is the point location of the central property in that postcode.\n",
      "for a full field list and explanations of values see the open flood risk by postcode documentation.\n",
      "acknowledgements\n",
      "open flood risk by postcode is derived from two open datasets:\n",
      "risk of flooding from rivers and sea\n",
      "open postcode geo\n",
      "both of these datasets are licensed under the ogl.\n",
      "the following attribution statements are required:\n",
      "contains os data © crown copyright and database right 2017\n",
      "contains royal mail data © royal mail copyright and database right 2017\n",
      "contains national statistics data © crown copyright and database right 2017\n",
      "contains environment agency data licensed under the open government licence v3.0.\n",
      "the dataset is maintained by getthedata.\n",
      "the latest version and full documentation is available here.\n",
      "inspiration\n",
      "example application:\n",
      "lookup or drill down to individual english postcodes to see a map of that postcode and its flood risk, alongside surrounding postcodes and their flood risks:\n",
      "application: flood map by postcode\n",
      "example postcode: rg9 2lp\n",
      "context\n",
      "since 1967, the freedom of information act (foia) has provided the public the right to request access to records from any federal agency. it is often described as the law that keeps citizens in the know about their government. federal agencies are required to disclose any information requested under the foia unless it falls under one of nine exemptions which protect interests such as personal privacy, national security, and law enforcement. as congress, the president, and the supreme court have all recognized, the act is a vital part of our democracy.\n",
      "a foia request can be made for any agency record. you can also specify the format in which you wish to receive the records (for example, printed or electronic form). the act does not require agencies to create new records or to conduct research, analyze data, or answer questions when responding to requests. each federal agency handles its own records in response to requests. there are currently one hundred agencies subject to the foia with several hundred offices that process foia requests.\n",
      "content\n",
      "annual freedom of information act reports are submitted to congress and published by foia.gov to promote agency accountability for the administration of the act.\n",
      "context\n",
      "the dog sled race covers 1000 miles of the roughest, most beautiful terrain mother nature has to offer from anchorage, in south central alaska, to nome on the western bering sea coast. she throws jagged mountain ranges, frozen river, dense forest, desolate tundra and miles of windswept coast at the mushers and their dog teams. add to that temperatures far below zero, winds that can cause a complete loss of visibility, the hazards of overflow, long hours of darkness and treacherous climbs and side hills, and you have the iditarod — the last great race on earth!\n",
      "the race route is alternated every other year, one year going north through cripple, ruby and galena and the next year south through iditarod, shageluk, anvik.\n",
      "content\n",
      "this dataset provides a record for mushers and dog teams at all seventeen checkpoints on the iditarod trail, including the musher's name and bib number, status in the race, country of origin, checkpoint name and location, distance in miles from the last checkpoint, time in hours from departure at the last checkpoint, average speed in miles per hour, date and time of arrival and departure, layover time at the checkpoint, and the number of dogs at arrival and departure.\n",
      "acknowledgements\n",
      "the data on mushers, checkpoints, and race standings was scraped from the iditarod website.\n",
      "inspiration\n",
      "which competitor had the highest average speed during the race? did race veterans outpace rookies on the trail? how did their strategies differ? did the competitors' speed slow with less dogs on their team?\n",
      "context\n",
      "kabaddi is a contact sport that originated in ancient india. more information\n",
      "the standard style kabaddi world cup, is an indoor international kabaddi competition conducted by the international kabaddi federation (ikf),contested by men's and women's national teams. the competition has been previously contested in 2004, 2007 and 2016. all the tournaments have been won by india. more information\n",
      "the 2016 kabaddi world cup, the third standard-style kabaddi world cup, was an international kabaddi tournament governed by the international kabaddi federation, contested from 7 to 22 october 2016 in ahmedabad, india. twelve countries had competed in the tournament. more information\n",
      "30 league matches played between teams. teams were deivided in 2 pools with 6 team in each pool. top 2 teams from each team were qualifid for semifinals and winner of semifianls played in finals.\n",
      "this dataset contains data for all 33 matches at granualirity level of attack, defense, allout and extra points. data set also includes toss results, super tackle count and all out count along with match results.\n",
      "content\n",
      "this dataset was manually prepared from taking necessary statistics from kabaddi world cup site. points acquired as per rules are main statistics.\n",
      "this dataset contains necessary statistics in today format and details of all variables are as per following.\n",
      "gameno : match number. sequential {integer}\n",
      "team : team name {factor}\n",
      "oppteam : opposition team name {factor}\n",
      "matchstage : tournament stage at which match was played. (0 - league, 1 - semifinal, 2 - final ) {factor}\n",
      "tossresult : results of toss to select either side or raid (0 - loss, 1 - win) {factor}\n",
      "alloutrec : no. of time team was all out yielding 2 point {integer}\n",
      "alloutgiv : no. of time opposition team was all out yielding 2 point {integer}\n",
      "stacklerec : no. of times super tackle by team yielding 2 point {integer}\n",
      "stacklegiv : no. of times super tackle by opposition team yielding 2 point {integer}\n",
      "touchpntsrec : no. of times player in raid touched opposition team player yiedling 1 point for every touch {integer}\n",
      "touchpntsgiv : no. of times opposition player in raid touched team player yiedling 1 point for every touch {integer}\n",
      "bonuspntsrec : no. of times player in raid crossed bonus line yiedling 1 point for every raid {integer}\n",
      "bonuspntsgiv : no. of times opposition player in raid crossed bonus line yiedling 1 point for every raid {integer}\n",
      "raidpntsrec : no. of total raid (attack) points by team, sum of touch points and bonus points {integer}\n",
      "raidpntsgiv : no. of total raid (attack) points by opposition team, sum of touch points and bonus points {integer}\n",
      "tacklepntsrec : no. of tackle (defense) points received by team yielding 1 point for normal tackle and 2 points for super tackle {integer}\n",
      "tacklepntsgiv : no. of tackle (defense) points received by opposition team yielding 1 point for normal tackle and 2 points for super tackle {integer}\n",
      "alloutpntsrec : no. of all out points received by team yielding 2 points per allout {integer}\n",
      "alloutpntsgiv : no. of all out points received by opposition team yielding 2 points per allout {integer}\n",
      "extrapntsrec : no. of extra (technical, penalty) points received by team {integer}\n",
      "extrapntsgiv : no. of extra (technical, penalty) points received by opposition team {integer}\n",
      "totalpntsrec : no. of total points received by team, sum of raid points, tackle points, allout points & extra points {integer}\n",
      "totalpntsgiv : no. of total points received by opposition team, sum of raid points, tackle points, allout points & extra points {integer}\n",
      "touchpntsdiff : no. of touch points difference from opposition team {integer}\n",
      "bonuspntsdiff : no. of bonus points difference from opposition team {integer}\n",
      "raidpntsdiff : no. of raid points difference from opposition team {integer}\n",
      "tacklepntsdiff : no. of tackle points difference from opposition team {integer}\n",
      "alloutpntsdiff : no. of allout points difference from opposition team {integer}\n",
      "extrapntsdiff : no. of extra points difference from opposition team {integer}\n",
      "totalpntsdiff : no. of total points difference from opposition team {integer}\n",
      "matchresults : results of the match (0 - loss, 1 - win) {factor}\n",
      "acknowledgements\n",
      "i would like to thank kabaddi world cup site for providing this data.\n",
      "inspiration\n",
      "this dataset was prepared for my research paper which aims to answer following questions\n",
      "is attack is better than defence?\n",
      "does bonus point lead to victory?\n",
      "what is the role of all out points on determining strength of attack?\n",
      "can we build predictive model for winning?\n",
      "how strong establish teams are compared to new teams?\n",
      "citation request: this dataset is public available for research. the details are described in [cortez et al., 2009]. please include this citation if you plan to use this database:\n",
      "p. cortez, a. cerdeira, f. almeida, t. matos and j. reis. modeling wine preferences by data mining from physicochemical properties. in decision support systems, elsevier, 47(4):547-553. issn: 0167-9236.\n",
      "available at: [@elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016 [pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib\n",
      "title: wine quality\n",
      "sources created by: paulo cortez (univ. minho), antonio cerdeira, fernando almeida, telmo matos and jose reis (cvrvv) @ 2009\n",
      "past usage:\n",
      "p. cortez, a. cerdeira, f. almeida, t. matos and j. reis. modeling wine preferences by data mining from physicochemical properties. in decision support systems, elsevier, 47(4):547-553. issn: 0167-9236.\n",
      "in the above reference, two datasets were created, using red and white wine samples. the inputs include objective tests (e.g. ph values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). each expert graded the wine quality between 0 (very bad) and 10 (very excellent). several data mining methods were applied to model these datasets under a regression approach. the support vector machine model achieved the best results. several metrics were computed: mad, confusion matrix for a fixed error tolerance (t), etc. also, we plot the relative importances of the input variables (as measured by a sensitivity analysis procedure).\n",
      "relevant information:\n",
      "the two datasets are related to red and white variants of the portuguese \"vinho verde\" wine. for more details, consult: http://www.vinhoverde.pt/en/ or the reference [cortez et al., 2009]. due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
      "these datasets can be viewed as classification or regression tasks. the classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). outlier detection algorithms could be used to detect the few excellent or poor wines. also, we are not sure if all input variables are relevant. so it could be interesting to test feature selection methods.\n",
      "number of instances: red wine - 1599; white wine - 4898.\n",
      "number of attributes: 11 + output attribute\n",
      "note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.\n",
      "attribute information:\n",
      "for more information, read [cortez et al., 2009].\n",
      "input variables (based on physicochemical tests): 1 - fixed acidity (tartaric acid - g / dm^3) 2 - volatile acidity (acetic acid - g / dm^3) 3 - citric acid (g / dm^3) 4 - residual sugar (g / dm^3) 5 - chlorides (sodium chloride - g / dm^3 6 - free sulfur dioxide (mg / dm^3) 7 - total sulfur dioxide (mg / dm^3) 8 - density (g / cm^3) 9 - ph 10 - sulphates (potassium sulphate - g / dm3) 11 - alcohol (% by volume) output variable (based on sensory data): 12 - quality (score between 0 and 10)\n",
      "missing attribute values: none\n",
      "description of attributes:\n",
      "1 - fixed acidity: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n",
      "2 - volatile acidity: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n",
      "3 - citric acid: found in small quantities, citric acid can add 'freshness' and flavor to wines\n",
      "4 - residual sugar: the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n",
      "5 - chlorides: the amount of salt in the wine\n",
      "6 - free sulfur dioxide: the free form of so2 exists in equilibrium between molecular so2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n",
      "7 - total sulfur dioxide: amount of free and bound forms of s02; in low concentrations, so2 is mostly undetectable in wine, but at free so2 concentrations over 50 ppm, so2 becomes evident in the nose and taste of wine\n",
      "8 - density: the density of water is close to that of water depending on the percent alcohol and sugar content\n",
      "9 - ph: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the ph scale\n",
      "10 - sulphates: a wine additive which can contribute to sulfur dioxide gas (s02) levels, wich acts as an antimicrobial and antioxidant\n",
      "11 - alcohol: the percent alcohol content of the wine\n",
      "output variable (based on sensory data): 12 - quality (score between 0 and 10)\n",
      "an individual’s annual income results from various factors. intuitively, it is influenced by the individual’s education level, age, gender, occupation, and etc.\n",
      "this is a widely cited knn dataset. i encountered it during my course, and i wish to share it here because it is a good starter example for data pre-processing and machine learning practices.\n",
      "fields the dataset contains 16 columns target filed: income -- the income is divide into two classes: <=50k and >50k\n",
      "number of attributes: 14 -- these are the demographics and other features to describe a person\n",
      "we can explore the possibility in predicting income level based on the individual’s personal information.\n",
      "acknowledgements this dataset named “adult” is found in the uci machine learning repository http://www.cs.toronto.edu/~delve/data/adult/desc.html\n",
      "the detailed description on the dataset can be found in the original uci documentation http://www.cs.toronto.edu/~delve/data/adult/adultdetail.html\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "i have used a web scrapper written in r to scrape the data from pib.nic.in website.\n",
      "acknowledgements\n",
      "this dataset was scrapped from press information bureau, government of india's website. banner photo by patrick tomasso on unsplash.\n",
      "the dataset contains two images taken at two wavelength (blue and red) of human metaphasic chromosomes (dapi stained) hybridized with a cy3 labelled telomeric probe. the two images can be combined into a color image. the previous dataset of overlapping chromosomes was generated chromosomes belonging to this metaphase.\n",
      "from http://www-bcf.usc.edu/~gareth/isl/data.html for the purpose of conducting the labs\n",
      "missing persons india\n",
      "taken from the pdf available at national crime records bureau. since the original was a pdf this is a table extracted from the original pdf using scripts.\n",
      "some level of noise is present in the data partly due to the original source and partly due to the extraction scripts.\n",
      "the goal of this dataset is to detect a certain type of pattern (called “spindle”) inside electroencephalogram (eeg) waves.\n",
      "data come from garches hospital sleep studies lab (france, paris area)\n",
      "they are extracted from a single eeg, mostly from c3-a1 and c4-a1 electrodes\n",
      "we have used 2 kinds of inputs to read these waves :\n",
      "the raw difference in potential measured by each captor\n",
      "the frequencies and power spectral densities issued from genecycle package periodogram function applied on the result of fast fourier transformation of differences in potential:\n",
      "raw difference in potential -> fft -> periodogram -> frequencies and power spectral densities the use of frequencies and densities gives much better results.\n",
      "we know that the spindles must occur in the middle of a certain type of waves (theta waves), so we take as a training set a mixture of spindles and theta waves, and associate the outcome value 1 (as factor) for spindles and value 0 for theta waves. the 1st part of the algorithm already identified the “spindle candidates” from windows of 1 second in the eeg raw waves and sent these candidates to the “authentication” program, which is the one described. so i used r neural networks (nnet) and kernel support vector machine (ksvm) for the authentication.\n",
      "here is the program with neural net.\n",
      "in version 2, i added raw eeg data of 30 mn of sleep (extrait_wsleeppage01.csv) and expected patterns (spindles.csv) manually extracted. if one of you guys has a method to more directly identify spindles from the sleep page, you are welcome !\n",
      "context\n",
      "do you know what is common among kung fu panda, alvin and the chipmunks, monster trucks, trolls, spongebob movie and monster vs aliens? they all were scripted by the same authors - jonathan aibel and glenn berger.\n",
      "kung fu panda is a 2008 animated movie by dreamworks production. it has made $631 million and its one of the most successful film on the box office from dreamworks.\n",
      "there is much talk and discussions on this movie beyond cinema-goers. some like to learn leadership lessons from it and few others try to link it with christianity, taoism, mysticism and islam.\n",
      "i was wondering if we can see the script from data science perspective and can answer some of the questions with significant implications in movie and other industries.\n",
      "i welcome you all to do data science martial arts with kung-fu-panda and see who survives\n",
      "content\n",
      "it’s a complete script of kung fu panda 1 and 2 in csv format with all background narrations, scene settings and movie dialogues by characters (po, master shufy, tai lung, tigress, monkey, viper, oogway, mr. ping, mantis and crane).\n",
      "acknowledgements\n",
      "kung fu panda is a production by dreamworks studios. all scripts were gathered from online public sources like this and this.\n",
      "inspiration\n",
      "some ideas worth exploring:\n",
      "• can we train the neural network to recognize the character by dialogue? for example, if i give any line from the script, your algorithm will be able to tell who’s more likely to say this in movie?\n",
      "• can we make the word cloud for each character (and perhaps compare it with other movie characters by same authors and see who is similar to who)\n",
      "• can we train a chat bot for oogway to po so kids can talk to it and it would respond the same way as oogway or po would\n",
      "• can we calculate the average length or dialogue\n",
      "• can we estimate the difficulty level of vocabulary being used and perhaps compare it with movies of other genre\n",
      "• can we compare the script with some religious text and find out similarities\n",
      "9 fatigue experiments done, on aluminium specimens. for every specimen, two sets (each 25 photos) of images in x200 and x400 were taken. the resolution of the photos is 1024x1280 pixels. the ground-truth database was built manually, using a gui (graphical user interface) that was specially created. the database contains 840 sem images, each has a correlated binary 'mask' image. the masks are binary images, at the same size of the sem photos, which composed of ones and zeros, where all the pixels that containing striations are 1, and the rest are 0. the images are saved as row vectors, that stacks one upon the other in .csv files.\n",
      "歐洲足球資料庫 背景:歐洲足球 內容:歐洲足球分析\n",
      "this data set gives you all funds given to development countries from norway in the time period 2010-2015. the dataset includes oecd markers.\n",
      "some inspiration:\n",
      "predict aid based on on oecd markers.\n",
      "predict implementation partner based on the available features.\n",
      "traffic violations followed the invention of the automobile: the first traffic ticket in the united states was allegedly given to a new york city cab driver on may 20, 1899, for going at the breakneck speed of 12 miles per hour. since that time, countless citations have been issued for traffic violations across the country, and states have reaped untold billions of dollars of revenue from violators.\n",
      "traffic violations are generally divided into major and minor types of violations. the most minor type are parking violations, which are not counted against a driving record, though a person can be arrested for unpaid violations.\n",
      "the most common type of traffic violation is a speed limit violation. speed limits are defined by state.\n",
      "context\n",
      "this dataset contains results of general elections to the lower house of the state legislatures in the united states over the last fifty years, up to 2012. this dataset was created by the princeton gerrymandering project as part of their effort to analyze and combat partisan gerrymandering. the supreme court will be hearing a very important case on this issue on october 3rd 2017. regardless of who wins, this dataset will be of interest to anyone hoping to defeat (or achieve!) a gerrymandering attempt.\n",
      "content\n",
      "each row represents one election, from the perspective of the winner. for example, the first row of the data should be read as a victory for a democrat who was not the incumbent.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the princeton gerrymandering project. you can find their copy, detailed discussion of the data, and their code here.\n",
      "context\n",
      "this dataset provides information on violence in california's criminal justice system, whether the victim was a civilian or an officer or if the incident occurred during the arrest or while a subject was in custody. it's composed of a few related datasets:\n",
      "use of force incidents: the use of force (ursus) incidents that result in serious bodily injury or death or involved the discharge of a firearm are reported annually from leas and other entities throughout the state that employ peace officers. the ursus data is narrowly defined and does not represent the totality of use of force incidents that occur in california. leas are only required to report use of force incidents that result in serious bodily injury or death of either the civilian or the officer and all incidents where there is a discharge of a firearm. as such, caution must be used when using the data for comparisons or in calculating rates.\n",
      "law enforcement officers killed: law enforcement officer's killed or assaulted (leoka) data are reported as part of the federal uniform crime reporting (ucr) program by leas throughout the state. leoka data are summary data, meaning it is a collection of information describing the totality of incidents, not a collection at the detailed, incident level. leoka is a federally mandated collection. from the 1960's until 1990, the cjsc did not retain any of the leoka data; the forms were passed along to the federal bureau of investigation (fbi). in 1990, the doj began to collect and retain the data from the leoka form for statistical purposes, but it wasn't until 2000, that full retention at the state level was defined and standardized.\n",
      "death in custody & arrest-related deaths: state and local law enforcement agencies and correctional facilities report information on deaths that occur in custody or during the process of arrest in compliance with section 12525 of the california government code. contributors include: california law enforcement agencies, county probation departments, state hospitals, and state correctional facilities. data are subject to revision as reports are received by the california department of justice (doj); figures in previous and current releases may not match.\n",
      "citizens' complaints against peace officers: state and local law enforcement agencies that employ peace officers provide citizens' complaints against peace officers (ccapo) data via an annual summary. the information includes the number of criminal and non-criminal complaints reported by citizens and the number of complaints sustained. assembly bill 953 (2015) modified the reporting requirements to expand the types of findings and also include complaints based upon racial and identity profiling claims. 2016 was the first year of collection under the new reporting requirements.\n",
      "acknowledgements\n",
      "this dataset was made available by the state of california's open justice program.\n",
      "photo by meric dagli.\n",
      "context\n",
      "this is a collection of all the works of charles darwin that are available through project gutenberg.\n",
      "this dataset is subject to the project gutenberg license.\n",
      "it is very possible that i have missed some of his works. please add them and update the \"last updated\" date below if you get the chance. otherwise, if you leave a comment on this dataset, i will try and do so myself.\n",
      "content\n",
      "last updated: october 13, 2017\n",
      "this dataset consists of the following works in txt format:\n",
      "\"on the origin of species by means of natural selection or the preservation of favoured races in the struggle for life.\" - pg22764.txt\n",
      "\"the formation of vegetable mould through the action of worms with observations on their habits\" - 2355-0.txt\n",
      "\"the descent of man and selection in relation to sex, vol. i (1st edition)\" - 34967-0.txt\n",
      "\"the descent of man and selection in relation to sex volume ii (1st edition)\" - 36520-0.txt\n",
      "\"a monograph on the sub-class cirripedia (volume 1 of 2)\" - pg31558.txt\n",
      "\"a monograph on the sub-class cirripedia (volume 2 of 2)\" - 46408-0.txt\n",
      "\"the foundations of the origin of species\" - pg22728.txt\n",
      "\"coral reefs\" - pg2690.txt\n",
      "\"more letters of charles darwin volume ii\" - pg2740.txt\n",
      "\"the variation of animals and plants under domestication volume i\" - pg2871.txt\n",
      "\"the variation of animals and plants under domestication volume ii\" - pg2872.txt\n",
      "\"south american geology\" or \"geological observations on south america\" - pg3620.txt\n",
      "\"the different forms of flowers on plants of the same species\" - pg3807.txt\n",
      "\"the effects of cross & self-fertilisation in the vegetable kingdom\" - pg4346.txt\n",
      "\"the movement and habits of climbing plants\" - 2485-0.txt\n",
      "\"the expression of emotion in man and animals\" - pg1227.txt\n",
      "\"the life and letters of charles darwin, volume i (of ii)\" - pg2087.txt\n",
      "\"the life and letters of charles darwin, volume ii (of ii)\" - pg2088.txt\n",
      "\"more letters of charles darwin volume i (of ii)\" - pg2739.txt\n",
      "\"a naturalist's voyage round the world the voyage of the beagle\" - pg3704.txt\n",
      "\"charles darwin: his life in an autobiographical chapter, and in a selected series of his published letters\" - pg38629.txt\n",
      "\"insectivorous plants\" - pg5765.txt\n",
      "acknowledgements\n",
      "content taken from project gutenberg\n",
      "image taken from wikimedia commons\n",
      "inspiration\n",
      "making this data available for any kind of textual analysis. i intend for this to be part of a series.\n",
      "content\n",
      "a ranked list (by frequency) of over 9k simplified chinese characters.\n",
      "acknowledgements\n",
      "all data scraped from hanzidb.org, which is based on jun da's modern chinese character frequency list.\n",
      "inspiration\n",
      "some possible questions:\n",
      "what is the distribution of radicals through the 100 most popular characters? 500? 1,000?\n",
      "does stroke count affect usage?\n",
      "is there an association between the number of strokes and the hsk level of characters?\n",
      "context\n",
      "this dataset lists the percent change of employment in u.s. manufacturing from the same quarter a year ago. it includes data collected on a quarterly basis from march 31, 1995 through june 30, 2016.\n",
      "content\n",
      "data includes the date of the quarterly collection and the percent change of employment in manufacturing from the same quarter a year ago.\n",
      "inspiration\n",
      "what is the general trend of employment in manufacturing?\n",
      "what points in time had the largest negative and positive changes? do they correlate with historical events or policy changes?\n",
      "the bls productivity database includes other datasets about manufacturing, such as unit labor costs and compensation. do any of these factors correlate with the change of employment?\n",
      "acknowledgement\n",
      "this dataset is part of the us department of labor bureau of labor statistics datasets (the bls productivity database), and the original source can be found here.\n",
      "context\n",
      "the data delineate the areal extent of wetlands and surface waters as defined by cowardin et al. (1979). certain wetland habitats are excluded from the national mapping program because of the limitations of aerial imagery as the primary data source used to detect wetlands. these habitats include seagrasses or submerged aquatic vegetation that are found in the intertidal and subtidal zones of estuaries and near shore coastal waters. some deepwater reef communities (coral or tuberficid worm reefs) have also been excluded from the inventory. these habitats, because of their depth, go undetected by aerial imagery. by policy, the service also excludes certain types of \"farmed wetlands\" as may be defined by the food security act or that do not coincide with the cowardin et al. definition. contact the service's regional wetland coordinator for additional information on what types of farmed wetlands are included on wetland maps.\n",
      "content\n",
      "the dataset includes:\n",
      "objectid\n",
      "attribute\n",
      "wetland_type\n",
      "acres\n",
      "globalid\n",
      "shapestarea\n",
      "shapestlength\n",
      "acknowledgement\n",
      "the original dataset and metadata can be found here.\n",
      "inspiration\n",
      "can you visualizes the differences in the wetlands shape by type?\n",
      "context\n",
      "this dataset contains japanese prime minister tweet. japanese culture, diplomatic problem ( north korea and tramp etc), time of disaster, economics... for example,14.april 2014 \"removing radiation contaminated water in all weather, 365/24 at fukushima. i am deeply thankful for dedication and commitment of our peers.\" maybe if you analyze his tweets about japanese economy this data will be useful for stock price forecasting etc.\n",
      "content\n",
      "this dataset contains following the data:\n",
      "url\n",
      "full name show\n",
      "user name dir\n",
      "tweet nav\n",
      "tweet nav_link\n",
      "tweet text size block\n",
      "tweet text size link\n",
      "tweet text size link_link\n",
      "profile tweet 1\n",
      "profile tweet 2\n",
      "profile tweet 3\n",
      "replay\n",
      "re tweet\n",
      "like\n",
      "inspiration\n",
      "inspired by trump vs clinton nlp\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "this is the food 101 dataset, also available from https://www.vision.ee.ethz.ch/datasets_extra/food-101/\n",
      "it contains images of food, organized by type of food. it was used in the paper \"food-101 – mining discriminative components with random forests\" by lukas bossard, matthieu guillaumin and luc van gool. it's a good (large dataset) for testing computer vision techniques.\n",
      "acknowledgements\n",
      "the food-101 data set consists of images from foodspotting [1] which are not property of the federal institute of technology zurich (ethz). any use beyond scientific fair use must be negociated with the respective picture owners according to the foodspotting terms of use [2].\n",
      "[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/\n",
      "context\n",
      "tatoeba is a crowd-sourced dataset made up of example sentences and their translations.\n",
      "this dump uploaded on kaggle is downloaded some time in oct/nov 2017. the latest dumps can be downloaded from https://tatoeba.org/eng/downloads\n",
      "acknowledgements\n",
      "credits goes to the maintainers and the crowd on https://tatoeba.org\n",
      "credits of the banner image goes to patrick tomasso\n",
      "license\n",
      "the official license is cc by-sa 2.0\n",
      "context\n",
      "cannabis strains\n",
      "content\n",
      "strain name: given name of strain\n",
      "type of strain: indica, sativa, hybrid\n",
      "rating: user ratings averaged\n",
      "effects: different effects optained\n",
      "taste: taste of smoke\n",
      "description: backround, etc\n",
      "acknowledgements\n",
      "leafly.com\n",
      "inspiration\n",
      "marijuana may get a bad rep in the media as far as the decriminalization debate goes, but its health benefits can no longer go unnoticed. with various studies linking long-term marijuana use to positive, health-related effects, there are more than just a few reasons to smoke some weed every day.\n",
      "a study done by the boston medical center and the boston university of medicine, examined 589 drug users—more than 8 out of 10 of whom were pot smokers. it determined that “weed aficionados” were no more likely to visit the doctor than non-drug users. if an increased risk of contracting ailments is what’s preventing you from smoking more weed, it looks like you’re in the clear!\n",
      "one of the greatest medicinal benefits of marijuana is its pain relieving qualities, which make it especially effective for treating chronic pain. from menstruation cramps to nerve pain, as little as three puffs of bud a day can help provide the same relief as synthetic painkillers. marijuana relieves pain by “changing the way the nerves function,” says mark ware, md and assistant professor of anesthesia and family medicine at mcgill university.\n",
      "studies have found that patients suffering from arthritis could benefit from marijuana use. this is because naturally occurring chemicals in cannabis work to activate pathways in the body that help fight off joint inflammation.\n",
      "context\n",
      "the global dataset of oil and natural gas production, prices, exports, and net exports.\n",
      "content\n",
      "oil production and prices data are for 1932-2014 (2014 data are incomplete); gas production and prices are for 1955-2014; export and net export data are for 1986-2013. country codes have been modified from earlier versions to conform to correlates of war (cow) and quality of government (qog) standards\n",
      "acknowledgements\n",
      "ross, michael; mahdavi, paasha, 2015, \"oil and gas data, 1932-2014\", doi:10.7910/dvn/ztpw0y, harvard dataverse\n",
      "inspiration\n",
      "how has the price varied from 1900s to 2000s?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this is an aggregate of the data i studied for my thesis titled, \"data mining in presidential debates and speeches: how campaign rhetoric shaped voter opinion in the 2016 u.s. presidential race\". the goal of my thesis was to use nlp techniques to understand how donald trump’s rhetoric impacted the opinions of various voter groups throughout his campaign. here is a summary of my findings:\n",
      "trump’s words were typically more common in an american english corpus and more extreme on both ends of the sentiment spectrum\n",
      "trump not only used rhetorical devices for persuasion but also adeptly coupled these devices with the right talking points based on the composition of his audience\n",
      "precise execution of the above strategy garnered him an unexpectedly large number of votes from the white female and hispanic demographics\n",
      "i hope that others can use this dataset to answer questions of their own about the 2016 presidential campaign.\n",
      "content\n",
      "collection of data from the 2016 u.s. presidential election campaign containing:\n",
      "transcripts of the three presidential debates, divided into separate trump and clinton text files\n",
      "transcripts of trump's 64 speeches delivered after the rnc and clinton's 35 speeches delivered after the dnc\n",
      "transcripts of select speeches delivered by candidates during the primary campaigns\n",
      "usc dornsife/la times presidential election poll, with daily breakdown by voter groups\n",
      "five thirty eight election poll, containing daily data from numerous pollsters\n",
      "acknowledgements\n",
      "debate and speech texts scraped from the american presidency project website.\n",
      "this first-of-its-kind citizen science project is a collection of photos submitted by a group of dedicated volunteers from locations across the united states during the august 21, 2017 total solar eclipse.\n",
      "the bigquery tables include metadata for the photos, links to the photos, and astronomical measurements extracted from the photos.\n",
      "acknowledgements\n",
      "this dataset was kindly prepared by google, uc berkeley, and thousands of volunteers. please see https://eclipsemega.movie/ for more information.\n",
      "inspiration\n",
      "can you map out the locations of the contributors to the project? how many of them were outside the path of totality?\n",
      "resnet-50\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "resnet-101\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "this dataset contains the data from the donations and quotas of the parliamentary people in brazil from 2009 to 2017. the original files were separated and can be found in this link here.\n",
      "important: this is suppose to be a first version, and better ones will come with more time spent in preparation. for the moment the job i had was to translate the headers and join the files. i would like to provide a data dictionary of the variables too, but even the brazilian government did not. hopefully most variables can be understood by its name.\n",
      "acknowledgements\n",
      "thank you everyone in advance for the suggestions and feedbacks. this is my first attempt of uploading a dataset here and i might make mistakes. be gentle, please.\n",
      "inspiration\n",
      "i would like governments to be more transparent and that people could audit more their data.\n",
      "context\n",
      "the aim of this dataset is to offer in a relatively small number of columns (~30) data to compare the performance of some football players, or to compare the efficiency of strikers in-between different european leagues.\n",
      "content\n",
      "inside the dataset are some performance indicators (goals, assists, minutes played, games played) for football strikers over (up to) the last 5 years.\n",
      "acknowledgements\n",
      "the data was extracted from https://www.transfermarkt.co.uk\n",
      "context\n",
      "i was looking for something ben hamner, kaggle's cto, tweeted a while back and it turned out just using r's twitter package was easier than scrolling through his timeline. since i collected all of his tweets, i figured i would share them here as well.\n",
      "content\n",
      "what you get: all of ben hamner's tweets current through today (12 december 2017).\n",
      "what's inside: the text from his tweets plus metadata like favorites, retweets, timestamps, etc. you can even see whether or not i've personally favorited or retweeted his tweets.\n",
      "acknowledgements\n",
      "thanks to ben for his insightful tweets! check out his tweets at @benhamner on twitter.\n",
      "context\n",
      "emotions detection is an interesting blend of psychology and technology.\n",
      "-this technology helps to build a companion robots,this robots can be friendly and have the ability to recognize users’ emotions and needs, and to act accordingly.\n",
      "-it’s essentially a way to determine how your consumers are reacting to your website, social media posts, and other forms of your online content this helps to transform the face of marketing and advertising by reading human emotions and then adapting consumer experiences to these emotions in real time.\n",
      "content\n",
      "now emotions sensor dataset helps to detect emotions in text or voice speech and you can easily build a sentiment analysis bot in few simple steps.\n",
      "emotions sensor data set contain top 1100 english words classified statistically into 7 basic emotion disgust, surprise ,neutral ,anger ,sad ,happy and fear.\n",
      "the words have been manually and automatically labeled using andbrain engine from over 1.185.540 classified words blogs,twitters and sentences .\n",
      "-you can see how to build sentiment analysis in few steps using emotional sensor data set\n",
      "questions or get full emotions sensor data set ?\n",
      "contact us lakadsens@gmail.com\n",
      "context\n",
      "this is the data for my project 2 from sta 108 with prof. chen fall '17 at u.c. davis.\n",
      "content\n",
      "data description: the data “diabetes.txt” contains 16 variables on 366 subjects who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central virginia for african americans. we will consider building regression models with glyhb as the response variable as glycosolated hemoglobin > 70 is often taken as a positive diagnostics of diabetes. the goal is to ﬁnd the “best” model for later use.\n",
      "acknowledgements\n",
      "i had a great deal of help with this project from my tutor and also from the ta cody carroll.\n",
      "inspiration\n",
      "i enjoyed using this data set, and thought that it'd be nice to share with the community.\n",
      "vgg19\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "inceptionv3\n",
      "rethinking the inception architecture for computer vision\n",
      "convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. we benchmark our methods on the ilsvrc 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. with an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.\n",
      "authors: christian szegedy, vincent vanhoucke, sergey ioffe, jonathon shlens, zbigniew wojna\n",
      "https://arxiv.org/abs/1512.00567\n",
      "inceptionv3 architecture\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "alexnet\n",
      "imagenet classification with deep convolutional neural networks\n",
      "we trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the imagenet lsvrc-2010 contest into the 1000 different classes. on the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. the neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. to make training faster, we used non-saturating neurons and a very efficient gpu implementation of the convolution operation. to reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. we also entered a variant of this model in the ilsvrc-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n",
      "authors: alex krizhevsky, ilya sutskever, geoffrey e. hinton\n",
      "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
      "top of the image is cut-off even in the original paper :d\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "densenet-201\n",
      "densely connected convolutional networks\n",
      "recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. in this paper, we embrace this observation and introduce the dense convolutional network (densenet), which connects each layer to every other layer in a feed-forward fashion. whereas traditional convolutional networks with l layers have l connections - one between each layer and its subsequent layer - our network has l(l+1)/2 direct connections. for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. densenets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. we evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (cifar-10, cifar-100, svhn, and imagenet). densenets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. code and models are available at this https url.\n",
      "authors: gao huang, zhuang liu, kilian q. weinberger, laurens van der maaten\n",
      "https://arxiv.org/abs/1608.06993\n",
      "densenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "inceptionv3\n",
      "rethinking the inception architecture for computer vision\n",
      "convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. we benchmark our methods on the ilsvrc 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. with an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.\n",
      "authors: christian szegedy, vincent vanhoucke, sergey ioffe, jonathon shlens, zbigniew wojna\n",
      "https://arxiv.org/abs/1512.00567\n",
      "inceptionv3 architecture\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "resnet-34\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "i know, i am not only one who got inspired after watching/reading moneyball and dreamt of making great contributions to what could've later transformed into cricket's version of sabermetrics. like many out there, i also started my journey by building web scrapers for collecting odi cricket match data and have spend a considerable amount of time, trying to make sense out of it. my current research interests involves alternate player metrics/statistics which can explain players' ability to contribute to a match win.\n",
      "the source data for the shared dataset is not my own (see the acknowledgement section), as i didn't want to deal with the headache of figuring out all the possible permissions and copyright issues that may arise if i released it online.\n",
      "content\n",
      "this dataset has ball by ball data for t20 international cricket matches. the data is available for matches played between 2005 and 2017.\n",
      "the columns included are:\n",
      "match_id - unique id for the matches\n",
      "team - team 1 (the numbering has no significance other than to differentiate between the teams)\n",
      "team2 - team 2 (the numbering has no significance other than to differentiate between the teams)\n",
      "season - season information\n",
      "date - date of the match\n",
      "series - the name of the series. a series is a collection of matches, generally played between two or more countries\n",
      "match_number - the number as part of the series\n",
      "venue - name of the cricket stadium/ground where the match was played\n",
      "city - name of the city where the match was played\n",
      "toss_winner - the team who won the toss\n",
      "toss_decision - the decision made by the team who won the toss\n",
      "player_of_match - man of the match, generally the player with the most significant contribution with bat or ball or both\n",
      "umpire, reserve_umpire, tv_umpire, match_referee - the umpiring team\n",
      "winner - the team which won the match\n",
      "winner_wickets - the number of wickets by which the team won if applicable\n",
      "winner_runs - the number of runs by which the team won if applicable\n",
      "neutralvenue - binary field indicating whether the match was played at a neutral venue\n",
      "method - indicates whether the d/l method was invoked during the match\n",
      "outcome - indicates special outcome like 'no result' (matching getting cancelled) or 'tie'\n",
      "bowl_out - a non-missing value indicates that the match went into a bowl out in order to break the tie\n",
      "competition - the name of the competition\n",
      "eliminator - a non-missing value indicates the use of eliminator method for breaking a tied result\n",
      "innings - the current inning\n",
      "overs - overs\n",
      "batting_team - the team which is batting\n",
      "striker - the batsman who faced the ball\n",
      "non_striker - the batsman at the non-strikers end\n",
      "bowler - the player who is bowling\n",
      "run_scored - run scored\n",
      "extras - extras scored\n",
      "dismissal - how the batsman got out, if applicable\n",
      "dismissed - the batsman who got dismissed\n",
      "acknowledgements\n",
      "the source data for shared dataset comes from the https://cricsheet.org website. thanks to cricsheet team for making the data available for use.\n",
      "the code used for consolidating the source dataset is available at my github page: https://github.com/shoaibnajeeb/cricsheet-data-preparation.\n",
      "the image used:\n",
      "england v sri lanka, twenty20 international - thursday 15 june 2006\n",
      "source: https://www.flickr.com/photos/badgerswan/173466044\n",
      "licensed under the creative commons attribution 2.0 generic license.\n",
      "inspiration\n",
      "how would you like it, if you can predict the probability of the team batting second winning the match?\n",
      "what if i tell you that, you can start making these predictions right from the start of the second innings?\n",
      "i have always been fascinated by the idea of making predictions about which team is going to win the match or how much the teams would score at the end of each innings. the availability of ball by ball data opens up a lot of possibilities for creating different kinds of aggregations at the player, match, team and other levels. i intend to add more datasets featuring aggregations built on top of the ball by ball data.\n",
      "this dataset was obtained as part of my project to rate player performances in a game and use it to model game outcomes. i was looking for an open dataset which included important in-game stats for players but couldn't find one. hence i ended up scraping data myself. subsequently, it has been successfully used to predict player performances in future games and build an optimum fantasy league team. i would be updating the dataset monthly to include newer games of the current season.\n",
      "the dataset includes 2 json files. one of the files describes in-game match stats for every match of the past 4 seasons (current season included) like player touches, passes, shots, yellow cards, saves etc. some of the stats are available as aggregate stats for the entire team and some of them are player specific. second, file describes general match outcomes like the full time and half-time score etc.\n",
      "data snapshot --\n",
      "{\n",
      "    \"1190174\":{\n",
      "        \"13\":{\n",
      "            \"team_details\":{\n",
      "                \"team_id\":\"13\",\n",
      "                \"team_name\":\"arsenal\",\n",
      "                \"team_rating\":\"7.30714285714286\",\n",
      "                \"date\":\"11/08/2017\"\n",
      "            },\n",
      "            \"aggregate_stats\":{\n",
      "                \"fk_foul_lost\":\"9\",\n",
      "                \"won_contest\":\"16\",\n",
      "                \"possession_percentage\":\"70\",\n",
      "                \"total_throws\":\"21\",\n",
      "                 .............\n",
      "             },\n",
      "            \"player_stats\":{\n",
      "                \"petr cech\":{\n",
      "                    \"player_details\":{\n",
      "                        \"player_id\":\"6775\",\n",
      "                        \"player_name\":\"petr cech\",\n",
      "                        \"player_position_value\":\"1\",\n",
      "                        \"player_position_info\":\"gk\",\n",
      "                        \"player_rating\":\"5.78\"\n",
      "                    },\n",
      "                    \"match_stats\":{\n",
      "                        \"good_high_claim\":\"1\",\n",
      "                        \"touches\":\"27\",\n",
      "                        \"total_tackle\":\"1\",\n",
      "                        \"total_pass\":\"20\",\n",
      "                        \"formation_place\":\"1\",\n",
      "                        \"accurate_pass\":\"16\"\n",
      "                    },\n",
      "this dataset could be used to predict player performances and how a particular player/team plays against another. can a game outcome be modeled on the player composition of the participating teams? are goals the most important factor that determines season outcomes or something other than historical goals be used to predict the future team performance in the league?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this dataset is the anonymized result of responses submitted to a survey collected by the canadian department of justice in 2016. this survey \"...focuses on the criminal justice system (cjs) to inform the current criminal justice system review...[this] involved a traditional public opinion research survey, in informed choice survey and in person and online focus groups...this work was undertaken to support reforms and new initiatives in this area.\"\n",
      "this dataset is the survey component of this review.\n",
      "content\n",
      "respondents were asked over 50 questions on their perception of how the canadian justice system works at large. this dataset was published in a typical survey output format, in that most questions are 1-10 rating scales or 0-1 true/false questions, with some free-text responses intermixed. to understand the fields, please see the attached data dictionary, or otherwise access it here.\n",
      "acknowledgements\n",
      "this data was published as-is by the government of canada, here. it is licensed under the open government license - canada.\n",
      "inspiration\n",
      "in a time of increasingly invective dialogue between police forces and the people they police, this dataset provides a window on the general level of satisfaction and concern that canadian government citizens have with their country's justice systems. these results are mostly generalizable to the developed world as a whole.\n",
      "history\n",
      "i made the database from my own photos of russian lowercase letters written by hand.\n",
      "content\n",
      "the github repository with examples\n",
      "handwritten letters on github\n",
      "the main dataset (letters3.zip)\n",
      "6600 (200x33) color images (32x32x3) with 33 letters and the file with labels letters3.txt.\n",
      "photo files are in the .png format and the labels are integers and values.\n",
      "additional letters3.csv file.\n",
      "the file lettercolorimages3.h5 consists of preprocessing images of this set: image tensors and targets (labels)\n",
      "the data can be combined with the database \"classification of handwritten letters\"\n",
      "letter symbols => letter labels\n",
      "а=>1, б=>2, в=>3, г=>4, д=>5, е=>6, ё=>7, ж=>8, з=>9, и=>10, й=>11, к=>12, л=>13, м=>14, н=>15, о=>16, п=>17, р=>18, с=>19, т=>20, у=>21, ф=>22, х=>23, ц=>24, ч=>25, ш=>26, щ=>27, ъ=>28, ы=>29, ь=>30, э=>31, ю=>32, я=>33\n",
      "background images => background labels\n",
      "striped=>0, gridded=>1, background=>2, graph paper=>3\n",
      "acknowledgements\n",
      "as an owner of this database, i have published it for absolutely free using by any site visitor.\n",
      "usage\n",
      "classification, image generation, etc. in a case of handwritten letters with a small number of images are useful exercises.\n",
      "improvement\n",
      "there are lots of ways for increasing this set and the machine learning algorithms applying to it. for example: add the same images but written by other person or add capital letters.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "donald trump's 'forbes richest 400 americans' rankings and estimated net worth from 1985 to 2017.\n",
      "content\n",
      "forbes magazine's yearly richest 400 americans list was first published in 1982. trump was on the list in 1982, 1983, and 1984, which are the only three years that i haven't been able to find his ranking. i left those years off the list. in 1982, according to \"trumpnation: the art of being the donald\" by timothy o'brien, \"forbes gave donald an undefined share of a family fortune the magazine estimate at $200 million - at at time when all donald owned personally was a half interest in the grand hyatt and a share of the yet-to-be completed trump tower. 1983- wealth: share of fred's estimated $400 million fortune...1984- wealth: fred has $200 million, donald has $400 million... 1985-rank:51 wealth: $600 million. donald becomes a solo forbes 400 act; fred disappears from list.\"\n",
      "the \"worth\" column contains trump's estimated net worth in billions. years when his ranking and net worth are \"na\" are years when he did not make the forbes 400 list (1990-1995).\n",
      "context\n",
      "dataset criado para realizar o projeto de conclusão do curso de engenheiro de machine learning pela udacity.\n",
      "content\n",
      "dados de candidatos a deputados estadual e federal eleições 2014. características dos candidatos (sexo, idade, raça/cor), valores dos bens declarados e doações recebidas.\n",
      "acknowledgements\n",
      "agradeço a felipe antunes por disponibilizar o dataset de doações aos candidatos. também agradeço heitor gomes, revisor de meu projeto pelas ótimas sugestões de melhorias.\n",
      "inspiration\n",
      "como os dados dos candidatos podem ser utilizados para realizar uma previsão se o candidato será ou não eleito melhorando o resultado obtido neste projeto?\n",
      "context\n",
      "sample of 17.000 github.com developers, and programming language they know - or want to -.\n",
      "content\n",
      "i acquired the data listing the 1.000 most starred repos dataset, and getting the first 30 users that starred each repo. cleaning the dupes. then for each of the 17.000 users, i calculate the frequency of each of the 1.400 technologies in the user and forked repositories metadata.\n",
      "acknowledgements\n",
      "thanks to jihye sofia seo, because their dataset top 980 starred open source projects on github is the source for this dataset.\n",
      "inspiration\n",
      "i am using this dataset for my github recommendation engine, i use it to find similar developers, to use his stared repositories as recommendation. also, i use this dataset to categorize developer types, trying to understand the weight of a developer in a team, specially when a developer leaves the company, so it is possible to draw the talent lost for the team and the company.\n",
      "context\n",
      "aggregate data from us citizenship and immigration services on deferred action for childhood arrivals program as of sept 4 2017 https://www.uscis.gov/tools/reports-studies/immigration-forms-data/data-set-form-i-821d-deferred-action-childhood-arrivals\n",
      "content\n",
      "country of birth: approximate active daca recipients\n",
      "notes:\n",
      "this table refers to individuals who were granted deferred action for childhood arrivals (daca) as of september 4, 2017. the number of individuals who were ever granted daca as of september 4, 2017 was approximately 800,000. this total excludes persons who applied for an initial grant of daca and were not approved, as well as initial daca requestors that were approved at first, but later had their initial request denied or terminated. nearly 40,000 daca recipients have adjusted to lawful permanent resident (lpr) status, leaving about 760,000 who are not lprs. about 70,000 individuals who were granted daca either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active daca recipients as of september 4, 2017.\n",
      "totals do not add due to rounding.\n",
      "countries with fewer than 10 active daca recipients are included in other.\n",
      "not available: data are not available in electronic systems. source: u.s. citizenship and immigration services,\"\n",
      "state of residence:\n",
      "notes:\n",
      "this table refers to individuals who were granted deferred action for childhood arrivals (daca) as of september 4, 2017. the number of individuals who were ever granted daca as of september 4, 2017 was approximately 800,000. this total excludes persons who applied for an initial grant of daca and were not approved, as well as initial daca requestors that were approved at first, but later had their initial request denied or terminated. nearly 40,000 daca recipients have adjusted to lawful permanent resident (lpr) status, leaving about 760,000 who are not lprs. about 70,000 individuals who were granted for daca either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active daca recipients as of september 4, 2017.\n",
      "state of residence at the time of most recent application.\n",
      "totals do not add due to rounding.\n",
      "territories with less than 10 residents are included in other.\n",
      "not available: data are not available in electronic systems. source: u.s. citizenship and immigration services, claims3 and elis systems.\"\n",
      "core-based statistical areas\n",
      "notes:\n",
      "this table refers to individuals who were granted deferred action for childhood arrivals (daca) as of september 4, 2017. the number of individuals who were ever granted daca as of september 4, 2017 was approximately 800,000. this total excludes persons who applied for an initial grant of daca and were not approved, as well as initial daca requestors that were approved at first, but later had their initial request denied or terminated. nearly 40,000 daca recipients have adjusted to lawful permanent resident (lpr) status, leaving about 760,000 who are not lprs. about 70,000 individuals who were granted daca either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active daca recipients as of september 4, 2017.\n",
      "core based statistical areas (cbsa) at the time of most recent application. cbsas are defined by the office of management and budget.\n",
      "totals may not add due to rounding.\n",
      "cbsas with fewer than 1,000 residents are included in other.\n",
      "not available: data are not available in electronic systems. source: u.s. citizenship and immigration services, claims3 and elis systems.\"\n",
      "age and sex\n",
      "notes:\n",
      "these tables refer to individuals who were granted deferred action for childhood arrivals (daca) as of september 4, 2017. the number of individuals who were ever granted daca as of september 4, 2017 was approximately 800,000. this total excludes persons who applied for an initial grant of daca and were not approved, as well as initial daca requestors that were approved at first, but later had their initial request denied or terminated. nearly 40,000 daca recipients have adjusted to lawful permanent resident (lpr) status, leaving about 760,000 who are not lprs. about 70,000 individuals who were granted daca either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active daca recipients as of september 4, 2017.\n",
      "age as of september 4, 2017 and marital status as of the time of most recent application.\n",
      "totals do not add due to rounding.\n",
      "interquartile range is the range between the 25th percentile and the 75th percentile. about half of the active daca recipients are 20 to 27 years old.\n",
      "not available: data are not available in electronic systems. source: u.s. citizenship and immigration services, claims3 and elis systems.\"\n",
      "marital status\n",
      "notes:\n",
      "this table refers to individuals who were granted deferred action for childhood arrivals (daca) as of september 4, 2017. the number of individuals who were ever granted daca as of september 4, 2017 was approximately 800,000. this total excludes persons who applied for an initial grant of daca and were not approved, as well as initial daca requestors that were approved at first, but later had their initial request denied or terminated. nearly 40,000 daca recipients have adjusted to lawful permanent resident (lpr) status, leaving about 760,000 who are not lprs. about 70,000 individuals who were granted daca either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active daca recipients as of september 4, 2017.\n",
      "marital status at time of most recent application.\n",
      "not available: data are not available in electronic systems.\n",
      "inspiration\n",
      "i converted this data set from the published pdf so i could practice making choropleth maps.\n",
      "acknowledgements\n",
      "i got the cbsa topojson from here: https://discourse.looker.com/t/custom-topojson-for-2014-core-based-statistical-areas-cbsa/2028\n",
      "context\n",
      "started in 2001, resident advisor (ra) has become the web's largest resource for information about underground electronic music around the world. the site maintains a huge database of music and tech reviews, artists, labels, news, podcasts, and events.\n",
      "content\n",
      "gathered using a web-scraping script, the dataset below is of the site's entire collection of music reviews from the start of the site through the end of 2017. it contains the following fields:\n",
      "release type (album or single)\n",
      "artist release\n",
      "title\n",
      "label\n",
      "release month\n",
      "release year\n",
      "style (genres of release listed by ra)\n",
      "rating (score out of 5 given by ra)\n",
      "date of review\n",
      "review author\n",
      "body of review\n",
      "release tracklist\n",
      "acknowledgements\n",
      "thanks to the ra team for the journalism over the years, and for (hopefully) being cool with this dataset being published here.\n",
      "context...\n",
      "ever wondered the what and where of dog ownership? so have we!\n",
      "content...\n",
      "have a look at a sample set of south australian and victorian animal registration data. data is publicly available from the data.gov.au website under a creative commons licence. information includes: breed, location, desexed and colour. datasets are for the 2015, 2016 & 2017 periods (depending on availability). sa information has been consolidated in ~82,500 lines of data!\n",
      "acknowledgements...\n",
      "a big thank you to the sa and victorian shires for having such great datasets!\n",
      "inspiration...\n",
      "we love dogs and really want to understand the distribution of pets across sa and victoria. we will leave it up to you the insights you want to create!\n",
      "results from the men's all-around final of the artistic gymnastics world championships which took place in montreal, canada in october 2017.\n",
      "includes: athlete name, athlete nationality, overall rank, apparatus scores separated into difficulty and execution scores, and apparatus ranks.\n",
      "data source: https://mtl2017gymcan.com/en/results/\n",
      "context\n",
      "for recognising handwritten forms, the very first step was to gather data in a considerable amount for training. which i struggled to collect for weeks.\n",
      "content\n",
      "the dataset contains 26 folders (a-z) containing handwritten images in size 28*28 pixels, each alphabet in the image is center fitted to 20*20 pixel box.\n",
      "each image is stored as gray-level\n",
      "note: might contain some noisy image as well\n",
      "acknowledgements\n",
      "the images are taken from nist(https://www.nist.gov/srd/nist-special-database-19) and nmist large dataset and few other sources which were then formatted as mentioned above.\n",
      "inspiration\n",
      "the dataset would serve beginners in machine learning for there created predictive model to recognise handwritten characters.\n",
      "introduction\n",
      "explore the archive of relevant economic information: relevant news on all indicators with explanations, data on past publications on the economy of the united states, britain, japan and other developed countries, volatility assessments and much more. for the construction of their forecast models, the use of in-depth training is optimal, with a learning model built on the basis of eu and forex data. the economic calendar is an indispensable assistant for the trader.\n",
      "data set\n",
      "the data set is created in the form of an excel spreadsheet (two files 2011-2013, 2014-2018), which can be found at boot time. you can see the source of the data on the site https://www.investing.com/economic-calendar/\n",
      "column - event date\n",
      "column - event time (time new york)\n",
      "column - country of the event\n",
      "column - the degree of volatility (possible fluctuations in currency, indices, etc.) caused by this event\n",
      "column - description of the event\n",
      "column - evaluation of the event according to the actual data, which came out better than the forecast, worse or correspond to it\n",
      "column - data format (%, k x103, m x106, t x109)\n",
      "column - actual event data\n",
      "column - event forecast data\n",
      "column - previous data on this event (with comments if there were any interim changes).\n",
      "inspiration\n",
      "use the historical eu in conjunction with the forex data (exchange rates, indices, metals, oil, stocks) to forecast subsequent forex data in order to minimize investment risks (combine fundamental market analysis and technical).\n",
      "historical events of the eu used as a forecast of the subsequent (for example, the calculation of the probability of an increase in the rate of the fed).\n",
      "investigate the impact of combinations of ec events on the degree of market volatility at different time periods.\n",
      "to trace the main trends in the economies of the leading countries (for example, a decrease in the demand for unemployment benefits).\n",
      "use the eu calendar together with the news background archive for this time interval for a more accurate forecast.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the labelme project has been run out of mit for many years, and allows users to upload and annotate images. since the labels are crowdsourced, they can be of poor quality. i have been proofreading these labels for several months, correcting spelling mistakes and coalescing similar labels into a single label when possible. i have also rejected many labels that did not seem to make sense.\n",
      "content\n",
      "the images in the labelme project as well as the raw metadata were downloaded from mit servers. all data is in the public domain. images within labelme may have been taken as far back as the early 2000s, and run up to the present day.\n",
      "i have worked through 5% of the labelme dataset thus far. i decided to create a dataset pertaining to meals (labels such as plate, glass, napkins, fork, etc.) since there were a fair number of those in the 5% i have curated thus far. most of the images in this dataset are of table settings.\n",
      "this dataset contains: 596 unique images 2734 labeled shapes outlining objects in these images 1782 labeled image grids, with a single number representing which portion of a grid cell is filled with a labeled object\n",
      "acknowledgements\n",
      "many thanks to the people of the labelme project!\n",
      "inspiration\n",
      "i want to see how valuable my curation efforts have been for the labelme dataset. i would like to see others build object recognition models using this dataset.\n",
      "context\n",
      "klcc car park sensor datasets - data about car park occupancy based on date and time.\n",
      "content\n",
      "the dataset contains:\n",
      "klcc label\n",
      "parking spot availability (number from 1-5500 - parking availability, full means no parking available, open means problem reading data - aka 'missing value')\n",
      "date and time when the parking spot availability queried from the sensors.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "when exactly is the busiest hour?\n",
      "detect when there is rush hour\n",
      "compare parking occupancy rate with weather data (does the rain impact parking rate occupancy?)\n",
      "whether there are seasonal changes or anomaly in certain date and time?\n",
      "compare parking occupancy rate before, during and after festive week\n",
      "compare parking occupancy rate between weekday and weekend or just before or after pay day.\n",
      "this dataset does not have a description yet.\n",
      "english word vectors\n",
      "about fasttext\n",
      "fasttext is a library for efficient learning of word representations and sentence classification. one of the key features of fasttext word representation is its ability to produce vectors for any words, even made-up ones. indeed, fasttext word vectors are built from vectors of substrings of characters contained in it. this allows you to build vectors even for misspelled words or concatenation of words.\n",
      "about the vectors\n",
      "these pre-trained vectors contain 1 million word vectors that were learned using wikipedia 2017, the umbc webbase corpus and the statmt.org news dataset. in total, it contains 16b tokens.\n",
      "the first line of the file contains the number of words in the vocabulary and the size of the vectors. each line contains a word followed by its vectors, like in the default fasttext text format. each value is space separated. words are ordered by descending frequency.\n",
      "acknowledgements\n",
      "these word vectors are distributed under the creative commons attribution-share-alike license 3.0.\n",
      "p. bojanowski*, e. grave*, a. joulin, t. mikolov, enriching word vectors with subword information\n",
      "a. joulin, e. grave, p. bojanowski, t. mikolov, bag of tricks for efficient text classification\n",
      "a. joulin, e. grave, p. bojanowski, m. douze, h. jégou, t. mikolov, fasttext.zip: compressing text classification models\n",
      "\n",
      "(* these authors contributed equally.)\n",
      "context\n",
      "melbourne real estate is booming. can you find the insight or predict the next big trend to become a real estate mogul... or even harder, to snap up a reasonably priced 2-bedroom unit?\n",
      "content\n",
      "this is a snapshot of a dataset created by tony pino.\n",
      "it was scraped from publicly available results posted every week from domain.com.au. he cleaned it well, and now it's up to you to make data analysis magic. the dataset includes address, type of real estate, suburb, method of selling, rooms, price, real estate agent, date of sale and distance from c.b.d.\n",
      "notes on specific variables\n",
      "rooms: number of rooms\n",
      "price: price in dollars\n",
      "method: s - property sold; sp - property sold prior; pi - property passed in; pn - sold prior not disclosed; sn - sold not disclosed; nb - no bid; vb - vendor bid; w - withdrawn prior to auction; sa - sold after auction; ss - sold after auction price not disclosed. n/a - price or highest bid not available.\n",
      "type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n",
      "sellerg: real estate agent\n",
      "date: date sold\n",
      "distance: distance from cbd\n",
      "regionname: general region (west, north west, north, north east ...etc)\n",
      "propertycount: number of properties that exist in the suburb.\n",
      "bedroom2 : scraped # of bedrooms (from different source)\n",
      "bathroom: number of bathrooms\n",
      "car: number of carspots\n",
      "landsize: land size\n",
      "buildingarea: building size\n",
      "councilarea: governing council for the area\n",
      "acknowledgements\n",
      "this is intended as a static (unchanging) snapshot of https://www.kaggle.com/anthonypino/melbourne-housing-market. it was created in september 2017. additionally, homes with no price have been removed.\n",
      "context\n",
      "cluster analysis for ad conversions data\n",
      "content\n",
      "the data used in this project is from an anonymous organisation’s social media ad campaign. the data file can be downloaded from here. the file conversion_data.csv contains 1143 observations in 11 variables. below are the descriptions of the variables.\n",
      "1.) ad_id: an unique id for each ad.\n",
      "2.) xyz_campaign_id: an id associated with each ad campaign of xyz company.\n",
      "3.) fb_campaign_id: an id associated with how facebook tracks each campaign.\n",
      "4.) age: age of the person to whom the ad is shown.\n",
      "5.) gender: gender of the person to whim the add is shown\n",
      "6.) interest: a code specifying the category to which the person’s interest belongs (interests are as mentioned in the person’s facebook public profile).\n",
      "7.) impressions: the number of times the ad was shown.\n",
      "8.) clicks: number of clicks on for that ad.\n",
      "9.) spent: amount paid by company xyz to facebook, to show that ad.\n",
      "10.) total conversion: total number of people who enquired about the product after seeing the ad.\n",
      "11.) approved conversion: total number of people who bought the product after seeing the ad.\n",
      "acknowledgements\n",
      "thanks to the anonymous data depositor\n",
      "inspiration\n",
      "social media ad campaign marketing is a leading source of sales conversion and i have made this data available for the benefit of businesses using google adwords to track conversions\n",
      "the included data file contains details about “leads”, where a lead is a person who wants to take up a course with “someschool”, a college. every row contains information about one lead. those leads with “closed” in the lead stage column are the people who have actually paid for the course.\n",
      "the purpose of the exercise is to determine which leads are more likely to close. a score should be assigned to each lead, with the possible values of “high”, “medium” and “low”, where “high” means that the lead is most likely to pay, and “low” means least likely.\n",
      "context\n",
      "this data set contains traffic violation with their different categories published in montgomery county website. the data set contains different categories of the traffic violations which should be useful for analyzing the data category wise.\n",
      "content\n",
      "the name of the fields are self explanatory. the data set contains geographical location details (latitude, longitude) which might be useful for analyzing the data on the geographical maps.\n",
      "acknowledgements\n",
      "i captured the data from the us govt website link: https://data.montgomerycountymd.gov/public-safety/traffic-violations/4mse-ku6q\n",
      "also i was looking for the full data set of the country - please let me know if anybody has the access of the usa all state traffic violation data.\n",
      "inspiration\n",
      "wanted to share the violations of the traffic rules - this might help people to avoid traffic violations as avoid road fatalities.\n",
      "data for sentiment analysis\n",
      "context: kanye west rap verses (243 songs, 364 verses)\n",
      "content: all verses are separated by empty lines. the data has been cleaned to remove any unnecessary words or characters not part of the actual verses.\n",
      "acknowledgements: the lyrics are owned by kanye west and his label, but the dataset was compiled by myself using rap genius.\n",
      "past research: ran the data through a rnn to try to generate new verses that sounded similar to kanye's existing verses.\n",
      "inspiration: it'll be interesting to see what analysis people can do on this dataset. although it's pretty small, it definitely seems like a fun dataset to mess around with.\n",
      "note: below is a list of all the songs used for verse extraction. songs labeled with (n) were excluded due to either not containing rap verses (only choruses), or me not being able to locate the actual lyrics.\n",
      "mercy\n",
      "niggas in paris\n",
      "clique\n",
      "bound 2\n",
      "no church in the wild\n",
      "father stretch my hand pt. 1\n",
      "new slaves\n",
      "blood on the leaves\n",
      "black skinhead\n",
      "don't like\n",
      "monster\n",
      "all day\n",
      "father stretch my hand pt. 2\n",
      "i am a god\n",
      "famous\n",
      "no more parties in la\n",
      "i'm in it\n",
      "hold my liquor\n",
      "facts\n",
      "power\n",
      "cold\n",
      "new god flow\n",
      "gotta have it\n",
      "blame game\n",
      "wolves\n",
      "fml\n",
      "runaway\n",
      "can't tell me nothing\n",
      "waves\n",
      "dark fantasy\n",
      "gorgeous\n",
      "gold digger\n",
      "devil in a new dress\n",
      "otis\n",
      "so appalled\n",
      "all falls down\n",
      "highlights\n",
      "all of the lights\n",
      "on sight\n",
      "who gon stop me\n",
      "guilt trip\n",
      "murder to excellence\n",
      "30 hours\n",
      "send it up\n",
      "through the wire\n",
      "stronger\n",
      "illest motherfucker alive\n",
      "flashing lights\n",
      "last call\n",
      "homecoming\n",
      "h·a·m\n",
      "the morning\n",
      "lost in the world\n",
      "saint pablo\n",
      "freestyle 4\n",
      "feedback\n",
      "jesus walks\n",
      "good morning\n",
      "the one\n",
      "good life\n",
      "touch the sky\n",
      "diamonds from sierra leone\n",
      "never let me down\n",
      "big brother\n",
      "new day\n",
      "hell of a life\n",
      "to the world\n",
      "hey mama\n",
      "heard 'em say\n",
      "white dress\n",
      "heartless\n",
      "champion\n",
      "that's my bitch\n",
      "everything i am\n",
      "gone\n",
      "made in america\n",
      "i wonder\n",
      "spaceship\n",
      "get em high\n",
      "christian dior denim flow\n",
      "we don't care\n",
      "family business\n",
      "see me now\n",
      "the glory\n",
      "welcome to the jungle\n",
      "looking for trouble\n",
      "drive slow\n",
      "the joy\n",
      "the new workout plan\n",
      "champions\n",
      "love lockdown\n",
      "primetime\n",
      "we major\n",
      "roses\n",
      "school spirit\n",
      "addiction\n",
      "lift off\n",
      "barry bonds\n",
      "bittersweet poetry\n",
      "welcome to heartbreak\n",
      "drunk and hot girls\n",
      "two words slow jamz\n",
      "paranoid\n",
      "crack music\n",
      "classic (nike air force remix)\n",
      "robocop\n",
      "breathe in breathe out\n",
      "late\n",
      "bring me down\n",
      "christmas in harlem\n",
      "celebration\n",
      "good night\n",
      "lord lord lord\n",
      "chain heavy\n",
      "eyes closed\n",
      "don't look down\n",
      "take one for the team\n",
      "mama's boyfriend\n",
      "apologize\n",
      "we can make it better\n",
      "when i see it\n",
      "because of you (remix)\n",
      "home\n",
      "throw some d's (remix)\n",
      "livin' in a movie\n",
      "another you\n",
      "impossible\n",
      "back niggaz\n",
      "birthday song\n",
      "back to basics\n",
      "line for line\n",
      "what you do to me\n",
      "in common (remix)\n",
      "pussy print\n",
      "guard down\n",
      "piss on your grave\n",
      "jukebox joints\n",
      "smuckers\n",
      "all your fault\n",
      "can't stop\n",
      "drunk in love (remix)\n",
      "welcome to the world\n",
      "blazing\n",
      "glenwood\n",
      "ayyy girl\n",
      "we fight we love (remix)\n",
      "anyone but him\n",
      "erase me\n",
      "diamonds (remix)\n",
      "hate\n",
      "ego (remix)\n",
      "alright\n",
      "i'm the shit (remix)\n",
      "flight school\n",
      "teriya-king\n",
      "punch drunk love (the eye)\n",
      "therapy\n",
      "digital girl\n",
      "promise land\n",
      "it's over\n",
      "go hard\n",
      "beat goes on\n",
      "everyone nose\n",
      "down\n",
      "in the mood\n",
      "southside\n",
      "my drink n my 2 step (remix)\n",
      "still dreaming\n",
      "tell me when to go (remix)\n",
      "fly away\n",
      "they say\n",
      "paid the price\n",
      "call some hoes\n",
      "the way that you do\n",
      "welcome back (remix)\n",
      "confessions pt. 2 (remix)\n",
      "my baby\n",
      "gettin' it in\n",
      "i changed my mind\n",
      "selfish\n",
      "higher\n",
      "talk about our love\n",
      "i see now\n",
      "getting out the game\n",
      "03 'til infinity\n",
      "so soulful\n",
      "oh oh\n",
      "u know\n",
      "candy\n",
      "the good, the bad and the ugly\n",
      "changing lanes\n",
      "the bounce\n",
      "let's get married (remix)\n",
      "pretty girl rock (remix)\n",
      "that part\n",
      "u mad\n",
      "blessings\n",
      "i won\n",
      "i wish you would\n",
      "marvin & chardonnay\n",
      "e.t.\n",
      "forever\n",
      "the big screen\n",
      "supernova\n",
      "make her say\n",
      "run this town\n",
      "gifted\n",
      "walkin' on the moon\n",
      "knock you down\n",
      "stay up! (viagra)\n",
      "put on\n",
      "american boy\n",
      "pro nails\n",
      "i still love h.e.r.\n",
      "wouldn't get far\n",
      "number one (with pharrell)\n",
      "grammy family\n",
      "extravaganza\n",
      "brand new\n",
      "wouldn't you like 2 ryde\n",
      "this way\n",
      "us placers\n",
      "don't stop!\n",
      "sanctified\n",
      "hurricane 2.0\n",
      "start it up\n",
      "in for the kill (remix)\n",
      "deuces (remix)\n",
      "alors on danse (remix)\n",
      "live fast die young\n",
      "maybach music 2\n",
      "swagga like us (remix)\n",
      "lollipop (remix)\n",
      "plastic\n",
      "finer things\n",
      "anything\n",
      "buy u a drank (remix)\n",
      "this ain't a scene, it's an arms race (remix)\n",
      "pusha man\n",
      "selfish\n",
      "real love\n",
      "hold on (remix)\n",
      "(n) coldest winter\n",
      "(n) ultralight beams\n",
      "(n) only one\n",
      "(n) i love kanye\n",
      "(n) why i love you\n",
      "(n) fade\n",
      "(n) welcome to the jungle\n",
      "(n) amazing\n",
      "(n) say you will\n",
      "(n) street lights\n",
      "(n) see you in my nightmares\n",
      "(n) awesome (freestyle)\n",
      "(n) rosalind ballroom\n",
      "(n) pinocchio story\n",
      "(n) god level\n",
      "(n) bad news\n",
      "(n) i feel like that\n",
      "(n) my way home\n",
      "(n) i'll fly away\n",
      "(n) all we got\n",
      "(n) m.p.a.\n",
      "(n) mula\n",
      "(n) the summer league\n",
      "(n) nobody\n",
      "(n) rollin'\n",
      "(n) touch it\n",
      "(n) we alright\n",
      "(n) punch drunk love (the eye)\n",
      "(n) more\n",
      "(n) take it as a loss\n",
      "(n) figure it out\n",
      "(n) one man can change the world\n",
      "(n) thank you\n",
      "(n) pride n joy\n",
      "(n) everybody\n",
      "(n) the corner\n",
      "(n) down and out\n",
      "(n) the food (n) welcome 2 chicago\n",
      "context\n",
      "the data set was extracted from fbi-ucr website for the year 2012 on population less than 250,000.\n",
      "content\n",
      "here is the list of its 12 variables; population, violent_crime_total, murder_and_manslaughter, forcible_rape, robbery, aggravated_assault, property_crime_total, burglary, larceny_theft, motor_vehicle_theft, lat, long.\n",
      "acknowledgements\n",
      "i really appreciate the fbi-ucr for their generosity.\n",
      "inspiration\n",
      "what impact does population have on crimes?\n",
      "context\n",
      "egyptian is an arabic dialect, and it is the only arabic dialect that has articles on wikipedia. that is why i decided to extract arabic-egyptian comparable corpus from wikipedia to make these resources available for linguists and computational linguists.\n",
      "content\n",
      "the dataset is composed of a set of text documents in both arabic (modern standard) and egyptian dialect aligned at document level. comparable documents share the same document id.\n",
      "acknowledgements\n",
      "thanks to wikipedia and wikipedia contributors who make these resource available. this corpus was collected by: m. saad and b. o. alijla, \"wikidocsaligner: an off-the-shelf wikipedia documents alignment tool,\" 2017 palestinian international conference on information and communication technology (picict), gaza, palestine, 2017, pp. 34-39. doi: 10.1109/picict.2017.27\n",
      "inspiration\n",
      "what are the most common words in egyptian and arabic? what are the most frequent words in egyptian and arabic? what are the least frequent (rare) words in egyptian and arabic?\n",
      "nan\n",
      "context\n",
      "this data-set contains information from the \"protocol gift util\" in the us department of state, which documents all of the official gifts accepted by the president and white house staff. quoting from the u.s. department of state website:\n",
      "the protocol gift unit within the office of the chief of protocol serves as the central processing point for all tangible gifts received from foreign sources by employees of the executive branch of the federal government. the unit is responsible for the creation and maintenance of the official record of all gifts presented by the department of state to officials of foreign governments. working closely with the chief of protocol and the staffs of the president, the vice president, and the secretary of state, the gift unit selects the gifts presented to foreign dignitaries. gifts received by the president, vice president, and the secretary of state and their spouses from foreign governments are also handled by the gift unit in the office of protocol.\n",
      "content\n",
      "the file contains data scraped from the the protocol gift unit website (the r script and more information about exclusions and possible issues can be found here.\n",
      "number of recorded gifts: 1913 (after some exclusions)\n",
      "years: 2002 to 2015\n",
      "encoding: utf8 (with many special characters)\n",
      "inspiration\n",
      "looking forward to see how people can use creative text mining techniques to extract more information about the different columns (for example classify givers / receivers, tag geographies, extract the gift object from the description text, etc.). you can find my future humble attempts here.\n",
      "context\n",
      "this dataset represents essentially the entirety of the astro-physics section for arxiv as the dataset spans from january 1993 to april 2003, beginning a few months after the inception of arxiv. it represents the co-authorship and collaboration network of this community in arxiv as an undirected graph where an edge between two nodes means they co-authored a paper before.\n",
      "content\n",
      "this dataset includes two files, one txt and the other csv. the two are identical in terms of content, they merely represent two different file types of the same dataset. each file has a description header at the top, followed by two columns which contain all of the edges in the network. each row represents an undirected edge between the two author id's i.e. these two authors co-authored a paper together.\n",
      "acknowledgements\n",
      "this dataset was first published in the following paper\n",
      "j. leskovec, j. kleinberg and c. faloutsos. graph evolution: densification and shrinking diameters. acm transactions on knowledge discovery from data (acm tkdd), 1(1), 2007.\n",
      "more information on the dataset can be found at more information on this particular dataset\n",
      "i posses no ownership over this data, please cite jure leskovec and andrej krevl if you use this dataset. similar large network datasets can be found at stanford large network dataset collection\n",
      "inspiration\n",
      "how do co-author networks evolve? can you confirm zipf's law? what are some different ways to calculate centrality or weight a particular author holds in the community?\n",
      "in text mining and natural language processing, stop words are words which are being eliminated on the pre-processing step to aim the greater accuracy. in many cases, stop words removal is to reduce potential noises, in some cases isn't.\n",
      "this dataset contains a list of stop words in bahasa indonesia\n",
      "acknowledgements : fadillah z tala\n",
      "past research : stemming algorithm comparison : porter vs nazief adriani, stemming effects on ir in bahasa indonesia\n",
      "context\n",
      "i am planning to compare above 18 years of male and female between the different class passengers in titanic data set\n",
      "content\n",
      "a century has sailed by since the luxury steamship rms titanic met its catastrophic end in the north atlantic, plunging two miles to the ocean floors after sideswiping an iceberg during its maiden voyage.rather than the intended port of new york, a deep-sea grave became the pride of the white star line’s final destination in the early hours of april 15, 1912.more than 1,500 people lost their lives in the disaster in this project i will be performing an exploratory analysis on the data\n",
      "acknowledgements\n",
      "i noticed that more women survived in raw number and percentage than men and opposite are true of 3rd class passengers. the bars are a good choice to show the difference between categories, but you may want to look into a grouped bar chart1 for an easier comparison of how many survived or didn't in each group. while there were far more men on the boat, less survived than the women. the class seemed to have a direct effect on a passenger's chance of survival. while it is good to see the difference in the numbers of those who survived to those who didn't.\n",
      "inspiration\n",
      "context\n",
      "stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users.\n",
      "content\n",
      "this dataset has got three files named as ratings.csv, movies.csv and tags.csv\n",
      "movies.csv in the 3 columns stored are the values of movieid, title and genre. the title has got the release year of movie in parenthesis. the movie list range from dickson greeting (1891) to movies of 2015. with the total of 27278 movies.\n",
      "ratings.csv the movies have been rated by 138493 users on the scale of 1 to 5, this file contains the information divided in the column 'userid', 'movieid', 'rating' and 'timestamp'.\n",
      "tags.csv this file has the data divided under category 'userid','movieid' and 'tag'\n",
      "acknowledgements\n",
      "i got this data from movielens, for a mini project. http://grouplens.org/datasets/movielens/20m/\"> this is the link to original data set\n",
      "inspiration\n",
      "you have got a ton data. you can use this to make fun decisions like which is the best movie series of all time or create a completely new story out of the data that you have.\n",
      "context:\n",
      "how frequently a word occurs in a language is an important piece of information for natural language processing and linguists. in natural language processing, very frequent words tend to be less informative than less frequent one and are often removed during preprocessing.\n",
      "this dataset contains frequency information on korean, which is spoken by 80 million people. for each item, both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided.\n",
      "content:\n",
      "this dataset contains six sub-files with frequency information. the files have been renamed in english, but no changes have been made to the file contents. the files and their headers are listed below. the text in this dataset is utf-8.\n",
      "frequency by jamo (letter)\n",
      "순위: rank\n",
      "빈도: frequency\n",
      "위치: location\n",
      "자모: jamo (hangul letter)\n",
      "frequency\n",
      "순위: rank\n",
      "빈도: frequency\n",
      "항목: location\n",
      "범주: category\n",
      "frequency by syllable\n",
      "순위: rank\n",
      "빈도: frequency\n",
      "음절: syllable\n",
      "borrowings\n",
      "순위: rank\n",
      "빈도: frequency\n",
      "항목: item\n",
      "풀이: root\n",
      "non standard words\n",
      "순위: rank\n",
      "빈도: frequency\n",
      "어휘: vocabulary\n",
      "풀이: notes\n",
      "품사: part of speech\n",
      "frequency (longer version)\n",
      "순위: rank\n",
      "빈도: frequency\n",
      "항목: location\n",
      "범주: category\n",
      "acknowledgements:\n",
      "this dataset was collected and made available by the national institute of korean language. the dataset and additional documentation (in korean) can be found here.\n",
      "this dataset is distributed under a korean open government liscence, type 4. it may be redistributed with attribution, without derivatives and not for commercial purposes.\n",
      "inspiration:\n",
      "what are the most frequent jamo (hangul characters) in korean? least frequent?\n",
      "what qualities do borrowed words have?\n",
      "is there a relationship between word length and frequency?\n",
      "you may also like:\n",
      "english word frequency\n",
      "japanese lemma frequency\n",
      "list of simplified chinese characters ordered by frequency rank\n",
      "stopword lists for african languages\n",
      "halloween isn't just a festival about spooky pumpkins, spooky ghosts, and eye-catching costumes. halloween is one of the largest datasets out there. in the trick-or-treat baskets little kids carry down the streets is a bank of knowledge and data that can unleash the trends found in communities around america. when i want trick-or-treating this halloween, i was intrigued to find what kind of candy i got, which one i got the most of, and most importantly, how did the price affect consumer choices in my community.\n",
      "when i got back from trick-or treating this october 31st, 2017 (data is fresh like a cucumber), i made sure to empty out what i collected in my middle-class community and sort it out by brand, type/category, price, flavor, etc. after deciding which factors were most relevant, i created three graphs - one bar graph showing the amount of candies collected for each brand, another bar graph looking at the quantity for each type/category of confectionery, and finally, a line graph that analyzed the relationship between unit price per piece and amount of candies collected. all of this is for the sole purpose of displaying how different factors such as brand and price affect consumer choices, appeal to the general population, and what kind of candies to be expecting next year! this is an analysis of middle class economics, brand appeal, and choice of consumers.\n",
      "i would like to thank my sister, who helped contribute to half of my dataset by trick-or-treating in different parts of the neighborhood, making sure the sample collected would be diverse! i also appreciate my dad's interest in this project and his own expertise that was involved in filtering everything out and making the dataset as accurate as possible!\n",
      "from publishing this dataset, i hope to inspire many more young data scientists such as myself to take on new projects that intrigue them or analyze their own halloween candy from their community/neighborhood! i wanted to see how simple festivals like halloween are affected by variables like consumer choice, price range, and brand appeal. keep in mind, this is an analysis of my middle-class community. there are many more datasets and demographics waiting to be analyzed!\n",
      "context\n",
      "this dataset includes 5671 requests collected from the reddit community random acts of pizza between december 8, 2010 and september 29, 2013 (retrieved on september 30, 2013). all requests ask for the same thing: a free pizza. the outcome of each request -- whether its author received a pizza or not -- is known. meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\n",
      "this dataset was featured in our completed playground competition entitled random acts of pizza. the objective of the competition was to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n",
      "content\n",
      "the data are stored in json format. each json entry corresponds to one request (the first and only request by the requester on random acts of pizza). we have removed fields from the test set which would not be available at the time of posting. the datasets include the following fields:\n",
      "\"giver_username_if_known\": reddit username of giver if known, i.e. the person satisfying the request (\"n/a\" otherwise).\n",
      "\"number_of_downvotes_of_request_at_retrieval\": number of downvotes at the time the request was collected.\n",
      "\"number_of_upvotes_of_request_at_retrieval\": number of upvotes at the time the request was collected.\n",
      "\"post_was_edited\": boolean indicating whether this post was edited (from reddit).\n",
      "\"request_id\": identifier of the post on reddit, e.g. \"t3_w5491\".\n",
      "\"request_number_of_comments_at_retrieval\": number of comments for the request at time of retrieval.\n",
      "\"request_text\": full text of the request.\n",
      "\"request_text_edit_aware\": edit aware version of \"request_text\". we use a set of rules to strip edited comments indicating the success of the request such as \"edit: thanks /u/foo, the pizza was delicous\".\n",
      "\"request_title\": title of the request.\n",
      "\"requester_account_age_in_days_at_request\": account age of requester in days at time of request.\n",
      "\"requester_account_age_in_days_at_retrieval\": account age of requester in days at time of retrieval.\n",
      "\"requester_days_since_first_post_on_raop_at_request\": number of days between requesters first post on raop and this request (zero if requester has never posted before on raop).\n",
      "\"requester_days_since_first_post_on_raop_at_retrieval\": number of days between requesters first post on raop and time of retrieval.\n",
      "\"requester_number_of_comments_at_request\": total number of comments on reddit by requester at time of request.\n",
      "\"requester_number_of_comments_at_retrieval\": total number of comments on reddit by requester at time of retrieval.\n",
      "\"requester_number_of_comments_in_raop_at_request\": total number of comments in raop by requester at time of request.\n",
      "\"requester_number_of_comments_in_raop_at_retrieval\": total number of comments in raop by requester at time of retrieval.\n",
      "\"requester_number_of_posts_at_request\": total number of posts on reddit by requester at time of request.\n",
      "\"requester_number_of_posts_at_retrieval\": total number of posts on reddit by requester at time of retrieval.\n",
      "\"requester_number_of_posts_on_raop_at_request\": total number of posts in raop by requester at time of request.\n",
      "\"requester_number_of_posts_on_raop_at_retrieval\": total number of posts in raop by requester at time of retrieval.\n",
      "\"requester_number_of_subreddits_at_request\": the number of subreddits in which the author had already posted in at the time of request.\n",
      "\"requester_received_pizza\": boolean indicating the success of the request, i.e., whether the requester received pizza.\n",
      "\"requester_subreddits_at_request\": the list of subreddits in which the author had already posted in at the time of request.\n",
      "\"requester_upvotes_minus_downvotes_at_request\": difference of total upvotes and total downvotes of requester at time of request.\n",
      "\"requester_upvotes_minus_downvotes_at_retrieval\": difference of total upvotes and total downvotes of requester at time of retrieval.\n",
      "\"requester_upvotes_plus_downvotes_at_request\": sum of total upvotes and total downvotes of requester at time of request.\n",
      "\"requester_upvotes_plus_downvotes_at_retrieval\": sum of total upvotes and total downvotes of requester at time of retrieval.\n",
      "\"requester_user_flair\": users on raop receive badges (reddit calls them flairs) which is a small picture next to their username. in our data set the user flair is either none (neither given nor received pizza, n=4282), \"shroom\" (received pizza, but not given, n=1306), or \"pif\" (pizza given after having received, n=83).\n",
      "\"requester_username\": reddit username of requester.\n",
      "\"unix_timestamp_of_request\": unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the utc timestamp -- which is incorrect since most raop users are from the usa).\n",
      "\"unix_timestamp_of_request_utc\": unit timestamp of request in utc.\n",
      "acknowledgements\n",
      "visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. if you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!\n",
      "this data was collected and graciously shared by althoff et al. (buy them a pizza -- data collection is a thankless and tedious job!) we encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:\n",
      "tim althoff, cristian danescu-niculescu-mizil, dan jurafsky. how to ask for a favor: a case study on the success of altruistic requests, proceedings of icwsm, 2014.\n",
      "context\n",
      "this dataset lists the number of employees (in the thousands) of the total private industry in the united states from march 31, 1939 through august 30, 2016. averages are taken quarterly and are seasonally adjusted.\n",
      "content\n",
      "data includes the date of the quarterly collection and the number of employees (in the thousands).\n",
      "inspiration\n",
      "is there a general trend of employment throughout the year?\n",
      "have certain historical events correlated with drastic changes in employment? what other datasets could you use to prove causation?\n",
      "several other bls datasets break down employment numbers by industry, gender, state, etc. what other employment trends can you find?\n",
      "acknowledgement\n",
      "this dataset is part of the us department of labor bureau of labor statistics datasets (the employment and unemployment database), and the original source can be found here.\n",
      "context\n",
      "california has been dealing with the effects of an unprecedented drought. september 2016 marks the 16th month since the state’s 400-plus urban water suppliers were directed to be in compliance with the emergency conservation standards that followed the governor’s april 1, 2015, executive order. the state water board has been requiring water delivery information from urban water suppliers for 28 consecutive months, following the historic july 2014 board action to adopt emergency water conservation regulations.\n",
      "on may 18, following the governor’s may 9 executive order, the board adopted a statewide water conservation approach that replaces the prior percentage reduction-based water conservation standard with a localized “stress test” approach that mandates urban water suppliers act now to ensure at least a three-year supply of water to their customers under drought conditions.\n",
      "content\n",
      "this fact sheet contains more details on how water conservation is monitored by the california epa. this dataset describes the cumulative savings and compliance of water suppliers from june 2015 - august 2016.\n",
      "inspiration\n",
      "what percentage of suppliers are actually meeting water conservation compliance standards?\n",
      "which hydrologic regions of california are in most danger of not meeting their residents' water needs?\n",
      "which city has the most residential gallons per capita per day (r-gpcd)? the least?\n",
      "are any cities outliers within their hydrological regions? why might they be more or less successful in their water conservation efforts?\n",
      "acknowledgement\n",
      "this dataset is part of the calepa water boards water conservation reporting, and the original source can be found here.\n",
      "context\n",
      "on the occasion of halloween, we thought of sharing a spooky dataset for the community to crunch on the data! this is a subset of larger dataset that goes back to 1911 containing more than 9500 movies. the complete dataset can be downloaded here.\n",
      "remember - \"this halloween could get a lot more spookier, but treats are guaranteed\".\n",
      "content\n",
      "the dataset goes back to 2012 and contains the following data fields:\n",
      "title\n",
      "genres\n",
      "release date\n",
      "release country\n",
      "movie rating\n",
      "review rating\n",
      "movie run time\n",
      "plot\n",
      "cast\n",
      "language\n",
      "filming locations\n",
      "budget\n",
      "acknowledgements\n",
      "the data was extracted by promptcloud's in-house data extraction solution.\n",
      "inspiration\n",
      "some of the things that can be explored are the following:\n",
      "number of horror movies released over the years\n",
      "number of movies released in terms of country\n",
      "rating and run time distribution\n",
      "spooky regions by considering the shooting location\n",
      "text mining on the description text\n",
      "kaggle’s march machine learning mania competition challenged data scientists to predict winners and losers of the men's 2017 ncaa basketball tournament. this dataset contains the selected predictions of all kaggle participants. these predictions were collected and locked in prior to the start of the tournament.\n",
      "the ncaa tournament is a single-elimination tournament that begins with 68 teams. there are four games, usually called the “play-in round,” before the traditional bracket action starts. due to competition timing, these games are included in the prediction files but should not be used in analysis, as it’s possible that the prediction was submitted after the play-in round games were over.\n",
      "data description\n",
      "each kaggle team could submit up to two prediction files. the prediction files in the dataset are in the 'predictions' folder. you can map the files to the teams by team_submission_key.csv.\n",
      "the submission format contains a probability prediction for every possible game between the 68 teams. refer to the competition documentation for data details. for convenience, we have included the data files from the competition dataset in the dataset (you may find tourneyslots.csv and tourneyseeds.csv useful for determining matchups). however, the focus of this dataset is on kagglers' predictions.\n",
      "context\n",
      "wars are one of the few things the human species isn't very proud of. in recent times, major political leaders have taken up steps that both increase and decrease the tension between the allies and the rivals. as being part of the data science community, i believe that this field isn't explored as much as it affects us. this era of comfort seems delusional when we visualise and try to predict how close the next great war might be.\n",
      "content\n",
      "version 1 : the data-set is very small for the initial version with the death tolls and the timelines of major wars. we can infer the participating countries (as much as possible from the names of the wars) and analyse how death toll has been on the rise/fall in the recent years.\n",
      "files : war.csv. columns\n",
      "name - war name\n",
      "time - the time period for the war (including start and end years for longer wars)\n",
      "casualties - number of deaths during that war\n",
      "subsequent version (on a good response) would include weapons used, participating countries and more entries.\n",
      "acknowledgements\n",
      "the data is scrapped from wikipedia with manual cleaning here and there.\n",
      "inspiration\n",
      "i hope that this data-set can start a butterfly effect which leads to an uprising against wars in general.\n",
      "context\n",
      "this is a list of the finishers of the boston marathon of 2017.\n",
      "content\n",
      "it contains the name, age, gender, country, city and state (where available), times at 9 different stages of the race, expected time, finish time and pace, overall place, gender place and division place.\n",
      "acknowledgements\n",
      "data was scrapped from the official marathon website. there are many other people that have done this type of scrapping and some of those ideas were use to get the data.\n",
      "inspiration\n",
      "i was a participant in the marathon, as well as a data science student, so it was a natural curiosity.\n",
      "context\n",
      "pisa stands for \"program for international student assessment\" and it is applied to 15 year-old students across the world to assess their performance in math, reading and science. these are the 2015 scores.\n",
      "content\n",
      "the dataset contains mean (pisa) attainment scores in math, reading and science by country and gender.\n",
      "acknowledgements\n",
      "queried from the world bank learning outcomes database http://datatopics.worldbank.org/education/wdataquery/qlearning.aspx\n",
      "inspiration\n",
      "how attainment compares by country? why some perform better than others? can pisa scores predict social environments such as freedom of press?\n",
      "the presidency of donald trump\n",
      "on jan 20th, 2017, donald j. trump was elected as the 45th president of the united states. this marked the end of a brutal and contentious campaign. he goes in as one of the most unpopular presidents in modern history(based on the popular vote).\n",
      "the inauguration and the women's march\n",
      "trump's election to the presidency led to the organization of the women's march , where millions of men and women took to the streets to protest the new government's stance on women's rights and healthcare. social media blew up with searchable terms like \"#womensmarch\" prompting major news organizations to cover the mass protests.\n",
      "data acquisition\n",
      "the data was acquired using the twitter package's searchtwitter() function. this function makes a call to the twitter api. a total of 30000 tweets containing #inauguration and #womensmarch were obtained (15000 for each).\n",
      "data set attributes\n",
      "1 \"x\" : serial number\n",
      "2 \"text\" : tweet text\n",
      "3 \"favorited\" : true/false\n",
      "4 \"favoritecount\" : number of likes\n",
      "5 \"replytosn\" : screen handle name of the receiver\n",
      "6 \"created\" : yyyy-mm-dd h:m:s\n",
      "7 \"truncated\" : if the tweet is truncated (true/false)\n",
      "8 \"replytosid\": id of the receiver\n",
      "9 \"id\" : id\n",
      "10 \"replytouid\": user id of the receiver\n",
      "11 \"statussource\": device information (web client,iphone,android etc)\n",
      "12 \"screenname\" : screen name of the tweeter\n",
      "13 \"retweetcount\": number of retweets\n",
      "14 \"isretweet\" : true/false\n",
      "15 \"retweeted\" : has this tweet been retweeted(true/false)\n",
      "16 \"longitude\" : longitude\n",
      "17 \"latitude\" : latitude\n",
      "some questions\n",
      "how do the polarity/number of tweets change by time? which locations had negative sentiments about the inauguration? what about the women's march? how to the retweet and mention networks look like for each case? number of tweets per day? which day has the most activity? what are the other hashtags used?\n",
      "context\n",
      "this dataset was created as part of the raspberry turk project. the raspberry turk is a robot that can play chess—it's entirely open source, based on raspberry pi, and inspired by the 18th century chess playing machine, the mechanical turk. the dataset was used to train models for the vision portion of the project.\n",
      "content\n",
      "in the raw form the dataset contains 312 480x480 images of chessboards with their associated board fens. each chessboard contains 30 empty squares, 8 orange pawns, 2 orange knights, 2 orange bishops, 2 orange rooks, 2 orange queens, 1 orange king, 8 green pawns, 2 green knights, 2 green bishops, 2 green rooks, 2 green queens, and 1 green king arranged in different random positions.\n",
      "scripts for data processing\n",
      "the raspberry turk source code includes several scripts for converting this raw data to a more usable form.\n",
      "to get started download the raw.zip file below and then:\n",
      "$ git clone git@github.com:joeymeyer/raspberryturk.git\n",
      "$ cd raspberryturk\n",
      "$ unzip ~/downloads/raw.zip -d data\n",
      "$ conda env create -f data/environment.yml\n",
      "$ source activate raspberryturk\n",
      "from this point there are two scripts you will need to run. first, convert the raw data to an interim form (individual 60x60 rgb/grayscale images) using process_raw.py like this:\n",
      "$ python -m raspberryturk.core.data.process_raw data/raw/ data/interim/\n",
      "this will split the raw images into individual squares and put them in labeled folders inside the interim folder. the final step is to convert the images into a dataset that can be loaded into a numpy array for training/validation. the create_dataset.py utility accomplishes this. the tool takes a number of parameters that can be used to customize the dataset (ex. choose the labels, rgb/grayscale, zca whiten images first, include rotated images, etc). below is the documentation for create_dataset.py.\n",
      "$ python -m raspberryturk.core.data.create_dataset --help\n",
      "usage: raspberryturk/core/data/create_dataset.py [-h] [-g] [-r] [-s sample]\n",
      "                                                 [-o] [-t test_size] [-e] [-z]\n",
      "                                                 base_path\n",
      "                                                 {empty_or_not,white_or_black,color_piece,color_piece_noempty,piece,piece_noempty}\n",
      "                                                 filename\n",
      "\n",
      "utility used to create a dataset from processed images.\n",
      "\n",
      "positional arguments:\n",
      "  base_path             base path for data processing.\n",
      "  {empty_or_not,white_or_black,color_piece,color_piece_noempty,piece,piece_noempty}\n",
      "                        encoding function to use for piece classification. see\n",
      "                        class_encoding.py for possible values.\n",
      "  filename              output filename for dataset. should be .npz\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -g, --grayscale       dataset should use grayscale images.\n",
      "  -r, --rotation        dataset should use rotated images.\n",
      "  -s sample, --sample sample\n",
      "                        dataset should be created by only a sample of images.\n",
      "                        must be value between 0 and 1.\n",
      "  -o, --one_hot         dataset should use one hot encoding for labels.\n",
      "  -t test_size, --test_size test_size\n",
      "                        test set partition size. must be value between 0 and\n",
      "                        1.\n",
      "  -e, --equalize_classes\n",
      "                        equalize class distributions.\n",
      "  -z, --zca             zca whiten dataset.\n",
      "example of how it can be used:\n",
      "$ python -m raspberryturk.core.data.create_dataset data/interim/ promotable_piece data/processed/example_dataset.npz --rotation --grayscale --one_hot --sample=0.3 --zca\n",
      "finally, the dataset is created and can be easily loaded into python either using raspberryturk.core.data.dataset.dataset or simply np.load.\n",
      "in [1]: from raspberryturk.core.data.dataset import dataset\n",
      "in [2]: d = dataset.load_file('data/processed/example_dataset.npz')\n",
      "or\n",
      "in [1]: with open('data/processed/example_dataset.npz', 'r') as f:\n",
      "      :     data = np.load(f)\n",
      "visit the data collection page of the raspberry turk website for more details.\n",
      "creator\n",
      "joey meyer\n",
      "content\n",
      "set of the daily returns for apple and microsoft stock from may 2000 to may 2017. this is a clean dataset used for learning purposes, so i deleted some outlier that should be included any model used in any real world application.\n",
      "acknowledgements\n",
      "the raw data come from yahoo finance.\n",
      "context - # quota for exercising parliamentary activity (ceap)\n",
      "the serenata de amor operation is a projet created by brazilian data scientists looking to data of the publich administration - all infomation you might need - information and more datasets - you can find here at the official site https://github.com/datasciencebr - by the way, everything was made available in english to make sure you understand up-front!\n",
      "data dictionary\n",
      "the quota for exercising parliamentary activity (aka ceap) is a montly quota available exclusively for covering costs of deputies with the exercise of parliamentary activity. the bureau act 43 of 2009 🇧🇷 describe the guidelines for its use.\n",
      "1. congressperson name (congressperson_name)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | nome parlamentar | congressperson name | | txnomeparlamentar | congressperson_name | | nome adotado pelo parlamentar ao tomar posse do seu mandato. compõe-se de dois elementos: um prenome e o nome; dois nomes; ou dois prenomes, salvo, a juízo do presidente da casa legislativa, que poderá alterar essa regra para que não ocorram confusões. | name used by the congressperson during his term in office. usually it is composed by two elements: a given name and a family name; two given names; or two forename, except if the head of the chamber of deputies explicitly alter this rule in order to avoid confusion. |\n",
      "2. unique identifier of congressperson (congressperson_id)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | identificador único do parlamentar | unique identifier of congressperson | | idecadastro | congressperson_id | | número que identifica unicamente um deputado federal na cd. | unique identifier number of a congressperson at the chamber of deputies. |\n",
      "3. congressperson document number (congressperson_document)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número da carteira parlamentar | congressperson document number | | nucarteiraparlamentar | congressperson_document | | documento usado para identificar um deputado federal na cd. pode alterar a cada legislatura nova. | document used to identify the congressperson at the chamber of deputies. may change from one term to another. |\n",
      "4. legislative period number (term)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número da legislatura | legislative period number | | nulegislatura | term | | legislatura: período de quatro anos coincidente com o mandato parlamentar dos deputados federais. no contexto da cota ceap, representa o ano base de início da legislatura e é utilizado para compor a carteira parlamentar, pois esta poderá ser alterada à medida que se muda de legislatura. | legislative period: 4 years period, the same period of the term of congresspeople. in the context of this allowance, it represents the initial year of the legislature. it is also used as part of the congressperson document number since it changes in between legislatures. |\n",
      "5. state (state)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | sigla da uf | state | | sguf | state | | no contexto da cota ceap, representa a unidade da federação pela qual o deputado foi eleito e é utilizada para definir o valor da cota a que o deputado tem. | in the context of this allowance it represents the state or federative unit that elected the congressperson; it is also used to define the value of the allowance to the congressperson. |\n",
      "6. party (party)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | sigla do partido | party | | sgpartido | party | | o seu conteúdo representa a sigla de um partido. definição de partido: é uma organização formada por pessoas com interesse ou ideologia comuns, que se associam com o fim de assumir o poder para implantar um programa de governo. tem personalidade jurídica de direito privado e goza de autonomia e liberdade no que diz respeito à criação, organização e funcionamento, observados os princípios e preceitos constitucionais. | it represents the abbreviation of a party. definition of party: it is an organization built by people with interests or ideologies in common. they form an association with the purpose of achieving power to implement a government program. they are legal entities, free and autonomous when it comes to their creation and self-organization, since they respect the constitutional commandments. |\n",
      "7. legislative period code (term_id)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | código da legislatura | legislative period code | | codlegislatura | term_id | | legislatura: período de quatro anos coincidente com o mandato parlamentar dos deputados federais. no contexto da cota ceap, o seu conteúdo representa o código identificador da legislatura, que um número ordinal sequencial, alterado de um em um, a cada início de uma nova legislatura (por exemplo, a legislatura que iniciou em 2011 é a 54ª legislatura). | legislative period: 4 years period, the same period of the term of congresspeople. in the context of this allowance it represents the identifying code of the legislature, an ordinal number incremented by one each new legislature (e.g. the 2011 legislature is the 54th legislature). |\n",
      "8. subquota number (subquota_number)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número da subcota | subquota number | | numsubcota | subquota_number | | no contexto da cota ceap, o conteúdo deste dado representa o código do tipo de despesa referente à despesa realizada pelo deputado e comprovada por meio da emissão de um documento fiscal, a qual é debitada na cota do deputado. | in the context of this allowance this is the code of the category group referring to the nature of the expense claimed by the congressperson's receipt, the receipt of what was debited from the congressperson's account. |\n",
      "9. subquota description (subquota_description)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | descrição da subcota | subquota description | | txtdescricao | subquota_description | | o seu conteúdo é a descrição do tipo de despesa relativo à despesa em questão. | the description of the category group referring to the nature of the expense. |\n",
      "10. subquota specification number (subquota_group_id)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número da especificação da subcota | subquota specification number | | numespecificacaosubcota | subquota_group_id | | no contexto da cota ceap, há despesas cujo tipo de despesa necessita ter uma especificação mais detalhada (por exemplo, “combustível”). o conteúdo deste dado representa o código desta especificação mais detalhada. | in the context of this allowance there are expenses under certain category groups that require further specifications (e.g. fuel). this variable represents the code of these detailed specification. |\n",
      "11. subquota specification description (subquota_group_description)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | descrição da especificação da subcota | subquota specification description | | txtdescricaoespecificacao | subquota_group_description | | representa a descrição especificação mais detalhada de um referido tipo de despesa. | description of the detailed specification required by certain category groups. |\n",
      "12. supplier (supplier)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | fornecedor | supplier | | txtfornecedor | supplier | | o conteúdo deste dado representa o nome do fornecedor do produto ou serviço presente no documento fiscal | name of the supplier of the product or service specified by the receipt. |\n",
      "13. cnpj/cpf (cnpj_cpf)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | cnpj/cpf | cnpj/cpf | | txtcnpjcpf | cnpj_cpf | | o conteúdo deste dado representa o cnpj ou o cpf do emitente do documento fiscal, quando se tratar do uso da cota em razão do reembolso despesas comprovadas pela emissão de documentos fiscais. | cnpj or cpf are identification numbers issued for, respectively, companies and people by federal revenue of brazil. cnpj are 14 digits long and cpf are 11 digits long. this field is the identification number (cnpj or cpf) of the legal entity issuing the receipt. the receipt is a proof of the expense and is a valid document used to claim for a reimbursement. |\n",
      "14. document number (document_number)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número do documento | document number | | txtnumero | document_number | | o conteúdo deste dado representa o número de face do documento fiscal emitido ou o número do documento que deu causa à despesa debitada na cota do deputado. | this field is the identifying number issued in the receipt, in the proof of expense declared by the congressperson in this allowance. |\n",
      "15. fiscal document type (document_type)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | indicativo de tipo de documento fiscal | fiscal document type | | indtipodocumento | document_type | | este dado representa o tipo de documento do fiscal – 0 (zero), para nota fiscal; 1 (um), para recibo; e 2, para despesa no exterior. | type of receipt — 0 (zero) for bill of sale; 1 (one) for simple receipt; and 2 (two) to expense made abroad. |\n",
      "16. issue date (issue_date)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | data de emissão | issue date | | datemissao | issue_date | | o conteúdo deste dado é a data de emissão do documento fiscal ou a data do documento que tenha dado causa à despesa. | issuing date of the receipt. |\n",
      "17. document value (document_value)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | valor do documento | document value | | vlrdocumento | document_value | | o seu conteúdo é o valor de face do documento fiscal ou o valor do documento que deu causa à despesa. quando se tratar de bilhete aéreo, esse valor poderá ser negativo, significando que o referido bilhete é um bilhete de compensação, pois compensa um outro bilhete emitido e não utilizado pelo deputado (idem para o dado vlrliquido abaixo). | value of the expense in the receipt. if it refers to fly tickets this value can be negative, meaning that it is a credit related to another fly tickets issued but not used by the congressperson (the same is valid for net_value). |\n",
      "18. remark value (remark_value)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | valor da glosa | remark value | | vlrglosa | remark_value | | o seu conteúdo representa o valor da glosa do documento fiscal que incidirá sobre o valor do documento, ou o valor da glosa do documento que deu causa à despesa. | remarked value of the expense concerning the value of the receipt, or remarked value of the expense. |\n",
      "19. net value (net_value)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | valor líquido | net value | | vlrliquido | net_value | | o seu conteúdo representa o valor líquido do documento fiscal ou do documento que deu causa à despesa e será calculado pela diferença entre o valor do documento e o valor da glosa. é este valor que será debitado da cota do deputado. caso o débito seja do tipo telefonia e o valor seja igual a zero, significa que a despesa foi franqueada. | net value of the receipt calculated from the value of the receipt and the remarked value. this is the value that is going to be debited from the congressperson's account. if the category group is telephone and the value is zero, it means the expense was franchised out. |\n",
      "20. month (month)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | mês | month | | nummes | month | | o seu conteúdo representa o mês da competência financeira do documento fiscal ou do documento que deu causa à despesa. é utilizado, junto com o ano, para determinar em que período o débito gerará efeito financeiro sobre a cota. | month of the receipt. it is used together with the year to determine in which month the debt will be considered in the context of this allowance. |\n",
      "21. year (year)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | ano | year | | numano | year | | o seu conteúdo representa o ano da competência financeira do documento fiscal ou do documento que deu causa à despesa. é utilizado, junto com o mês, para determinar em que período o débito gerará efeito financeiro sobre a cota. | year of the receipt. it is used together with the month to determine in which month the debt will be considered in the context of this allowance. |\n",
      "22. installment number (installment)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número da parcela | installment number | | numparcela | installment | | o seu conteúdo representa o número da parcela do documento fiscal. ocorre quando o documento tem de ser reembolsado de forma parcelada. | the number of the installment of the receipt. used when the receipt has to be reimbursed in installments. |\n",
      "23. passenger (passenger)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | passageiro | passenger | | txtpassageiro | passenger | | o conteúdo deste dado representa o nome do passageiro, quando o documento que deu causa à despesa se tratar de emissão de bilhete aéreo. | name of the passenger when the receipt refers to a fly ticket. |\n",
      "24. leg of the trip (leg_of_the_trip)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | trecho | leg of the trip | | txttrecho | leg_of_the_trip | | o conteúdo deste dado representa o trecho da viagem, quando o documento que deu causa à despesa se tratar de emissão de bilhete aéreo. | leg of the trip when the receipt refers to a fly ticket. |\n",
      "25. batch number (batch_number)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número do lote | batch number | | numlote | batch_number | | no contexto da cota ceap, o número do lote representa uma capa de lote que agrupa os documentos que serão entregues à câmara para serem ressarcidos. este dado, juntamente com o número do ressarcimento, auxilia a localização do documento no arquivo da casa. | in the context of this allowance the batch number refers to the cover number of a batch grouping receipts handed in to the chamber of deputies to be reimbursed. this data together with the reimbursement number helps in finding the receipt in the lower house archive. |\n",
      "26. reimbursement number (reimbursement_number)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | número do ressarcimento | reimbursement number | | numressarcimento | reimbursement_number | | no contexto da cota ceap, o número do ressarcimento indica o ressarcimento do qual o documento fez parte por ocasião do processamento do seu reembolso. este dado, juntamente com o número do ressarcimento, auxilia a localização do documento no arquivo da casa. | in the context of this allowance the reimbursement number points to document issued in the reimbursement process. this data together with the reimbursement number helps in finding the receipt in the chamber of deputies archive. |\n",
      "27. reimbursement value (reimbursement_value)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | valor da restituição | reimbursement value | | vlrrestituicao | reimbursement_value | | o seu conteúdo representa o valor restituído do documento fiscal que incidirá sobre o valor do documento. | reimbursement value referring to the document value. |\n",
      "28. applicant identifier (applicant_id)\n",
      "| 🇧🇷 | 🇬🇧 | |:------:|:------:| | identificador do solicitante | applicant identifier | | nudeputadoid | applicant_id | | número que identifica um parlamentar ou liderança na transparência da cota para exercício da atividade parlamentar. | identifying number of a congressperson or the chamber of deputies leadership for the sake of transparency and accountability within this allowance. |\n",
      "acknowledgements\n",
      "all the activists involved in this great initiative.\n",
      "inspiration\n",
      "here are just some questions!\n",
      "in which period of the year do congressmen spend more? is there a relation between spending and session attendance patterns? which party spends more, in average, in each of the 27 states? in average, which are the top spenders? do congressmen spend more on their birthdays? which professions are associated with most spending? does education level relates with spending patterns? what is the relation between spending patterns and congressmen profile characteristics?\n",
      "context\n",
      "the competition is over 2 yrs ago. i just wanna play around the dataset.\n",
      "content\n",
      "the labeled data set consists of 50,000 imdb movie reviews, specially selected for sentiment analysis. the sentiment of reviews is binary, meaning the imdb rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. no individual movie has more than 30 reviews. the 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. in addition, there are another 50,000 imdb reviews provided without any rating labels.\n",
      "id - unique id of each review\n",
      "sentiment - sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
      "review - text of the review\n",
      "acknowledgements\n",
      "the origin place is here. awesome tutorial is here, we can play with it.\n",
      "inspiration\n",
      "just for study and learning\n",
      "rollercoaster tycoon data\n",
      "when i was a kid i loved rollercoaster tycoon. i recently found out that atari had ported the classic game for ios, and i immediately purchased it. now i play whenever i get a free moment. usually i manage to play a few games a week while i watch tv or listen to podcasts.\n",
      "if you do not know, a main aspect of the game involves designing custom roller coasters. when you build something new, you can go to a special window to view the ride statistics:\n",
      "the most important metrics are at the top: you want to maximize excitement without making the ride too intense or nauseating.\n",
      "when i was a kid i had no patience for numbers (i created rides for maximum awesomeness), but these days i'm fond of that sort of thing. after i completed a few park scenarios, i began to wonder if i can use my own data to build better roller coasters. so i started saving my ride stats in a spreadsheet: after completing each scenario, i just logged the stats from each ride into the spreadsheet. this repository contains that data, as well as any analyses i might conduct on it.\n",
      "the data\n",
      "i still play a few games a week (duh), so occasionally i will update the database with my future creations (feel free to send me your own!). each ride is cataloged based on the info from the window above:\n",
      "park_id: integer. unique park identifier, zero-indexed.\n",
      "theme: string. title of the park scenario.\n",
      "rollercoaster_type: string. category of roller coaster.\n",
      "custom_design: boolean. true/false on whether i designed the ride, or if it was pre-designed.\n",
      "excitement: float. ride excitement from 0 (very low) with no specified maximum, but it is very rarely above 10.0. larger numbers are always better.\n",
      "intensity: float. ride intensity from 0 (very dull) with no specified maximum, though most (well-designed) rides are under 10.0. each customer has their own intensity preference.\n",
      "nausea: float. ride nausea from 0 (very low) with no specified maximum, but lower is better and rides rarely have values above 10.0.\n",
      "excitement_rating, intensity_rating, `nausea_rating: string. descriptors of the excitement, intensity, and nausea ratings.\n",
      "max_speed: integer. maximum speed (mph).\n",
      "avg_speed: integer. average speed (mph).\n",
      "ride_time: integer. total duration of the ride in seconds.\n",
      "ride_length: integer. length of the ride in feet.\n",
      "max_pos_gs, max_neg_gs, max_lateral_gs: float. values describing the maximum observed positive, negative, and lateral g-forces.\n",
      "total_air_time: float. number of seconds in which riders experience weightlessness.\n",
      "drops: integer. number of downhill segments.\n",
      "highest_drop_height: integer. highest height (with 0 being sea-level?) from which a drop takes place. note: rides can drop to -6 in the game.\n",
      "inversions: integer. number of times riders are upside-down during the ride. this accounts for loops, corkscrews, etc. values of -1 indicate missing information (see caveat #2).\n",
      "caveats\n",
      "so far i've only been keeping track of roller coasters, so the data does not include customizable thrill rides or water rides (excepting the dinghy slide, which is classified as a roller coaster in the game).\n",
      "it is unfortunate that the first handful of rides i built did not have any inversions, and it took me several weeks to realize that the game does not show this info unless there are inversions. during that time, i simply ignored the inversions count, so we just do not have that info for many rides. some rides cannot have inversions, and i filled in that information after the fact. so, a value for inversions = -1 indicates that the ride could have had inversions.\n",
      "this dataset is part of the mars express power challenge which spans over several competitions and hackathons. the data is licensed as creative commons and to know how you can share it, follow the link below:\n",
      "license: esa cc by-sa 3.0 igo\n",
      "description\n",
      "it has now been more than 12 years that the mars express orbiter (mex) provides science data from mars about its ionosphere and ground subsurface composition. the 3d imagery of mars has provided the community with unprecedented information about the planet. today, thanks to the work of careful and expert operators, mars express orbiter still provides information that supports ground exploration missions on mars (curiosity, opportunity, ...) and a lot of other research.\n",
      "the mars express orbiter is operated by the european space agency from its operations centre (darmstadt, germany) where all the telemetry is analysed. the health status of the spacecraft is carefully monitored to plan future science observations and to avoid power shortages.\n",
      "operators of mars express keep track of the thermal power consumption thanks to the telemetry data. the spacecraft uses electric power coming from the solar arrays (or batteries, during eclipses) not only to supply power to the platform units, but also to the thermal subsystem, which keeps the entire spacecraft within its operating temperature range. the remaining available power can be used by the payloads to do science operations:\n",
      "sciencepower = producedpower − platformpower − thermalpower\n",
      "the mars express power challenge focuses on the difficult problem of predicting the thermal power consumption. three full martian years of mars express telemetry are made available and you are challenged to predict the thermal subsystem power consumption on the following martian year. if successful, the winning method will be a new tool helping the mars express orbiter deliver science data for a longer period of time.\n",
      "more details on the data could be found there: https://kelvins.esa.int/mars-express-power-challenge/\n",
      "license of the data: esa cc by-sa 3.0 igo\n",
      "context\n",
      "i thought it might be neat to do some simple analytics on the bachelor contestant data.\n",
      "content\n",
      "contestant info (incomplete) from seasons 1, 2, 5, 9-21 of abc's the bachelor.\n",
      "acknowledgements\n",
      "pulled from wikipedia and reddit user u/nicolee314.\n",
      "inspiration\n",
      "are there any predictors for success on reality dating shows? has the typical contestant changed over the years? are certain qualities under/over-represented in these contestants?\n",
      "context\n",
      "this represents race data for woodbine track in toronto, on from the period of 06/2015 - 04/2017\n",
      "context\n",
      "in the powerball game, the numbers that the lottery selects are random, but the numbers that players choose to play are not. this data could be used to build models that predict prize amounts as a function of the numbers drawn or other interesting correlations\n",
      "content\n",
      "this dataset is space delimited and represents winning powerball numbers dating back to 1997\n",
      "acknowledgements\n",
      "these numbers were acquired from http://www.powerball.com/powerball/pb_numbers.asp\n",
      "inspiration\n",
      "with machine learning advances accelerating rapidly, could someone create a random number generator that could predict powerball winning numbers ?\n",
      "context\n",
      "active volcanoes in the philippines, as categorized by the philippine institute of volcanology and seismology (phivolcs), include volcanoes in the country having erupted within historical times (within the last 600 years), with accounts of these eruptions documented by humans; or having erupted within the last 10,000 years (holocene) based on analyses of datable materials. however, there is no consensus among volcanologists on how to define an \"active\" volcano. as of 2012, phivolcs lists 23 volcanoes as active in the philippines, 21 of which have historical eruptions; one, cabalian, which is strongly fumarolic volcano[further explanation needed]; and one, leonard kniaseff, which was active 1,800 years ago (c14).\n",
      "there are 50 philippines volcanoes listed by the royal smithsonian institution's global volcanism program (gvp) at present,of which 20 are categorized as \"historical\" and 59 as \"holocene\".the gvp lists volcanoes with historical, holocene eruptions, or possibly older if strong signs of volcanism are still evident through thermal features like fumaroles, hot springs, mud pots, etc.\n",
      "content\n",
      "name\n",
      "m\n",
      "ft\n",
      "coordinates\n",
      "province\n",
      "eruptions\n",
      "acknowledgements\n",
      "philippine institute of volcanology and seismology (phivolcs)\n",
      "not all world citizens are treated equally. one way we might think about measuring inequality in the value of a person’s citizenship is by the number of countries in which they can freely travel. this data set contains information on the global reach of national passports for 199 countries. data identify the number of countries in which each passport is granted visa free travel, a visa on arrival, and the number of countries in which each passport is welcomed by destination countries. the data were downloaded from https://www.passportindex.org/byindividualrank.php on april 11, 2016 using the following r scraper package (https://github.com/sdorius/passportr).\n",
      "information about variables: visarank: passport index rank (was originally labeled 'global rank'). visafree: number of countries in which the passport allows for visa-free travel (originally labeled 'individual rank'). visaoa: number of countries in which the passport allows for visa-on-arrival (originally labeled 'individual rank'). visawelc number of passports accepted for travel by the destination country (originally labeled 'welcoming rank').\n",
      "context\n",
      "launched in 1979, the world economic forum's global competitiveness report is the longest-running, most comprehensive assessment of the drivers of economic development. the 2016-2017 edition covers 138 economies. please contact gcp@weforum.org for any question.\n",
      "go here for more information about the global competitiveness report and index.\n",
      "content\n",
      "the data contains all the components of the global competitiveness index, that's 170 time series, including 113 individual indicators. the file contains the following tabs:\n",
      "about this dataset: all the legal stuff and important disclaimers\n",
      "data: contains the data organised as follows: entities (economies and regions) are listed across. for each data point, the value, rank, period, source, source date, and note (if any) are reported. the dataset includes the gci editions 2007-2008 to 2016-2017. editions are \"stacked\" vertically, starting with the most recent. earlier editions are not included due to change in the methodology.\n",
      "entities: list of all entities, i.e. economies, with iso codes and groups to which they belong (i.e. world bank's income group, imf's regional classification, and forum's own regional classification)\n",
      "meta data: list all series with reference id, descriptions, units, placement in the index, etc. detailed methodology\n",
      "context\n",
      "the non-profit organization grownyc operates a network of 50+ greenmarkets and 15 youthmarkets throughout the city, including the flagship union square greenmarket, to ensure that all new yorkers have access to the freshest, healthiest local food.\n",
      "acknowledgements\n",
      "the farmers market directory was published by the nyc department of health and mental hygiene, and the population statistics were provided by the us census bureau's 2015 american community survey.\n",
      "context\n",
      "the newspaper publications on the internet increases every day. there are many news agencies, newspapers and magazines with digital publications on the big network. published documents made available to users who, in turn, use search engines to find them. to deliver the closest searched documents, these documents must be previously indexed and classified. with the huge volume of documents published every day, many researches have been carried out in order to find a way of dealing with the automatic document classification.\n",
      "content\n",
      "the \"tribuna\" database is of journalistic origin with its digital publication, a factor that may be important for professionals of the area, also serving to understand other similar datasets. in order to carry out the experiment, we adopted the \"a tribuna\" database, whose main characteristics presented previously, show that the collection is a good source of research, since it is already classified by specialists and has 21 classes that can be displayed in the table below.\n",
      "acknowledgements\n",
      "my thanks to the company \"a tribuna\" that gave all these text files for experiment at the federal university of espírito santo. to the high desermpenho computation laboratory (lcad) for all the help in the experiments. thanks also to prof. phd oliveira, elias for all the knowledge shared.\n",
      "inspiration\n",
      "there are two issues involving this dataset:\n",
      "what is the best algorithm for sorting these documents?\n",
      "what are the elements that describe each of the 21 classes in the collection?\n",
      "context\n",
      "the global open data index is an annual effort to measure the state of open government data around the world. the crowdsourced survey is designed to assess the openness of specific government datasets according to the open definition.\n",
      "the global open data index is not an official government representation of the open data offering in each country, but an independent assessment from a citizen’s perspective. it is a civil society audit of open data, and it enables government progress on open data by giving them a measurement tool and a baseline for discussion and analysis of the open data ecosystem in their country and internationally from a key user’s perspective.\n",
      "the global open data index plays a powerful role in sustaining momentum for open data around the world and in convening civil society networks to use and collaborate around this data. governments and open data practitioners can review the index results to see how accessible the open data they publish actually appears to their citizens, see where improvements are necessary to make open data truly open and useful, and track their progress year to year.\n",
      "according to the common open data assessment framework there are four different ways to evaluate data openess — context, data, use and impact. the global open data index is intentionally narrowly focused on the data aspect, hence, limiting its inquiry only to the datasets publication by national governments. it does not look at the broader societal context, seek to assess use or impact in a systematic way, or evaluate the quality of the data. this narrow focus of data publication enables it to provide a standardized, robust, comparable assessment of the state of the publication of key data by governments around the world.\n",
      "acknowledgements\n",
      "the global open data index is compiled by open knowledge international with the assistance of volunteers from the open knowledge network around the world.\n",
      "inspiration\n",
      "what is the state of open data around the world? which countries or regions score the highest in all the data categories? did any countries receive lower open data scores than in previous years?\n",
      "context\n",
      "this datasets comes from a personal project that begun with my msc thesis in data mining at buenos aires university. there i detect slums and informal settlements for la matanza (buenos aires) district. the algorithm developed there helps to reduce to 15% of the total territory to analyze.\n",
      "after successfully finish the thesis, i created a map of slums for whole argentina. map and thesis content are available at fedebayle.github.io/potencialesvya.\n",
      "as far as i know, this is the first research of its kind in argentina, which i think would help my country to contribute to un millennium development goal 7, target 11, \"improving the lives of 100 million slum dwellers\".\n",
      "content\n",
      "this datasets contains georeferenced images about urban slums and informal settlements for two districts in argentina: buenos aires and córdoba (~ 15m habitants).\n",
      "the image of cordoba was taken on 2017-06-09 and the images of buenos aires on 2017-05-04.\n",
      "each image comes from sentinel-2 sensor, with 32x32px and 4 bands (bands 2, 3, 4, 8a, 10 meter resolution). those who prefix is \"vya_\" contains slums (positive class). sentinel-2 is an earth observation mission developed by esa as part of the copernicus programme to perform terrestrial observations in support of services such as forest monitoring, land cover changes detection, and natural disaster management.\n",
      "images are in .tif format.\n",
      "image names consist of:\n",
      "(vya_)[tile id]_[raster row start]_[raster row end]_[raster column start]_[raster column end].tif\n",
      "this is a highly imbalanced class problem:\n",
      "córdoba: 37 of 13,004 images corresponds to slums.\n",
      "buenos aires: 1,008 of 46,047 images corresponds to slums.\n",
      "acknowledgements\n",
      "i would not have been able to create this dataset if the sentinel program did not exist. thanks to european space agency!\n",
      "inspiration\n",
      "the cost of conducting a survey of informal settlements and slums is high and requires copious logistical resources. in argentina, these surveys have been conducted only each 10 years at census.\n",
      "algorithms developed with this data could be used in different countries and help to fight poverty around the world.\n",
      "context\n",
      "hourly weather data for new york city. extracted from online web sources. the following data set is cleaned for the purpose for nyc taxi eta calculation.\n",
      "content\n",
      "we have features such as date, time, temperature (f), dew point (f), humidity, wind speed (mph), condition.\n",
      "acknowledgements\n",
      "the cleaned version is user owned. used in past research for weather data analysis in boston. performed the similar calculation to extract the dataset.\n",
      "inspiration\n",
      "the hourly dataset is cleaned with no missing values. along with temperature the dataset also consists of features like humidity and condition such as snow, rain etc.\n",
      "context\n",
      "so i was trying to use a vgg19 pretrained model with keras but the docker instance couldn't download the model file. there's an open ticket for this issue here: https://github.com/kaggle/docker-python/issues/73\n",
      "content\n",
      "just starting off with vgg16 and vgg19 for now. if this works, i'll upload some more. the weights for the full vgg16 and vgg19 files were too large to upload as a single files. i tried uploading them in parts but there wasn't enough room to extract them in the working directory.\n",
      "here's an example on how to use the model files:\n",
      "keras_models_dir = \"../input/keras-models\"\n",
      "model = applications.vgg16(include_top=false, weights=none) model.load_weights('%s/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' % keras_models_dir)\n",
      "here's some more examples on how to use it: https://www.kaggle.com/ekkus93/keras-models-as-datasets-test\n",
      "acknowledgements\n",
      "i downloaded the files from here: https://github.com/fchollet/deep-learning-models\n",
      "inspiration\n",
      "i just wanted try out something with the dogs vs cats dataset and vgg19.\n",
      "context\n",
      "the dataset for this project originates from the uci machine learning repository. the boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in boston, massachusetts.\n",
      "acknowledgements\n",
      "https://github.com/udacity/machine-learning\n",
      "https://archive.ics.uci.edu/ml/datasets/housing\n",
      "history of locations of an android mobile in the month of october 2014.\n",
      "the history of locations of each of the mobile can be downloaded from this link: source: [https: //takeout.google.com/settings/takeout/custom/location_history][1]\n",
      "content\n",
      "these data are interesting to perform geolocation studies associated with time. format file: json\n",
      "acknowledgements\n",
      "thanks to google for allowing us to download our history of our data\n",
      "every year forbes.com releases a list of top 2000 companies worldwide. these companies are ranked by various metrics like the company size, its market value, sales, profit,etc.\n",
      "so this is the forbes 2000 companies for 3 years viz 2013-2015-2017. the data was scraped from forbes.com. the files contain the list of the top 2000 companies for the 3 years mentioned. the attributes in the dataset are:\n",
      "rank- rank of the company for that year\n",
      "company- name of the company\n",
      "country- country it belongs to\n",
      "sales- sales that corresponding year\n",
      "market values- market value in billions\n",
      "profit- profit earned that year\n",
      "assests- total revenue the company has\n",
      "sector- expertise of the company like finance or technology,etc.\n",
      "context\n",
      "the volume of text data is increasing at a humongous rate everyday which has made it almost impossible to evaluate the data manually. in order to make the process of analyzing this text automatic there are various machine learning techniques that could be applied. this data set is for those enthusiasts who are willing to play with text data and perform sentiment analysis / text classification.\n",
      "content\n",
      "the data has been scraped from trivago,india. a python script was run to examine the get requests and make those requests explicitly in order to obtain required data in json. this data was further parsed and written into a csv file.\n",
      "the data is in the form of a csv file with over 4000 reviews. there are 5 columns:\n",
      "column 1: name of the hotel column 2: title of the review column 3: text of the review column 4: sentiment of the review*( 1: negative 2:average 3:positive) column 5: rating percentage\n",
      "*there are three values for sentiment as mentioned above. a value of 1 represents a negative reviews whereas a value of 3 represents a positive one.\n",
      "acknowledgements\n",
      "i would like to thank my friend iniquitouspsyche for helping me out in scraping the data from trivago.\n",
      "inspiration\n",
      "this data set consists of actual reviews from real people. so this data set will give a real time experience as to how to deal with textual data .\n",
      "author's note:\n",
      "this dataset was originally coined: \"speed limits in new york city\". since then, i have changed the name of the dataset to \"describing new york city roads\" to better reflect the contents of the dataset.\n",
      "- curtis\n",
      "context\n",
      "new york city speed limits\n",
      "the new york department of transportation regulates the speed limits for its roads (afterall, we can't be hitting 88 mph on a regular day). this dataset describes the speed limits for particular road segments of new york city streets.\n",
      "the new york city centerline\n",
      "which streets are inherently faster? how will speed limits come into play? how will nearby bike lanes slow down vehicles (and ultimately taxis)? these are the kinds of questions that can only be answered with contextual data of the streets themselves.\n",
      "fortunately, most major cities provide a public centerline file that describes the path of all railroads, ferry routes, and streets in the city. i've taken the new york city centerline and packaged a dataset that tries to extract meaning out of all the road connections within the city.\n",
      "content\n",
      "new york city speed limits\n",
      "every speed limit region is a straight line. (which represents a segment of road). these lines are expressed by two pairs of coordinates.\n",
      "lat1 - the first latitude coord\n",
      "lon1 - the first longitude coord\n",
      "lat2 - the second latitude coord\n",
      "lat2 - the second longitude coord\n",
      "street - the name of the street the speed limit is imposed on\n",
      "speed - the speed limit of that road section\n",
      "signed - denotes if there is a physical sign on the street that displays the speed limit to cars.\n",
      "region - the city region that the road resides in. there are 5 regions: (bronx, brooklyn, manhattan, queens, and staten island)\n",
      "distance - the length of the speed limit road section (in miles).\n",
      "the new york city centerline\n",
      "street - the name of the street\n",
      "post_type* - the extension for the street name.\n",
      "st_width - the width of the street (in feet). there are varying widths for the size of a street so it was hard to derive a lane count/ street using this feature. as a rule of thumb, the average lane is around 12 feet wide.\n",
      "bike_lane - defines which segments are part of the bicycle network as defined by the nyc department of transportation. there are 11 classes:\n",
      "1 = class i\n",
      "2 = class ii\n",
      "3 = class iii\n",
      "4 = links\n",
      "5 = class i, ii\n",
      "6 = class ii, iii\n",
      "7 = stairs\n",
      "8 = class i, iii\n",
      "9 = class ii, i\n",
      "10 = class iii, i\n",
      "11 = class iii, ii\n",
      "bike class information: https://en.wikipedia.org/wiki/cycling_in_new_york_city#bikeway_types\n",
      "bike_traf_dir** - describes the direction of traffic: (ft = with, tf = against, tw = two-way)\n",
      "traf_dir** - describes the direction of traffic: (ft = with, tf = against, tw = two-way)\n",
      "rw_type - the type of road. there are 6 types of roads: (1 = street, 2 = highway, 3 = bridge, 4 = tunnel, 9 = ramp, 13 = u-turn). note: i parsed awkward path types such as \"ferry route\" and \"trail\".\n",
      "start_contour*** - numeric value indicating the vertical position of the feature's \"from\" node relative to grade level.\n",
      "end_contour*** - numeric value indicating the vertical position of the feature's \"to\" node relative to grade level.\n",
      "snow_pri - the department of sanitation (dsny) snow removal priority designation.\n",
      "v = non-dsny\n",
      "c = critical (these streets have top priority)\n",
      "s = sector (these streets are second priority)\n",
      "h = haulster (small spreaders with plows attached for treating areas with limited accessibility - can hold two tons of salt)\n",
      "region - the city region that the road resides in. there are 5 regions: (bronx, brooklyn, manhattan, queens, and staten island)\n",
      "length - the length of the road (in miles).\n",
      "points - the coordinates that define the road. each coordinate is separated by '|' and the lat and lon values per coordinate are separated by ';'. (side note: round road sections are plotted by points along the curve).\n",
      "*for those who may not be aware, road names are based on a convention. \"avenue\"s, \"boulevard\"s, and \"road\"s are different for distinct reasons. i left these fields in the dataset in case you wish to find any patterns that are pertinent to those types of roads. to learn more about road conventions, visit this link: http://calgaryherald.com/news/local-news/in-naming-streets-strict-rules-dictate-roads-rises-trails-and-more\n",
      "**to explain how direction works i'll provide you with an image: http://imgur.com/a/uflwx. think of every road on the centerline as a vector. it points from one location to another. it always points from the very first coordinate to the very last coordinate. now pay attention to the direction of the road (circled). note how it points in the same direction as the vector denoted by the centerline data. the \"traf_dir\" attribute of the street is \"ft\" because the vector is headed in the same direction as traffic is (it is a one-way street). for \"traf_dir\" with a value of \"tw\", the direction of the vector doesn't matter as the road is a two-way street.\n",
      "***i've had little luck finding what the \"grade levels\" represent. the original aliases are \"to_lvl_co\" and \"frm_lvl_co\". i'll keep searching tonight and will try to dig up what elevation these grades represent. i highly suspect the grades are contour lines because i know they have some relevance to elevation. in the meantime here are the \"grades\" that each value represents:\n",
      "1 = below grade 1\n",
      "2 = below grade 2\n",
      "3 = below grade 3\n",
      "4 = below grade 4\n",
      "5 = below grade 5\n",
      "6 = below grade 6\n",
      "7 = below grade 7\n",
      "8 = below grade 8\n",
      "9 = below grade 9\n",
      "10 = below grade 10\n",
      "11 = below grade 11\n",
      "12 = below grade 12\n",
      "13 = at grade\n",
      "14 = above grade 1\n",
      "15 = above grade 2\n",
      "16 = above grade 3\n",
      "17 = above grade 4\n",
      "18 = above grade 5\n",
      "19 = above grade 6\n",
      "20 = above grade 7\n",
      "21 = above grade 8\n",
      "22 = above grade 9\n",
      "23 = above grade 10\n",
      "24 = above grade 11\n",
      "25 = above grade 12\n",
      "26 = above grade 13\n",
      "99 = not applicable\n",
      "all in all, their documentation could be better and here is a reference to it if you want to look at the source: (https://data.cityofnewyork.us/api/views/exjm-f27b/files/cba8af99-6cd5-49fd-9019-b4a6c2d9dff7?download=true&filename=centerline.pdf)\n",
      "acknowledgements\n",
      "i want to thank the new york city department of transportation (nycdot) and the city of new york for aggregating the original data sets.\n",
      "new york city speed limits http://www.nyc.gov/html/dot/html/about/vz_datafeeds.shtml 28‐11 queens plaza, 8th fl long island city, new york 11101\n",
      "the new york city centerline https://catalog.data.gov/dataset/nyc-street-centerline-cscl data.cityofnewyork.us new york, ny 10007\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "gsmarena phones dataset is a labeled dataset extracted from gsmarena , one of the most popular online provider of phone information, and holds a large collection of phone specification. the original purpose of this dataset was data exploration and potentially some machine learning.\n",
      "the dataset\n",
      "there are 108 unique phone brands with 39 variables: network_technology , 2g_bands , 3g_bands , 4g_bands, network_speed, gprs, edge ,announced ,status ,dimentions ,weight_g ,weight_oz ,sim ,display_type ,display_resolution ,display_size ,os ,cpu ,chipset , gpu ,memory_card ,internal_memory ,ram ,primary_camera ,secondary_camera ,loud_speaker ,audio_jack ,wlan ,bluetooth ,gps ,nfc ,radio ,usb ,sensors ,battery ,colors ,approx_price_eur ,img_url\n",
      "notes\n",
      "multivalued columns use \"|\" or \"/\" as delimiters.\n",
      "the time period when the data was scraped will be mentioned in the dataset description below. the number of devices, prices and other variable are bound to change with time.\n",
      "more details available about variables in column description below.\n",
      "the scraper\n",
      "the dataset was scraped using a little cli i wrote in c#, check it out on my github https://github.com/arwinneil/phone-dataset\n",
      "context\n",
      "in machine learning there is a long path from understanding to intuition. i have created many data files of traditional electronics test pattern to see the response of different activation, loss, optimizers, and metrics in keras. these files should give some ability to test drive your chosen type of machine learning with a very deliberate data set.\n",
      "i wanted something that was infinitely predicable to see how all the different settings effected the algorithms to set a base line for me to gain intuition as to how they should behave once i make more complex models.\n",
      "content\n",
      "these files most contain single line 10,000 example patterns in sine, cosine, triangle, and others. frequency and amplitude in some change through out the set. one has 2,500 example with 4 features of a sine wave 90 degrees out of phase from each other. the values are all between zero and one so no scaling should be necessary.\n",
      "cosinedecampfreqinc, cosinedecreasingamp, cosineincampfreqinc, cosineincampfreqslowing, exponentialdecaytenwaves, exponentialrisetenwaves, foursinewaves, linearfall, linearrise, lorentz, multitone, pulse10waves, pulse10wavesinverted, randomsamples, sinfivewaves, sinfourtywaves, sintenwaves, sintwentywaves, 30,000 squarefivewaves, squaretenwaves, sweeponetofive, sweeponetotwo, sweeponetotwopointfive, syncpattern, trianglefivewaves, triangletenwaves.csv\n",
      "acknowledgements\n",
      "some were generated using tektronics arbexpress and modified in excel for scale. some i generated in c#.\n",
      "inspiration\n",
      "how about a good toy example of a lstm in keras with multivariate data and a single prediction of one of the columns. i did the 4 sine wave .csv to try this. so far the examples i have found just average all of them.\n",
      "context\n",
      "since 2001, palestinian militants have launched thousands of rocket and mortar attacks on israel from the gaza strip as part of the continuing arab–israeli conflict. from 2004 to 2014, these attacks have killed 27 israeli civilians, 5 foreign nationals, 5 idf soldiers, and at least 11 palestinians and injured more than 1900 people, but their main effect is their creation of widespread psychological trauma and disruption of daily life among the israeli population.\n",
      "source:\n",
      "wikipedia[https://en.wikipedia.org/wiki/palestinian_rocket_attacks_on_israel]\n",
      "content\n",
      "this dataset contains the latest rocket alerts released by the israeli \"home front security\". the data was aggregated in the http://tzeva-adom.com site.\n",
      "the column contains date in dd/mm/yy format, time mm:hh format and the name of the area in hebrew (and sometimes messages like:\n",
      "הופעלה בישוב בעוטף עזה- ככה''נ איכון שווא which means that it's a false alarm. those messages can be easily distinguished by the length of them.\n",
      "area sizes may differ and does not report exact coordinates.\n",
      "a list of all the possible areas and messages is in a separate file.\n",
      "data is reported from 2013-2014.\n",
      "acknowledgements\n",
      "context was taken from the wikipedia page: palestinian rocket attacks on israel. data generated by the israeli home front command and was made easily accessible to developers(many apps were created based on it. reporting the alarm during the last conflict).\n",
      "the data was aggregated at the site http://tzeva-adom.com.\n",
      "inspiration\n",
      "israel has a system called \"iron dome\" which intercepts rockets(but only from certain distance). the challenge for israel is where those systems should be deployed and at what times. furthermore, it will be interesting to find patterns in the times rockets were being launched, trying to see if different places were targeted in different times of day. also, areas that were targeted at the same time find if it's possible to cluster the places into different groups of areas.\n",
      "operation \"protective edge\" took place from 8 july until 26 august 2014. after it ended, the rocket attack ended (more or less) until today(june 2017). it will be interesting to check out how the operation effects the alerts, the launching patterns, targeted areas, etc.\n",
      "though the names in this dataset are in hebrew, no hebrew knowledge is needed for working with this dataset. i tried to find an appropriate automatic transliteration service to english, but non of them proved useful. if anyone knows how to get them in english, even using some list from the internet of the cities names in english and their corresponding hebrew names, i'll appreciate your contribution to the dataset's github repository:\n",
      "https://github.com/tomersa/tzeva_adom_dataset.git\n",
      "also, you may contact me and i'll add the changes.\n",
      "context\n",
      "this represents race data for seoul, je-ju, pu-kyoung track in korea, on from the period of 2005 - 2014\n",
      "this dataset is made for korean kaggle user.\n",
      "content\n",
      "2005 - 2014 horse racing dataset from korea\n",
      "p file has\n",
      "id/rctrck/race_de/race_no/partcpt_no/rank/rchose_nm hrsmn/rcord/arvl_dffnc/each_sctn_pas ge_rank/a_win_sutm_expect_alot/win_sta_expect_alot\n",
      "s file has\n",
      "id/race_de/prdctn_nation_nm/sex/age/bnd_wt/trner/rchose_ownr_nm/rchose_bdwgh\n",
      "context\n",
      "a taste of http://reddit.com/r/4chan4trump as seen at http://4chan.org/pol\n",
      "sqlite databases of http://4chan.org/pol threads. sporadic but multiple day scrapping. only finds the keyword trump then scrapes. /r/4chan4trump has screen caps for verification, also, helpful reddit bots which give you some quick wiki and other link facts, although the posting scrapper malfunctioned, all the posts are in the db. if anyone wants too find the thread, the post's id is in the reddit thread. everythings parsable by the 4chan id so just make sure you look through any thread with posts before posting something.\n",
      "if anyone wants to contribute to the threads, they should be easy enough to parse & post. alot of the wiki/youtube bots are supper helpful in referencing the links. there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "{ 'utc': unix timestamp, 'link': 'http://boards.4chan.org/pol/thread/129419703/demand-to-know-the-truth-about-seth-rich', 'reported': whether it was posted on /r/4chan4trump, 'id': serial 4chan id (basically a parallel processing, ie, the order of posts continually goes up) 'text': the entire scrape of the text, contains a unique id per original post, so you can identify the same party 'country': location told to users on 4chan, likely just a ip location database 'link_ids': not used, but could be used to parse the text, 'body': just the message without the header found in text 'identity': how 4chan identifies users on a per thread basis 'thread': id of thread, the same as an id but has subthreads. if id === thread, then it's the original post, but also as the table name for good measure } for those unfamilair, 4chan is an image forum, so some posts just are image responses, so there's some holes in the dataset, but i'm sure the nsa has a bot to fill in the gaps.\n",
      "acknowledgements\n",
      "crazy is crazy, conspiracy is conspiracy, meme is meme and propaganda is one, all, and part.\n",
      "inspiration\n",
      "was bored, and knew that there's some zeitgeist going on when it came to the 2016 presidential escapades and aftermath. i would recommend thinks like levenstein distances in comparison with places like reddit.com/r/the_donald to detect some unhealthy cross overs and the viral spread.\n",
      "overview identification country rwanda title integrated household living conditions survey 2010-2011\n",
      "translated title enquête intégrale sur les conditions de vie des ménages 2010-2011\n",
      "study type income/expenditure/household survey series information this is the third in a series of periodic standardized income and expenditure surveys. the rwanda eicv is conducted with a periodicity of 5 years. the surveys in the series are as follows:\n",
      "eicv1 2000-2001\n",
      "eicv2 2005-2006\n",
      "eicv3 2010-2011\n",
      "id number rwa-nisr-eicv3-02 version version description version 2.0: final public-use dataset\n",
      "production date 2012-10-19 notes version 2.0\n",
      "the date of this version corresponds to the date of nisr approval of the final public-use datasets.\n",
      "overview abstract the 2010/11 integrated household living conditions survey or eicv3 (enquête intégrale sur les conditions de vie des ménages) is the third in the series of surveys which started in 2000/01 and is designed to monitor poverty and living conditions in rwanda. the survey methodology has changed little over its 10 years, making it ideal for monitoring changes in the country. in 2010/11, for the first time the achieved sample size of 14,308 households in the eicv3 was sufficient to provide estimates which are reliable at the level of the district.\n",
      "kind of data sample survey data [ssd]\n",
      "units of analysis for the purposes of this study, the following units of analysis are considered:\n",
      "-communities\n",
      "-households\n",
      "-persons\n",
      "scope notes the scope of survey is defined by the need to evaluate poverty determinants and effects of poverty in various domains. this includes gathering data in specific sectors and examning summary statistics and computed indicators by consumption indicator, gender etc. the survey primarily seeks to compute household consumption aggregates and correlate consumption to the following areas are within the scope and integrated into the survey:\n",
      "education (education expenditures): general education, curriculum, vocational training and, higher learning, school-leaving, literacy and apprenticeship.\n",
      "health (health expenditures): disability and health problems, general health and preventative vaccination over the past 12 months.\n",
      "migration (travel expenditures): rural-urban migration, internal and external migration.\n",
      "housing (expenditures on utilities, rent etc.): status of the housing occupancy, services and installations, physical characteristics of the dwelling, access and satisfaction towards basic services.\n",
      "economic activity (revenue): unemployment, underemployment and job search, occupation, wage or salaried employment characteristics, vup activities, all other activities, domestic work.\n",
      "non-agricultural activities (revenue): activity status, formal and informal sector activity.\n",
      "agriculture (income and expenditure) : livestock, land and agricultural equipment, details of holding parcels/blocs and agricultural policy changes, crop harvests and use on a large and small scale crop production, harvests and use, transformation (processing) of agricultural products.\n",
      "in addition to the specific sector information, consumption and/or wealth holding information was collected:\n",
      "consumption: expenditure on non food items, food expenditure, subsistence farming (own consumption) with different recall periods.\n",
      "other cash flows : transfers out by household, transfers received by the household, income support programs & other revenues (excluding all incomes accrued from saving), vup, ubudehe & rssp schemes, other expenditure (excluding expenditures related to any form of saving).\n",
      "stock items: credit, durable assets and savings (household assets and liabilities)\n",
      "topics topic vocabulary uri consumption/consumer behaviour [1.1] cessda http://www.nesstar.org/rdf/common economic conditions and indicators [1.2] cessda http://www.nesstar.org/rdf/common education [6] cessda http://www.nesstar.org/rdf/common general health [8.4] cessda http://www.nesstar.org/rdf/common employment [3.1] cessda http://www.nesstar.org/rdf/common unemployment [3.5] cessda http://www.nesstar.org/rdf/common housing [10.1] cessda http://www.nesstar.org/rdf/common time use [13.9] cessda http://www.nesstar.org/rdf/common migration [14.3] cessda http://www.nesstar.org/rdf/common information technology [16.2] cessda http://www.nesstar.org/rdf/common coverage geographic coverage this is a national survey with representivity at the (5) provicial and (30) district level and includes urban and rural households.\n",
      "geographic unit the cluster\n",
      "universe all household members.\n",
      "producers and sponsors primary investigator(s) name affiliation national institute of statistics of rwanda (nisr) ministry of finance and economics planning (minecofin) other producer(s) name affiliation role oxford policy management dfid permanante assistance geoffrey greenwell undp designer of data system david megill undp statistician metadata production metadata produced by name abbreviation affiliation role juste nitiema oxford policy management (opm) developed the document geoffrey greenwell undp reviewed and edited document ruben muhayiteto nisr revision of metadata date of metadata production 2011-06-02 ddi document version version 1.0 (oct. 19,2012)\n",
      "this version of the document represents the first draft of the public-use dataset of the eicv 3 study.\n",
      "version 1.1 (june 28th ,2016): changed the title from french into english\n",
      "ddi document id rwa-nisr-ddi-eicv3-02\n",
      "overview\n",
      "the aristo mini corpus contains 1,197,377 (very loosely) science relevant sentences drawn from public data. it provides simple science-relevant text that may be useful to help answer elementary science questions. it was used in the aristo mini system (http://allenai.org/software/) and is also available as a resource in its own right.\n",
      "reference\n",
      "please refer to this corpus as \"the aristo mini corpus (dec 2016 release)\" if you mention it in a publication, with a pointer to this dataset so others can obtain it.\n",
      "contents\n",
      "the aristo mini corpus is primarily a \"science\" subset of simple wikipedia. sentences were taken from the simple wikipedia pages either within the \"science\" category or from pages whose titles also occurs in a 4th grade study guide (by barron’s). also included are approximately 32 thousand definitions from simple wiktionary, and around 50 thousand 4th grade science-like sentences drawn from the web.\n",
      "inspiration\n",
      "this dataset was originally collected with the purpose of a question answering system that could answer 4th grade science questions. the simple structure and single (broad) subject makes it a useful resource for other natural language processing tasks as well. some interesting projects might include:\n",
      "domain-specific word embeddings\n",
      "testing sentence-level parsers\n",
      "entity recognition across contexts -training a markov chain to generate new sciency-sounding sentences\n",
      "context\n",
      "the national traffic and motor vehicle safety act (1966) gives the department of transportation’s national highway traffic safety administration (nhtsa) the authority to issue vehicle safety standards and to require manufacturers to recall vehicles that have safety-related defects or do not meet federal safety standards. more than 390 million cars, trucks, buses, recreational vehicles, motorcycles, and mopeds, 46 million tires, 66 million pieces of motor vehicle equipment, and 42 million child safety seats have been recalled to correct safety defects since 1967.\n",
      "manufacturers voluntarily initiate many of these recalls, while others are influenced by nhtsa investigations or ordered by nhtsa via the courts. if a safety defect is discovered, the manufacturer must notify nhtsa, vehicle or equipment owners, dealers, and distributors. the manufacturer is then required to remedy the problem at no charge to the owner. nhtsa is responsible for monitoring the manufacturer’s corrective action to ensure successful completion of the recall campaign.\n",
      "acknowledgements\n",
      "this dataset was compiled and published by the nhtsa's office of defects investigation (odi).\n",
      "context\n",
      "in 2016, california investigators used state wiretapping laws 563 times to capture 7.8 million communications from 181,000 people, and only 19% of these communications were incriminating. the year's wiretaps cost nearly $30 million.\n",
      "we know this, and much more, now that the california department of justice (cadoj) for the first time has released to eff the dataset underlying its annual wiretap report to the state legislature.\n",
      "content\n",
      "the yearly “electronic interceptions report” includes county-by-county granular data on wiretaps on landlines, cell phones, computers, pagers1, and other devices. each interception is accompanied by information on the number of communications captured and the number of people those communications involved, as well as what percentage of the messages were incriminating. the report also discloses the criminal justice outcomes of the wiretaps (e.g. drugs seized, arrests made) and the costs to the public for running each surveillance operation.\n",
      "under california’s sunshine law, government agencies must provide public records to requesters in whatever electronic format they may exist. and yet, for the last three years, cadoj officials resisted releasing the data in a machine-readable format. in fact, in 2015, cadoj initially attempted to only release the “locked” version of a pdf of the report until eff publicly called out the agency for ignoring these provisions of the california public records act.\n",
      "eff sought the dataset because the formatting of the paper version of the report was extremely difficult to scrape or export in a way that would result in reliable and accurate data. tables in the reports have sometimes spanned more than 70 pages.\n",
      "this year, eff has scored a major victory for open data: in response to our latest request, cadoj has released not only an unlocked pdf, but a spreadsheet containing all the data.\n",
      "what’s especially interesting about the data is that it includes data not previously disclosed in the formal report, including information on when wiretaps targeted multiple locations, devices, and websites, such as facebook. at the same time, the data does not include some information included in the official report, such as the narrative summary of the outcome of each wiretap.\n",
      "inspiration\n",
      "some of the highlights contained in the data.\n",
      "wiretap application in riverside county dropped from 620 wiretap applications in 2015 to 106 in 2016. this is likely due to reforms in the riverside county district attorney’s office following a series of investigative reports from usa today that showed many wiretaps were likely illegal.\n",
      "as in previous years, many of the wiretaps captured voluminous amounts of communications from large groups of people. the largest in terms of communications was a wiretap in a los angeles narcotics case in which 559,000 communications were captured from cell phones over 30 days. the largest in terms of number of people were caught up in a wiretap was a riverside narcotics case in which 91,000 people each had a single piece of communication captured over 120 days.\n",
      "the most expensive wiretap cost $1 million, mostly in personnel costs, to target a single person’s text message in a los angeles murder case. the most expensive wiretap in terms of non-personnel resources (i.e. equipment) cost $193,000. two arrests were made in the associated narcotics case.\n",
      "explore the 2016 data (reproduced here as a csv file) and the full report. previous years’ reports are also made available here in a zip archive (pdfs). let eff know if you discover something interesting in the data by emailing dm@eff.org.\n",
      "acknowledgments\n",
      "this description is reproduced with slight changes from the original blog post introducing the dataset published by dave maass on june 9, 2017. the dataset and contents from eff are released under a cc-by license and redistributed here in accordance with eff's copyright policy.\n",
      "start an analysis\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes 1.6 gb of stop data from north carolina, covering all of 2010 onwards. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context\n",
      "rxnorm was created by the u.s. national library of medicine (nlm) to provide a normalized naming system for clinical drugs, defined as the combination of {ingredient + strength + dose form}. in addition to the naming system, the rxnorm dataset also provides structured information such as brand names, ingredients, drug classes, and so on, for each clinical drug. typical uses of rxnorm include navigating between names and codes among different drug vocabularies and using information in rxnorm to assist with health information exchange/medication reconciliation, e-prescribing, drug analytics, formulary development, and other functions.\n",
      "content\n",
      "the full technical documentation is available here.\n",
      "please note that the nlm updates rxnorm on a regular basis; you should assume that this version is out of date.\n",
      "acknowledgements\n",
      "this dataset uses publicly available data from the u.s. national library of medicine (nlm), national institutes of health, department of health and human services. please cite this dataset as: rxnorm meta2016ab full update 2017_03_06 bethesda, md national library of medicine\n",
      "use this dataset with bigquery\n",
      "you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too: https://cloud.google.com/bigquery/public-data/rxnorm.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "this dataset contains detail about the international air traffic from and to indian territories from jan-2015 to mar-2017 in the below level. a) country wise b) city pair wise c) airline wise\n",
      "content\n",
      "time period: 2015q1 - 2017q1 granularity: quarterly and monthly\n",
      "acknowledgements\n",
      "directorate general of civil aviation, india has published this dataset at their website.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context:\n",
      "*annual health survey : woman schedule *\n",
      "this dataset contains data on annual health survey : woman schedule. woman schedule comprised two sections. section-i (this dataset) contains information relating to the outcome of pregnancy(s) (live birth/still birth/abortion); birth history; type of medical attention at delivery; details of maternal health care(ante-natal/natal/post-natal); immunization of children; breast feeding practices including supplements; occurrence of child diseases (pneumonia, diarrhoea and fever); registration of births, etc. use, sources and practices of family planning methods; details relating to future use of contraceptives and unmet need;awareness about rti/sti, hiv/aids, administration of haf/ort/ors during diarrhoea and danger signs of ari/pneumonia. it also includes more information relating the ever married women (emw) like conception details, usage of npt kit, registration of pregnancy, health problems and subsequent treatments during ante-natal/natal/post-natal period, cost incurred by the woman during delivery etc.\n",
      "there are total of 197 variables/columns in this dataset.\n",
      "survey:\n",
      "base line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)\n",
      "questionnaire: refer page no 131 to 151 of this fact sheet for the survey questions.\n",
      "the survey was conducted in the below 9 states.\n",
      "a. empowered action group (eag) states\n",
      "uttarakhand (05)\n",
      "rajasthan (08)\n",
      "uttar pradesh (09)\n",
      "bihar (10)\n",
      "jharkhand (20)\n",
      "odisha (21)\n",
      "chhattisgarh (22)\n",
      "madhya pradesh (23)\n",
      "b. assam. (18)\n",
      "these nine states, which account for about 48 percent of the total population, 59 percent of births, 70 percent of infant deaths, 75 percent of under 5 deaths and 62 percent of maternal deaths in the country, are the high focus states in view of their relatively higher fertility and mortality.\n",
      "acknowledgements\n",
      "department of health and family welfare, govt. of india has published this dataset in open govt data platform india portal under govt. open data license - india.\n",
      "thanks marco for sharing questionnaire details.\n",
      "inspirations\n",
      "is it possible to predict the pregnancy outcome(live birth/still birth/abortion)?\n",
      "context\n",
      "hypernymy is an important lexical-semantic relation for nlp tasks. for instance, knowing that tom cruise is an actor can help a question answering system answer the question \"which actors are involved in scientology?\". while semantic taxonomies, like wordnet define hypernymy relations between word types, they are limited in scope and domain. therefore, automated methods have been developed to determine, for a given term-pair (x, y), whether y is an hypernym of x, based on their occurrences in a large corpus. to facilitate training neural methods for hypernymy detection, which typically require a large amount of training data, we followed the common methodology of creating a dataset using distant supervision from knowledge resources, and extracted hypernymy relations from wordnet, dbpedia, wikidata, and yago.\n",
      "content\n",
      "all instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least one of the resources. these resources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. to avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (table 1), which we manually selected from the set of hypernymy indicating relations in shwartz et al. (2015).\n",
      "term-pairs related by other relations (including hyponymy), are considered as negative instances, and there is a ratio of 1:4 positive to negative pairs in the dataset.\n",
      "resourcerelations wordnetinstance hypernym, hypernym dbpediatype wikidatasubclass of, instance of yagosubclass of\n",
      "table 1: hypernymy relations in each resource.\n",
      "we provide two dataset splits: random - 70/25/5 ratio for train/test/validation, and lexical - where each set has a distinct vocabulary, preventing models from overfitting to the most common class of x/y (this split maintains a similar ratio).\n",
      "files:\n",
      "train_rnd.csv, test_rnd.csv, val_rnd.csv - the training, test, and validation set of the random split.\n",
      "train_lex.csv, test_lex.csv, val_lex.csv - the training, test, and validation set of the lexical split.\n",
      "each file is a comma-separated file with the following fields:\n",
      "x - the first term\n",
      "y - the second term\n",
      "label - true if y is a hypernym of x, else false\n",
      "acknowledgements\n",
      "if you use this dataset for research purposes, please cite the following publication:\n",
      "improving hypernymy detection with an integrated path-based and distributional method.\n",
      "vered shwartz, yoav goldberg and ido dagan. acl 2016.\n",
      "context\n",
      "recognizing lexical inference is an important component in semantic tasks. various lexical semantic relations, such as synonomy, class membership, part-of, and causality may be used to infer the meaning of one word from another, in order to address lexical variability.\n",
      "as many of the existing lexical inference datasets are constructed from wordnet, important linguistic components that are missing from them are proper-names (lady gaga) and recent terminology (social networks). this dataset contains both components.\n",
      "to construct the dataset, we sampled articles from different topics in online magazines. as candidate (x, y) pairs, we extracted pairs of noun phrases x and y that belonged to the same paragraph in the original text, selecting those in which x is a proper-name. these pairs were manually annotated. to balance the ratio of positive and negative pairs in the dataset, we sampled negative examples according to the frequency of y in positive pairs, creating \"harder\" negative examples, such as (sherlock, lady) and (kylie minogue, vice president).\n",
      "content\n",
      "this dataset contains pairs of (x, y) terms in which x is a proper-name and y is a common noun, annotated to whether x is a y. for instance, (lady gaga, singer) is true, but (lady gaga, film) is false.\n",
      "files:\n",
      "full_dataset.csv: the full dataset\n",
      "train.csv: the training set\n",
      "test.csv: the test set\n",
      "validation.csv: the validation set\n",
      "each file is a comma-separated file with the following format:\n",
      "x: the x term (proper-name)\n",
      "y: the y term (common noun)\n",
      "label: true if x is a y, else false\n",
      "acknowledgements\n",
      "if you use the dataset for any published research, please include the following citation:\n",
      "\"learning to exploit structured resources for lexical inference\".\n",
      "vered shwartz, omer levy, ido dagan and jacob goldberger. conll 2015.\n",
      "context\n",
      "we want to explore india with the help of data and numbers.\n",
      "content\n",
      "in this dataset we will know about india and it^s people, and what problem they are facing.(this dataset is small right now and it will get upadted as time flow. )\n",
      "acknowledgements\n",
      "it wouldn't be possible without the help of niti aayog and other govt sources. http://niti.gov.in\n",
      "inspiration\n",
      "india is 2nd most populous country in the world, and facing a billion problem but the good thing is it have billion minds to solve it.\n",
      "context\n",
      "india-crime-list-2014-and-2015\n",
      "content\n",
      "persons arrested under ipc crimes and percentage variation in 2015 over 2014 (all india) note: $: custodial rapes in 2015 include all types of custodial rapes including rape in police custody whereas prior to 2014, only rapes in police custody were collected; *: sections of crime head have been modified therefore no comparison can be made prior to 2014.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of data.gov.in.\n",
      "inspiration\n",
      "we want to study different types of crime and how things are related and number of arrest\n",
      "context\n",
      "this dataset was used for my undergraduate research. this is my first dataset in kaggle so do not be surprised if you find any errors and mistakes.\n",
      "content\n",
      "i scraped the website of coursera and pre-labelled the dataset depending on their rating. for a 5-star rating, the review was labelled as very positive, positive for 4-star, neutral for 3-star, negative for 2-star, and very negative for 1-star. there are 2 files, reviews.json and reviews_by_course.json. the reviews.json file has no grouping, just an array of course reviews and its corresponding label. for the reviews_by_course.json, each json object corresponds to the course reviews and labels in a specific course.\n",
      "reviews.json\n",
      "data\n",
      "the actual course review.\n",
      "id\n",
      "the unique identifier for a review.\n",
      "label\n",
      "the rating of the course review.\n",
      "reviews_by_course.json\n",
      "data\n",
      "an array of reviews from a specific course.\n",
      "id\n",
      "the course tag. this is in the url of the course. for example, in this url, machine-learning would be the course tag.\n",
      "label\n",
      "an array of labels from a specific course. each label corresponds to a review from the data array depending on their array index.\n",
      "context\n",
      "in this dataset we have a compilation of demand components of the gdp - gross domestic product -of catalonia -one of the 17 autonomous comunities of spain-, and the spanish region with the highest gdp output.\n",
      "content\n",
      "columns :\n",
      "gdp\n",
      "domestic demand\n",
      "consumer expenditure household\n",
      "consumer public adm gross capital\n",
      "equip. goods others\n",
      "const.\n",
      "ext. balance foreign balance\n",
      "total exports goods and services\n",
      "exports goods and services\n",
      "foreign consump. territory total imports goods and services\n",
      "imports goods and services\n",
      "national residents consump. abroad\n",
      "acknowledgements\n",
      "all units of the dataframe are presented in millions of euros (base 2010). the data has been extracted from the idescat, economic annual accounts of catalonia.\n",
      "context\n",
      "metar is a format for reporting weather information. a metar weather report is predominantly used by pilots in fulfillment of a part of a pre-flight weather briefing, and by meteorologists, who use aggregated metar information to assist in weather forecasting.\n",
      "content\n",
      "this is the metars aggregated information for 2016 in knyc.\n",
      "acknowledgements\n",
      "thanks to wunderground for providing the data\n",
      "inspiration\n",
      "this dataset is ment to be used as a extra information for those willing to extract conclusions from their own dataset where hourly the weather information could be useful for their predictions / analysis. you can contact me if you have any doubt or suggestion.\n",
      "context:\n",
      "urban dictionary is an online dictionary of informal language that anyone can add to. as a result, a lot of the user-provided dictionary entries contain interesting variant spellings. while some are (presumably) typos, others are new linguistic innovations.\n",
      "content:\n",
      "this dataset contains 716 variant spellings found in text scraped from urban dictionary, as well as the standard spellings of those words (in uk english).\n",
      "acknowledgements:\n",
      "if you use this dataset in your work, please cite the following paper:\n",
      "saphra, n., & lopez, a. (2016). evaluating informal-domain word representations with urbandictionary. acl 2016, 94. url: http://www.anthology.aclweb.org/w/w16/w16-2517.pdf\n",
      "inspiration:\n",
      "what’s the average edit distances between a variant spelling and the standard spelling? does this differ by part of speech? (you can find part of speech using the natural language toolkit pos_tag function, which is available in python kernels.)\n",
      "can you automatically classify whether a variant spelling is a typo or an intentional innovation (like gr8t)?\n",
      "can you come up with an edit distance metric that takes into account how close letters are to each other on a standard keyboard? does that help you identify typos?\n",
      "can you build an standard english to urban dictionary “translator”?\n",
      "context:\n",
      "the fact that some languages extensively use suffixes and prefixes to convey grammatical meaning(e.g. subject-verb agreement) poses a challenge to most current human language technology (hlt). suffixes and prefixes in such languages can more generally be called morphemes, which are defined as the meaningful subparts of words. the rules that languages use to combine morphemes, together with the actual morphemes that they use (i.e. suffixes and prefixes themselves), are both referred to as a language's morphology. languages which make extensive use of morphemes to build words are said to be morphologically-rich. these include languages such as turkish and can be contrasted with so-called analytic languages such as mandarin chinese, which does not use suffixes or prefixes all.\n",
      "the goal of the universal morphological feature schema is to allow an inflected word from any language to be dened by its lexical meaning (typically carried in the root or stem) and by a rendering of its inflectional morphemes in terms of features from the schema (i.e. a vector of universal morphological features). when an inflected word is defined in this way, it can then be translated into any other language since all other inflected words from all other languages can also be defined in terms of the universal morphological feature schema. although building an interlingual representation for the semantic content of human language as a whole is typically seen as prohibitively difficult, the comparatively small extent of grammatical meanings that are conveyed by overt, affixal inflectional morphology places a natural bound on the range of meaning that must be expressed by an interlingua for inflectional morphology.\n",
      "content:\n",
      "this dataset contains unimorph morphological annotations for 352 languages. each language’s annotations are in a separate file, and each file has a different number of words.\n",
      "many cells in each file are empty. this is because not every feature that is annotated applies to every part of speech. nouns, for example, do not have a tense. in addition, not every language makes use of every possible morphological marking. for instance, english does not have an evidentiality inflection, while other languages, like mongolian and eastern pomo, do.\n",
      "acknowledgments:\n",
      "the unimorph framework was developed by john sylak-glassman. if you use this framework in your work, please cite the following paper:\n",
      "sylak-glassman, j. (2016). the composition and use of the universal morphological feature schema (unimorph schema). technical report, department of computer science, johns hopkins university.\n",
      "you may also like:\n",
      "extinct languages: number of endangered languages in the world, and their likelihood of extinc\n",
      "stopword lists for 19 languages: lists of high-frequency words usually removed during nlp analysis\n",
      "atlas of pidgin and creole language structures: information on 76 creole and pidgin languages\n",
      "context:\n",
      "the penn world table has long been a standard data source for those interested in comparing living standards across countries and explaining differences in cross-country growth. the article describing version 5.6 (summers and heston 1991), is among the most widely cited papers in economics with well over 1000 citations. this version (9.0) attempts to mitigate many concerns raised since. see this article for additional discussion.\n",
      "content:\n",
      "database with information on relative levels of income, output, input and productivity, covering 182 countries between 1950 and 2014. see legend, user guide and source for additional information.\n",
      "acknowledgements:\n",
      "this file contains the data of pwt 9.0, as available on www.ggdc.net/pwt. please refer to www.ggdc.net/pwt for extensive documentation of the different concepts and how these data were constructed.\n",
      "when using these data, please refer to the following paper available for download at www.ggdc.net/pwt:\n",
      "feenstra, robert c., robert inklaar and marcel p. timmer (2015), \"the next generation of the penn world table\"\" american economic review, 105(10), 3150-3182.\n",
      "context\n",
      "global positioning systems (gps) available in many consumer products such as mobile phones has mostly solved the problem of navigation but remains a challenge for indoor locations. a possible solution exists with 802.11 wireless networks and location based services (lbs) that are able to compute location of a wireless station (ws) using triangulation of telemetry such as receiver signal strength indicators (rssi) from nearby wireless access points (wap). the ws coordinates have inaccuracies due to it being “a function of distance, geometry, and materials” (mengual, marbán & eibe, 2010) making distance travelled calculation inaccurate. in an experiment plotting a moving workstation the estimated distanced travelled was 483 metres compared to the actual distance of 149 metres (322% difference).\n",
      "content\n",
      "the lbs system receives data from the wireless network, computes location information for each workstation and stores the data for later retrieval. the data can be sourced from the lbs using an rest api that returns json formatted data. to enable comparison to the estimated calculation, a controlled experiment with a wireless station moving to 20 known locations and turning on the wireless interface for 90 sec periods at a time was conducted.\n",
      "the continual stream of coordinates from the lbs can change not only due to the ws physically moving but also due to the errors in the location calculation itself. these errors can be significant and render any distance calculation meaningless. the experiment captured the calculated position from the wireless network and the actual measured x and y coordinates of a workstation in 20 locations in an office building. the challenge is to figure out using the wireless location ways to improve the accuracy of the prediction.\n",
      "field name description time - conversion of singapore time to to seconds, from 00:00:00 x - x axis coordinates of floor map in feet y - y axis coordinates of floor map, origin is top left of floor map cf - 95% confidence in feet of radius away from x and y client likely to be. realx - x axis measured coordinates of the real location of the test subject realy - x axis measured coordinates of the real location of the test subject\n",
      "inspiration\n",
      "the distance travelled by the workstation was 149 mtrs. how close can you get to this calculation used the predicted locations ?\n",
      "modern american congressional campaigns usually spend millions of dollars. this dataset provides a detailed breakdown of where that money goes. however, the descriptions are provided as unstructured text. can you provide a useful clustering of the expenses?\n",
      "this data comes from the us federal election commission. you can find the original dataset here.\n",
      "context:\n",
      "public health is a large and expensive problem for policymakers to understand in order to provide health services and prevent future epidemics. self-reported data can be tricky due to many sampling issues, but it can paint an interesting picture of how healthy a given area’s population might be.\n",
      "content:\n",
      "data includes small area samples of residents from 500 us cities. recorded is the percent of residents who answered a public health-related question affirmatively (see here). in addition to crude data, additional data is provided with age adjustment applied. 95% confidence intervals also provided for both datapoints.\n",
      "acknowledgements:\n",
      "this data was collected by centers for disease control and prevention, national center for chronic disease prevention and health promotion, division of population health. 500 cities project data [online]. 2016 [accessed aug 10, 2017]. url: https://www.cdc.gov/500cities.\n",
      "inspiration:\n",
      "are there any regional health trends?\n",
      "any unusual hotspots of declining health? higher levels of wellness?\n",
      "can you split the data by geography and predict neighboring cities health?\n",
      "who's healthier, larger or smaller cities?\n",
      "this database presents population and other demographic estimates and projections from 1960 to 2050. they are disaggregated by age-group and gender and cover approximately 200 economies.\n",
      "this dataset was kindly made available by the world bank.\n",
      "name ambiguity has long been viewed as a challenging problem in many applications, such as scientific literature management, people search, and social network analysis. when we search a person name in these systems, many documents (e.g., papers, webpages) containing that person’s name may be returned. which documents are about the person we care about? although much research has been conducted, the problem remains largely unsolved, especially with the rapid growth of the people information available on the web.\n",
      "content\n",
      "this data set contains 110 author names and their disambiguation results (ground truth). for each author, there are 3 json entries. the most important files are xxx_xml, xxx(classify)_txt, and xxx_txt. the xxx(classify)_txt contains the ground truth and the other two files (xxx_xml and xxx_txt) provide features to perform the disambiguation. at the high-level, the xxx_xml file includes title, venue, coauthor, affiliation, and the xxx.txt further contains citation, co-affiliation-occur and homepage.\n",
      "let us use \"ajay gupta\" as the example to explain what information contained in each file.\n",
      "ajay gupta.xml. the raw file. is formatted as a xml file. in the xml file, the author name is associated with a number of publications. an example of a publication is as follow: \" explanation-based failure recovery 1987 ajay gupta aaai 13048 0 null \" where denotes the title of the publication; denotes the publication year; denotes the publication venue; denotes the publication id; denotes the labeled person, e.g., all publications with \"0\" can be considered as published by the same person; denotes the affiliation of the author(s).\n",
      "ajay gupta(classify).txt: the answer file is the ground truth. it is actually extracted from the raw-file by viewing publications with the same \"0\" as a person. the format is in plain text. the following is an example: \" ajay gupta -1:13048 388794 596099 1265282 1179332 675629 39153 258611 -2:988870 1490190 -3:1393934 -4:1398544 -5:1739014 -6:1671104 515636 1678096 -7:1126381 1205032 275987 277587 276300 1549674 1034401 -8:600181 846439 149270 175996 264268 264291 299548 1384744 300057 302056 545651 1212517 -9:1316053 \" where the first line denotes the author name and each of the following line indicates a disambiguated person. for example the first line indicates that an author published 8 papers. the corresponding ids of those papers are respectively 13048, 388794, 596099, 1265282, 1179332, 675629, 39153, 258611.\n",
      "ajay gupta.txt: the intermediate feature files. it contains 8 matrices, which respectively represents 8 features: co-affiliation, coauthor, citation, co-venue, google (ignored), co-affiliation-occur, titlesim, homepage. each matrix records the correlation between any two papers published by ajay gupta. each element, e.g., m^0_{ij}, the i-th row and the j-column in the 0-th matrix, denotes whether the two papers (i and j) contain the same affiliation. in this sense, the problem of name disambiguation can be basically considered as a pairwise clustering problem. the second matrix records the number of same coauthors, except ajay gupta. the third matrix records whether the a paper cites another paper. the fourth matrix records whether a paper is published at the same venue with another paper. the fifth matrix records whether the two papers (titles) can be found at a same web page (e.g., conference page). (this matrix is not complete). the sixth matrix records whether the affiliation of author \"a\" of a paper appears in the content of another paper, or vice versa. the seventh matrix records the cosine similarity between titles of any two papers. the eighth matrix records whether two papers appear on the same homepage. please note that the 5th-8th matrixes cannot be extracted from the raw-data file (xxx.xml) and they are generated using other program.\n",
      "this dataset is a lightly edited from the version provided by aminer. the three core files for each author have been bundled into a single json for convenience.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by aminer. use the data for publication, please kindly cite the following papers:\n",
      "@article{tang:12tkde, author = {jie tang and alvis c.m. fong and bo wang and jing zhang}, title = {a unified probabilistic framework for name disambiguation in digital library}, journal ={ieee transactions on knowledge and data engineering}, volume = {24}, number = {6}, year = {2012}, }\n",
      "@inproceedings{ wang:adana:, author = \"xuezhi wang and jie tang and hong cheng and philip s. yu\", title = \"adana: active name disambiguation\", booktitle = \"icdm'11\", pages = {794-803}, year = {2011}\n",
      "the goal of the \"library of southern literature\" is to make one hundred of the most important works of southern literature published before 1920 available world-wide for teaching and research. currently, this collection includes over eighty titles that were digitized with special funding from the chancellor and the university library of the university of north carolina at chapel hill. the southern united states has been a distinct region since the colonial period, and its literature has developed in connection with, but also divergently from american literature as a whole. the south claims prominent and world-renowned authors, including edgar allan poe and mark twain, but also lesser known authors who created works that reflected southern attitudes and experiences. teachers of literature as well as historians and other scholars of southern culture need access to literary texts that illustrate these differences, and many of these important southern works are no longer in print and are not widely held in libraries. noting this lack of available titles, the late dr. robert bain volunteered to help documenting the american south by developing the bibliography for it. dr. bain was a faculty member at the university of north carolina at chapel hill and taught american and southern literature from 1964 until 1995. he also co-edited five scholarly works on southern writers. to prepare this bibliography of southern literature, dr. bain wrote to some fifty scholars throughout the united states who specialize in southern and american literature requesting that they nominate what they considered to be the ten most important works of southern literature published before 1920. from their responses, dr. bain compiled the bibliography on which this collection is based. he completed the list three months before his death in july 1996. with additional funding from the university library, this collection continues to grow. dr. joe flora, professor of english at unc-chapel hill, guides the expansion of this collection beyond dr. bain's original bibliography. the original texts for the \"library of southern literature\" come from the university library of the university of north carolina at chapel hill, which includes the southern historical collection, one of the largest collections of southern manuscripts in the country and the north carolina collection, the most complete printed documentation of a single state anywhere. the docsouth editorial board, composed of faculty and librarians at unc and staff from the unc press, oversees this collection and all other collections on documenting the american south.\n",
      "context\n",
      "the north american slave narratives collection at the university of north carolina contains 344 items and is the most extensive collection of such documents in the world.\n",
      "the physical collection was digitized and transcribed by students and library employees. this means that the text is far more reliable than uncorrected ocr output which is common in digitized archives.\n",
      "more information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh\n",
      "the plain text files have been optimized for use in voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. you may wish to delete these in order to focus your analysis on just the narratives.\n",
      "the .csv file acts as a table of contents for the collection and includes title, author, publication date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with voyant: http://voyant-tools.org/).\n",
      "copyright statement and acknowledgements\n",
      "with the exception of \"fields's observation: the slave narrative of a nineteenth-century virginian,\" which has no known rights, the texts, encoding, and metadata available in open docsouth are made available for use under the terms of a creative commons attribution license (cc by 4.0:http://creativecommons.org/licenses/by/4.0/). users are free to copy, share, adapt, and re-publish any of the content in open docsouth as long as they credit the university library at the university of north carolina at chapel hill for making this material available.\n",
      "if you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. send any feedback to wilsonlibrary@unc.edu.\n",
      "about the docsouth data project\n",
      "doc south data provides access to some of the documenting the american south collections in formats that work well with common text mining and data analysis tools.\n",
      "documenting the american south is one of the longest running digital publishing initiatives at the university of north carolina. it was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.\n",
      "doc south data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. we have made it easy to use tools such as voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis.\n",
      "context\n",
      "baby names for children recently born in new york city. this dataset is notable because it includes a breakdown by the ethnicity of the mother of the baby: a source of ethnic information that is missing from many other similar datasets published on state and national levels.\n",
      "content\n",
      "this dataset includes columns for the name, year of birth, sex, and mother's ethnicity of the baby. it also includes a rank column (that name's popularity relative to the rest of the names on the list).\n",
      "acknowledgements\n",
      "this data is published as-is by the city of new york.\n",
      "inspiration\n",
      "how do baby names in new york city differ from national trends?\n",
      "what names are most, more, or less popular amongst different ethnicities?\n",
      "the objective of fda regulatory programs is to assure compliance with the federal food, drug, and cosmetic act (the act). specific enforcement activities include actions to correct and prevent violations, remove violative products or goods from the market, and punish offenders. the type of enforcement activity fda uses will depend on the nature of the violation. the range of enforcement activities include issuing a letter notifying the individual or firm of a violation and requesting correction, to criminal prosecution of the individual or firm. adulteration or misbranding is usually the result of an individual failing to take steps to assure compliance with the law. such an individual may be liable for a violation of the act and, if found guilty, be subject to the penalties specified by the law.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the united states food and drug administration. you can find the most current version of the dataset here.\n",
      "inspiration\n",
      "all but two out of every thousand drug enforcement actions were voluntary recalls. does this hold for food and medical devices as well? was there anything special about the non-voluntary enforcement actions that leads the industry to largely self-police?\n",
      "context\n",
      "federal air marshals fly undercover on passenger planes and are trained to intervene in the event of a hijacking. this database contains information on 5,214 cases of misconduct committed by federal air marshals by date and field office and what discipline was meted out in response.\n",
      "content\n",
      "data covers november 2002 to february 2012. i cleaned the data to remove some extraneous columns and to add an easier \"target\" column (disciplinary results, with duplicates merged and the number of days in a suspension removed). the original \"final disposition\" columns remains unchanged. columns: field office , allegation, date case opened , final disposition.\n",
      "acknowledgements\n",
      "data gathered and distributed from the transportation security administration by propublica: https://www.propublica.org/datastore/dataset/federal-air-marshal-misconduct-database\n",
      "propublica is an independent, non-profit newsroom that produces investigative journalism in the public interest. falls under propublica data terms of use.\n",
      "inspiration\n",
      "what types of misconduct can result in suspension, or a \"slap on the wrist\"?\n",
      "what types of misconduct are ignored and occurred?\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "data set information\n",
      "this data set contains 416 liver patient records and 167 non liver patient records.the data set was collected from test samples in north east of andhra pradesh, india. 'is_patient' is a class label used to divide into groups(liver patient or not). this data set contains 441 male patient records and 142 female patient records. any patient whose age exceeded 89 is listed as being of age \"90\".\n",
      "attribute information\n",
      "age age of the patient\n",
      "gender gender of the patient\n",
      "*tot_bilirubin* total bilirubin\n",
      "*direct_bilirubin* direct bilirubin\n",
      "alkphos alkaline phosphotase\n",
      "sgpt alamine aminotransferase\n",
      "sgot aspartate aminotransferase\n",
      "*tot_proteins* total protiens\n",
      "albumin albumin\n",
      "*ag_ratio* albumin and globulin ratio\n",
      "is_patient selector field used to split the data into two sets (labeled by the experts)\n",
      "acknowledgements\n",
      "the data set has been elicit from uci machine learning repository. my sincere thanks to them.\n",
      "context\n",
      "r/place was a wildly successful april fool's joke perpetrated by reddit over the course of 72 hours april 1-3, 2017. the rules of place, quoting u/drunken_economist were:\n",
      "there is an empty canvas.\n",
      "you may place a tile upon it, but you must wait to place another.\n",
      "individually you can create something.\n",
      "together you can create something more.\n",
      "1.2 million redditors used these premises to build the largest collaborative art project in history, painting (and often re-painting) a million-pixel canvas with 16.5 million tiles in 16 colors.\n",
      "the canvas started out completely blank, and ended looking like this:\n",
      "how did that happen?\n",
      "content\n",
      "this dataset is a full time placement history for r/place over time. each record is a single move: one user changing one pixel to one of 15 different colors.\n",
      "acknowledgements\n",
      "this data was published as-is by reddit.\n",
      "inspiration\n",
      "users were heavily rate-limited in their ability to place pixels, so this dataset shows what happens when users of similar stripes \"band together\" to build something greater than themselves. with a pixel-by-pixel history, what can you tell about the relative popularity of different regions in the figure? can you use image analysis techniques to segment the image into different regions, and measure what happens to them over time?\n",
      "context\n",
      "the southern poverty law center maintains a list of publicly supported symbols of the confederacy. it is available here. there data table can be downloaded in multiple formats here\n",
      "content\n",
      "i cleaned up the data set a bit.\n",
      "removed the following columns: cartodb_id, the_geom,field_1,uid,secondary_class_for_internal_use\n",
      "changed 'unknown' dates to na\n",
      "arranged records by states and by feature_name\n",
      "removed holidays\n",
      "changed year_dedicated for the following feature names: - 'city of confederate corners' from '1860's (late)' to '1865' (source:http://www.amap1.org/images/2008%20folder/amap%20newsletter%2012-08.pdf ...says '1865-ish', so just an estimate)\n",
      "- 'confederate monument, la plaza del constitucion' from '~1880' to 1879. source: http://www.drbronsontours.com/bronsonconfederatememorial.html\n",
      "\n",
      "- changed 'confederate women fountain (women of the sixties)' from 1911-1920 to 1916. source: http://www.arkansaspreservation.com/national-register-listings/pdf/pu4770s.nr.pdf\n",
      "\n",
      "- changed 'confederate monument', 'carline county courthouse' from 'unknown (perhaps 1906: see last link in sources)' to 1906. \n",
      "source:  'a history of caroline county, virginia', by marshall wingfield, page 243 (https://books.google.com/books?id=xxvhymoh3usc&pg=pa276&lpg=pa276&dq=%22caroline+county%22+confederate+memorial+1906&source=bl&ots=qyrgtv13js&sig=ii-ka__3bhzg9wztad5vckhb3b8&hl=en&sa=x&ved=0ahukewjq-eqj-uhvahukjcykhu-aas4q6aeiuzam#v=snippet&q=monument&f=false)\n",
      "\n",
      "- changed \"to our soldiers of the confederacy\", \"king william courthouse\" from '1901-1903' to 1903. source: http://docsouth.unc.edu/commland/monument/15/ \n",
      "added rededicated column. removed rededicated values from year_dedicated column. rededicated column also includes: -remodeled, replace,reopened, and relocated monuments.\n",
      "-readopted flags\n",
      "acknowledgements\n",
      "would like to thank the splc for making the dataset that this dataset is based on available and for their interesting and important report on the history of confederate public symbols.\n",
      "context\n",
      "dataset contains list of every ufc ppv event + the estimated number of ppv sales (source: https://www.tapology.com/search/mma-event-figures/ppv-pay-per-view-buys-buyrate.\n",
      "content\n",
      "ufc's 171 and 156 are missing because those events were cancelled.\n",
      "acknowledgements\n",
      "thanks to tapology.com for providing the raw data.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "about this project\n",
      "this is my first pet project ever. in this project i'm going to do exploratory analysis on the top rated imdb movies of all time. i wanted to analyse the the box office success of a highly rated movie and its relationship with various variables like, the age, gender of user/voter. for this purpose i looked at the top 100 movies of all time on imdb. i used octoparse for scraping.\n",
      "i realized that it would be too complicated to analyse movies over various generations, because the tastes of every generation is different, so i narrowed down onto movies of this decade, i.e. movies released between 2010-2016 ) both years included, i know 2010 doesn't really fall in to this decade, but included it in order to have a decent sized data set. then i narrowed down to movies targeting a particular culture, because indian movies and american movies are quite different, and trying to predict what people like, with both kinds of movies mixed up didn't seem like a good idea.\n",
      "i excluded movies released in 2017 as it takes time for ratings to stabilize and box office reports to be finalized.\n",
      "so from this list, i further shortened the list to movies made in english, and finally ended up with a dataset of 118 movies and 55 variables. one of the most important variable, the box office collections of each movie could not be scraped due to technical difficulties, this is an area i really need help with, so if someone can contribute here, it would be great!\n",
      "objectives\n",
      "what are the goals or questions you're investigating with this project?\n",
      "visualize and analyse the ratings given by different age groups for a genre.\n",
      "visualize and analyse the ratings given by male and female groups for a genre.\n",
      "visualize and analyse the number of votes given by different age groups for a genre.\n",
      "visualize and analyse the number of votes given by male and female groups for a genre.\n",
      "visualize and analyse the box office success of a movie in u.s. with various variables.\n",
      "visualize and analyse the box office success of a movie outside the u.s. with various variables.\n",
      "visualize and analyse the overall box office success of a movie with various variables.\n",
      "revisit the project after a couple of years and see if any of the models we built have made any accurate predictions.\n",
      "get involved\n",
      "how can others contribute to this project? are there tasks that need to be done, or skills you're looking for?\n",
      "scrape data to get the u.s. box office data and non-u.s. box office data of each movie. completed (check imdb2.csv)\n",
      "integrate that data into our dataset. completed (check imdb2.csv)\n",
      "clean the data.\n",
      "scrape data to understand how much each of these movies was pirated.\n",
      "work on the objectives.\n",
      "external resources\n",
      "https://github.com/saipranava/imdb\n",
      "photo by jakob owens on unsplash\n",
      "overview\n",
      "this dataset is a wikipedia-based mr dataset called relocar, which is tailored towards locations as well as improving previous defi- ciencies in annotation guidelines. the corpus is designed to evaluate the capability of a classifier to distinguish literal, metonymic and mixed location mentions. in terms of dataset size, relocar contains 1,026 training and 1,000 test instances. the data was sampled using wikipedia’s random article api. we kept the sentences, which contained at least one of the places from a manually compiled list of countries and capitals of the world. the natural distribution of literal versus metonymic examples is approximately 80/20 so we had to discard the excess literal examples during sampling to balance the classes.\n",
      "annotation guidelines\n",
      "relocar has three classes, literal, metonymic and mixed. literal reading comprises territorial interpretations (the geographical territory, the land, soil and physical location) i.e. inanimate places that serve to point to a set of coordinates (where something might be located and/or happening) such as “the treaty was signed in italy.”, “peter comes from russia.”, “britain’s andy murray won the grand slam today.”, “us companies increased exports by 50%.”, “china’s artists are among the best in the world.” or “the reach of the transmission is as far as brazil.”.\n",
      "a metonymic reading is any location occurrence that expresses animacy (coulson and oakley, 2003) such as “jamaica’s indifference will not improve the negotiations.”, “sweden’s budget deficit may rise next year.”. the following are other metonymic scenarios: a location name, which stands for any persons or organisations associated with it such as “we will give aid to afghanistan.”, a location as a product such as “i really enjoyed that delicious bordeaux.”, a location posing as a sports team “india beat pakistan in the playoffs.”, a governmental or other legal entity posing as a location “zambia passed a new justice law today.”, events acting as locations “vietnam was a bad experience for me”.\n",
      "the mixed reading is assigned in two cases: either both readings are invoked at the same time such as in “the central european country of slovakia recently joined the eu.” or there is not enough context to ascertain the reading i.e. both are plausible such as in “we marvelled at the art of ancient mexico.”. in difficult cases such as these, the mixed class is assigned.\n",
      "acknowledgements\n",
      "this dataset was collected by milan gritta, mohammad taher pilehvar, nut limsopatham and nigel collier. it is redistributed here under a gnu general public license. if you use this data in your work, please cite the following paper:\n",
      "gritta, m., pilehvar, m. t., limsopatham, n., & collier, n. 2017. “vancouver welcomes you! minimalist location metonymy resolution”. proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: long papers) url: http://aclweb.org/anthology/p/p17/p17-1115.pdf\n",
      "in addition, code to replicate the paper’s results can be found here.\n",
      "overview\n",
      "six degrees of francis bacon is a digital reconstruction of the early modern social network (emsn). historians and literary critics have long studied the way that early modern people associated with each other and participated in various kinds of formal and informal groups. by data-mining existing scholarship that describes relationships between early modern persons, documents, and institutions, we have created a unified, systematized representation of the way people in early modern england were connected.\n",
      "contents\n",
      "this dataset contains information on 171419 relationships between 15824 early modern figures (including, of course, the titular francis bacon). the individuals have been separated into 109 distinct labelled groups and the relationships fall under one of 64 labelled categories.\n",
      "this dataset contains the following files:\n",
      "sdfb_groups.csv: a list of the groups of individuals in the dataset\n",
      "sdfb_people.csv: a list of all individuals in the dataset\n",
      "sdfb_relationships_100000000_100020000.csv: this and the following “relationships” files contain information on relationships between specific individuals\n",
      "sdfb_relationships_100020001_100040000.csv\n",
      "sdfb_relationships_100040001_100060000.csv\n",
      "sdfb_relationships_100060001_100080000.csv\n",
      "sdfb_relationships_100080001_100100000.csv\n",
      "sdfb_relationships_100100001_100120000.csv\n",
      "sdfb_relationships_100120001_100140000.csv\n",
      "sdfb_relationships_100140001_100160000.csv\n",
      "sdfb_relationships_100160001_100180000.csv\n",
      "sdfb_relationships_greater_than_100180000.csv\n",
      "sdfb_relationshiptypes.csv: the types of relationships found in the database\n",
      "table-of-contents.csv: a table of contents for the files\n",
      "acknowledgements\n",
      "please cite six degrees of francis bacon as follows:\n",
      "sdfb team, six degrees of francis bacon: reassembling the early modern social network. www.sixdegreesoffrancisbacon.com (august 29, 2017).\n",
      "inspiration\n",
      "this dataset is an excellent place to explore network analysis and visualization. each individual is a node, and each relationship an edge.\n",
      "can you visualize this social network?\n",
      "who is the most central figure in this social network?\n",
      "do different groups have different degrees of connectivity? plexity?\n",
      "context:\n",
      "this dataset contains pending and registered trademark text data (no drawings/images) to include word mark, serial number, registration number, filing date, registration date, goods and services, classification number(s), status code(s), design search code(s), pseudo mark(s) from the april 7, 1884 - present. the file format .json, converted from extensible markup language (xml) in accordance with the u.s. trademark applications version 2.0 document type definition (dtd).\n",
      "content:\n",
      "this dataset is made up of one .json file with the following fields. (note that the documentation was written for an xml file, which was subsequently converted into .json). the following is a description of some of the fields; full documentation can be found in the applications-documentation.pdf file.\n",
      "the trademark-applications-daily element is mandatory and will occur one time identifying the beginning of a daily application process. the trademark applications daily element will contain one occurrence of the version and data processed indicator elements, and zero or more occurrences of the file segments element between the trademark-applications-daily start and trademark-applications-daily end tags.\n",
      "the version element is mandatory and will occur one time between the version start and version end tags identifying the version number and version date of the trademark applications dtd. the version element will contain one occurrence of the version number and version date elements.\n",
      "the version-no element is mandatory and will occur one time between the version-no start and version-no end tags containing the version number of the trademark applications dtd. example: the first production version will be 1.0\n",
      "the version-date element is mandatory and will occur one time between the version-date start and version-date end tags containing the date of the trademark applications dtd, an 8-position numeric in the format yyyymmdd.\n",
      "application information section\n",
      "the application-information element is optional and will occur zero or one times between the application-information start and application-information end tags containing the daily trademark applications data.\n",
      "the data-available-code element is optional and will occur zero or one times when trademark data is not present. the data-available-code start tag and the data-available-code end tag will be present and contain a one-position “n” indicating that trademark data is not present. this element will not be present when data is available.\n",
      "the file-segments element is optional and will occur zero or more times between the file-segments start and file- segments end tags identifying the beginning of the trademark applications file. the file-segments element will contain zero or more occurrences of the file segment and action keys elements.\n",
      "the file-segment element is optional and will occur zero or more times between the file-segment start and file-segment end tags containing the file segment text data, a four-position alphabetic field identifying the type of data in the trademark daily xml process. the trademark applications file-segment will always have a constant of “trmk”. the”trmk” file segment contains new trademark data and modifications made to existing trademark data.\n",
      "the action-keys element is optional and will occur zero or more times between the action-keys start and action-keys end tags. the action keys element will contain zero or more occurrences of the action key and case file elements.\n",
      "the action-key element is optional and will occur zero or more times between the action-key start and action-key end tags containing the key action, a two-position alphanumeric field. the contents of the action key will be one of the following values for marks appearing in the trademark official gazette (og).\n",
      "case file section\n",
      "the case file section and case-file end tags containing the following elements in the sequence as follows:\n",
      "the serial-number element is mandatory and will occur one time between the serial-number start and serial-number end tags containing the serial number, an eight-position numeric field consisting of the following:\n",
      "serial number\n",
      "position 1-2 will contain a series code:\n",
      "series code, filing date\n",
      "70, 1881 – 03/31/05\n",
      "71, 04/01/05 – 12/31/55\n",
      "72, 01/01/56 – 08/31/73\n",
      "73, 09/01/73 – 11/15/89\n",
      "74, 11/16/89 – 09/30/95\n",
      "75, 10/01/95 – 03/19/2000\n",
      "76, 03/20/2000 – present - (76 - will be used for all paper filed applications.)\n",
      "78, 03/20/2000 – present - (78 - will be used for electronically filed (e-teas) applications.)\n",
      "position 3-8 will be a six-position serial number right justified with leading zeros.\n",
      "note: when the serial-number begins with 80, 81 or 82, the actual serial number is unknown and the serial number is created by placing an “8” before the seven-digit registration number. if the registration number is less than seven digits in length, it is preceded by zeros.\n",
      "when the serial-number begins with 89, non-registration data consists of information entered in the database because of treaty obligations, u.s. statutes, or other requirements.\n",
      "the registration-number element is optional and will occur zero or one times between the registration-number start and registration-number end tags containing the registration number, a seven-position numeric field right justified with leading zeros. note: if a mark does not contain a registration number, the registration number element will contain zeros.\n",
      "the transaction-date element is optional and will occur zero or one times between the transaction-date start and transaction-date end tags containing the transaction date, an eight position date in the format yyyymmdd. the transaction date is the date of the trademark daily xml process for action key entries.\n",
      "case file header section\n",
      "the case-file-header element is optional and occurs zero or one times between the case-file-header start and case-file-header end tags identifying the first record sequence of a trademark application document, which contains the equivalent of a twtf/genx record. each case file header element will contain the following optional elements. note: any of the following elements that do not have the required date will contain zeros.\n",
      "the filing-date element is optional and occurs zero or one times between the filing-date start and filing-date end tags containing the filing date, an eight position date in the format yyyymmdd, which is the date on which a statutorily complete trademark application is filed at the uspto.\n",
      "the registration date element is optional and occurs zero or one times between the registration-date start and registration-date end tags containing the registration date, an eight-position date in the format yyyymmdd that identifies the date the mark was registered.\n",
      "the status-code element is optional and occurs zero or one times between the status-code start and status-code end tags containing the status code, a three position numeric field which identifies the status of the mark.\n",
      "the status codes can be found in the trademark-status-codes file.\n",
      "the status-date element is optional and occurs zero or one times between the status-date start and status-date end tags containing the status date, an eight-position date in the format yyyymmdd, which is the date on which the current status was reported to the system.\n",
      "the mark-identification element is optional and occurs zero or one times between the mark-identification start and mark-identification end tags containing the mark-1-lin, a variable length alphanumeric field containing the characters of the actual mark.\n",
      "the mark-drawing-code element is optional and occurs zero or one times between the mark-drawing-code start and mark-drawing-code end tags containing the mark drawing code, a four-position alphanumeric field. the first position identifies the physical characteristics of the mark.\n",
      "the published-for-opposition-date element is optional and occurs zero or one times between the published-for-opposition-date start and published-for-opposition-date end tags containing the date published for opposition, an eight-position date in the format yyyymmdd, which is the date that the mark published for opposition in the official gazette.\n",
      "the amend-to-register-date element is optional and occurs zero or one times between the amend-to-register-date start and amend-to-register end tags containing the amended to register date, an eight-position date in the format of yyyymmdd. this element contains the date on which a case is entered as an amendment to a register.\n",
      "the abandonment-date element is optional and occurs zero or one times between the abandonment-date start and abandonment-date end tags containing the date abandoned, an eight-position date in the format yyyymmdd, which is the date that the mark is abandoned.\n",
      "the cancellation-code element is a optional and occurs zero or one times between the cancellation-code start and cancellation-code end tags containing the cancellation code, a one-position alphanumeric which identifies the section of the statute under which an entire registration is being cancelled or under which some classes in a multiple class registration are being cancelled.\n",
      "cc, definition\n",
      "0, no entry\n",
      "1, section 7(d) entire registration\n",
      "2, section 8 entire registration\n",
      "3, section 18 entire registration\n",
      "4, section 24 entire registration\n",
      "5, section 37 entire registration\n",
      "6, entire registration inadvertently issued\n",
      "7, inadvertently issued-entire registration restored to pendency\n",
      "a, section 7 (d ) - class(es) in multiple class registration\n",
      "b, section 8 - class(es) in multiple class registration\n",
      "c, section 18 - class(es) in multiple class registration\n",
      "d, section 24 - class(es) in multiple class registration\n",
      "e, section 37 - class(es) in multiple class registration\n",
      "the cancellation-date element is optional and occurs zero or one times between the cancellation-date start and cancellation-date end tags containing the cancellation date, an eight-position date in the format yyyymmdd, which is the date that the cancellation of the entire registration was recorded.\n",
      "the republished 12c-date element is optional and occurs zero or one times between the republished-12c-date start and republished-12c-date end tags containing the date published under section 12(c), an eight-position date in the format yyyymmdd, which is the date of publication under section 12(c).\n",
      "the domestic-representative-name element is optional and occurs zero or one times between the domestic-representative-name start and domestic-representative-name end tags and contains the domestic representative information for the application.\n",
      "the attorney-docket-number element is optional and occurs zero or one times between the attorney-docket-number start and attorney-docket-number end tags containing the attorney docket number, a twelve-position number containing the reference or identification number of a case as assigned and used in the office of the attorney filing the application.\n",
      "the attorney-name element is optional and occurs zero or one times between the attorney-name start and attorney-name end tags and contains the attorney information from the ownx record.\n",
      "the principal-register-amended-indicator element is optional and occurs zero or one times between the principal-register-amended-in start and principal-register-amended-in end tags containing the flag amended to the principal register, a one-position alphabetic indicating the register has been amended for an application on the supplemental register. a “t” in this field indicates an amendment to the principal register. an “f” in this field indicates no amendment to the principal register.\n",
      "the supplemental-register-amended-indicator element is optional and occurs zero or one times between the supplemental-register-amended-in start and supplemental-register-amended-in end tags containing the flag amended to the supplemental register, a one-position field indicating the register has been amended for an application on the principal register. a “t” in this field indicates an amendment to the supplemental register. an “f” in this field indicates no amendment to the supplemental register.\n",
      "prior registration applications section\n",
      "the prior-registration-applications element is optional and occurs zero or one times between the prior-registration-applications start and prior-registration-applications end tags, which contains the equivalent of all twtf/prus records. each prior registration applications element will contain the optional other related in and prior registration application elements.\n",
      "the other-related-indicator element is optional and occurs zero or more times between the other-related-in start and other-related-in end tags containing the and others, a one-position alphabetic field. a “t” in this field would indicate that the words “and others” appears in conjunction with a list of prior registrations that are claimed as being related to this mark. an “f” would indicate that the statement is not present.\n",
      "acknowledgements:\n",
      "this dataset is provided by the united states government and is in the public domain. daily uploads of this dataset are available online here.\n",
      "context:\n",
      "311 calls are a good snapshot of public complaints, and provide interesting analytical data to predict future resource allocation by policymakers.\n",
      "content:\n",
      "date, time, location, description, handling office, and status are included.\n",
      "acknowledgements:\n",
      "this dataset was compiled by the city of austin and published on google cloud public data.\n",
      "inspiration:\n",
      "any notable trends in location or volume of certain calls?\n",
      "can you predict future 311 calls?\n",
      "dataset description\n",
      "use this dataset with bigquery you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too.\n",
      "context:\n",
      "free law project seeks to provide free access to primary legal materials, develop legal research tools, and support academic research on legal corpora. we work diligently with volunteers to expand our efforts at building an open source, open access, legal research ecosystem. currently free law project sponsors the development of courtlistener, juriscraper, and recap. courtlistener is a free legal research website containing millions of legal opinions from federal and state courts. with courtlistener, lawyers, journalists, academics, and the public can research an important case, stay up to date with new opinions as they are filed, or do deep analysis using our raw data.\n",
      "content:\n",
      "this dataset contains the corpus of all supreme court opinions and some additional supporting information. corpus was initially collated as individual json objects here; these json objects were joined into a single csv. note that the actual opinions column is rendered in html. citations are links to additional courtlistener api calls that are not included in this corpus.\n",
      "acknowledgements:\n",
      "courtlistener scraped and assembled this and other similar legal datasets for public use.\n",
      "inspiration:\n",
      "can you join this data with other supreme court data like here and here?\n",
      "sentiment analysis would be particularly illuminating.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this dataset provides user vote data on which video from a pair of videos was funnier. youtube comedy slam was a discovery experiment running on youtube 2011 and 2012. in the experiment, pairs of videos were shown to users and the users voted for the video that they found funniest.\n",
      "content\n",
      "the datasets includes roughly 1.7 million votes recorded chronologically. the first 80% are provided here as the training dataset and the remaining 20% as the testing dataset.\n",
      "each row in this text file represents one anonymous user vote and there are three comma-separated fields.\n",
      "the first two fields are youtube video ids.\n",
      "the third field is either 'left' or 'right'.\n",
      "left indicates the first video from the pair was voted to be funnier than the second. right indicates the opposite preference.\n",
      "acknowledgements\n",
      "sanketh shetty, 'quantifying comedy on youtube: why the number of o's in your lol matter,' google research blog, https://research.googleblog.com/2012/02/quantifying-comedy-on-youtube-why.html.\n",
      "dataset was downloaded from uci ml repository: https://archive.ics.uci.edu/ml/datasets/youtube+comedy+slam+preference+data\n",
      "inspiration\n",
      "predict which videos are going to be funny!\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 42,000 hotels) that was created by extracting data from cleartrip.com, a leading travel portal in india.\n",
      "content\n",
      "analyses can be performed on the hotel description, facilities and various ratings to name a few.\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 2.4 million profiles) that was created by extracting data from avvo.com.\n",
      "content\n",
      "this dataset has following fields:\n",
      "address\n",
      "categories\n",
      "description\n",
      "image_count\n",
      "name\n",
      "payment_option\n",
      "phone\n",
      "profile_id\n",
      "profile_url\n",
      "video_count\n",
      "website\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "analysis can be performed on the profile category.\n",
      "commercial paper, in the global financial market, is an unsecured promissory note with a fixed maturity of not more than 270 days.\n",
      "commercial paper is a money-market security issued (sold) by large corporations to obtain funds to meet short-term debt obligations (for example, payroll), and is backed only by an issuing bank or company promise to pay the face amount on the maturity date specified on the note. since it is not backed by collateral, only firms with excellent credit ratings from a recognized credit rating agency will be able to sell their commercial paper at a reasonable price. commercial paper is usually sold at a discount from face value, and generally carries lower interest repayment rates than bonds due to the shorter maturities of commercial paper. typically, the longer the maturity on a note, the higher the interest rate the issuing institution pays. interest rates fluctuate with market conditions, but are typically lower than banks' rates.\n",
      "commercial paper – though a short-term obligation – is issued as part of a continuous rolling program, which is either a number of years long (as in europe), or open-ended (as in the u.s.)\n",
      "acknowledgements\n",
      "this dataset was made available by the federal reserve. you can find the original dataset, updated daily, here.\n",
      "inspiration\n",
      "based solely on this dataset, when would you say the great recession financial crisis started? how does that compare with media reports?\n",
      "context\n",
      "the averaged_perceptron_tagger.zip contains the pre-trained english [part-of-speech (pos]](https://en.wikipedia.org/wiki/part_of_speech) tagger in nltk.\n",
      "the nltk.tag.averagedperceptrontagger is the default tagger as of nltk version 3.1. the model was trained on on sections 00-18 of the wall street journal sections of ontonotes 5\n",
      "the original implementation comes from matthew honnibal, it outperforms the predecessor maximum entropy pos model in nltk.\n",
      "the version from textblob was ported over to nltk in pull-request #122.\n",
      "acknowledgements\n",
      "credit goes to matthew honnibal.\n",
      "the reimplementation in textblob by steven loria\n",
      "re-reimplementation in nltk by long duong\n",
      "transaction data gives numbers of applications for first registrations, leases, transfers of part, dealings, official copies and searches lodged with hm land registry by account holders in the preceding month. the information is divided into data showing all applications lodged, transactions for value, by region and local authority district. transactions for value include freehold and leasehold sales.\n",
      "the data published on this page gives you information about the number and types of applications. the data reflects the volume of applications lodged by customers using an hm land registry account number on their application form. the data does not include applications that are not yet completed, or were withdrawn.\n",
      "content\n",
      "this dataset has been altered from its original format. specifically, the monthly files have been aggregated and columns whose names changed over time have been merged to use the current title. some acronyms that will be helpful to know while reading the column names, per the documentation:\n",
      "acronym title description\n",
      "dfl dispositionary first lease an application for the registration of a new lease granted by the proprietor of registered land\n",
      "dlg dealing an application in respect of registered land. this includes transfers of title, charges and notices\n",
      "fr first registration an application for a first registration of land both freehold and leasehold. for leasehold this applies when the landlord’s title is not registered\n",
      "tp transfer of part an application to register the transfer of part of a registered title\n",
      "os(w) search of whole an application to protect a transaction for value, such as purchase, lease or charge for the whole of a title\n",
      "os(p) search of part an application to protect a transaction for value, such as purchase, lease or charge for part of a title\n",
      "os(npw) non-priority search of whole an application to search the whole of the register without getting priority\n",
      "os(npp) non-priority search of part an application to search a part of the register without getting priority\n",
      "oc1 official copy an application to obtain an official copy of a register or title plan represents a true record of entries in the register and extent of the registered title at a specific date and time. the data includes historical editions of the register and title plan where they are kept by the registrar in electronic form\n",
      "oc2 official copy of a deed or document an application to obtain a copy of a document referred to in the register or relates to an application. this includes correspondence, surveys, application forms and emails relating to applications that are pending, cancelled or completed\n",
      "sim search of the index map an application to find out whether or not land is registered and, if so, to obtain the title number\n",
      "acknowledgements\n",
      "this data was kindly released by hm land registry under the open government license 3.0. you can find their current release here.\n",
      "inspiration\n",
      "-what does this dataset tell us about the hm land registry's records of housing prices paid? are searches a leading indicator of price changes?\n",
      "the code of federal regulations (cfr) is the codification of the general and permanent rules and regulations (sometimes called administrative law) published in the federal register by the executive departments and agencies of the federal government of the united states.\n",
      "the 50 subject matter titles contain one or more individual volumes, which are updated once each calendar year, on a staggered basis. the annual update cycle is as follows: titles 1-16 are revised as of january 1; titles 17-27 are revised as of april 1; titles 28-41 are revised as of july 1; and titles 42-50 are revised as of october 1. each title is divided into chapters, which usually bear the name of the issuing agency. each chapter is further subdivided into parts that cover specific regulatory areas. large parts may be subdivided into subparts. all parts are organized in sections, and most citations to the cfr refer to material at the section level.\n",
      "the cfr is published in multiple formats by the us government publishing office. you can find the latest version of the xml format here: http://www.gpo.gov/fdsys/bulkdata/cfr.\n",
      "context\n",
      "iclr (international conference on learning representations) is a premier machine learning conference. unlike the other two flagship machine learning conferences icml and nips, iclr chooses a single-blind public review process in which the reviews and their rebuttals are both carried out transparently and in the open. this dataset was created by crawling the public iclr 2017 paper review site. it seems iclr is going double-blind from 2018, so my guess is that authors will remain anonymous during the review process. so, this dataset is unique because it captures a public academic review process with academic affiliations and all paper decisions including rejections.\n",
      "content\n",
      "the dataset consists of two csv files:\n",
      "iclr2017_papers.csv: this file has a row per submission. it includes the paper title, authors, author conflicts, abstracts, tl;dr (a simplified abstract), and final decision (accept/oral, accept/poster, accept/invitetoworkshop, reject). each row has a unique identifier key called the \"paper_id.\"\n",
      "iclr2017_conversations.csv: this file has a row per textual review, rebuttal, or comment. it is related to the previous papers dataset using the secondary key \"paper_id.\" all rows talking about a single paper share the same \"paper_id.\" the conversations associated with each paper can be thought of as a forest. each tree in the forest begins with a review followed by rebuttals and further comments/conversation. each such textual entry composed by an individual is listed in its own row. the nodes of the tree are connected using the fields \"child_id\" and \"parent_id\" which can be used to construct the entire conversation hierarchy.\n",
      "acknowledgements\n",
      "all rights for abstracts rest with the paper authors. reproduction of abstracts here is solely for purposes of research. thanks to the authors of beautiful soup 4 python package which considerably simplified the process of curating this dataset.\n",
      "inspiration\n",
      "this dataset was created to understand gender disparities in paper submissions and acceptances. annotating each author with a binary gender is a pending task. the dataset can also be used to model communication processes employed in negotiation, persuasion, and decision-making. another use of this dataset could be in modeling and understanding textual time-series data.\n",
      "context\n",
      "this dataset contains crops production and yield over the years in argentina. data is provided by province and district for each seed or harvest campaign from 1969 to 2017.\n",
      "content\n",
      "sup. sembrada (ha) --- hectares sown\n",
      "sup. cosechada (ha) --- hectares harvested\n",
      "produccion (tn) --- tonnes produced\n",
      "rendimiento (kg/ha) --- harvest performance\n",
      "acknowledgements\n",
      "this dataset was kindly made publicly available by datos argentina under the odbl license.\n",
      "photo by benjamin davies on unsplash.\n",
      "inspiration\n",
      "my data science adventure start here. my objective is to use this dataset together with kaggle kernels as a starting point in my learning process.\n",
      "context\n",
      "this dataset contains every line from every season of the hbo tv show game of thrones.\n",
      "content\n",
      "each season has one json file. in eachjson file there is a key for each episode and each episode is further mapped at a dialogue level.\n",
      "inspiration\n",
      "the idea is to use this data set to see if one can create a summary of what transpired in each episode or season.\n",
      "context\n",
      "in commercial agriculture it is common practice to retain so-called crop residue (agricultural product left on the field after a harvest) on planting fields. this is a commonly recommended practice in conservation agriculture.\n",
      "this dataset is a measurement of the adoption and impact of the methodology for a selection of agricultural fields. six different crop residue coverage measurement methods are included: i) interviewee (respondent) estimation; ii) enumerator estimation visiting the field; iii) interviewee with visual-aid without visiting the field; iv) enumerator with visual-aid visiting the field; v) field picture collected with a drone and analyzed with image-processing methods and vi) satellite picture of the field analyzed with remote sensing methods.\n",
      "content\n",
      "this dataset is a csv file with a selection of characteristics about fields included in the sample.\n",
      "acknowledgements\n",
      "this dataset was created as part of the following study: \"on the ground or in the air? a methodological experiment on crop residue cover measurement in ethiopia\".\n",
      "inspiration\n",
      "what are the characteristics of the ethiopian farmers sampled in this dataset? how well do they follow conservation practices?\n",
      "context\n",
      "there are no any data-sets for distinguishing paddy plants from other weeds. this data-set was taken from different fields from sri lanka.\n",
      "content\n",
      "this data-set contains 1200 images of paddy and other weeds.\n",
      "acknowledgements\n",
      "this data can be be used for any sort of machine learning projects. use this for learning purposes only.\n",
      "inspiration\n",
      "this data-set was developed for semester project of the university.\n",
      "a sample of betfair data, normally available to those who spend a lot of money wagering. horse races only. other sports are available at https://www.kaggle.com/zygmunt/betfair-sports.\n",
      "see http://data.betfair.com/ for a description.\n",
      "the file has 688184 data rows. it is 143 mb uncompressed.\n",
      "columns:\n",
      "event_id\n",
      "country\n",
      "full_description\n",
      "course\n",
      "scheduled_off\n",
      "event\n",
      "actual_off\n",
      "selection\n",
      "settled_date\n",
      "odds\n",
      "latest_taken (when these odds were last matched on the selection)\n",
      "first_taken (when these odds were first matched on the selection)\n",
      "in_play (ip - in-play, pe - pre-event, ni - event did not go in-play)\n",
      "number_bets (number of individual bets placed)\n",
      "volume_matched (sums the stakes of both back and lay bets)\n",
      "sports_id\n",
      "selection_id\n",
      "win_flag (1 if the selection was paid out as a full or partial winner, 0 otherwise)\n",
      "context\n",
      "i'm trying to make a choropleth map over time of home sale prices by block in brooklyn for the last 15 years to visualize gentrification. i have the entire dataset for all 5 boroughs of new york, but am starting with brooklyn.\n",
      "content and acknowledgements\n",
      "primary dataset is the nyc housing sales data found in this link: http://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page\n",
      "the data in all the separate excel spreadsheets for 2003-2017 was merged via vba scripting in excel and further cleaned & de-duped in r\n",
      "additionally, in my hunt for shapefiles i discovered these wonderful shapefiles from nycpluto: https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page\n",
      "i left joined it by \"block\" & \"lot\" onto the primary data frame, but 25% of the block/lot combo's ended up not having a corresponding entry in the pluto shapefile and are nas.\n",
      "note that as in other uploaded datasets of nyc housing on kaggle, many of these transactions have a sale_price of $0 or only a nominal amount far less than market value. these are likely property transfers to relatives and should be excluded from any analysis of market prices.\n",
      "inspiration\n",
      "can you model brooklyn home prices accurately?\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "quotables\n",
      "a corpus of quotes.\n",
      "quotable quote\n",
      "\"i fear not the man who has practiced 10,000 kicks once, but i fear the man who has practiced one kick 10,000 times.\" - bruce lee\n",
      "content\n",
      "two-columns file: authors are in the first column, quotes in the second, separated by a tab\n",
      "statistics:\n",
      "36,165 quotes with\n",
      "878,450 words from\n",
      "2,297 people\n",
      "acknowledgements\n",
      "originally from https://github.com/alvations/quotables\n",
      "dataset image comes from mona eendra\n",
      "google taxonomy\n",
      "this product taxonomy lists seemed to be used by merchants in tagging their products on google shopping, see https://support.google.com/merchants/answer/6324436?hl=en\n",
      "these lists were also used in the semeval taxonomy evaluation tasks:\n",
      "http://alt.qcri.org/semeval2015/task17/\n",
      "http://alt.qcri.org/semeval2016/task13/\n",
      "content\n",
      "contains the product category lists for:\n",
      "czech\n",
      "danish\n",
      "german (swiss / germany)\n",
      "english (australian / british / american)\n",
      "spanish (spanish)\n",
      "french (swiss / france)\n",
      "italian (swiss / italy)\n",
      "japanese\n",
      "dutch\n",
      "norwegian\n",
      "polish\n",
      "portuguese (brazillian)\n",
      "russian\n",
      "swedish\n",
      "turkish\n",
      "chinese\n",
      "this post might be helpful for others too: https://www.en.advertisercommunity.com/t5/google-shopping-and-merchant/taxonomy-list-countries-some-missing/td-p/599656\n",
      "acknowledgements\n",
      "the individual google product taxonomy files came from\n",
      "http://www.google.com/basepages/producttype/taxonomy-with-ids.<language_code>-<country_code>.txt\n",
      "dataset image comes from edho pratama\n",
      "disclaimer\n",
      "i am not affiliated to google or own these product categories lists. if this offends any copyrights/licenses, please request for me to remove it.\n",
      "context\n",
      "a broad-coverage corpus such as the human language project envisioned by abney and bird (2010) would be a powerful resource for the study of endangered languages. seedling was created as a seed corpus for the human language project to cover a broad range of languages (guy et al. 2014).\n",
      "taus (translation automation user society) also see the importance of the human language project in the context of keeping up with the demand for capacity and speed for translation. taus' definition of the human language project can be found on https://www.taus.net/knowledgebase/index.php/human_language_project\n",
      "a detailed explanation of how to use the corpus can be found on https://github.com/alvations/seedling\n",
      "content\n",
      "the seedling corpus on this repository includes the data from:\n",
      "odin: online database of interlinear text\n",
      "omniglot: useful foreign phrases from www.omniglot.com\n",
      "udhr: universal declaration of human rights\n",
      "acknowledgements\n",
      "citation:\n",
      "guy emerson, liling tan, susanne fertmann, alexis palmer and michaela regneri . 2014. seedling: building and using a seed corpus for the human language project. in proceedings of the use of computational methods in the study of endangered languages (computel) workshop. baltimore, usa.\n",
      "@inproceedings{seedling2014,\n",
      "  author    = {guy emerson, liling tan, susanne fertmann, alexis palmer and michaela regneri},\n",
      "  title     = {seedling: building and using a seed corpus for the human language project},\n",
      "  booktitle = {proceedings of the use of computational methods in the study of endangered languages (computel) workshop},\n",
      "  month     = {june},\n",
      "  year      = {2014},\n",
      "  address   = {baltimore, usa},\n",
      "  publisher = {association for computational linguistics},\n",
      "  pages     = {},\n",
      "  url       = {}\n",
      "}\n",
      "references:\n",
      "steven abney and steven bird. 2010. the human language \n",
      "project: building a universal corpus of the world’s languages. \n",
      "in proceedings of the 48th annual meeting of the association \n",
      "for computational linguistics, pages 88–97.\n",
      "\n",
      "sime ager. omniglot - writing systems and languages \n",
      "of the world. retrieved from www.omniglot.com.\n",
      "\n",
      "william d lewis and fei xia. 2010. developing odin: a multilingual \n",
      "repository of annotated language data for hundreds of the world’s \n",
      "languages. literary and linguistic computing, 25(3):303–319.\n",
      "\n",
      "un general assembly, universal declaration of human rights, \n",
      "10 december 1948, 217 a (iii), available at: \n",
      "http://www.refworld.org/docid/3ae6b3712c.html \n",
      "[accessed 26 april 2014]\n",
      "inspiration\n",
      "this corpus was created in a span a semester in saarland university by a linguist, a mathematician, a data geek and two amazing mentors from the coli department. it wouldn't have been possible without the cross-disciplinary synergy and the common goal we had.\n",
      "expand/explore the human language project.\n",
      "go to the field and record/document their language. make them computationally readable.\n",
      "grow the seedling!\n",
      "o censo escolar é um levantamento de dados estatístico-educacionais de âmbito nacional realizado todos os anos e coordenado pelo inep. ele é feito com a colaboração das secretarias estaduais e municipais de educação e com a participação de todas as escolas públicas e privadas do país.\n",
      "trata-se do principal instrumento de coleta de informações da educação básica, que abrange as suas diferentes etapas e modalidades: ensino regular (educação infantil e ensinos fundamental e médio), educação especial e educação de jovens e adultos (eja). o censo escolar coleta dados sobre estabelecimentos, matrículas, funções docentes, movimento e rendimento escolar.\n",
      "essas informações são utilizadas para traçar um panorama nacional da educação básica e servem de referência para a formulação de políticas públicas e execução de programas na área da educação, incluindo os de transferência de recursos públicos como merenda e transporte escolar, distribuição de livros e uniformes, implantação de bibliotecas, instalação de energia elétrica, dinheiro direto na escola e fundo de manutenção e desenvolvimento da educação básica e de valorização dos profissionais da educação (fundeb).\n",
      "além disso, os resultados obtidos no censo escolar sobre o rendimento (aprovação e reprovação) e movimento (abandono) escolar dos alunos do ensino fundamental e médio, juntamente com outras avaliações do inep (saeb e prova brasil), são utilizados para o cálculo do índice de desenvolvimento da educação básica (ideb), indicador que serve de referência para as metas do plano de desenvolvimento da educação (pde), do ministério da educação.\n",
      "para saber mais sobre o censo escolar: http://portal.inep.gov.br/basica-censo\n",
      "apresentados em formato ascii, os microdados são acompanhados de inputs, ou seja, canais de entrada para leitura dos arquivos por meio da utilização dos softwares sas e spss.\n",
      "os microdados passaram a ser estruturados em formato csv (comma-separated values), e seus dados estão delimitados por pipe ( | ), de modo a garantir que praticamente qualquer software estatístico, inclusive open source, consiga importar e carregar as bases de dados.\n",
      "devido à amplitude de nossas bases, os arquivos foram divididos por região geográfica (norte, nordeste, sudeste, sul e centro-oeste), tanto para as variáveis de matrículas, quanto para as de docentes.\n",
      "data set information: this data arises from a large study to examine eeg correlates of genetic predisposition to alcoholism. it contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 hz (3.9-msec epoch) for 1 second.\n",
      "there were two groups of subjects: alcoholic and control. each subject was exposed to either a single stimulus (s1) or to two stimuli (s1 and s2) which were pictures of objects chosen from the 1980 snodgrass and vanderwart picture set. when two stimuli were shown, they were presented in either a matched condition where s1 was identical to s2 or in a non-matched condition where s1 differed from s2.\n",
      "attribute information\n",
      "each trial is stored in its own file and will appear in the following format.\n",
      "trial number sensor position sample num sensor value subject identifier matching condition channel name time\n",
      "0 fp1 0 -8.921 a s1 obj 0 co2a0000364 0\n",
      "0 af8 87 4.14 a s1 obj 33 co2a0000364 0.33\n",
      "the columns of data are:\n",
      "the trial number,\n",
      "sensor position,\n",
      "sample number (0-255),\n",
      "sensor value (in micro volts),\n",
      "subject identifier(alcoholic(a) or control (c)),\n",
      "matching condition(a single object shown (s1 obj), object 2 shown in a matching condition (s2 match), and object 2 shown in non matching condition (s2 nomatch)),\n",
      "channel number(0-63),\n",
      "name(a serial code assigned to each subject),\n",
      "time(inverse of sample num measured in seconds))\n",
      "acknowledgements\n",
      "there are no usage restrictions on this data.\n",
      "acknowledgments for this data should made to henri begleiter at the neurodynamics laboratory at the state university of new york health center at brooklyn.\n",
      "you can check out more info about it on: https://archive.ics.uci.edu/ml/datasets/eeg+database\n",
      "context:\n",
      "it is now a common practice to compare models of human language processing by predicting participant reactions (such as reading times) to corpora consisting of rich naturalistic linguistic materials. however, many of the corpora used in these studies are based on naturalistic text and thus do not contain many of the low-frequency syntactic constructions that are often required to distinguish processing theories. the corpus includes self-paced reading time data for ten naturalistic stories.\n",
      "content:\n",
      "this is a corpus of naturalistic stories meant to contain varied, low-frequency syntactic constructions. there are a variety of annotations and psycholinguistic measures available for the stories. the stories in with their various annotations are coordinated around the file words.tsv, which specifies a unique code for each token in the story under a variety of different tokenization schemes. for example, the following lines in words.tsv cover the phrase the long-bearded mill owners.:\n",
      "1.54.whole the\n",
      "1.54.word the\n",
      "1.54.1 the\n",
      "1.55.whole long - bearded\n",
      "1.55.word long - bearded\n",
      "1.55.1 long\n",
      "1.55.2 -\n",
      "1.55.3 bearded\n",
      "1.56.whole mill\n",
      "1.56.word mill\n",
      "1.56.1 mill\n",
      "1.57.whole owners .\n",
      "1.57.word owners\n",
      "1.57.1 owners\n",
      "1.57.2 .\n",
      "the first column is the token code; the second is the token itself. for example, 1.57.whole represents the token owners.and 1.57.word represents the token owners. the token code consists of three fields:\n",
      "the id of the story the token is found in,\n",
      "the number of the token in the story,\n",
      "an additional field whose value is whole for the entire token including punctuation, word for the token stripped of punctuation to the left and right, and then 1 through n for each sub-token in whole as segmented by nltk's treebankwordtokenizer.\n",
      "the various annotations (frequencies, parses, rts, etc.) should reference these codes so that we can track tokens uniformly.\n",
      "this dataset contains reading time data collected for 10 naturalistic stories. participants typically read 5 stories each. the data is contained in batch1_pro.csv and batch2_pro.csv\n",
      "all_stories.tok contains the 10 stories, with one word per row. item is the story number, zone is the region where the word falls within the story. note that some wordforms in all_stories.tok differ from those in words.tsv, reflecting typos in the spr experiment as run.\n",
      "acknowledgements:\n",
      "if you use this dataset in your work, please cite the following paper:\n",
      "futrell, r., gibson, e., tily, h., blank, i., vishnevetsky, a., piantadosi, s. t., & fedorenko, e. (2017). the natural stories corpus. arxiv preprint arxiv:1708.05763.\n",
      "a more complete version of this dataset, with additional supporting files, can be found in this github repository maintained by by richard futrell at massachusetts institute of technology, titus von der malsburg at the university of potsdam and cory shain at the ohio state university.\n",
      "inspiration:\n",
      "what words do participants tend to read more slowly or quickly?\n",
      "are certain parts of speech read more quickly or slowly?\n",
      "how much variation in reading speed is there between individuals?\n",
      "what’s the relationship between word length & reading speed?\n",
      "context:\n",
      "this dataset is trash. who in austin makes it, who takes it, and where does it go?\n",
      "content:\n",
      "data ranges 2008-2016 and includes dropoff site, load id, time of load, type of load, weight of load, date, route number, and route type (recycling, street cleaning, garbage etc).\n",
      "acknowledgements:\n",
      "this dataset was created by austin city government and hosted on google cloud platform. you can use kernels to analyze, share, and discuss this data on kaggle, but if you’re looking for real-time updates and bigger data, check out the data on bigquery, too\n",
      "inspiration:\n",
      "how much trash is austin generating?\n",
      "which are the trashiest routes? who recycles the best?\n",
      "any seasonal changes?\n",
      "try to predict trash route usage from historical trash data\n",
      "context\n",
      "we developed this platform to be able to study driver behavior in a controlled environment and perform various experiments economically and with greater flexibility.\n",
      "vehicularlab - university of luxembourg\n",
      "content\n",
      "more information will become available with the publication of the corresponding article that is currently submitted to ieee vnc 2017.\n",
      "acknowledgements\n",
      "to be acknowledged!\n",
      "inspiration\n",
      "this dataset can be used to discover differences in individuals driving behavior.\n",
      "context\n",
      "although there have been lot of studies undertaken in the past on factors affecting life expectancy considering demographic variables, income composition and mortality rates. it was found that affect of immunization and human development index was not taken into account in the past. also, some of the past research was done considering multiple linear regression based on data set of one year for all the countries. hence, this gives motivation to resolve both the factors stated previously by formulating a regression model based on mixed effects model and multiple linear regression while considering data from a period of 2000 to 2015 for all the countries. important immunization like hepatitis b, polio and diphtheria will also be considered. in a nutshell, this study will focus on immunization factors, mortality factors, economic factors, social factors and other health related factors as well. since the observations this dataset are based on different countries, it will be easier for a country to determine the predicting factor which is contributing to lower value of life expectancy. this will help in suggesting a country which area should be given importance in order to efficiently improve the life expectancy of its population.\n",
      "content\n",
      "the project relies on accuracy of data. the global health observatory (gho) data repository under world health organization (who) keeps track of the health status as well as many other related factors for all countries the data-sets are made available to public for the purpose of health data analysis. the data-set related to life expectancy, health factors for 193 countries has been collected from the same who data repository website and its corresponding economic data was collected from united nation website. among all categories of health-related factors only those critical factors were chosen which are more representative. it has been observed that in the past 15 years , there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing nations in comparison to the past 30 years. therefore, in this project we have considered data from year 2000-2015 for 193 countries for further analysis. the individual data files have been merged together into a single data-set. on initial visual inspection of the data showed some missing values. as the data-sets were from who, we found no evident errors. missing data was handled in r software by using missmap command. the result indicated that most of the missing data was for population, hepatitis b and gdp. the missing data were from less known countries like vanuatu, tonga, togo, cabo verde etc. finding all data for these countries was difficult and hence, it was decided that we exclude these countries from the final model data-set. the final merged file(final dataset) consists of 22 columns and 2938 rows which meant 20 predicting variables. all predicting variables was then divided into several broad categories:immunization related factors, mortality factors, economical factors and social factors.\n",
      "acknowledgements\n",
      "the data was collected from who and united nations website with the help of deeksha russell and duan wang.\n",
      "inspiration\n",
      "the data-set aims to answer the following key questions: 1. does various predicting factors which has been chosen initially really affect the life expectancy? what are the predicting variables actually affecting the life expectancy? 2. should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan? 3. how does infant and adult mortality rates affect life expectancy? 4. does life expectancy has positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc. 5. what is the impact of schooling on the lifespan of humans? 6. does life expectancy have positive or negative relationship with drinking alcohol? 7. do densely populated countries tend to have lower life expectancy? 8. what is the impact of immunization coverage on life expectancy?\n",
      "this is a dataset containing all grammy winners categorized by award title. i saw this data for sale yesterday for $3xx.xx, so i decided to scrape it and host it on kaggle for free. it's stored as json, with the following keys:\n",
      "\"name\"\n",
      "name of the individual receiving the award\n",
      "\"awardtype\"\n",
      "this indicates if the award was for an artist or for an album. if the \"awardtype\" == \"work\" it's for something they published. if \"awardtype\" == \"individual\" it was given to the artist specifically (ie artist of year)\n",
      "\"category\"\n",
      "this describes the award category (ie \"artist of year\", \"album of year\", etc)\n",
      "\"annualgrammy\"\n",
      "this is the # grammy it was. so last nights grammy was #60, the first was #1, etc.\n",
      "\"awardfor\"\n",
      "this is whatever is being recognized with the award. this can be an artist, a song, an album, etc. (ie \"alesia cara\" or \"transcendental\")\n",
      "here is the script used to scrape this information.\n",
      "context\n",
      "the dataset consists of data collected from heavy scania trucks in everyday usage. the system in focus is the air pressure system (aps) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. the datasets' positive class consists of component failures for a specific component of the aps system. the negative class consists of trucks with failures for components not related to the aps. the data consists of a subset of all available data, selected by experts.\n",
      "content\n",
      "the training set contains 60000 examples in total in which 59000 belong to the negative class and 1000 positive class. the test set contains 16000 examples. there are 171 attributes per record.\n",
      "the attribute names of the data have been anonymized for proprietary reasons. it consists of both single numerical counters and histograms consisting of bins with different conditions. typically the histograms have open-ended conditions at each end. for example, if we measuring the ambient temperature \"t\" then the histogram could be defined with 4 bins where:\n",
      "the attributes are as follows: class, then anonymized operational data. the operational data have an identifier and a bin id, like \"identifier_bin\". in total there are 171 attributes, of which 7 are histogram variables. missing values are denoted by \"na\".\n",
      "acknowledgements\n",
      "this file is part of aps failure and operational data for scania trucks. it was imported from the uci ml repository.\n",
      "inspiration\n",
      "the total cost of a prediction model the sum of cost_1 multiplied by the number of instances with type 1 failure and cost_2 with the number of instances with type 2 failure, resulting in a total_cost. in this case cost_1 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop, while cost_2 refer to the cost of missing a faulty truck, which may cause a breakdown. cost_1 = 10 and cost_2 = 500, and total_cost = cost_1*no_instances + cost_2*no_instances.\n",
      "can you create a model which accurately predicts and minimizes [the cost of] failures?\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"timit\" name=\"timit corpus sample\"\n",
      "     sample=\"true\"\n",
      "     license=\"this corpus sample is copyright 1993 linguistic data consortium, and is distributed under the terms of the creative commons attribution, non-commercial, sharealike license.  http://creativecommons.org/\"\n",
      "     webpage=\"http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc93s1\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the datasciencebowl covered the whole process of diagnosing lung cancer and i am to make the individual steps more clear. after segmenting lungs and identifying suspicious nodes, it is important to classify them as malignant or benign.\n",
      "content\n",
      "this dataset consists of several thousand examples formatted in multipage tiff (for use with tools like imagej and knime) and hdf5 (for python and r).\n",
      "acknowledgements\n",
      "the data were preprocessed and extracted partially from the luna16 competition (https://luna16.grand-challenge.org/description/) and should be used with the same policy that data has.\n",
      "inspiration\n",
      "the dataset is more for practice with medical images and cnn's but it would be interesting to see how the best manually created features (hog, sift, ...) perform against various deep learning approaches. it would also be quite interesting to try and visualize exactly which parts of an image made the algorithm guess malignant or benign.\n",
      "context\n",
      "on february 14, 2012, the president signed public law 112-95 , the \"faa modernization and reform act of 2012.\" section 311 amended title 18 of the united states code (u.s.c.) chapter 2, § 39, by adding § 39a, which makes it a federal crime to aim a laser pointer at an aircraft. as a result of this law, the faa has compiled a report of laser incidents\n",
      "content\n",
      "there is a datafile for each year and the column headers changed a little from year to year so keep that in mind when you're loading the data.\n",
      "date - date of report\n",
      "time (utc) - time of laser incident\n",
      "acid - aircraft id (ac/id, aircraft id, etc)\n",
      "no. a/c - number of aircraft\n",
      "type a/c - type of aircraft\n",
      "alt - altitude\n",
      "major city - nearest major city abbreviation\n",
      "color - color of laser\n",
      "injury reported - were there injuries?\n",
      "city - nearest city\n",
      "state - state\n",
      "acknowledments\n",
      "original file was converted into separate csv files for each year. original dataset can be found here: https://www.faa.gov/about/initiatives/lasers/laws/\n",
      "as the summer split regular season comes to a close. we can look back on which players and teams performed well in terms of fantasy points.\n",
      "next to the column names. in the parenthesis is how many points are awarded. for example in team stats dragon kills(+1) gives 1 point per dragon killed. baron kills gives 2 points. in player stats \"10+k/a(+2)\" means +2 points when kill+assists is greater than 10 in a match. in player stats \"3k(+2),4k(+5),5k(+10)\" means +2 points per triple kill, +5 points per quadra, and +10 points per penta\n",
      "these are the point breakdowns according to the fantasy lcs website\n",
      "\"lcs players are scored accordingly:\n",
      "2 points per kill\n",
      "-0.5 points per death\n",
      "1.5 points per assist\n",
      "0.01 points per creep kill\n",
      "2 points for a triple kill\n",
      "5 points for a quadra kill (doesn't also count as a triple kill)\n",
      "10 points for a penta kill (doesn't also count as a quadra kill)\n",
      "2 points if a player attains 10 or more assists or kills in a game (this bonus only applies once)\n",
      "lcs teams are scored accordingly:\n",
      "2 points per win\n",
      "2 points per baron nashor killed\n",
      "1 point per dragon killed\n",
      "2 points per first blood earned\n",
      "1 point per tower destroyed\n",
      "2 points if the team wins in less than 30 minutes\"\n",
      "source: http://fantasy.na.lolesports.com/en-us/stats\n",
      "context\n",
      "the fantasy premier league has become more popular every year. in the fpl, people pick fantasy teams of real-life players, and every week, receive points based on their picks' real-life performance.\n",
      "within this dataset, we have some historical data for the player performance in previous seasons, as well as future match fixtures.\n",
      "content\n",
      "the three main components currently in this dataset are:\n",
      "the individual players' current performance stats.\n",
      "the individual players' past performance stats (how much historical data depends on the player).\n",
      "a list of future match fixtures.\n",
      "all the data was taken from the official fantasy premier league website.\n",
      "n.b. a lot of the data was cobbled together from the output of publicly accessible json endpoints, therefore there are a lot of duplications (as fixture data was initially from the perspective of the individual players). also, since a lot of this data is used to drive the ui of a web application, there are a lot of redundancies, all of which could do with being cleaned up.\n",
      "inspiration\n",
      "a lot of my friends are massively into all aspects of the premier league (fantasy or otherwise), so my main motivation in putting this dataset together was to see was it possible to gain a competitive advantage over my very domain knowledgeable friends, with little to no domain knowledge myself.\n",
      "the obvious questions that could be answered with this data correspond to predicting the future performance of players based on historical metrics.\n",
      "context\n",
      "digit frequencies of the values of well-known mathematical series are a curiousity within the field of number theory. however, some are quite costly to compute and may cause stack overflow and out-of-memory issues. i am publishing this factorial digit frequency dataset for the convenience of fellow data enthusiasts who are interested in the field of number theory.\n",
      "content\n",
      "this dataset contains decimal digit (0-9) frequencies of the number 0! to 8000! (total of 8001 rows) there are 10 columns - one for each digit. note: the csv file contains header (0-9).\n",
      "acknowledgements\n",
      "this dataset was generated by using the clojure language and the trampoline function which avoids annoying stack overflow issues when doing very deep recursion. i would like to thank rich hickey and his colleagues in creating the clojure language.\n",
      "replication\n",
      "the script to generate the numbers can be found here: https://github.com/ianchute/factorial-number-frequencies/blob/master/generate.clj (you may need to convert the resulting json file to csv.)\n",
      "this data set contains all the match information of the last 47 years of la liga. extract as much insights possible from this data set and give some statistics of this high standard football league.\n",
      "this dataset is a single large shapefile of the buildings in southeast england. you can use it to make gorgeous maps or join it with other datasets for some really nice visualizations.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by alasdair rae, with the underlying raw data from the british ordnance survey. you can find the original shapefiles here, plus shapefiles for the rest of the uk.\n",
      "description\n",
      "company name - name of the fortune 500 company\n",
      "number of employees - total number of employees in the company\n",
      "previous rank - rank for the year 2016\n",
      "revenues - revenue of the company for the year 2016-17 in $ millions.\n",
      "revenue change - percentage of revenue change from last year\n",
      "profits - profits of the company in $ million\n",
      "profit change - change in the percentage of profit from previous year.\n",
      "assets - value of assets in $ millions\n",
      "market value - market value of the company as of 3/31/17\n",
      "acknowledgements\n",
      "this dataset is compiled by fortune magazine (www.fortune.com/fortune500). this is the limited version of the original data.\n",
      "context:\n",
      "oreos are traditionally chocolate sandwich cookies with a (vegan) creme filling. however, there are literally dozens of oreo flavors and varieites (over 50, by this count). with such a variety of choice, how do you know which flavors are tasty enough to make it worth grabbing a whole pack?\n",
      "content:\n",
      "this dataset contains of twelve different oreo varieties from five tasters, collected at an oreo tasting party. not every taster tasted every type of cookie, due in part to food allergies.\n",
      "the varieties included in this dataset are:\n",
      "pb&j\n",
      "mega stuf\n",
      "lemon\n",
      "chocolate\n",
      "birthday cake\n",
      "double stuf\n",
      "red velvet\n",
      "dunkin' donuts mocha\n",
      "mini\n",
      "coconut (thins)\n",
      "mint\n",
      "cinnamon bun\n",
      "each rater rated flavors on a five point scale, with 1 being the lowest rating and 5 the highest. a hyphen (-) indicates no rating for that flavor from that rater. the notes and discussion were contributed by all tasters.\n",
      "acknowledgements:\n",
      "this dataset was collected by rachael tatman and a group of willing volunteers. oreo is a trademark of nabisco.\n",
      "inspiration:\n",
      "which oreo flavor was there the most disagreement on? the most agreement?\n",
      "how does the strength of a flavor relate to the ratings it's given?\n",
      "context:\n",
      "shoreline bacteria are routinely monitored at fifteen stations around the perimeter of san francisco where water contact recreation may occur. these include three stations within the candlestick point state recreation area, one station at islais creek, two stations at aquatic park, two stations along crissy field beach, three stations at baker beach, one station at china beach, and three stations along ocean beach.\n",
      "content:\n",
      "dataset represents 552 samples taken across 15 locations over summer of 2017. additional monitoring is conducted whenever a treated discharge from the city’s combined sewer system occurs that affects a monitored beach.\n",
      "acknowledgements:\n",
      "the beach monitoring program is a cooperative effort between the san francisco public utilities commission and the san francisco department of public health. samples are collected weekly year round. read more about the combined sewer system and a detailed explanation of the beach monitoring program.\n",
      "inspiration:\n",
      "are there any patterns in beach water quality?\n",
      "the dataset is taken from data.gov.uk and contains all traffic-related deaths in the uk in 2015.\n",
      "source: https://data.gov.uk/dataset/road-accidents-safety-data/resource/ceb00cff-443d-4d43-b17a-ee13437e9564\n",
      "context\n",
      "this dataset represents the % of registered unemployment in the city of barcelona (spain) from year 2012 till 2016.\n",
      "registered unemployment corresponds to the job demands pending cover by the last day of each month, excluding employees who want to change jobs, the ones that do not have readily available or incompatible situation, the ones that are asking for a specific occupation and the temporary agricultural beneficiaries special unemployment benefit.\n",
      "content\n",
      "all files in this dataset have the same format. every row represents a hood from the city.\n",
      "district number\n",
      "hood name\n",
      "number of citizens from this hood with ages between 16 and 64 (legal ages for having a job)\n",
      "12 columns (one per month), % of unemployment\n",
      "in barcelona we have hoods and districts. every hood belongs to a district. a district is formed by several hoods.\n",
      "acknowledgements\n",
      "this data can be found in \"open data bcn - barcelona's city hall open data service\", which is the owner of the csv files.\n",
      "inspiration\n",
      "a few weeks ago i needed this datasets for testing purposes.\n",
      "i have uploaded this information here, because, in my honest opinion, \"data\" and \"research\" should be shared with everybody. enjoy!\n",
      "context\n",
      "this dataset was used in the paper \"a novel hybrid rbf neural networks model as a forecaster, statistics and computing\" (akbilgic et al) to show off a new forecasting algorithm. the paper showed good results when using a hrbf-nn model for predicting daily stock movements.\n",
      "content\n",
      "the data was collected (by the owners) from imkb.gov.tr and finance.yahoo.com and is organized by working days in the istanbul stock exchange.\n",
      "columns:\n",
      "istanbul stock exchange national 100 index\n",
      "standard & poor's 500 return index\n",
      "stock market return index of germany\n",
      "stock market return index of uk\n",
      "stock market return index of japan\n",
      "stock market return index of brazil\n",
      "msci european index\n",
      "msci emerging markets index\n",
      "acknowledgements\n",
      "akbilgic, o., bozdogan, h., balaban, m.e., (2013) a novel hybrid rbf neural networks model as a forecaster, statistics and computing. doi 10.1007/s11222-013-9375-7 phd thesis: oguz akbilgic, (2011) hibrit radyal tabanlä± fonksiyon aäÿlarä± ile deäÿiåÿken seã§imi ve tahminleme: menkul kä±ymet yatä±rä±m kararlarä±na ä°liåÿkin bir uygulama, istanbul university\n",
      "this dataset was downloaded from the uci ml repository: https://archive.ics.uci.edu/ml/datasets/istanbul+stock+exchange\n",
      "inspiration\n",
      "use this dataset to create predictive algorithms. then get rich!\n",
      "data from: https://data.medicare.gov/hospital-compare/payment-and-value-of-care-hospital/c7us-v4mf more information coming soon!\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "this dataset is a checklist of plants known to grow in north america. the list is maintained by the united states department of agriculture.\n",
      "content\n",
      "this data includes scientific names and common names of all plants in the united states.\n",
      "acknowledgements\n",
      "this dataset is published as-is by the united states department of agriculture.\n",
      "inspiration\n",
      "what words are commonly used in plant names? can you detect any trends in, say, adjectives commonly used in plant names that are less commonly used in the english language?\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (more than 61,000 properties) that was created by extracting data from stayzilla.com, an indian airbnb-like startup founded in 2005 that closed its operations in 2017.\n",
      "content\n",
      "this dataset has following fields:\n",
      "additional_info - special considerations regarding this property.\n",
      "amenities - pipe (|) delimited list of amenities offered at the property.\n",
      "check_in_date\n",
      "check_out_date\n",
      "city\n",
      "country\n",
      "crawl_date\n",
      "description - textual description of the property, as entered into the site by the lister.\n",
      "highlight_value - property highlights, as entered into the site by the lister.\n",
      "hotel_star_rating - in case the property is a hotel, its out-of-five star rating. not all hotels have ratings.\n",
      "image_count - number of images posted to the site by the lister.\n",
      "image_urls\n",
      "internet - does this property have internet access yes/no.\n",
      "landmark\n",
      "latitude\n",
      "longitude\n",
      "occupancy - how many adults and children may book the listing.\n",
      "pageurl\n",
      "property_address\n",
      "property_id\n",
      "property_name\n",
      "property_type - home? hotel? resort? etc.\n",
      "qts - crawler timestamp.\n",
      "query_time_stamp - copy of qts.\n",
      "room_price\n",
      "room_types - number of beds and baths for the room.\n",
      "search_term\n",
      "service_value - whether or not the property is verified with stayzilla (plus some junk entries).\n",
      "similar_hotel - some similar listings by name.\n",
      "sitename\n",
      "things_to_do - nearby activities as entered by the lister.\n",
      "things_to_note - special notes entered by the lister.\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "what is the shape of the indian property-sharing market, and how does it differ from that of, say, the united states? (try comparing this dataset to, say, the boston airbnb dataset).\n",
      "what are the contents of textual descriptions for properties?\n",
      "where are stayzilla properties located geographically?\n",
      "context:\n",
      "thor is a painstakingly cultivated database of historic aerial bombings from world war i through vietnam. thor has already proven useful in finding unexploded ordinance in southeast asia and improving air force combat tactics. our goal is to see where public discourse and innovation takes this data. each theater of warfare has a separate data file, in addition to a thor overview.\n",
      "content:\n",
      "the thor data contains data for allied aircraft carrying a combined bomb load of more than a million pounds over 1,437 recorded missions. this theater history of operations (thor) dataset combines digitized paper mission reports from wwi. it can be searched by date, conflict, geographic location and more than 30 additional data attributes forming a live-action sequence of events. see the data dictionary here and additonal background here.\n",
      "acknowledgements:\n",
      "thor is a dataset project initiated by lt col jenns robertson and continued in partnership with data.mil, an experimental project, created by the defense digital service in collaboration with the deputy chief management officer and data owners throughout the u.s. military.\n",
      "inspiration:\n",
      "can you create animated maps of certain campaigns? see, for example, this map of the argonne-meuse offensive.\n",
      "can you match weather data with campaigns?\n",
      "where were the most campaigns?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes over 1 gb of stop data from illinois, covering all of 2010 onwards. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context\n",
      "a record of every campaign donation made during the 2013 new york city election cycle. this dataset includes the donor, recipient, and dollar amount of contributions to campaigns both for the mayor of new york city (for the 2013-2017 term, which was won by bill de blasio) and for the variety of other publicly elected positions in the city of new york.\n",
      "content\n",
      "this dataset includes identifying information about the name of the person and/or corporation which made the donation, the dollar amount of the donation, whether or not it was later refunded (if so, a date of refund is provided), the archetype the donation falls under, the name of the candidate being donated to, and identifying information about the donor (including self-identified work area or profession).\n",
      "this dataset includes a field for campaign finance codes, read here for more information on what these are.\n",
      "acknowledgements\n",
      "this data is published as-is by the city of new york.\n",
      "inspiration\n",
      "what is the distribution of donations made across candidates?\n",
      "what is the distribution of small-to-large donations?\n",
      "who is doing the donating? how are they spatially distributed through the city?\n",
      "context\n",
      "it is difficult to determine how many coins are in a large pile of money, we want to make an app to automatically count them.\n",
      "content\n",
      "each folder has a different type of coin (1fr = 1 franc, 50rp = 50 rappen / cents) and since all of the franc coins have the same back we have a fr_back folder and all of the rappen have the same we have a rp_back\n",
      "acknowledgements\n",
      "the data was collected by https://github.com/zarkonnen/aimeetup_coins\n",
      "inspiration\n",
      "making a tool to classify coints\n",
      "context\n",
      "realising which routes a taxi takes while going from one location to another gives us deep insights into why some trips take longer than others. also, most taxis rely on navigation from google maps, which reinforces the use case of this dataset. on a deeper look, we can begin to analyse patches of slow traffic and number of steps during the trip (explained below).\n",
      "content\n",
      "the data, as we see it contains the following columns :\n",
      "trip_id, pickup_latitude, pickup_longitude (and equivalents with dropoff) are picked up from the original dataset.\n",
      "distance : estimates the distance between the start and the end latitude, in miles.\n",
      "start_address and end_address are directly picked up from the google maps api\n",
      "params : details set of parameters, flattened out into a single line. (explained below)\n",
      "parameters\n",
      "the parameters field is a long string of a flattened out json object. at its very basic, the field has space separated steps. the syntax is as follows :\n",
      "step1:{ ... }, step2:{ ...\n",
      "each step denotes the presence of an intermediate point.\n",
      "inside the curly braces of each of the steps we have the distance for that step measured in ft, and the start and end location. the start and end location are surrounded by round braces and are in the following format :\n",
      "step1:{distance=x ft/mi start_location=(latitude, longitude) end_location ...}, ...\n",
      "one can split the internal params over space to get all the required values.\n",
      "acknowledgements\n",
      "all the credit for the data goes to the google maps api, though limited to 2000 queries per day. i believe that even that limited amount would help us gain great insights.\n",
      "future prospects\n",
      "more data : since the number of rows processed are just 2000, with a good response we might be able to get more. if you feel like contributing, please have a look at the script here and try and run in for the next 2000 rows.\n",
      "driver instructions : i did not include the driver instruction column in the data from the google api as it seemed to complex to use in any kind of models. if that is not the general opinion, i can add it here.\n",
      "loans data\n",
      "context\n",
      "this dataset contains hourly measurements of the water level in venezia from 1983 to 2015.\n",
      "content\n",
      "the original data come from centro previsioni e segnalazioni maree it has been cleaned and aggregated to obtain this dataset. the original data and the script used in the process can be seen in github\n",
      "acknowledgements\n",
      "centro previsioni e segnalazioni maree\n",
      "inspiration\n",
      "this dataset can be used to do exploratory analysis and forecasting models.\n",
      "context\n",
      "in this dataset you'll find the captions from 990 videos of the late show with stephen colbert. captions were cleaned in order to make text mining easier.\n",
      "content\n",
      "this dataset has only three columns: id, link, and captions. the first one contains the video's id, the second one contains its link, and the last one contains the video's captions.\n",
      "the data was collected in the week of 2017-06-19 with a script i created. i also tried to collect more information about the videos (like date and title), but youtube's api was being too stubborn.\n",
      "acknowledgements\n",
      "the content of all videos belong to the late show. captions were extracted with the help of ccsubs.\n",
      "inspiration\n",
      "i collected this data to write the blog post \"colbert's fixation\".\n",
      "this is the ball by ball data of all the ipl cricket matches till season 9. (i will be coming up with season 10 data as soon as possible)\n",
      "source: http://cricsheet.org/ (data is available on this website in the yaml format. this is converted to csv format by using r script ,sql,ssis.\n",
      "research scope: predicting the winner of the next season of ipl based on past data, visualizations, perspectives, etc.\n",
      "sentiment analysis on rare disesases facebook groups.\n",
      "the origin of this file is 'rare diseases on facebook groups', also available in kaggle. this file contained facebook posts from rare diseases groups, in spanish.\n",
      "based on that file, the post_message was extracted and translated into english using google translate. sentimient analysis was performed to obtain the polarity and subjectivity using python (textblob).\n",
      "the result is this output file, containing the post_message both in spanish and english and the polarity and subjectivity of the message.\n",
      "context\n",
      "using ta-lib : technical analysis library. backtest on the spy index data, using support and resistance indicators or any other indicator.\n",
      "content\n",
      "data contains daily spy index: date open high low close adj close volume\n",
      "acknowledgements\n",
      "support for resistance: technical analysis wiki link: https://en.wikipedia.org/wiki/support_and_resistance\n",
      "inspiration\n",
      "do your best for the backtest and technical indicator implementation\n",
      "context\n",
      "the funniness of joke is very subjective. having more than 70,000 users rate jokes, can an algorithm be written to identify the universally funny joke?\n",
      "content\n",
      "the data file are in .csv format.\n",
      "the complete dataset is 100 rows and 73422 columns.\n",
      "the complete dataset is split into 3 .csv files.\n",
      "joketext.csv contains the id of the joke and the complete joke string.\n",
      "userratings1.csv contains the ratings provided by the first 36710 users.\n",
      "userratings2.csv contains the ratings provided by the last 36711 users.\n",
      "the dataset is arranged such that the initial users have rated higher number of jokes than the later users.\n",
      "the rating is a real value between -10.0 and +10.0.\n",
      "the empty values indicate that the user has not provided any rating for that particular joke.\n",
      "acknowledgements\n",
      "the dataset is associated with the below research paper.\n",
      "eigentaste: a constant time collaborative filtering algorithm. ken goldberg, theresa roeder, dhruv gupta, and chris perkins. information retrieval, 4(2), 133-151. july 2001.\n",
      "more information and datasets can be found at http://eigentaste.berkeley.edu/dataset/\n",
      "inspiration\n",
      "since funniness is a very subjective matter, it will be very interesting to see if data science can bring out the details on what makes something funny.\n",
      "==========================================\n",
      "bike sharing dataset\n",
      "hadi fanaee-t\n",
      "laboratory of artificial intelligence and decision support (liaad), university of porto inesc porto, campus da feup rua dr. roberto frias, 378 4200 - 465 porto, portugal\n",
      "=========================================\n",
      "background\n",
      "bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. through these systems, user is able to easily rent a bike from a particular position and return back at another position. currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\n",
      "apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. this feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. hence, it is expected that most of important events in the city could be detected via monitoring these data.\n",
      "=========================================\n",
      "data set\n",
      "bike-sharing rental process is highly correlated to the environmental and seasonal settings. for instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. the core data set is related to\n",
      "the two-year historical log corresponding to years 2011 and 2012 from capital bikeshare system, washington d.c., usa which is publicly available in http://capitalbikeshare.com/system-data. we aggregated the data on two hourly and daily basis and then extracted and added the corresponding weather and seasonal information. weather information are extracted from http://www.freemeteo.com.\n",
      "=========================================\n",
      "associated tasks\n",
      "- regression: \n",
      "    predication of bike rental count hourly or daily based on the environmental and seasonal settings.\n",
      "\n",
      "- event and anomaly detection:  \n",
      "    count of rented bikes are also correlated to some events in the town which easily are traceable via search engines.\n",
      "    for instance, query like \"2012-10-30 washington d.c.\" in google returns related results to hurricane sandy. some of the important events are \n",
      "    identified in [1]. therefore the data can be used for validation of anomaly or event detection algorithms as well.\n",
      "=========================================\n",
      "files\n",
      "- readme.txt\n",
      "- hour.csv : bike sharing counts aggregated on hourly basis. records: 17379 hours\n",
      "- day.csv - bike sharing counts aggregated on daily basis. records: 731 days\n",
      "=========================================\n",
      "dataset characteristics\n",
      "both hour.csv and day.csv have the following fields, except hr which is not available in day.csv\n",
      "- instant: record index\n",
      "- dteday : date\n",
      "- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
      "- yr : year (0: 2011, 1:2012)\n",
      "- mnth : month ( 1 to 12)\n",
      "- hr : hour (0 to 23)\n",
      "- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
      "- weekday : day of the week\n",
      "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
      "+ weathersit : \n",
      "    - 1: clear, few clouds, partly cloudy, partly cloudy\n",
      "    - 2: mist + cloudy, mist + broken clouds, mist + few clouds, mist\n",
      "    - 3: light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds\n",
      "    - 4: heavy rain + ice pallets + thunderstorm + mist, snow + fog\n",
      "- temp : normalized temperature in celsius. the values are divided to 41 (max)\n",
      "- atemp: normalized feeling temperature in celsius. the values are divided to 50 (max)\n",
      "- hum: normalized humidity. the values are divided to 100 (max)\n",
      "- windspeed: normalized wind speed. the values are divided to 67 (max)\n",
      "- casual: count of casual users\n",
      "- registered: count of registered users\n",
      "- cnt: count of total rental bikes including both casual and registered\n",
      "=========================================\n",
      "license\n",
      "use of this dataset in publications must be cited to the following publication:\n",
      "[1] fanaee-t, hadi, and gama, joao, \"event labeling combining ensemble detectors and background knowledge\", progress in artificial intelligence (2013): pp. 1-15, springer berlin heidelberg, doi:10.1007/s13748-013-0040-3.\n",
      "@article{ year={2013}, issn={2192-6352}, journal={progress in artificial intelligence}, doi={10.1007/s13748-013-0040-3}, title={event labeling combining ensemble detectors and background knowledge}, url={http://dx.doi.org/10.1007/s13748-013-0040-3}, publisher={springer berlin heidelberg}, keywords={event labeling; event detection; ensemble learning; background knowledge}, author={fanaee-t, hadi and gama, joao}, pages={1-15} }\n",
      "=========================================\n",
      "contact\n",
      "for further information about this dataset please contact hadi fanaee-t (hadi.fanaee@fe.up.pt)\n",
      "context\n",
      "in 2015 i was looking for subletting an apartment in tel aviv. since, housing in tel-aviv is in high demand i thought i'll try to scrap the facebook groups of tel aviv sublets in order to find one. this dataset is in hebrew mostly, eventhough, some posts are in english. it is best suited for people who are looking for nlp challenges and to try working with an hebrew dataset (which is a challenge in itself).\n",
      "content\n",
      "the dataset is made of real posts that i've scraped in 2015 from a facebook group for subletting in tel-aviv named \"sublets in telaviv for short periods\" or in hebrew סאבלטים בתל אביב לתקופות קצרות.\n",
      "acknowledgements\n",
      "using the facebook graph api which allows to get the data easily.\n",
      "inspiration\n",
      "this dataset holds several challenges. first of all, i wonder how much a sublet for a month should cost in the city and how the prices change over the years (the dataset hold a few years of posts). also, it can be interesting to find the differences in prices between different regions of the city(הצפון הישן, מרכז העיר, פלורנטין).\n",
      "context\n",
      "i get this dataset from uci machine learning. i very interested with this dataset because one of our global warming problem is about air quality in some big city very serious. in uci ml get this data from sensor device that located in italy. also you can read about the dataset in the description.\n",
      "content\n",
      "i get this data from uci machine learning. here is about descripstion rows and column also another description. \"the dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an air quality chemical multisensor device. the device was located on the field in a significantly polluted area, at road level,within an italian city. data were recorded from march 2004 to february 2005 (one year)representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses. ground truth hourly averaged concentrations for co, non metanic hydrocarbons, benzene, total nitrogen oxides (nox) and nitrogen dioxide (no2) and were provided by a co-located reference certified analyzer. evidences of cross-sensitivities as well as both concept and sensor drifts are present as described in de vito et al., sens. and act. b, vol. 129,2,2008 (citation required) eventually affecting sensors concentration estimation capabilities. missing values are tagged with -200 value. \"\n",
      "acknowledgements\n",
      "thank to uci https://archive.ics.uci.edu/ml/index.php\n",
      "inspiration\n",
      "i would like to see another method to classify or cluster this dataset with timeseries purpose.\n",
      "context\n",
      "rankings are a constant phenomenon in society, with a persistent interest in the stratification of items in a set across various disciplines. in sports, rankings are a direct representation of the performance of a team or player over a certain period. given the straightforward nature of rankings in sports (points based system) there is the opportunity to statistically explore rankings of sports disciplines.\n",
      "content\n",
      "the dataset comprises weekly rankings data of the top 1000 golf players between sep 2000 and june april 2015. the data is housed in a single csv file.\n",
      "acknowledgements\n",
      "data was sourced from the official golf world rankings (ogwr) website: ogwr.com\n",
      "inspiration\n",
      "this dataset could be of use to anyone interested in the distribution of rankings in competitive events\n",
      "this is my first ever dataset. any suggestions would be welcome.\n",
      "there are two main files in this dataset. pokemon.csv gives a general info on all pokemons, including mega evolutions, but excluding alternate forms of pokemons. the move.csv gives info about the moves learned by the pokemon but only through level ups*.\n",
      "i would like to thank pokemon.com bulbapedia\n",
      "the data is yours to play with :) in the future, i might add many more files. i have the data, just need to process it.\n",
      "context:\n",
      "packages for the r programming language often include datasets. this dataset collects information on those datasets to make them easier to find.\n",
      "content:\n",
      "rdatasets is a collection of 1072 datasets that were originally distributed alongside the statistical software environment r and some of its add-on packages. the goal is to make these data more broadly accessible for teaching and statistical software development.\n",
      "acknowledgements:\n",
      "this data was collected by vincent arel-bundock, @vincentarelbundock on github. the version here was taken from github on july 11, 2017 and is not actively maintained.\n",
      "inspiration:\n",
      "in addition to helping find a specific dataset, this dataset can help answer questions about what data is included in r packages. are specific topics very popular or unpopular? how big are datasets included in r packages? what the naming conventions/trends for packages that include data? what are the naming conventions/trends for datasets included in packages?\n",
      "license:\n",
      "this dataset is licensed under the gnu general public license .\n",
      "context:\n",
      "this dataset was collected to answer questions about where in tweets users put clap emoji, especially when clap emoji are used 👏 between 👏 every 👏 word. 👏\n",
      "content:\n",
      "this dataset is made of up information on 27035 unique tweets continaing at least on clap emoji collected on july 7th, 2017. tweets were collected through fireant using the twitter streaming api. for each tweet, every word in the tweet is marked as either the clap emoji or a different word.\n",
      "acknowledgements:\n",
      "this dataset was collected by rachael tatman during the process of linguistic research on the clap emoji. a blog post on an analysis of this data can be found here. the dataset here is released to the public domain.\n",
      "inspiration:\n",
      "while this dataset was originally collected to look specific at the clap-word-clap-word pattern, it can also be used to investigate other problems. -do most tweets which contain the tweet emoji contain more than one? -when multiple claps are used together (👏👏👏) , are they more likely to show up at the beginning or the end of the tweet? -can you predict the distribution of claps over the tweet? (perhaps by using a bernoulli distribution?) -how can you visualize where tweet emoji are used?\n",
      "context:\n",
      "when groups of people who don’t share a spoken language come together, they will often create a new language which combines elements of their first languages. these languages are known as “pidgins”. if they are then learned by children as their first language they become fully-fledged languages known as “creoles”. this dataset contains information on both creoles and pidgins spoken around the world.\n",
      "content:\n",
      "this dataset includes information on the grammatical and lexical structures of 76 pidgin and creole languages. the language set contains not only the most widely studied atlantic and indian ocean creoles, but also less well known pidgins and creoles from africa, south asia, southeast asia, melanesia and australia, including some extinct varieties, and several mixed languages.\n",
      "this dataset is made up of several tables, each of which contains different pieces of information:\n",
      "language: a table of language names & the unique id’s associated with them.\n",
      "language_data: a table of data on the different languages, including the name speakers’ call their language (autoglossonym), other names the language is called, how many speakers it has, the language which contributed the most words to the language (major lexifier), other languages which contribute to that language, where it is spoken, and where it is an official language fro. the column language_id has the id linked to the language table.\n",
      "language_source: the sources referenced on each language (referencing the language and source tables).\n",
      "langauge_table: information on the geographic location of each language.\n",
      "source: information on the scholarly sources referenced for information on language.\n",
      "acknowledgements:\n",
      "this dataset contains information from the online portion of the atlas of pidgin and creole language structures (apics). it is distributed under a creative commons attribution 3.0 unported license . if you use this dataset in your work, please use this citation:\n",
      "salikoko s. mufwene. 2013. kikongo-kituba structure dataset. in: michaelis, susanne maria & maurer, philippe & haspelmath, martin & huber, magnus (eds.) atlas of pidgin and creole language structures online. leipzig: max planck institute for evolutionary anthropology. (available online at http://apics-online.info/contributions/58, accessed on 2017-07-28.)\n",
      "inspiration:\n",
      "which areas of the world have the most creoles/pidgins?\n",
      "which language has contributed to the most creoles/pidgins? why might this be?\n",
      "can you map the areas of influence of the various lexicalized major lexifier languages?\n",
      "you may also be interested in:\n",
      "world language family map\n",
      "the sign language analyses (slay) database\n",
      "world atlas of language structures: information on the linguistic structures in 2,679 languages\n",
      "hourly weather data for the new york city taxi trip duration challange\n",
      "here is some detailed weather data for the new york city taxi trip duration challange. i noticed that many contenders use daily weather data and thought that the ml could be improved with hourly data for nyc (default knyc station) since pickup_datetime is given. python code on github can return same data for any city\n",
      "content\n",
      "wundergrounds api provides hourly weather data in json format, but i assume most people just want the complete data set in csv. i stands for imperial, m for metric so the difference stands in the relative unit for the returned value (ex. fahrenheit vs. celsius).\n",
      "note that values will = -9999 or -999 for null or non applicable (na) variables. (replaced with nan in version 2) wundergrounds full phrase glossary\n",
      "datetime: date and time of day (est)\n",
      "tempm: temperature in celcius\n",
      "tempi: temperature in fahrenheit\n",
      "dewptm: dewpoint in celcius\n",
      "dewpti: dewpoint in fahrenheit\n",
      "hum: humidity %\n",
      "wspdm: wind speed in kph\n",
      "wspdi: wind speed in mph\n",
      "wgustm: wind gust in kph\n",
      "wgusti: wind gust in mph\n",
      "wdird: wind direction in degrees\n",
      "wdire: wind direction description\n",
      "vism: vivibility in km\n",
      "visi: visibility in miles\n",
      "pressurem: pressure in mbar\n",
      "pressurei: pressure in inhg\n",
      "windchillm: wind chill in celcius\n",
      "windchilli: wind chill in fahrenheit\n",
      "heatindexm: heat index celcius\n",
      "heatindexi: heat index fahrenheit\n",
      "precipm: precipitation in mm\n",
      "precipi: precipitation in inches\n",
      "conds: conditions: see full list of conditions\n",
      "icon\n",
      "fog: boolean\n",
      "rain: boolean\n",
      "snow: boolean\n",
      "hail: boolean\n",
      "thunder: boolean\n",
      "tornado: boolean\n",
      "thanks to wunderground\n",
      "context\n",
      "the goal of this corporate prosecutions registry is to provide comprehensive and up-to-date information on federal organizational prosecutions in the united states, so that we can better understand how corporate prosecutions are brought and resolved. it includes detailed information about every federal organizational prosecution since 2001, as well as deferred and non-prosecution agreements with organizations since 1990.\n",
      "dataset description\n",
      "these data on deferred prosecution and non-prosecution agreements were collected by identifying agreements through news searches, press releases by the department of justice and u.s. attorney’s office, and also when practitioners brought agreements to our attention. the government accountability office conducted a study of federal deferred prosecution and non-prosecution agreements with organizations, and in august 2010, the gao provided a list of those agreements in response to an information request. finally, searches of the bloomberg dockets database located additional prosecution agreements with companies that had not previously been located. jon ashley has contacted u.s. attorney’s offices to request agreements. an effort by the first amendment clinic at the university of virginia school of law to litigate freedom of information act requests resulted in locating a group of missing agreements which are now available on the registry.\n",
      "this registry only includes information about federal organizational prosecutions, and not cases brought solely in state courts. nor does this registry include leniency agreements entered through the antitrust division’s leniency program, which are kept confidential. the registry also does not include convictions overturned on appeal, or cases in which the indictment was dismissed or the company was acquitted at a trial.\n",
      "the u.s. sentencing commission reports sentencing data concerning organizational prosecutions each year. that data does not include cases resolved without a formal sentencing, such as deferred and non-prosecution agreements.\n",
      "acknowledgements\n",
      "the corporate prosecutions registry is a project of the university of virginia school of law. it was created by professor brandon garrett (bgarrett@virginia.edu) and jon ashley (jaa6c@virginia.edu). please cite this dataset as: “brandon l. garrett and jon ashley, corporate prosecutions registry, university of virginia school of law, at http://lib.law.virginia.edu/garrett/corporate-prosecution-registry/index.html”\n",
      "inspiration\n",
      "which industries face the most prosecutions?\n",
      "which government organizations have been the most successful at pursuing cases against corporations?\n",
      "not a single case in the dataset led to a trial conviction. can you link these corporate cases to criminal cases against the individuals involved? how many of them were convicted instead?\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes over 1 gb of stop data from ohio. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context\n",
      "\"law enforcement in cook county, which includes chicago, seized items from residents ranging from a cashier's check for 34 cents to a 2010 rolls royce ghost with an estimated value of more than $200,000. they also seized xbox controllers, televisions, nunchucks, 12 cans of peas, a pair of rhinestone cufflinks, and a bayonet.\"\n",
      "-reason.com\n",
      "content\n",
      "there wasn't much documentation on the dataset, but the data fields are somewhat explanatory.\n",
      "inventnumb: inventory number\n",
      "polrptnumb: police report number\n",
      "inumber: ?\n",
      "seizedate: date siezed\n",
      "seizeaddress: address where it was siezed\n",
      "seizecity: city where it was siezed\n",
      "seizestate: state where it was siezed\n",
      "seizezip: zip code for where it was siezed\n",
      "invitemnumb: invenotry item number?\n",
      "descr: description of item\n",
      "estvalue: estimated value of item\n",
      "vin: vin number? (vehicles)\n",
      "findings: ?\n",
      "forfdate: forfieture date?\n",
      "forfvalue: value of forfieture?\n",
      "casenumb: case number\n",
      "ad: ?\n",
      "addate: date?\n",
      "acknowledgements\n",
      "this dataset is the result of foia request by lucy parsons labs, a chicago-based transparency non-profit. it contains information about to the seizures of assets in cook county (chicago, il).\n",
      "these data were presented in a reason.com article showing the disparity of police seizures between poor and wealthier parts of cook county, il. http://reason.com/blog/2017/06/13/poor-neighborhoods-hit-hardest-by-asset\n",
      "the original excel file was converted to csv\n",
      "inspiration\n",
      "this dataset comes from a freedom of information act request. i love datasets like these because of the potential for finding new socioeconomic insights and improving government accountability.\n",
      "image segmentation\n",
      "the data and kernels here are connected to the lecture material on image segmentation from the quantitative big imaging course at eth zurich (kmader.github.io/quantitative-big-imaging-2017). the specific description of the exercise tasks can be found (https://github.com/kmader/quantitative-big-imaging-2016/blob/master/exercises/03-description.md)\n",
      "introduction\n",
      "the basic inspiration was the inability to search through lots of alternative routes while traveling over easter weekend and having to manually to point-to-point searches. the hope is by using a bit of r/python the search for the best routes can be made a lot easier.\n",
      "data structure\n",
      "the data is organized in a format called gtfs which is explained in detail here but only available in german. kernels should make it clear how to work with most of the data\n",
      "source / attribution\n",
      "the data all comes from opentransportdata.swiss and can be downloaded in the original format by following this link\n",
      "cyrillic-oriented mnist\n",
      "comnist services\n",
      "a repository of images of hand-written cyrillic and latin alphabet letters for machine learning applications.\n",
      "the repository currently consists of 20,000+ 278x278 png images representing all 33 letters of the russian alphabet and the 26 letters of the english alphabet. find original source on my github these images have been hand-written on touch screen through crowd-sourcing.\n",
      "the dataset will be regularly extended with more data as the collection progresses\n",
      "an api that reads words in images\n",
      "comnist also makes available a web service that reads drawing and identifies the word/letter you have drawn. on top of an image you can submit an expected word and get back the original image with mismtaches highlighted (for educational purposes)\n",
      "the api is available at this address: http://35.187.34.5:5002/api/word it is accessible via a post request with following input expected: { 'img': mandatory b64 encoded image, with letters in black on a white background 'word': optional string, the expected word to be read 'lang': mandatory string, either 'en' or 'ru', respectively for latin or cyrillic (russian) alphabets 'nb_output': mandatory integer, the \"tolerance\" of the engine }\n",
      "the return information is the following: { 'img': b64 encoded image, if a word was supplied as an input, then modified version of that image highlighting mismatches 'word': string, the word that was read by the api }\n",
      "participate\n",
      "the objective is to gather at least 1000 images of each class, therefore your contribution is more that welcome! one minute of your time is enough, and don't hesitate to ask your friends and family to participate as well.\n",
      "english version - draw latin only + common to cyrillic and latin\n",
      "french version - draw latin only + common to cyrillic and latin\n",
      "russian version - draw cyrillic only\n",
      "find out more about comnist on my blog\n",
      "credits and license\n",
      "a big thanks to all the contributors!\n",
      "these images have been crowd-sourced thanks to the great web-design by anna migushina available on her github.\n",
      "comnist logo by sophie valenina\n",
      "**the ultimate soccer database for data analysis and machine learning\n",
      "what you get:**\n",
      "+25,000 matches\n",
      "+10,000 players\n",
      "11 european countries with their lead championship\n",
      "seasons 2008 to 2016\n",
      "players and teams' attributes* sourced from ea sports' fifa video game series, including the weekly updates - - team line up with squad formation (x, y coordinates)\n",
      "betting odds from up to 10 providers\n",
      "detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\n",
      "*16th oct 2016: new table containing teams' attributes from fifa !\n",
      "original data source:\n",
      "you can easily find data about soccer matches but they are usually scattered across different websites. a thorough data collection and processing has been done to make your life easier. i must insist that you do not make any commercial use of the data. the data was sourced from:\n",
      "http://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events\n",
      "http://www.football-data.co.uk/ : betting odds. click here to understand the column naming system for betting odds:\n",
      "http://sofifa.com/ : players and teams attributes from ea sports fifa games. fifa series and all fifa assets property of ea sports.\n",
      "when you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. i have called those foreign keys \"api_id\".\n",
      "improving the dataset:\n",
      "you will notice that some players are missing from the lineup (null values). this is because i have not been able to source their attributes from fifa. this will be fixed overtime as the crawling algorithm is being improved. the dataset will also be expanded to include international games, national cups, champion's league and europa league. please ask me if you're after a specific tournament.\n",
      "please get in touch with hugo mathien if you want to help improve this dataset.\n",
      "click here to access the project github\n",
      "important note for people interested in using the crawlers: since i first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. the existing script to crawl players ('player spider') will not work until i've updated it.\n",
      "exploring the data:\n",
      "now that's the fun part, there is a lot you can do with this dataset. i will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! here are some ideas for you:\n",
      "the holy grail... ... is obviously to predict the outcome of the game. the bookies use 3 classes (home win, draw, away win). they get it right about 53% of the time. this is also what i've achieved so far using my own svm. though it may sound high for such a random sport game, you've got to know that the home team wins about 46% of the time. so the base case (constantly predicting home win) has indeed 46% precision.\n",
      "probabilities vs odds\n",
      "when running a multi-class classifier like svm you could also output a probability estimate and compare it to the betting odds. have a look at your variance vs odds and see for what games you had very different predictions.\n",
      "explore and visualize features\n",
      "with access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into the beautiful game . who knows, guardiola himself may hire one of you some day! database released under open database license, individual papers copyright their original authors\n",
      "citation request: this dataset is public available for research. the details are described in [cortez et al., 2009]. please include this citation if you plan to use this database:\n",
      "p. cortez, a. cerdeira, f. almeida, t. matos and j. reis. modeling wine preferences by data mining from physicochemical properties. in decision support systems, elsevier, 47(4):547-553. issn: 0167-9236.\n",
      "available at: [@elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016 [pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib\n",
      "title: wine quality\n",
      "sources created by: paulo cortez (univ. minho), antonio cerdeira, fernando almeida, telmo matos and jose reis (cvrvv) @ 2009\n",
      "past usage:\n",
      "p. cortez, a. cerdeira, f. almeida, t. matos and j. reis. modeling wine preferences by data mining from physicochemical properties. in decision support systems, elsevier, 47(4):547-553. issn: 0167-9236.\n",
      "in the above reference, two datasets were created, using red and white wine samples. the inputs include objective tests (e.g. ph values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). each expert graded the wine quality between 0 (very bad) and 10 (very excellent). several data mining methods were applied to model these datasets under a regression approach. the support vector machine model achieved the best results. several metrics were computed: mad, confusion matrix for a fixed error tolerance (t), etc. also, we plot the relative importances of the input variables (as measured by a sensitivity analysis procedure).\n",
      "relevant information:\n",
      "the two datasets are related to red and white variants of the portuguese \"vinho verde\" wine. for more details, consult: http://www.vinhoverde.pt/en/ or the reference [cortez et al., 2009]. due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
      "these datasets can be viewed as classification or regression tasks. the classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). outlier detection algorithms could be used to detect the few excellent or poor wines. also, we are not sure if all input variables are relevant. so it could be interesting to test feature selection methods.\n",
      "number of instances: red wine - 1599; white wine - 4898.\n",
      "number of attributes: 11 + output attribute\n",
      "note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.\n",
      "attribute information:\n",
      "for more information, read [cortez et al., 2009].\n",
      "input variables (based on physicochemical tests): 1 - fixed acidity (tartaric acid - g / dm^3) 2 - volatile acidity (acetic acid - g / dm^3) 3 - citric acid (g / dm^3) 4 - residual sugar (g / dm^3) 5 - chlorides (sodium chloride - g / dm^3 6 - free sulfur dioxide (mg / dm^3) 7 - total sulfur dioxide (mg / dm^3) 8 - density (g / cm^3) 9 - ph 10 - sulphates (potassium sulphate - g / dm3) 11 - alcohol (% by volume) output variable (based on sensory data): 12 - quality (score between 0 and 10)\n",
      "missing attribute values: none\n",
      "description of attributes:\n",
      "1 - fixed acidity: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n",
      "2 - volatile acidity: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n",
      "3 - citric acid: found in small quantities, citric acid can add 'freshness' and flavor to wines\n",
      "4 - residual sugar: the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n",
      "5 - chlorides: the amount of salt in the wine\n",
      "6 - free sulfur dioxide: the free form of so2 exists in equilibrium between molecular so2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n",
      "7 - total sulfur dioxide: amount of free and bound forms of s02; in low concentrations, so2 is mostly undetectable in wine, but at free so2 concentrations over 50 ppm, so2 becomes evident in the nose and taste of wine\n",
      "8 - density: the density of water is close to that of water depending on the percent alcohol and sugar content\n",
      "9 - ph: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the ph scale\n",
      "10 - sulphates: a wine additive which can contribute to sulfur dioxide gas (s02) levels, wich acts as an antimicrobial and antioxidant\n",
      "11 - alcohol: the percent alcohol content of the wine\n",
      "output variable (based on sensory data): 12 - quality (score between 0 and 10)\n",
      "context\n",
      "this is a forked subset of the uber pickups in new york city, enriched with weather, borough, and holidays information.\n",
      "content\n",
      "i created this dataset for a personal project of exploring and predicting pickups in the area by merging data that intuitively seem possible factors for the analysis. these are:\n",
      "uber pickups in new york city, from 01/01/2015 to 30/06/2015 (uber-raw-data-janjune-15.csv). (by fivethirtyeight via kaggle.com)\n",
      "weather data from national centers for environmental information.\n",
      "locationid to borough mapping. (by fivethirtyeight)\n",
      "nyc public holidays.\n",
      "the main dataset contained over 10 million observations of 4 variables which aggregated per hour and borough, and then joined with the rest of the datasets producing 29,101 observations across 13 variables. these are:\n",
      "pickup_dt: time period of the observations.\n",
      "borough: nyc's borough.\n",
      "pickups: number of pickups for the period.\n",
      "spd: wind speed in miles/hour.\n",
      "vsb: visibility in miles to nearest tenth.\n",
      "temp: temperature in fahrenheit.\n",
      "dewp: dew point in fahrenheit.\n",
      "slp: sea level pressure.\n",
      "pcp01: 1-hour liquid precipitation.\n",
      "pcp06: 6-hour liquid precipitation.\n",
      "pcp24: 24-hour liquid precipitation.\n",
      "sd: snow depth in inches.\n",
      "hday: being a holiday (y) or not (n).\n",
      "acknowledgements / original datasets:\n",
      "uber pickups in new york city, from 01/01/2015 to 30/06/2015 (uber-raw-data-janjune-15.csv). (by fivethirtyeight via kaggle.com)\n",
      "weather data from national centers for environmental information.\n",
      "locationid to borough mapping. (by fivethirtyeight)\n",
      "(picture credits: buck ennis)\n",
      "context\n",
      "data related to estimated fraud or payment errors for unemployment and food assistance, used in discussion about alleged widespread fraud and abuse of welfare.\n",
      "content\n",
      "data pulled from gov't websites on february 3rd, 2017.\n",
      "snap rates come from :https://www.fns.usda.gov/sites/default/files/snap/2014-rates.pdf ui error data comes from : https://www.dol.gov/general/maps/data > > > 2016 ipia 1-year data [07/01/2015 - 06/30/2016] >> tab: integrity rates with ci\n",
      "inspiration\n",
      "want to plot this against election results or possibly find polling data about alleged abuse of welfare system to match against actual findings\n",
      "content\n",
      "in case you want to test a crawler, verify marketing ideas, we prepared a random dataset which includes 100,000 of registered domain names.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "this dataset collects characteristics of the population in each region (age distribution, unemployment rate, immigration percent and primary economic sector) and cross it with the votes per each political part.\n",
      "it has 52 fields:\n",
      "1) code [string]: region code of the different spanish areas. there are 8126 different regions, but the dataset only contains 8119, because some sources were incomplete.\n",
      "2) regionname [string]: name of the region.\n",
      "3) population [int]: amount of people living in that area (1st january 2015)\n",
      "4) totalcensus [int]: number of people over 18 years old, which means that can vote.\n",
      "5) totalvotes [int]: number of total votes.\n",
      "6) abstentionptge [float]: percent of the people that have not votes in the election. (totalcensus-totalvotes)/totalcensus*100 %\n",
      "7) blankvotesptge [float]: percent of votes that were blank. calculated as follows: blankvotes/totalvotes*100 %\n",
      "8) nullvotesptge [float]: percent of votes that were null. calculated as follows: nullvotes/totalvotes*100 %\n",
      "9) pp_ptge [float]: percent of the votes given to the political party called “partido popular”. (pp_votes)/totalvotes*100 %\n",
      "10) psoe_ptge [float]: percent of the votes given to the political party called “partido socialista obrero español” (psoe_votes)/totalvotes*100 %\n",
      "11) podemos_ptge [float]: percent of the votes given to the political party called “podemos” (podemos_votes)/totalvotes*100 %\n",
      "12) ciudadanos_ptge [float]: percent of the votes given to the political party called “ciudadanos” (ciudadanos_votes)/totalvotes*100 %\n",
      "13) others_ptge [float]: percent of the votes given to the others political parties (∑▒minoritaryvotes)/totalvotes*100 %\n",
      "14) age_0-4_ptge [float]: percent of the populations which age is between 0 and 4 years old. it is calculated as follows: (number of people in (0-4))/totalpopulation*100 %\n",
      "15) age_5-9_ptge [float]: percent of the populations which age is between 5 and 9 year old.\n",
      "16) age_10-14_ptge [float]: percent of the populations which age is between 10 and 14 years old\n",
      "17) age_15-19_ptge [float]: percent of the populations which age is between 15 and 19 years old\n",
      "18) age_20-24_ptge [float]: percent of the populations which age is between 20 and 24 years old\n",
      "19) age_25-29_ptge [float]: percent of the populations which age is between 25 and 29 years old\n",
      "20) age_30-34_ptge [float]: percent of the populations which age is between 30 and 34 years old\n",
      "21) age_35-39_ptge [float]: percent of the populations which age is between 35 and 39 years old\n",
      "22) age_40-44_ptge [float]: percent of the populations which age is between 40 and 44 years old\n",
      "23) age_45-49_ptge [float]: percent of the populations which age is between 45 and 49 years old\n",
      "24) age_50-54_ptge [float]: percent of the populations which age is between 50 and 54 years old\n",
      "25) age_55-59_ptge [float]: percent of the populations which age is between 55 and 59 years old\n",
      "26) age_60-64_ptge [float]: percent of the populations which age is between 60 and 64 years old\n",
      "27) age_65-69_ptge [float]: percent of the populations which age is between 65 and 69 years old\n",
      "28) age_70-74_ptge [float]: percent of the populations which age is between 70 and 74 years old\n",
      "29) age_75-79_ptge [float]: percent of the populations which age is between 75 and 79 year old\n",
      "30) age_80-84_ptge [float]: percent of the populations which age is between 80 and 84 years old\n",
      "31) age_85-89_ptge [float]: percent of the populations which age is between 85 and 89 year old\n",
      "32) age_90-94_ptge [float]: percent of the populations which age is between 90 and 94 years old\n",
      "33) age_95-99_ptge [float]: percent of the populations which age is between 95 and 99 years old\n",
      "34) age_100+_ptge [float]: percent of the populations which is older than 100 years old.\n",
      "35) manpopulationptge [float]: percentage of masculine population in a region. calculated as follows: manpopulation/totalpopulation*100\n",
      "36) womanpopulationptge [float]: percentage of masculine population in a region. calculated as follows: womanpopulation/totalpopulation*100\n",
      "37) spanishptge [float]: percentage of people with spanish nationality in a region. calculated as follows: nativespanishpopulation/totalpopulation*100\n",
      "38) foreignersptge [float]: percentage of foreign people in a region. calculated as follows: foreignpopulation/totalpopulation*100\n",
      "39) samecomautonptge [float]: percentage of people who live in the same autonomic community (same province) that was born. calculated as follows: samecomautonpopulation/totalpopulation*100\n",
      "40) samecomautondiffprovptge [float]: percentage of people who live in the same autonomic community (different province) that was born. calculated as follows: samecomautondiffprovpopulation/totalpopulation*100\n",
      "41) difcomautonptge [float]: percentage of people who live in different autonomic community that was born. calculated as follows: samecomautondiffprovpopulation/totalpopulation*100\n",
      "42) unemployless25_ptge [float]: percent of unemployed people that are under 25 years and older than 18. it is calculated over the total amount of unemployment. (unemploymentless25_man+ unemploymentless25_woman)/totalunemployment*100\n",
      "43) unemploy25_40_ptge [float]: percent of unemployed people that are 25-40 years over the total amount of unemployment. (unemployment(25-40)_man+ unemployment(25-40)_woman )/totalunemployment*100\n",
      "44) unemploymore40_ptge [float]: percent of unemployed people that are older that 40 and younger than 69 years over the total amount of unemployment. (unemployment(40-69)_man+unemployment(40-69)_woman)/totalunemployment*100\n",
      "45) unemployless25_population_ptge [float]: percent of unemployed people younger than 25 and older than 18, over the total population of the region. note that the percent is calculated over the total population and not over the total active population. (unemploymentless25_man+ unemploymentless25_woman)/totalpopulation*100\n",
      "46) unemploy25_40_population_ptge [float]: percent of unemployed people (25-40) years old, over the total population of the region. note that the percent is calculated over the total population and not over the total active population. (unemployment(25-40)_man+ unemployment(25-40)_woman )/totalpopulation*100\n",
      "47) unemploymore40_population_ptge [float]: percent of unemployed people (40-69) years old, over the total population of the region. note that the percent is calculated over the total population and not over the total active population. (unemploymentless25_man+ unemploymentless25_woman)/totalpopulation*100\n",
      "48) agricultureunemploymentptge [float]: percent of unemployment in the agriculture sector relative to the total amount of unemployment. peopleunemployedinagriculture/totalunemployment*100\n",
      "49) industryunemploymentptge [float]: percent of unemployment in the industry sector relative to the total amount of unemployment. peopleunemployedinindustry/totalunemployment*100\n",
      "50) constructionunemploymentptge [float]: percent of unemployment in the construction sector relative to the total amount of unemployment. peopleunemployedinconstruction/totalunemployment*100\n",
      "51) servicesunemploymentptge [float]: percent of unemployment in the services sector relative to the total amount of unemployment. peopleunemployedinservices/totalunemployment*100\n",
      "52) notjobbeforeunemploymentptge [float]: percent of unemployment of people that didn’t have an employ before, over the total amount of unemployment. peopleunemployedwithoutemploybefore/totalunemployment*100\n",
      "references:\n",
      "[1] unemployment: www.datos.gob.es/es/catalogo/e00142804-paro-registrado-por-municipios\n",
      "[2] age distribution per region relation between spanish and foreigners relation between woman and man relation between people born in the same area or different areas of spain http://www.ine.es/dynt3/inebase/index.htm?type=pcaxis&file=pcaxis&path=%2ft20%2fe245%2fp05%2f%2fa2015\n",
      "[3] congress elections result of spanish election (june 2016) http://www.infoelectoral.interior.es/min/areadescarga.html?method=inicio\n",
      "context\n",
      "i am the director of a technical recruiting team and i would love to get a better look at their peaks, valleys, hurdles, and hot spots to see how i can better manager and show them the value in process improvements. the data is dirty at best they didnt always track or enter in the correct pieces. i have been here for 6 months and it has been a steady improvement. i welcome any recommendations or data you can suggest.\n",
      "you will see our client numbers from open projects, to the activity and team production\n",
      "content\n",
      "i am attaching what i call the mbo form \"management by objective\". the form is completed weekly by my team to help track how they are doing from an activity perspective. its a name i'm living with not loving just yet. this data is a year long look into how many people they screened how many people they submitted to projects and how many people they put on projects. the other issue is some of the data changed when i joined the team so please look at in in 2 pieces. jan - june 2016 july - dec 2016... i just started in june and added more granularity. there has also been some turn over so you will see partial data from some of my team\n",
      "acknowledgements\n",
      "my team is responsible for data gathering, im responsible for review and strategy adjustments\n",
      "inspiration\n",
      "i would like to get a better view into what their energy cycle looks like. (they work in waves). i would like to know what the sweet spot for motivation might be. (early in the month-later in the month) i would like to identify activity trends on a week to week basis help me understand if this is a constant - consistent level of activity or if i can get more juice from this orange i also dont know what i dont know... any insight is greatly appreciated\n",
      "context\n",
      "the interactive fiction competition is the internet's oldest game programming competition, starting in 1995 and continuing to this day. interactive fiction is an unusual genre because it has been centralized in the interactive fiction database for the last decade, and essentially all games of interest are recorded there. it is also a niche genre, and the amount of public interest has varied much less than other genres since 1995.\n",
      "content\n",
      "this database contains the placement (i.e. 1st place, 2nd place, etc.) of all games ever entered into the ifcomp, together with their genre, number of ifdb ratings and average ifdb rating. it also contains the system; a few programming languages have tended to dominate the competition. it also contains the forgiveness rating, which indicates how difficult the game can be expected to be. much of the information is null.\n",
      "note that the ifdb was created in 2006, 11 years after the competition began.\n",
      "inspiration\n",
      "ifdb ratings represent lasting interest, as these have been gathered over many years, while ifcomp rankings represent a short, 6-week period.\n",
      "it would be interesting to see the relationship between the two, how that relationship has changed over time, etc.\n",
      "also, inform and tads have always dominated the competition, but it's recently been changing. the change in systems over the years should be interesting.\n",
      "finally, it would be interesting to see time-adjusted averages of reviews for comp games over time, to see if new games are receiving less interest than old games.\n",
      "acknowledgements\n",
      "mike roberts has been the curator of ifdb since its inception. the ifcomp is currently run by jason macintosh under the aegis of the interactive fiction technology foundation.\n",
      "context\n",
      "data from: uci machine learning repository http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names\n",
      "content\n",
      "\"each record represents follow-up data for one breast cancer case. these are consecutive patients seen by dr. wolberg since 1984, and include only those cases exhibiting invasive breast cancer and no evidence of distant metastases at the time of diagnosis.\n",
      "the first 30 features are computed from a digitized image of a\n",
      "fine needle aspirate (fna) of a breast mass.  they describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "a few of the images can be found at\n",
      "http://www.cs.wisc.edu/~street/images/\n",
      "\n",
      "the separation described above was obtained using\n",
      "multisurface method-tree (msm-t) [k. p. bennett, \"decision tree\n",
      "construction via linear programming.\" proceedings of the 4th\n",
      "midwest artificial intelligence and cognitive science society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "the actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[k. p. bennett and o. l. mangasarian: \"robust linear\n",
      "programming discrimination of two linearly inseparable sets\",\n",
      "optimization methods and software 1, 1992, 23-34].\n",
      "\n",
      "the recurrence surface approximation (rsa) method is a linear\n",
      "programming model which predicts time to recur using both\n",
      "recurrent and nonrecurrent cases.  see references (i) and (ii)\n",
      "above for details of the rsa method. \n",
      "\n",
      "this database is also available through the uw cs ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/wpbc/\n",
      "1) id number 2) outcome (r = recur, n = nonrecur) 3) time (recurrence time if field 2 = r, disease-free time if field 2 = n) 4-33) ten real-valued features are computed for each cell nucleus:\n",
      "a) radius (mean of distances from center to points on the perimeter)\n",
      "b) texture (standard deviation of gray-scale values)\n",
      "c) perimeter\n",
      "d) area\n",
      "e) smoothness (local variation in radius lengths)\n",
      "f) compactness (perimeter^2 / area - 1.0)\n",
      "g) concavity (severity of concave portions of the contour)\n",
      "h) concave points (number of concave portions of the contour)\n",
      "i) symmetry \n",
      "j) fractal dimension (\"coastline approximation\" - 1)\"\n",
      "acknowledgements\n",
      "creators:\n",
      "dr. william h. wolberg, general surgery dept., university of\n",
      "wisconsin,  clinical sciences center, madison, wi 53792\n",
      "wolberg@eagle.surgery.wisc.edu\n",
      "\n",
      "w. nick street, computer sciences dept., university of\n",
      "wisconsin, 1210 west dayton st., madison, wi 53706\n",
      "street@cs.wisc.edu  608-262-6619\n",
      "\n",
      "olvi l. mangasarian, computer sciences dept., university of\n",
      "wisconsin, 1210 west dayton st., madison, wi 53706\n",
      "olvi@cs.wisc.edu \n",
      "inspiration\n",
      "i'm really interested in trying out various machine learning algorithms on some real life science data.\n",
      "annotated (bio) corpus for named entity recognition\n",
      "this corpus is made up of texts of news sites and built specifically to train the classifier to predict named entities such as per, loc, etc.\n",
      "annotation scheme: sentence parts of speech bio tags\n",
      "number of tagged entities:\n",
      "'o': 167112 'per': 11692 'org': 9736 'loc': 8431 'misc': 4195 count of sentences: 14000\n",
      "essential info about entities:\n",
      "o = other per = person org = organization loc = geographical entity misc = miscellaneous\n",
      "industrial demand/ response iot data for iot analytics. can be used for academic purpose only.\n",
      "highlights\n",
      "•\n",
      "facility energy management systems require interconnection and interoperation.\n",
      "•\n",
      "an iot-based communication framework with a common information model is proposed.\n",
      "•\n",
      "a practical example of an iot-based energy-management platform.\n",
      "•\n",
      "validation of the ability of the platform to enhance system interoperability.\n",
      "•\n",
      "the platform with improved energy management achieves energy saving targets.\n",
      "don't forget to cite:\n",
      "wei, min, seung ho hong, and musharraf alam. \"an iot-based energy-management platform for industrial facilities.\" applied energy 164 (2016): 607-619.\n",
      "context\n",
      "i have been a nascar fan since the 1970s and share the amazing history of this sport by creating and posting a simple dataset focused on 67 years of nascar champion history (1949-present).\n",
      "content\n",
      "there are five columns (year, driver, car number, manufacturer and wins) and 116 rows in this dataset. i will update the dataset every every year going forward after the champion has been determined. please suggest other data to be included in the dataset!\n",
      "acknowledgements\n",
      "the nascar information was compiled from information found on https://en.wikipedia.org/wiki/list_of_monster_energy_nascar_cup_series_champions\n",
      "background:\n",
      "the survey stands out as one of the most significant methodological innovations in the history of the social sciences. the instrument has made possible the efficient and reliable collection of measurements on individual attitudes, beliefs, values, behaviors, traits, and states (alwin 2009), and coupled with modern sampling techniques, surveys have allowed researchers to generalize findings to much larger populations with remarkable precision. survey data have helped governments to obtain essential descriptive information on, for example, their citizen’s living standards, demographic attributes, health status, purchasing intentions. such data have, in turn, been used to craft evidence-based social and economic policies, more accurately forecast labor market participation and project population distributions, and monitor poverty rates. commercial firms, especially those in the fields of market research and consumer behavior, have also been influential in the development of methods and the production of designed data (groves 2011). social scientists use survey data to study a wide range of social, economic, political, demographic, and cultural topics and to collect measurements of voter preferences, personality traits, social networks, and public opinions, to name a few. survey data are an essential component of evidence-based social policy-making and program monitoring and evaluation and vital tool for understanding the worldviews and aspirations of publics around the world.\n",
      "the progression of the social survey from its origins as a community and, later, national measuring device was attended by greater consensus and less variation in survey design and administration. over the course of more than 60 years of cross-cultural survey research, the standardized survey (gobo and mauceri 2014; harkness 2010) became the de facto one, with hallmark features including an almost exclusive reliance on input harmonized questionnaires and closed-ended questions, english language parent survey back translated into local-language-administered surveys, and best efforts at nationally representative sampling techniques that allow for generalization to, in most cases, the non-institutionalized adult population. with few exceptions, survivor programs originated in rich, western countries, and among survey programs originating in non-western countries, survey design and content has often been patterned after surveys designed to measure attitudes, beliefs, behaviors, states, and institutions endemic to western, industrialized societies. this is not to say that cross-national survey programs are monolithic or lacking in variety of design, content, international membership, and purposes.\n",
      "the data:\n",
      "to get a clearer understanding of how the social survey has spread around the world over the last 60 years, we collected information from just under 40 international survey programs and compiled it into a time-series data set that allows researchers to empirically evaluate the global social survey data infrastructure from a life-history perspective. collectively, these 40 survey programs have fielded over 7000 national social surveys comprising nearly 8 million completed interviews on a broad range of social, economic, and political topics. units of analysis in the data set are survey programs and national social surveys. additional data about programs and surveys include samples sizes, data collection methods, fielding dates, and countries surveyed, to name a few.\n",
      "participating social survey programs:\n",
      "how nations see each other (hnseo) pattern of human concerns data (phcd) civic culture study (ccs) attitudes toward europe study (ate) political participation and equality (ppe) eurobarometer (eurob) european values study (evs) world values survey (wvs) international social survey programme (issp) the political culture of southern europe (pcse) central and eastern eurobarometer (ceeb) post-communist publics (pcp) new european barometer (neb) comparative national elections project (cnep) new soviet citizen surveys (nscs) values and political change in post-communist europe (vpcp) latinobarometro (latinb) coping with government in the former soviet union (cgfe) afrobarometer (afrob) asia europe survey (ases) voice of the people surveys (votp) asian barometer (asianb) candidate countries eurobarometer (cceb) comparative study of electoral systems (cses) european social survey (ess) pew global attitudes surveys (pgas) worldviews 2002 (world)* asia barometer (asiab) transatlantic trends survey (tts) americasbarometer (amerab) arab barometer (arabb) east asia social survey (eass) the globalization of personal data project (gpd) a quest for citizenship in an ever closer europe (intune) caucasus barometer (caucab) transatlantic trends: immigration (tti) eu neighbourhood barometer (eunb)\n",
      "new: see this dataset visualized in d3\n",
      "context\n",
      "this data, acquired from the recreation information database catalog, contains campsite info for all campground facilities run by the united states national park service, the united states forest service, the bureau of land management and other federal government agencies. read the api documentation.\n",
      "content\n",
      "fields include facility id, campsite id, campsite type, facility name, facility location (latitude, longitude, state), and managing agency.\n",
      "acknowledgements\n",
      "a humble thank you to the federal agencies that collect these data and maintain public access, including usfs, nps, blm, usace, fws, and bor. thank you to american tax payers for keeping these facilities afloat. thank you to levi gadye for granting permission for the use of the cover photo.\n",
      "inspiration\n",
      "i was looking for a campsite for a group of friends in between salt lake city and grand teton np, and was struggling to find a tent-only campsite along the highway corridors leading to grand teton np. in the past i had always felt that finding tent only campsites in california was quite easy compared to other places i’ve camped in the american west. i tidied this data set from ridb to determine whether or not my ease in booking tent-only campsites in ca was due to a larger number of those sites, or if the state of ca had a relative enrichment for tent-only campsites.\n",
      "about\n",
      "the data is based on images i have taken with my lytro illum camera (https://pictures.lytro.com/ksmader) they have been exported as image data and depth maps. the idea is to make and build tools for looking at lytro image data and improving the results\n",
      "data\n",
      "the data are from the lytro illum and captured as 40mp images which are then converted to 5mp rgb+d images. all of the required data for several test images is provided\n",
      "questions/challenges\n",
      "build a neural network which automatically generates depth information from 2d rgb images\n",
      "build a tool to find gaps or holes in the depth images and fixes them automatically\n",
      "build a neural network which can reconstruct 3d pixel data from rgbd images\n",
      "content\n",
      "this dataset contains territorial claims across the entire interstate system between 1816-2001 and includes information on participants, dates, the significance of the claimed territories, and militarization of these claims. a territorial claim is defined as explicit contention between two or more nation-states claiming sovereignty over a specific piece of territory. official government representatives (i.e., individuals who are authorized to make or state foreign policy positions for their governments) must make explicit statements claiming sovereignty over the same territory.\n",
      "our goal is to identify cases where nation-states have disagreed over specific issues in the modern era, as well as measuring what made those issues valuable to them and studying how they chose to manage or settle those issues. the issue correlates of war (icow) project does not endorse official positions on any territorial claim. inclusion/exclusion of specific cases, and coding of details related to those cases, follows strict guidelines presented in the project's coding manuals.\n",
      "acknowledgements\n",
      "this data was collected by professor paul hensel of the university of north texas and his research assistants.\n",
      "los angeles residents have made roughly 4,000 complaints since 2011 about abandoned and vacant buildings in the city according to an analysis by the la times. this dataset was originally collected and analyzed for \"fire officials were concerned about westlake building where 5 died in a blaze\", a june 15, 2016, story by the los angeles times.\n",
      "lists of open and closed complaints filed with the los angeles department of building and safety were downloaded from the city's data portal. the two files were combined into a single spreadsheet. a new column called \"year received\" was generated from the existing \"date received\" field using libreoffice's year() function. the new file was named combined_complaints.csv.\n",
      "acknowledgements\n",
      "data and analysis originally published on the la times data desk github.\n",
      "this is a database dump from the website that is used to report on us college sailing results.\n",
      "this dataset is being upload for the purposes of creating a new ranking system for us college sailing.\n",
      "in the days of slide rules and punch cards, rand corp.'s a million random digits was a treasured reference work for statisticians and engineers. you can easily spend an afternoon browsing through the hundreds of loving tributes to the book on its amazon page.\n",
      "this dataset, used for testing our uploader, is a tribute to that classic tome.\n",
      "context: what is the subject matter of the dataset?\n",
      "content: what fields does it include?\n",
      "acknowledgements: who owns the dataset and how was it compiled?\n",
      "past research: in brief, what other analysis has been done on this data?\n",
      "inspiration: why is this dataset worthy of further analysis? what questions would you like answered by the community? what feedback would be helpful on the data itself?\n",
      "in 2013, students of the statistics class at fsev uk were asked to rate how much they like each one of 39 paintings (on a scale from 1 to 5). these comprise of 13 different art movements (exactly 3 paintings for each art movement).\n",
      "s1-s48: students' ratings, where one means \"don't like at all\" (integer)\n",
      "art movement: the art movement the painting belongs to (categorical)\n",
      "artist: the author of the painting (categorical)\n",
      "painting: the name of the painting (categorical)\n",
      "this is a usgs dataset which includes the position, type, and size of every wind turbine in the united states.\n",
      "the data was compiled by usgs in part using data from the federal aviation administration digital obstacle file to help identify wind turbines. see the link for a full description of the data and how it was collected.\n",
      "the dataset focus on find those zombie followers(fake account created by automated registration bot). all the fake accounts are human-like with both profile image and some personal information. they also have a lot of followers and posts. all the data are collected from weibo, a chinese twitter like platform. the dataset contains fake account's profile page and contents.\n",
      "please cite the following paper if you want to use it:\n",
      "linqing liu, yao lu, ye luo, renxian zhang, laurent itti and jianwei lu. \"detecting \"smart\" spammers on social network: a topic model approach.\" proceedings of naacl-hlt. 2016.\n",
      "this dataset contains information about the complaints made to the nypd from 2010 until the present. obtained from: https://nycopendata.socrata.com/social-services/311-service-requests-from-2010-to-present\n",
      "near-earth objects (neos) are comets and asteroids that have been nudged by the gravitational attraction of nearby planets into orbits that allow them to enter the earth's neighborhood. composed mostly of water ice with embedded dust particles, comets originally formed in the cold outer planetary system while most of the rocky asteroids formed in the warmer inner solar system between the orbits of mars and jupiter. the scientific interest in comets and asteroids is due largely to their status as the relatively unchanged remnant debris from the solar system formation process some 4.6 billion years ago. the giant outer planets (jupiter, saturn, uranus, and neptune) formed from an agglomeration of billions of comets and the left over bits and pieces from this formation process are the comets we see today. likewise, today's asteroids are the bits and pieces left over from the initial agglomeration of the inner planets that include mercury, venus, earth, and mars.\n",
      "as the primitive, leftover building blocks of the solar system formation process, comets and asteroids offer clues to the chemical mixture from which the planets formed some 4.6 billion years ago. if we wish to know the composition of the primordial mixture from which the planets formed, then we must determine the chemical constituents of the leftover debris from this formation process - the comets and asteroids.\n",
      "in the dataset files there are news from a turkish newspaper: hurriyet. the news are separated in sentences. stopwords, numbers and puctuation characters are all removed from the sentences. the sentences are separated from each other by a newline character. words are separated by a space. files are in utf-8 text format.\n",
      "this training.csv file of this dataset contains wind directions and speeds for 3 different street. the test.csv dataset contains wind directions and speeds for the first 2 streets. the goal is to predict the direction for street #3.\n",
      "mammography is the most effective method for breast cancer screening available today. however, the low positive predictive value of breast biopsy resulting from mammogram interpretation leads to approximately 70% unnecessary biopsies with benign outcomes. to reduce the high number of unnecessary breast biopsies, several computer-aided diagnosis (cad) systems have been proposed in the last years.these systems help physicians in their decision to perform a breast biopsy on a suspicious lesion seen in a mammogram or to perform a short term follow-up examination instead. this data set can be used to predict the severity (benign or malignant) of a mammographic mass lesion from bi-rads attributes and the patient's age. it contains a bi-rads assessment, the patient's age and three bi-rads attributes together with the ground truth (the severity field) for 516 benign and 445 malignant masses that have been identified on full field digital mammograms collected at the institute of radiology of the university erlangen-nuremberg between 2003 and 2006. each instance has an associated bi-rads assessment ranging from 1 (definitely benign) to 5 (highly suggestive of malignancy) assigned in a double-review process by physicians. assuming that all cases with bi-rads assessments greater or equal a given value (varying from 1 to 5), are malignant and the other cases benign, sensitivities and associated specificities can be calculated. these can be an indication of how well a cad system performs compared to the radiologists.\n",
      "class distribution: benign: 516; malignant: 445\n",
      "attribute information:\n",
      "6 attributes in total (1 goal field, 1 non-predictive, 4 predictive attributes)\n",
      "bi-rads assessment: 1 to 5 (ordinal, non-predictive!)\n",
      "age: patient's age in years (integer)\n",
      "shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
      "margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
      "density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
      "severity: benign=0 or malignant=1 (binominal, goal field!)\n",
      "missing attribute values: - bi-rads assessment: 2 - age: 5 - shape: 31 - margin: 48 - density: 76 - severity: 0\n",
      "i acknowledge that this dataset is not mine and i have only reformatted the data and uploaded it to kaggle. source:\n",
      "matthias elter fraunhofer institute for integrated circuits (iis) image processing and medical engineering department (bmt) am wolfsmantel 33 91058 erlangen, germany matthias.elter '@' iis.fraunhofer.de (49) 9131-7767327\n",
      "prof. dr. rüdiger schulz-wendtland institute of radiology, gynaecological radiology, university erlangen-nuremberg universitätsstraße 21-23 91054 erlangen, germany\n",
      "relevant papers:\n",
      "m. elter, r. schulz-wendtland and t. wittenberg (2007) the prediction of breast cancer biopsy outcomes using two cad approaches that both emphasize an intelligible decision process. medical physics 34(11), pp. 4164-4172\n",
      "citation request:\n",
      "m. elter, r. schulz-wendtland and t. wittenberg (2007) the prediction of breast cancer biopsy outcomes using two cad approaches that both emphasize an intelligible decision process. medical physics 34(11), pp. 4164-4172\n",
      "movie revenue depends on multiple factors such as cast, budget, film critic review, mpaa rating, release year, etc. because of these multiple factors there is no analytical formula for predicting how much revenue a movie will generate. however by analyzing revenues generated by previous movies, one can build a model which can help us predict the expected revenue for a movie. such a prediction could be very useful for the movie studio which will be producing the movie so they can decide on expenses like artist compensations, advertising, promotions, etc. accordingly. plus investors can predict an expected return-on-investment.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "corpus is the latin word for \"body.\" the coursera sponsored data science capstone corpus is extracted from blogs, news and twitter text files archived in the hc corpora english language online database, a collection of free text corpora\n",
      "facial keypoints detection -> improving prediction\n",
      "context & content\n",
      "the dataset is used to perform sentiment analysis and contains words that are specific to the pharma sector along with their respective polarity (pos/neg).\n",
      "this is my portfolio for my next step in a data science career. for the last 5 years i worked in a ukrainian bank and all this time i asked to myself: \"how can i use market risk knowledge, where i'll use yet to maturity calculation or expected lose methodology?\" several months ago my old school friend showed me betfair. and i started wondering how i can use match, web scraping techniques and data mining experience to pick up some money. in this case i give some ideas to kaggle users and show my results.\n",
      "past research\n",
      "i start from these questions:\n",
      "1. what kind of horses win more money then lose?\n",
      "i found 6 mask: i load data for 8 years and create pivot table with race results and type of races by distance and horses options. after that i calculate sum of ppmin and ppmax for this horse. and finally, i calculate dispersion to filter more stable masks.\n",
      "2. how many selected horses run daily/monthly? is something change when the season changes?\n",
      "3. what kind of bets do i need to use and how much money do i need to start? when should i start?\n",
      "content\n",
      "data storage = http://www.betfairpromo.com/betfairsp/prices/index.php\n",
      "betfair rules:\n",
      "minimal bet = 4$\n",
      "base commission for win on bf platform = 6.5%\n",
      "inspiration\n",
      "i'm looking for a job and similar developers who can join me in other projects.\n",
      "context\n",
      "the dataset of registered second-level domains fetched from domains-index lists. used dataset for our internet second-level domain names research.\n",
      "content\n",
      "just one column with second-level domain names, one per line. about 20m of records.\n",
      "past research\n",
      "the results of the research we've made are here 1\n",
      "understanding liquid foam is essential for a number of applications for fighting fires to making food and even extracting oil. one of the best techniques for looking inside 3d structures of foam is x-ray tomography. these type of measurements, however, cannot see bubbles, they can only see the 'plateau borders' where water collects.\n",
      "the challenge for this data is to find the bubbles from the outlines left by the plateau borders. the dataset include a segmented 3d image of the borders for one sample. the bubbles must be found and extracted and output to a list of x,y,z, volume points indicating the bubble center (x,y,z) and size (volume in voxels).\n",
      "context\n",
      "this is a dataset containing some fictional job class specs information. typically job class specs have information which characterize the job class- its features, and a label- in this case a pay grade - something to predict that the features are related to.\n",
      "content\n",
      "the data is a static snapshot. the contents are id column - a sequential number job family id job family description job class id job class description paygrade- numeric education level experience organizational impact problem solving supervision contact level financial budget pg- alpha label for paygrade\n",
      "acknowledgements\n",
      "this data is purely fictional\n",
      "inspiration\n",
      "the intent is to use machine learning classification algorithms to predict pg from educational level through to financial budget information.\n",
      "typically job classification in hr is time consuming and cumbersome as a manual activity. the intent is to show how machine learning and people analytics can be brought to bear on this task.\n",
      "challenge: can you predict the house price?\n",
      "this is a challenging task, as it requires to first parse the features and the identify the relevant features for house prediction. as is the case with many real world datasets, in this dataset not all houses have the same features or some houses even have - for the prediction - irrelevant features, so maybe you can think of some clever way to union all the features before you process them with your favourite machine learning algorithm.\n",
      "in case you need more fresh data ore you need more houses for your algorithm to work, you can find the yet to be improved scrapy script here this page is a forum for data scientist i started, in hope , that you will participate and maybe even improve the scrapy script.\n",
      "kind regards\n",
      "orges leka\n",
      "context\n",
      "specint2006 rate results for intel xeon scalable processors\n",
      "content\n",
      "data collected on october 30, 2017\n",
      "acknowledgements\n",
      "the standard performance evaluation corporation (spec)\n",
      "intel ark\n",
      "photo by samuel zeller on unsplash\n",
      "inspiration\n",
      "intel introduced new processor names: platinum, gold, silver and bronze. it would be nice to visualise difference between them.\n",
      "increased automobile ownership and use in china over the last two decades has increased energy consumption, worsened air pollution, and exacerbated congestion. the government of shanghai has adopted an auction system to limit the number of license plates issued for every month. the dataset contains historical data of auctions from jan 2002 to oct 2017.\n",
      "how the auction system works: a starting price is given at the beginning of the auction, bidders can only bid up to 3 times for each auction and can only mark up or down within 300 cny ( roughly 46 usd ) for each bid. at the end of each auction, only the top n ( number of plates that will be issued for the month ) bids will get the license plates at the cost of their bids. the nth bid will be the lowest deal price for the month. please note that the auctions are conducted online and each bidder will not be able to see other bids.\n",
      "columns:\n",
      "date: jan 2002 to oct 2017 ( note that feb 2008 is missing)\n",
      "*num_bidder*: number of citizens who participate the auction for the month\n",
      "*num_plates*: number of plates that will be issued by the government for the month\n",
      "*lowest_deal_price*: explained above, in cny\n",
      "*avg_deal_price*: average deal price, in cny ( note that since each bid can only be marked up or down within 300, it is not drifting too far away from the lowest deal price)\n",
      "the goal is to predict the lowest_deal_price for each month, the actual result will be updated at the end of every month\n",
      "the dataset is scraped from http://www.51chepai.com.cn/paizhaojiage/\n",
      "contact: ran_su147@hotmail.com\n",
      "the 500 cities project is a collaboration between cdc, the robert wood johnson foundation, and the cdc foundation. the purpose of the 500 cities project is to provide city- and census tract-level small area estimates for chronic disease risk factors, health outcomes, and clinical preventive service use for the largest 500 cities in the united states. these small area estimates will allow cities and local health departments to better understand the burden and geographic distribution of health-related variables in their jurisdictions, and assist them in planning public health interventions\n",
      "christopher columbus, or cristóbal colón in spanish, is an amazingly difficult man to pin down. born about 1451, he died on 20 may 1506. we know that he sailed the atlantic and reached the americas on october 12, 1492 under the sponsorship of the spanish kingdom of castile.\n",
      "his voyage in 1492 marked the beginning of european exploration of the americas.\n",
      "more information :http://www.christopher-columbus.eu/logs.htm\n",
      "content:\n",
      "date: complete date: year-month-day\n",
      "day: day of month\n",
      "month: month in character\n",
      "year: 1492 or 1493\n",
      "text: the logbook itself\n",
      "nmonth: month as decimal number (1–12)\n",
      "leagues: distance in leagues\n",
      "course: course of the voyage\n",
      "more things that can be done: what says christopher columbus and what bartolome de las casas, locations, voyage indications, etc\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this dataset was created during my phd (http://www.tdg-seville.info/fogallego/personal%20info) at the university of seville. we didn't found any datasets with labelled conditions so we decided to build one since our main goal for the phd was to be able to identify conditions without relying on user-defined patterns or requiring any specific-purpose dictionaries, taxonomies, or heuristics.\n",
      "content\n",
      "the reviews in english and spanish were randomly gathered from ciao.com between april 2017 and may 2017. the sentences were classified into 15 domains according to their sources, namely: adults, baby care, beauty, books, cameras, computers, films, headsets, hotels, music, ovens, pets, phones, tv sets, and video games.\n",
      "our dataset consist of two files: sentences.csv and conditions.csv. the first one contains the whole set of sentences and the second one the manually labelled conditions.\n",
      "in order to better understand the meaning of each column, i'll explain them in detail:\n",
      "sentence.csv:\n",
      "sentence_uuid: the unique identifier of the sentence\n",
      "sentence_text: the text of the sentence\n",
      "language: the language of the sentence\n",
      "domain: the domain of the sentence\n",
      "labelled: whether the sentence was labelled or not\n",
      "conditions.csv:\n",
      "sentence_uuid: the unique identifier of the corresponding labelled sentence\n",
      "condition_uuid: the unique identifier of the condition\n",
      "begin_connective: the character position where the connective of the condition starts\n",
      "end_connective: the character position where the connective of the condition ends\n",
      "begin_condition: the character position where the rest of the condition starts\n",
      "end_condition: the character position where the rest of the condition ends\n",
      "language: the language of the corresponding labelled sentence\n",
      "domain: the domain of the corresponding labelled sentence\n",
      "acknowledgements\n",
      "my phd and this dataset were supported by opileak.com and the spanish r&d programme (grants tin2013- 40848-r and tin2013-40848-r).\n",
      "context\n",
      "can you tell geographical stories about the world using data science?\n",
      "content\n",
      "world countries with their corresponding continents , official english names, official french names, dial,itu,languages and so on.\n",
      "acknowledgements\n",
      "this data was gotten from https://old.datahub.io/\n",
      "inspiration\n",
      "exploration of the world countries: - can we graphically visualize countries that speak a particular language? - we can also integrate this dataset into others to enhance our exploration. - the dataset has now been updated to include longitude and latitudes of countries in the world.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "my objective sharing this data is to make studies about stock quotes using real data from the brazilian stock market.\n",
      "content\n",
      "this is the daily stock quotes from b3 for the 2017 year. b3 is the unique stock exchange in brazil, here we don't have competition in this sector as in usa.\n",
      "acknowledgements\n",
      "this data is directly extracted from b3 site, without changes.\n",
      "context\n",
      "see description for unvectorized full text dataset. post clustering, the same dataset. level_0 is the manually picked out clusters; level_1 is the hdbscan clusters. also left in the text hash this time. looking back through the data, i am still amazed by what a great job hdbcsan did at breaking everything up (down?). check out leland's project and try it for yourself.\n",
      "context\n",
      "this dataset contains information on all 71 cards of clash royale. the information contained in this dataset include card name, card level, cost, count, crown tower damage, damage per second etc. the information was scraped from clash royale wikipedia page\n",
      "content\n",
      "card level (spawn level): name of the card/character\n",
      "cost: elixir\n",
      "count: number of troops in the card\n",
      "crown tower damage: damage to the tower\n",
      "damage: damage per hit\n",
      "damage per second: hits per second * damage\n",
      "death damage: damage just before death\n",
      "health (+shield): hit points\n",
      "hit speed: hit speed of the card\n",
      "level: level of the card\n",
      "maximum spawned: in case of spawn troops\n",
      "radius: radius covered in the arena when deployed\n",
      "range: range within the arena\n",
      "spawn dps: damage per second\n",
      "spawn damage: damage\n",
      "spawn health: hit points\n",
      "spawn speed: speed of the character\n",
      "spawner health: character health\n",
      "troop spawned: number of troops spawned\n",
      "type: damaging spells, spawners, troops and defenses\n",
      "acknowledgements\n",
      "data was scraped from clash royale wikipedia page using beautiful soup 4 library.\n",
      "inspiration\n",
      "clash royale is an interesting mobile game. i have been playing this game since some months now. wonder if this could be run by a bot using some ai library. this dataset is very basic and does not cover the detailed information about the cards transformation as per levels. with this dataset can we answer the following questions -:\n",
      "group cards having similar impact considering the range, speed, damage etc ?\n",
      "which cards can be used best against the other ?\n",
      "i will update the data set with the level data soon. thanks.\n",
      "context\n",
      "urdu is an indo-aryan language with over 100 million speakers. it is mainly used in pakistan. while often mutually intelligible with hindi, the two languages use different scripts and as a result nlp tools developed for hindi are often not extendible to urdu.\n",
      "some words, like “the” or “and” in english, are used a lot in speech and writing. for most natural language processing applications, you will want to remove these very frequent words. this is usually done using a list of “stopwords” which has been complied by hand.\n",
      "content\n",
      "this dataset is a list of 517 high-frequency urdu words. very high frequency words are not generally useful for most nlp tasks and are generally removed as part of pre-processing.\n",
      "the .json and .txt files have the same words in them.\n",
      "acknowledgements:\n",
      "this dataset is copyright (c) 2016 gene diaz and distributed here under an mit license. see the attached license file for more information.\n",
      "inspiration:\n",
      "this is more of a utility nlp dataset than one that is interesting in its own right. try using it with these other urdu datasets:\n",
      "urdu-nepali parallel corpus: a part of speech tagged corpus for urdu & nepali\n",
      "the holy quran (in 21 languages, including urdu)\n",
      "old newspapers: a cleaned subset of hc corpora newspapers\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "a dataset of information on deaths related to the northern ireland conflict (1969-2005).\n",
      "content\n",
      "fatalities: a list of deaths related to the northern ireland conflict (fact table)\n",
      "location: a list of locations where the fatalities took place [including gps coordinates] (links to fact table via the \"location\" field)\n",
      "agency: this refers to groups responsible for fatal incidents [including agency groups] (links to fact table via the \"agency\" field)\n",
      "status: this refers to the role of fatality victims [including status groups] (links to fact table via the \"status\" field)\n",
      "dimension remarks\n",
      "name: spellings might vary from other sources\n",
      "year: relates to year of death rather than fatal incident\n",
      "religion: only that of northern ireland residents is listed\n",
      "agency: this refers to groups responsible for fatal incidents\n",
      "status: this refers to the role of fatality victims\n",
      "location: westminster electoral areas are employed for northern ireland fatalities\n",
      "rationale: this refers to the inferred purpose underlying the fatal act\n",
      "causality: this probes the degree of inferred purposiveness of a fatal event\n",
      "context: this distinguishes between incidents such as gun fire, explosions, beatings\n",
      "new incident: this offers a count of discrete fatal incidents\n",
      "1st fatality: this distinguishes multiple fatality incidents\n",
      "acknowledgements\n",
      "the information was compiled by michael mckeown and was contributed by him to the cain web site. michael mckeown has taken the decision (june 2009) to make the dataset freely available via the cain site. while users are free to download the dataset for research purposes, the database remains copyright © of michael mckeown.\n",
      "inspiration\n",
      "\"the following study represents both the revisiting and continuation of a task which had occupied me for over twenty years. the concluded work highlights complexities and ambiguities in the patterns of the violence in northern ireland over the past three decades which are often obscured by the polar interpretations offered by partizan commentaries. for that reason i believe it should be inserted into the public arena for further consideration and possibly as a methodological model for further enquiry.\" michael mckeown (2001).\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "mr. robot is all about data whether it's corrupting it, encrypting it, or deleting it. i wanted to dig up some data on my favorite show.\n",
      "content\n",
      "each episode has a corresponding .csv file with the first column being the word and the second column being how often it appears in that episode.\n",
      "my python code can be found at https://github.com/emmabel96/wordoccurrences\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this is the job data for the last one month (300+ u.s.-based companies) downloaded from jobspikr - a job data delivery platform that extracts job data from various company sites across the globe on daily basis powered by machine learning techniques.\n",
      "content\n",
      "following data fields are available in this dataset:\n",
      "url\n",
      "job title\n",
      "job text\n",
      "job posting date\n",
      "last date to apply\n",
      "inspiration\n",
      "it'd be interesting to perform text mining on the job description (perhaps lda to classify the job types).\n",
      "context\n",
      "how do you design a computer vision algorithm that is able to detect and segment people when they are captured by a visible light camera, thermal camera, and a depth sensor? and how do you fuse the three inherently different data streams such that you can reliably transfer features from one modality to another? feel free to download our dataset and try it out yourselves!\n",
      "content\n",
      "the dataset features a total of 5724 annotated frames divided in three indoor scenes. activity in scene 1 and 3 is using the full depth range of the kinect for xbox 360 sensor whereas activity in scene 2 is constrained to a depth range of plus/minus 0.250 m in order to suppress the parallax between the two physical sensors. scene 1 and 2 are situated in a closed meeting room with little natural light to disturb the depth sensing, whereas scene 3 is situated in an area with wide windows and a substantial amount of sunlight. for each scene, a total of three persons are interacting, reading, walking, sitting, reading, etc.\n",
      "every person is annotated with a unique id in the scene on a pixel-level in the rgb modality. for the thermal and depth modalities, annotations are transferred from the rgb images using a registration algorithm found in registrator.cpp.\n",
      "we have used our aau vap multimodal pixel annotator to create the ground-truth, pixel-based masks for all three modalities.\n",
      "acknowledgements\n",
      "palmero, c., clapés, a., bahnsen, c., møgelmose, a., moeslund, t. b., & escalera, s. (2016). multi-modal rgb–depth–thermal human body segmentation. international journal of computer vision, pp 1-23.\n",
      "context\n",
      "video context\n",
      "description\n",
      "build your own version of the seefood app from the tv show silicon valley. whether you've seen the show or not, you should watch a refresher on how it works. this dataset has everything you need to build the seefood app.\n",
      "this data was extracted from the food 101 dataset. a full version of the dataset is available here.\n",
      "acknowledgements\n",
      "original data from this paper.\n",
      "context\n",
      "bootstrapped and manually annotated ner swedish web news from 2012. ner stands for named entity recognition, and its used to describe entities in a text such as organisations, locations and people for instance.\n",
      "its a very common operation in general nlp pipeline, and several algorithms can be used to train a model. traditionally many ner systems were trained using some kind of crf (conditionally random fields) approach, but nowadays many people successfully uses lstm:s or other sequence based deep learning techniques.\n",
      "a tutorial on how to use this dataset to train an ner for stanford corenlp is available here https://medium.com/@klintcho/training-a-swedish-ner-model-for-stanford-corenlp-part-2-20a0cfd801dd\n",
      "content\n",
      "the dataset is very simple and can easily be adapted into other formats, it is specifically adapted to corenlp ner. thus the first column is a word. second column (tab separated) is either the ner category (org, per, loc, misc) or a 0 if it does not belong to any category (not an entity). each word is separated by a new line, and each sentence is separated by an empty new line.\n",
      "sample structure (of two sentences, one three word sentence, and another 4 word sentence): apple org is 0 nice 0 . 0\n",
      "per per is 0 not 0 sad 0\n",
      "acknowledgements\n",
      "text is annotated from http://spraakbanken.gu.se/eng/resource/webbnyheter2012. thanks norah klintberg sakal for helping out with the annotation and reviewing all annotations as well.\n",
      "inspiration\n",
      "feel free to use this for whatever you like. as most datasets it would definitely benefit from becoming larger, feel free to create a pull request https://github.com/klintan/swedish-ner-corpus/ or update it here on kaggle.\n",
      "context\n",
      "malicious websites are of great concern. unfortunately there is a lack of datasets that classify malicious vs benign web characteristics. this dataset is a research production of my bachelor students that aims to fill this gap.\n",
      "content\n",
      "the project sought to evaluate classification models to predict malicious and benign websites, based on application layer and network characteristics. the data were obtained by using different verified sources of benign and malicious url, in a low interactive client honeypot to isolate network traffic. we used additional tools to get other information, such as server country with whois.\n",
      "this is the first version and we have some initial results from applying machine learning classifiers in a bachelor thesis. further details on the data process making and the data description can be found in the article below.\n",
      "acknowledgements\n",
      "if your papers or other works use our dataset, please cite our paper:\n",
      "urcuqui, c., navarro, a., osorio, j., & garcıa, m. (2017). machine learning classifiers to detect malicious websites. ceur workshop proceedings. vol 1950, 14-17.\n",
      "if you need a review article of website cybersecurity state of the art (in english and spanish):\n",
      "urcuqui, c., peña, m. g., quintero, j. l. o., & cadavid, a. n. (2017). antidefacement. sistemas & telemática, 14(39), 9-27\n",
      "if you have any question or feedback, please contact me: ccurcuqui@icesi.edu.co\n",
      "this dataset does not have a description yet.\n",
      "sign language digits dataset\n",
      "dataset github page: github.com/ardamavi/sign-language-digits-dataset\n",
      "by turkey ankara ayrancı anadolu high school students\n",
      "turkey ankara ayrancı anadolu high school's sign language digits dataset\n",
      "this dataset prepared by our school students.\n",
      "dataset preview:\n",
      "details of datasets:\n",
      "image size: 64x64\n",
      "color space: grayscale\n",
      "file format: npy\n",
      "number of classes: 10 (digits: 0-9)\n",
      "number of participant students: 218\n",
      "number of samples per student: 10\n",
      "details of datasets in github repo:\n",
      "repo: github.com/ardamavi/sign-language-digits-dataset\n",
      "image size: 100x100\n",
      "color space: rgb\n",
      "project executives:\n",
      "zeynep dikle & arda mavi\n",
      "turkey ankara ayrancı anadolu high school students\n",
      "for development:\n",
      "processing dataset:\n",
      "for processing the dataset, look up arda mavi's github gist: gist.github.com/ardamavi/get_dataset.py\n",
      "content\n",
      "plain text, pulled from github, sorted and concatenated into one file per language. among those languages are:\n",
      "abap: 0.138gb\n",
      "actionscript: 0.684gb\n",
      "ada: 0.002gb\n",
      "assembly: 2.134gb\n",
      "c: 4.452gb\n",
      "clojure: 0.136gb\n",
      "cobol: 0.483gb\n",
      "code: 7.725gb\n",
      "cpp: 3.248gb\n",
      "crystal: 0.069gb\n",
      "csharp: 1.205gb\n",
      "css: 0.881gb\n",
      "cuda: 0.275gb\n",
      "d: 0.990gb\n",
      "dart: 0.655gb\n",
      "delphi: 0.514gb\n",
      "erlang: 0.343gb\n",
      "fortran: 1.127gb\n",
      "go: 4.471gb\n",
      "haskell: 0.447gb\n",
      "html: 2.158gb\n",
      "java: 1.049gb\n",
      "js: 4.863gb\n",
      "julia: 0.144gb\n",
      "lua: 0.301gb\n",
      "matlab: 0.257gb\n",
      "perl: 0.585gb\n",
      "php: 1.300gb\n",
      "prolog: 0.146gb\n",
      "python: 0.911gb\n",
      "r: 0.214gb\n",
      "ruby: 0.625gb\n",
      "rust: 0.434gb\n",
      "sas: 0.272gb\n",
      "scala: 0.458gb\n",
      "shell: 0.175gb\n",
      "tex: 0.554gb\n",
      "vbnet: 0.389gb\n",
      "xml: 5.160gb\n",
      "coffeescript: 0.106gb\n",
      "lisp: 0.699gb\n",
      "useful things\n",
      "data loader written in python and simple classifying lstm (tensorflow), are going to be available here, once i put them there.\n",
      "acknowledgements\n",
      "i would like to thank to all contributors to any repository on github as it's hard to thank only to the contributors of repositories presented in this dataset. but i'll try anyway: if you've contributed to one or more repository in this list, thank you.\n",
      "i'm sorry, if i forgot to mention you, even though your code is in the dataset, it's hard to keep in memory list of this size. if you feel your repository should be there, feel free to write me.\n",
      "also, i'd like to thank those guys for providing this cheesy hi-res stock image of code.\n",
      "context:\n",
      "in working on unicode implementations, it is often useful to access the full content of the unicode character database (ucd). for example, in establishing mappings from characters to glyphs in fonts, it is convenient to see the character scalar value, the character name, the character east asian width, along with the shape and metrics of the proposed glyph to map to; looking at all this data simultaneously helps in evaluating the mapping.\n",
      "this is a machine-readable version of the unicode character database in json format.\n",
      "content:\n",
      "the majority of information about individual codepoints is represented using properties. each property, except for the special_case_condition and name_alias properties, is represented by an attribute. in an xml data file, the absence of an attribute (may be only on some code-points) means that the document does not express the value of the corresponding property. conversely, the presence of an attribute is an expression of the corresponding property value; the implied null value is represented by the empty string.\n",
      "the name_alias property is represented by zero or more name-alias child elements. unlike the situation for properties represented by attributes, it is not possible to determine whether all of the aliases have been represented in a data file by inspecting that data file.\n",
      "the name of an attribute is the abbreviated name of the property as given in the file propertyaliases.txt in version 6.1.0 of the ucd. for the unihan properties, the name is that given in the various versions of the unihan database (some properties are no longer present in version 6.1.0).\n",
      "for catalog and enumerated properties, the values are those listed in the file propertyvaluealiases.txt in version 6.1.0 of the ucd; if there is an abbreviated name, it is used, otherwise the long name is used. note that the set of possible values for a property captured in this schema may change from one version to the next.\n",
      "the following properties are associated with code points:\n",
      "age property\n",
      "name properties\n",
      "name aliases\n",
      "block\n",
      "general category\n",
      "combining properties\n",
      "bidirectionality properties\n",
      "decomposition properties\n",
      "numeric properties\n",
      "joining properties\n",
      "linebreak properties\n",
      "east asian width property\n",
      "case properties\n",
      "script properties\n",
      "iso comment properties\n",
      "hangul properties\n",
      "indic properties\n",
      "identifier and pattern and programming language properties\n",
      "properties related to function and graphic characteristics\n",
      "properties related to boundaries\n",
      "properties related to ideographs\n",
      "miscellaneous properties\n",
      "unihan properties\n",
      "tangut data\n",
      "nushu data\n",
      "for additional information, please consult the full documentation on the unicode website.\n",
      "acknowledgements:\n",
      "copyright © 1991-2017 unicode, inc. all rights reserved. distributed under the terms of use in http://www.unicode.org/copyright.html.\n",
      "permission is hereby granted, free of charge, to any person obtaining a copy of the unicode data files and any associated documentation (the \"data files\") or unicode software and any associated documentation (the \"software\") to deal in the data files or software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, and/or sell copies of the data files or software, and to permit persons to whom the data files or software are furnished to do so, provided that either (a) this copyright and permission notice appear with all copies of the data files or software, or (b) this copyright and permission notice appear in associated documentation.\n",
      "this dataset does not have a description yet.\n",
      "as the tagline of ‘american association of suicidology’ says i strongly believe that suicide prevention is everyone’s business. the act of ending one’s own life stating the reasons to be depression, alcoholism or any other mental disorders for that matter is not a considerable idea keeping in mind that anything can be overcome with reliable help and lifestyle. we can choose to stand together in the face of a society which may often feel like a lonely and disconnected place, and we can choose to make a difference by making lives more livable for those who struggle to cope. through this project, i am hoping to identify the trends of suicidal rates by country, gender, age and ethnicity. and relate the trends to the possible reasons that leads to the drastic decision, which might help us to curb the thought in the very beginning.\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too. data on suicides is deficient for two reasons, first of all, there is a problem with the frequency and reliability of vital registration data in many countries – an issue that undermine the quality of mortality estimates in general, not just suicide. secondly, there are problems with the accuracy of the official figures made available, since suicide registration is a complicated process involving several responsible authorities with medical and legal concerns. moreover, the illegality of suicidal behavior in some countries contributes to under reporting and misclassification. i was lucky enough to obtain enough data from different reliable resources. i will be starting off the project with the most reliable datasets available for us on suicide.\n",
      "•world health organization (who) dataset which contains entity wise suicide rates, crude suicide rates per gender and country which are age standardized which has a geographical coverage of 198 countries. the time spanning from 1950-2011.\n",
      "•samaritans statistics report 2017 including data for 2013-2015, in order to reduce the time, it takes to register deaths, the maximum time between a death and registration is eight days.\n",
      "•american association of suicidology facts and statistics which are categorized by age, gender, region and ethnicity.\n",
      "inspiration: to visualize the trends and patterns by merging different datasets available regarding the subject matter from different organizations, deriving the major causes for the drastic stride. and also observing the changes in patterns over the years by country, sex and ethnicity\n",
      "understanding the data: it is always tricky to understand the suicide statistics as they may not be so straight forward as they appear to be. generally, the rate is per 100,000. it is done this way to adjust the underlying population size. ‘age-standardized’ rates have been standardized to the world population to increase the confidence while making the comparisons. on the other hand, ‘crude rates’ have not been standardized like the prior, so they are just the basic calculation of number of deaths divided by the population (x100,000). the size of the population and specific cohort is also to be taken into account as smaller groups often produce less reliable rates per 100,000. when examining the suicide trends over a period of time it is also important to look over a relatively long period. increases and decreases for a year at a time should not be considered in isolation.\n",
      "context\n",
      "i want my computer to react to simple, short, predefined commands. i do not need it to understand any meaning or do complex things, just to recognize and react. i created this dataset to explore possible audio classification algorithms that can ultimately be transferred to the real world.\n",
      "content\n",
      "i have selected single-syllable english verbs, obtained their pronounciations (phonemes) via to the british english example pronciation dictionary, and let espeak pronounce it varying the pronounciations, stress, pitch, speed and speaker.\n",
      "you can play individual words samples with any audio player that understands ogg (e.g. vlc). the samples still sound a bit mechanic, but it is a start.\n",
      "for simplicity, consider the dog sub-sample, which only contains four commands: \"chase\" \"fetch\" \"sit\" \"walk\". they are fairly easy for the ear to distinguish, even in poor quality audio.\n",
      "to create the classification data set (db.dog.hdf5), i added aurora noise with varying volume and convertedd the audio data to 24x24 frequency-vs-time \"images\" (spectrograms). the file comes with class labels (word id).\n",
      "example python script to load and train: https://github.com/johannesbuchner/spoken-command-recognition/blob/master/traincommanddetect_svm.py\n",
      "more details and generating scripts can be found at https://github.com/johannesbuchner/spoken-command-recognition\n",
      "acknowledgements\n",
      "pronounciation dictionary: beep: http://svr-www.eng.cam.ac.uk/comp.speech/section1/lexical/beep.html noise samples: aurora: https://www.ee.columbia.edu/~dpwe/sounds/noise/ espeak: http://espeak.sourceforge.net/ and mbrola voices http://www.tcts.fpms.ac.be/synthesis/mbrola/mbrcopybin.html\n",
      "inspiration\n",
      "the open question is if these computer-generated data sets, robustified by pronounciation and noise variations, can be transferred into real applications.\n",
      "at the moment i see companies trying to solve a hard problem (map arbitrary, open-ended speech to text and identify meaning), while the easier problem of detecting a predefined word and mapping it to a predefined action should be solvable with currently available tools. machine learning audio training data is lacking, and this aims to solve that.\n",
      "this dataset does not have a description yet.\n",
      "content\n",
      "dataset created by jose portilla and pierian data for his udemy course (python for data science and machine learning bootcamp) if you want to be a data scientist i cannot recommend this course enough\n",
      "inspiration\n",
      "explore the data to find predict who is more likely to click the ad!\n",
      "context\n",
      "this is the dataset from the 2017 expat insider survey conducted through internations.\n",
      "content\n",
      "this is the dataset from the 2017 expat insider survey conducted through internations. the data is based on surveys conducted on expats regarding different aspects of living in their respective foreign countries. all values are rankings. i was not able to find an official dataset, so all data was hand typed from the official report found here https://www.internations.org/expat-insider/. this dataset only includes countries who had values in all of the conducted surveys.\n",
      "acknowledgements\n",
      "find the official report here https://cms-internationsgmbh.netdna-ssl.com/cdn/file/2017-09/expat_insider_2017_the_internations_survey.pdf.\n",
      "inspiration\n",
      "i wanted a dataset to use to compare different aspects of life for expats in their respective countries and maybe to compare with the lives of the actual citizens of each country or perhaps other related topics.\n",
      "hi! i'm new here and have zero knowledge of anything data science, but would love to one day and will study it at uni. all i've been able to do is deseasonalise this data and find an increasing linear trend. but i've just graduated high school and would love to see how it's done properly.\n",
      "the data was collected by the laverton raaf station 087031 (37.86°s, 144.76°e) from october 1, 1943 and has been meticulously collated up to august 8, 2017. the data was sourced from the australian bureau of meteorology (the first year of data can be found here). there was also a period from 1999-2003 where only integer values were take (go figure) and this can be seen within the right half of this page's banner image.\n",
      "i originally collated the data to see how climate change was affecting temperatures in my local area here and now, and now with a sparked passion for data science, am keen to learn how it could be processed for long term forecasting.\n",
      "info\n",
      "te'udat bagrut (hebrew: תעודת בגרות‬) is a certificate which attests that a student has successfully passed israel's high school matriculation examination. bagrut is a prerequisite for higher education in israel. a bagrut certificate is awarded to students who pass the required written (and in some cases oral) subject-matter examinations with a passing mark (56% or higher) in each exam. the bagrut certificate however should not be confused with a high school diploma (te'udat g'mar tichon, hebrew: תעודת גמר תיכון‬), which is a certificate awarded by the ministry of education attesting that a student has completed 12 years of study. (source: wikipedia)\n",
      "content\n",
      "this file contains over 60,000 average bagrut grades that were taken between 2013 and 2016 by over 1800 schools in various subjects.\n",
      "this data was posted under the israeli freedom of information law and was formatted by me\n",
      "context\n",
      "indian hindi cinema, popularly known as bollywood has witnessed exponential growth in terms of volume of business, manpower employed, number of movies produced each year and also the global reach. hence, it could be of great commercial importance to develop a model which could predict the success of a movie before it's release. however, it is not easy to forecast demand for a movie. there are a number of factors like actors, directors, time of release, genre, production house etc. which affect the outcome of a movie.\n",
      "the primary requirement to develop such a model would be the availability of bollywood movie data. thus, i created this dataset while working on my senior year research project, titled 'predicting success of upcoming bollywood movies'.\n",
      "content\n",
      "the data has been created manually by visiting different websites. the primary ones being wikipedia, boxofficeindia.com and imdb. the data contains 1285 rows with movies released between the years 2001 to 2014.\n",
      "the hitflop column contains values from 1 to 9 with\n",
      "1 - disaster\n",
      "2 - flop\n",
      "3 - below average\n",
      "4 - average\n",
      "5 - semi hit\n",
      "6 - hit\n",
      "7 - super hit\n",
      "8 - blockbuster\n",
      "9 - all-time blockbuster\n",
      "acknowledgements\n",
      "research guide - dr. s.k. saha\n",
      "inspiration\n",
      "can we save the time and money wasted by movie viewers on viewing flop and disaster movies?\n",
      "can we suggest must-watch movies to movie viewers even before movies release?\n",
      "can we classify upcoming movies into 1 of 9 categories even before their release?\n",
      "densenet-169\n",
      "densely connected convolutional networks\n",
      "recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. in this paper, we embrace this observation and introduce the dense convolutional network (densenet), which connects each layer to every other layer in a feed-forward fashion. whereas traditional convolutional networks with l layers have l connections - one between each layer and its subsequent layer - our network has l(l+1)/2 direct connections. for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. densenets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. we evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (cifar-10, cifar-100, svhn, and imagenet). densenets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. code and models are available at this https url.\n",
      "authors: gao huang, zhuang liu, kilian q. weinberger, laurens van der maaten\n",
      "https://arxiv.org/abs/1608.06993\n",
      "densenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "resnet-18\n",
      "deep residual learning for image recognition\n",
      "deeper neural networks are more difficult to train. we present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. we explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. we provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. on the imagenet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than vgg nets but still having lower complexity.\n",
      "an ensemble of these residual nets achieves 3.57% error on the imagenet test set. this result won the 1st place on the ilsvrc 2015 classification task. we also present analysis on cifar-10 with 100 and 1000 layers.\n",
      "the depth of representations is of central importance for many visual recognition tasks. solely due to our extremely deep representations, we obtain a 28% relative improvement on the coco object detection dataset. deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions, where we also won the 1st places on the tasks of imagenet detection, imagenet localization, coco detection, and coco segmentation.\n",
      "authors: kaiming he, xiangyu zhang, shaoqing ren, jian sun\n",
      "https://arxiv.org/abs/1512.03385\n",
      "architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-16\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-13\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "this is a refactored version of https://www.kaggle.com/calmdownkarm/ufcdataset\n",
      "the original dataset was separated only by fights, not by fights and fighters, replicating the metric columns and making it hard to work with.\n",
      "content\n",
      "additionally the statistics you would see would be the sum of all the fights before the fight that is referenced in the row itself. every time a new fighter appeared their metrics would be na for their first fight. if you're interested in predicting fights or knowing what metrics are the most important in a fight itself i think it's best separated. so i've undone this so every fight what you see is the statistics for the fight itself. if you want the sum that's easy enough to compute in r or python.\n",
      "acknowledgements\n",
      "all credit goes to https://www.kaggle.com/calmdownkarm for making this dataset\n",
      "inspiration\n",
      "mma is a highly unpredictable sport. it's the epitome of competition in my opinion where fighters pour their hearts out on the line for us.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "i started to scrape tripadvisor reviews for a personal project on sentiment analysis. i thought it could be good to share my data on kaggle, since this can help other with similar ideas.\n",
      "content\n",
      "the dataset represents a corpus of more than 220k reviews scraped from tripadvisor (in reviews.csv). accompanying information on the b&bs (in properties.csv) are also provided.\n",
      "each review has the following fields: - review_title, - review_date, - review_rating, - review_text, - review_language, - review_user, - review_id, - property_id\n",
      "each property has the following fields: - property_id, - property_name, - property_reviews (the total number of reviews of the b&b), - property_rating (the average rating of the b&b as displayed on tripadvisor), - property_address, - property_latitude, - property_longitude\n",
      "the reviews cover a time slice running from 2002 up to the first days of december 2017.\n",
      "all data has been scraped from tripadvisor website using chesf (the chrome headless scraping framework). chesf is really useful when you have to scrape javascript intensive web pages.\n",
      "inspiration\n",
      "other than sentiment analisys, this dataset could be useful to investigate strategies against fake reviews. i performed a simple analysis that identified several possibly fake users. other people smarter than me could find more efficient strategies.\n",
      "context\n",
      "i extracted 20,000 english language tweets relating to #jerusalemembassy . in the wake of usa's decision to move its embassy in israel to jerusalem (december 6, 2017), there has been an outpouring of tweets under this hashtag. this will be a good data set for data scientists interested in politics to carry out sentiment analysis and nlp. the data can be used for data visualization and geo-location based plotting. these tweets were collected on december 9, 2017.\n",
      "content\n",
      "the csv contains the tweets extracted under #jerusalemembassy . the column text contains the actual text of the tweets. other columns are:\n",
      "favorited : false means the tweets was not favorited by anyone. true means that people clicked the heart symbol to indicate they liked the content of the tweet\n",
      "favoritecount : number of times the heart symbol was clicked, it will be 0 for the false of the above\n",
      "replytosn\n",
      "created: the date and time when the tweet was created isretweet:\n",
      "whether tweet is a re-tweet or not\n",
      "retweetcount : number of times a tweet was re-tweeted longitude & latitude: geo-location info\n",
      "inspiration\n",
      "politics and current affairs interest me as as much as data science does (almost!! ). the world is in a state of flux and while maintaining a politically neutral stand i want to see how advances in data science can help us understand current affairs better. some of the questions that can be answered include:\n",
      "1) what are the overall sentiments associated with these tweets? (an analysis of 6k english tweets extracted on december 6, 2017 may surprise you: http://completestatisticaldatanalysis.com/o-jerusalem-jerusalemembassy-hashtag-on-december-6-2017/)\n",
      "2) what are the dominant words that turn up in the word cloud?\n",
      "3) where are most of tweets coming from?\n",
      "4) which tweets were re-tweeted the most?\n",
      "5) which user received the most responses?\n",
      "6) was there any difference in the sentiments of tweets emanating from different countries?\n",
      "7) how did the tweeting and re-tweeting intensity vary over a day?\n",
      "context\n",
      "this is a set of wikipedia title/content pairs where the content is always based on a person. maybe person is not the best word, because this set does not necessarily contain humans and the content can be about fictitious characters. i came across this data set by trying to remove this content from my primary set and thought it might be useful to someone else.\n",
      "content\n",
      "there are two columns wiki_title and content. this data was collected through the wikipedia api and all the content is raw data from wikipedia.page. content is from page.content and does contain the full page content. the people described can be characters, real people, fake people, humans, non-humans. although, the majority of this data will be real people.\n",
      "the wiki_title almost always contains the name of the person in the content or their nickname, but i know this is not true 100% of the time. it is too much data for me to verify the 100,000+ examples so i cannot verify all content is in fact about a person/character, but if there are improvements to be made i would like to do so.\n",
      "acknowledgements\n",
      "wikipedia\n",
      "python wikipedia api https://github.com/goldsmith/wikipedia\n",
      "photo by paul dufour on unsplash\n",
      "inspiration\n",
      "i think it would be interesting to try and match the celebrities in this data to the celebrity image data set and try and predict appearance of people based on their wikipedia content.\n",
      "everyone is wondering if the players (or balls) are 'juiced' this year after we observed a significant uptick in the number of home runs. if so, would the home run ball exit velocity change? take a look.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "i collected this dataset for my school project. the project is to train gan to generate new pokemon. i had a difficult time to find training dataset that is complete and clean. so i gather this collection of image and publish it here in hope that it will help others who need similar dataset.\n",
      "you can find my project on my github\n",
      "content\n",
      "819 transparent pokemon images in png format size 256x256.\n",
      "acknowledgements\n",
      "i collected the image mostly from this website https://veekun.com/dex/downloads\n",
      "banner image is taken from https://viking011.deviantart.com/art/pokemon-poster-436455502\n",
      "inspiration\n",
      "since i failed to generate new pokemon with clarity (i can only generate the shape) i wish there will be others that could do it with this dataset. if you managed to, please share it!\n",
      "context\n",
      "random shopping cart\n",
      "content\n",
      "date to add register\n",
      "id transaction\n",
      "product for id transaction\n",
      "acknowledgements\n",
      "the dataset is transform from random shopping cart https://www.kaggle.com/fanatiks/shopping-cart\n",
      "inspiration\n",
      "the inspiration is the testing the association rule learning https://en.wikipedia.org/wiki/association_rule_learning\n",
      "introduction this file contains the values of the price for top 10 cryptocurrencies (including scams) recorded on daily base, i decide to include all coins. all this dataset come from coinmarketcap historical pages, grabbed using just an r script. thanks coinmarketcap to making this data available for free (and for every kind of usage). the datasets will be updated on a regular basis (once a week).\n",
      "available columns in the dataset:\n",
      "date - the day of recorded values\n",
      "open - the opening price (in usd)\n",
      "high - the highest price (in usd)\n",
      "low - the lowest price (in usd)\n",
      "close - the closing price (in usd)\n",
      "volume - total exchanged volume (in usd)\n",
      "market.cap - the total market capitalization for the coin (in usd)\n",
      "coin - the name of the coin\n",
      "delta - calculated as (close - open) / open\n",
      "i m a student of bsc in it. this semester i m having a course on \"advanced analytics\". as needed for my studies, i have to do analysis on a specific topic for which i have chosen cryptocurrency. due to lack of updated data i have worked in this and arranged this file. hope this will be helpful.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this is a pre-crawled dataset, taken as subset of a bigger dataset (close to 4 million job listings) that was created by extracting data from dice.com, a leading us-based technology job board.\n",
      "the complete dataset can be downloaded from datastock — a repository of clean and ready to use web datasets with historical records.\n",
      "content\n",
      "the dataset has the following data fields:\n",
      "country_code\n",
      "date_added\n",
      "job_board\n",
      "job_description\n",
      "job_title\n",
      "job_type\n",
      "location\n",
      "organization\n",
      "page_url\n",
      "phone_number\n",
      "salary\n",
      "sector\n",
      "time period: 2016\n",
      "acknowledgements\n",
      "this dataset was created by promptcloud's in-house web-crawling service.\n",
      "inspiration\n",
      "following are some of the analyses that can be performed\n",
      "job title depending on the location/sector\n",
      "job description with respect to the skills\n",
      "job type (full time, contractual, etc.) depending on the location/sector\n",
      "inception-resnet-v2\n",
      "inception-v4, inception-resnet and the impact of residual connections on learning\n",
      "very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. one example is the inception architecture that has been shown to achieve very good performance at relatively low computational cost. recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ilsvrc challenge; its performance was similar to the latest generation inception-v3 network. this raises the question of whether there are any benefit in combining the inception architecture with residual connections. here we give clear empirical evidence that training with residual connections accelerates the training of inception networks significantly. there is also some evidence of residual inception networks outperforming similarly expensive inception networks without residual connections by a thin margin. we also present several new streamlined architectures for both residual and non-residual inception networks. these variations improve the single-frame recognition performance on the ilsvrc 2012 classification task significantly. we further demonstrate how proper activation scaling stabilizes the training of very wide residual inception networks. with an ensemble of three residual and one inception-v4, we achieve 3.08 percent top-5 error on the test set of the imagenet classification (cls) challenge\n",
      "authors: christian szegedy, sergey ioffe, vincent vanhoucke, alex alemi\n",
      "https://arxiv.org/abs/1602.07261\n",
      "inceptionresnetv2 architecture\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "xception\n",
      "xception: deep learning with depthwise separable convolutions\n",
      "we present an interpretation of inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). in this light, a depthwise separable convolution can be understood as an inception module with a maximally large number of towers. this observation leads us to propose a novel deep convolutional neural network architecture inspired by inception, where inception modules have been replaced with depthwise separable convolutions. we show that this architecture, dubbed xception, slightly outperforms inception v3 on the imagenet dataset (which inception v3 was designed for), and significantly outperforms inception v3 on a larger image classification dataset comprising 350 million images and 17,000 classes. since the xception architecture has the same number of parameters as inception v3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.\n",
      "author: françois chollet\n",
      "https://arxiv.org/abs/1610.02357\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "densenet-121\n",
      "densely connected convolutional networks\n",
      "recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. in this paper, we embrace this observation and introduce the dense convolutional network (densenet), which connects each layer to every other layer in a feed-forward fashion. whereas traditional convolutional networks with l layers have l connections - one between each layer and its subsequent layer - our network has l(l+1)/2 direct connections. for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. densenets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. we evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (cifar-10, cifar-100, svhn, and imagenet). densenets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. code and models are available at this https url.\n",
      "authors: gao huang, zhuang liu, kilian q. weinberger, laurens van der maaten\n",
      "https://arxiv.org/abs/1608.06993\n",
      "densenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "densenet-161\n",
      "densely connected convolutional networks\n",
      "recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. in this paper, we embrace this observation and introduce the dense convolutional network (densenet), which connects each layer to every other layer in a feed-forward fashion. whereas traditional convolutional networks with l layers have l connections - one between each layer and its subsequent layer - our network has l(l+1)/2 direct connections. for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. densenets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. we evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (cifar-10, cifar-100, svhn, and imagenet). densenets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. code and models are available at this https url.\n",
      "authors: gao huang, zhuang liu, kilian q. weinberger, laurens van der maaten\n",
      "https://arxiv.org/abs/1608.06993\n",
      "densenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "squeezenet 1.0\n",
      "squeezenet: alexnet-level accuracy with 50x fewer parameters and <0.5mb model size\n",
      "recent research on deep neural networks has focused primarily on improving accuracy. for a given accuracy level, it is typically possible to identify multiple dnn architectures that achieve that accuracy level. with equivalent accuracy, smaller dnn architectures offer at least three advantages: (1) smaller dnns require less communication across servers during distributed training. (2) smaller dnns require less bandwidth to export a new model from the cloud to an autonomous car. (3) smaller dnns are more feasible to deploy on fpgas and other hardware with limited memory. to provide all of these advantages, we propose a small dnn architecture called squeezenet. squeezenet achieves alexnet-level accuracy on imagenet with 50x fewer parameters. additionally, with model compression techniques we are able to compress squeezenet to less than 0.5mb (510x smaller than alexnet).\n",
      "authors: forrest n. iandola, song han, matthew w. moskewicz, khalid ashraf, william j. dally, kurt keutzer\n",
      "https://arxiv.org/abs/1602.07360\n",
      "squeezenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-16\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-19\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "this dataset does not have a description yet.\n",
      "u.s. department of labor employment and training administration office of foreign labor certification public disclosure file: federal fiscal year: reporting period: h-1b icert lca 2017 october 1, 2016, through september 30, 2017 important note: this public disclosure file contains administrative data from employers’ labor condition applications (eta forms 9035 & 9035e) and the certification determinations processed by the department’s office of foreign labor certification, employment and training administration where the date of the determination was issued on or after october 1, 2016, and on or before september 30, 2017. all data were extracted from the office of foreign labor certification’s icert visa portal system; an electronic filing and application processing system of employer requests for h-1b nonimmigrant workers.\n",
      "this dataset does not have a description yet.\n",
      "clash royale ladder battles dataset\n",
      "i expect to update this dataset daily\n",
      "i encourage you to make an eda or even train a model to predict if i win or lose!\n",
      "notes:\n",
      "\"op\" means opponent\n",
      "my/op_troops, my/op_buildings, my/op_spells, my/op_commons, my/op_rares, my/op_epics, my/op_legendaries: number of cards of that type.\n",
      "my/op_name_of_card: level of that card. if 0 then that card wasn't used in that battle.\n",
      "this is my second clash royale account and i don't play 2v2 matches, 1v1 ladder matches only.\n",
      "context\n",
      "word2vec is a group of related models that are used to produce word embeddings. these models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
      "content\n",
      "existing word2vec embeddings. googlenews-vectors-negative300.bin glove.6b.50d.txt glove.6b.100d.txt glove.6b.200d.txt glove.6b.300d.txt\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "over the past decade, bicycle-sharing systems have been growing in number and popularity in cities across the world. bicycle-sharing systems allow users to rent bicycles for short trips, typically 30 minutes or less. with the latest technologies, it is easy for a user of the system to access a dock within the system to unlock or return bicycles. these technologies also provide a wealth of data that can be used to explore how these bike-sharing systems are used. the data consists of bikeshare information for three large cities in the us - new york city, chicago, and washington, dc.\n",
      "content\n",
      "sample data\n",
      "'tripduration', '839'\n",
      "'starttime', '1/1/2016 00:09:55'\n",
      "'stoptime', '1/1/2016 00:23:54'\n",
      "'start station id', '532'\n",
      "'start station name', 's 5 pl & s 4 st'\n",
      "'start station latitude', '40.710451'\n",
      "'start station longitude', '-73.960876'\n",
      "'end station id', '401'\n",
      "'end station name', 'allen st & rivington st'\n",
      "'end station latitude', '40.72019576'\n",
      "'end station longitude', '-73.98997825'\n",
      "'bikeid', '17109'\n",
      "'usertype', 'customer'\n",
      "'birth year', ''\n",
      "'gender', '0'\n",
      "acknowledgements\n",
      "thanks to udacity for the data.\n",
      "inspiration\n",
      "this data set can be a very good inspirations for not only new users but experienced veterans who wants to explore more insights.\n",
      "context\n",
      "the no show problem is one of the bigest on the health industry, about 30% of the patient fail theirs appointments.\n",
      "content\n",
      "61k points, from 2017.01.01 to 2017.04.30 and 19 features to work with\n",
      "data dictionary\n",
      "especialidad : what kind of specialist is going to. ie dematologist, etc.\n",
      "edad: age\n",
      "sexo: sex, 1: male, 2: female\n",
      "reserva_mes_d : discrete value for the month of the appointment, 1: jan, 2: feb...\n",
      "reserva_mes_c : continue value for the month of the appointment, the formula is cos(2*reserva_mes_d*pi/12)\n",
      "reserva_dia_d : day of the week for the appointment, 1: mon... 7: sun\n",
      "reserva_dia_c : continous value for the day of the week, the formula is cos(2*reserva_dia_d*pi/7)\n",
      "reserva_hora_d : discrete value for hour of the appointment\n",
      "reserva_hora_c : continous value for the hour of the appointment, the formula is cos(2*reserva_hora_d*pi/24)\n",
      "creacion_mes_d : discrete value for the month when the appointment was created\n",
      "creacion_mes_c : continous value for the month when the appointment was created, the formula is cos(2*creacion_mes_d*pi/12)\n",
      "creacion_dia_d : same as reserva_dia_d, but considering the day when the appointment was created\n",
      "creacion_dia_c : same as reserva_dia_c, but considering the day when the appintment was created\n",
      "creacion_hora_d : hour when the appointment was created\n",
      "creacion_hora_c : continous value for the creacion_hour_d, the formula is cos(2*creacion_hora_d*pi/24)\n",
      "latencia : number of days between the appointment and the date when it was created\n",
      "canal : channel used for the creation of the apppointment, 1: call center, 2: personal, 3: web\n",
      "tipo : type of appointment, 1: medical, 2: procedures\n",
      "show : 0: no show, 1: show\n",
      "inspiration\n",
      "can we use it to predict if a patient is going to show up for his appointment?\n",
      "plate number recognition datasets\n",
      "this datasets provided for plate number recognition in indonesia and images are containing for indramayu west java and yogyakarta region\n",
      "contribution\n",
      "yes! check out on my github repository\n",
      "context:\n",
      "character encodings are sets of mappings from raw bits (0’s and 1’s) to text characters. when a text encoded with a specific encoder is decoded with a different encoder, it changes the output text. sometimes this results in completely unreadable text.\n",
      "this dataset is intended to provide a list of example texts in different character encodings to help you diagnose which file encoding your source file actually in.\n",
      "content\n",
      "this dataset is made up of six text files that represent five different character encodings and six different languages. the character encodings represented in this dataset are iso-8859-1 (also known as latin 1),\n",
      "ascii, windows 1251, utf-16 that has been successfully converted into the utf-8 and big-5. more information on the files is available in the file_guide.csv file.\n",
      "each text file contains a header and footer. the body text is delimited by this text:\n",
      "* start of the project gutenberg ebook [title of book goes here] *\n",
      "* end of the project gutenberg ebook [title of book goes here]*\n",
      "acknowledgements:\n",
      "the texts in this dataset were prepared by project gutenberg volunteers. these texts are in the public domain.\n",
      "inspiration:\n",
      "can you build an tool to automatically detect when a file in the wrong encoding is read in?\n",
      "you can use this dataset to explore what happens when you read in text using different encoders.\n",
      "context:\n",
      "the spoken wikipedia project unites volunteer readers of wikipedia articles. hundreds of spoken articles in multiple languages are available to users who are – for one reason or another – unable or unwilling to consume the written version of the article. this is time-aligned corpus of these spoken articles, well suited to research and fostering new ways of interacting with the material.\n",
      "content:\n",
      "all spoken articles use a template with slots: filename, speaker, date and revision spoken, ... to insert the audio player and a display of the meta-data on the wikipedia page. the template also adds spoken articles to a root category. templates, categories, and meta-data vary between language communities!\n",
      "the dutch language portion of this corpus contains 3073 articles read by 145 speakers. there are 224 hours of speech, of which 79 hours is aligned at the word level.\n",
      "each article is tokenized into sections, sentences, and tokens. each token is normalized and the normalization is aligned to the audio. complete information on the annotation schema can be found in the schema file. for each article, the corpus contains:\n",
      "audio file(s)\n",
      "original wikitext\n",
      "html generated by mediawiki\n",
      "cleaned and normalized text\n",
      "alignment between text and audio\n",
      "meta-information (who, when, what)\n",
      "for additional information and updates, please see the project website.\n",
      "acknowledgements:\n",
      "this dataset was collected by arne köhn, florian stegen and timo baumann. if you use this dataset in your work, please cite the following paper:\n",
      "köhn, a., stegen, f., & baumann, t. (2016). mining the spoken wikipedia for speech data and beyond. in the proceedings of the tenth international conference on language resources and evaluation (lrec 2016).\n",
      "inspiration:\n",
      "some possible uses for this corpus include:\n",
      "train or evaluate automatic speech recognition systems\n",
      "improve accessibility / spoken article navigation\n",
      "top contributors speak >30 hours, which is enough audio to train synthesis voices\n",
      "analyze prosody of reading (large amounts of diversely read text)\n",
      "analyze prosody of information structure (accessible through links, research on semantic wikipedia, dbpedia, ...)\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "contains leaderboards of 2017 pga tour season.\n",
      "content\n",
      "location course yardage purse pos player earnings to par thru r1 r2 r3 r4 total score\n",
      "acknowledgements\n",
      "scrapped on cssports.com\n",
      "inspiration\n",
      "best players regarding yardage ?\n",
      "context\n",
      "starting with 20 years data scrapped from nuforc (from 97 to 2017), plus mr. ajayrana ufo report dataset (1949 to 2000), my idea was since the beginning to be able to predict, given sight data, which shape the ufo would be. therefore, besides nuforc data, i captured data from wunderground related to those occurrences.\n",
      "content\n",
      "first of all, i did some web scrapping over nuforc website collecting data from 1997 to 2017, using r. then i created another python program running periodically (every 30 minutes) reading 10 rows at a time from wunderground, but only for those cities, dates having sight records\n",
      "acknowledgements\n",
      "thanks to wunderground for letting me capture small amounts of historical data every day. and thanks to alura, one of the best online educating platforms i ever used, and for letting me host some of my courses about big data. also, i must thank mr. ajayrana for his nice work\n",
      "inspiration\n",
      "it is an ongoing work. soon i will have more data and be able to create an awesome prediction model\n",
      "this dataset is comprised of the chest x-rays of 5 different diseases. they are: atelectasis infiltration effusion cardiomegaly fibrosis\n",
      "all of the training sets contain 150+ images. all of the validation/test sets contain 30+ images. it contains folder wise arranged images for all diseases in both the training and validation sets.\n",
      "the aim is to train a neural network to classify a given chest x-ray image into one of these categories.\n",
      "context\n",
      "the data extraction of the reviews was done for three iconic spots (the eiffel tower, the statue of libery and taj mahal) to analyze how people from different countries review different places. in this dataset we have released the reviews posted for the eiffel tower.\n",
      "the data set for all the three locations can be downloaded from datastock -- a repository of datasets containing historical records extracted from the web.\n",
      "content\n",
      "data extraction was done by promptcloud's internal web crawling solution in december, 2017. it has the following fields:\n",
      "review text\n",
      "review title\n",
      "rating (1-5)\n",
      "reviewer location\n",
      "landmark name\n",
      "initial analyses\n",
      "we compared the reviews posted by the visitors from the uk and the us. it can be accessed here - https://goo.gl/cru8zj.\n",
      "inspiration\n",
      "text mining techniques can be applied on this data to unveil sentiment, word frequency, build network graph, perform topic modelling and more.\n",
      "context\n",
      "this dataset was created by machine learning-based wordpress crawler developed by promptcloud.\n",
      "content\n",
      "this a json file containing the following data:\n",
      "page url\n",
      "page title\n",
      "post text content\n",
      "image url\n",
      "timestamp of blog post\n",
      "inspiration\n",
      "this dataset can be used to figure out how the fashion trend changes according to year and season.\n",
      "vgg-13\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-19\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "podcast exists for near two decades. but it really takes over in recent two years. the podcast meta data may be useful for research in fields like machine learning, social science, or media in general.\n",
      "content\n",
      "listen notes is the podcast search engine that actually works. it has the most comprehensive podcast database that you can find on the internet.\n",
      "this dataset includes the meta data of (almost) all podcast episodes that were published in december 2017.\n",
      "data source: rss feed of podcasts.\n",
      "acknowledgements\n",
      "thanks for all the podcasters who produce those inspiring / entertaining shows.\n",
      "inspiration\n",
      "n/a\n",
      "context\n",
      "from national chicken wings day to national video game day to national clean your virtual desktop day, this dataset covers it all.\n",
      "content\n",
      "event: name of the holiday/occasion\n",
      "day: date the occasion is observed. a couple of occasions may appear more than once a year\n",
      "type: activity, food, cause, family, etc\n",
      "acknowledgements\n",
      "the dataset is scrapped with rvest from nationaltoday.com which collected the data.\n",
      "inspiration\n",
      "the dataset itself can be used for all sorts of timeline visualization. how this dataset can be merged with other public data is only limited by your imagination.\n",
      "context\n",
      "on november 21st, 2017, the us federal communications commission (fcc) introduced a plan to repeal net neutrality rules.\n",
      "content\n",
      "this dataset contains details of 90,000 public comments made on fcc proceeding 17-108, \"restoring internet freedom\" between april and may 2017 while regulations regarding net neutrality were being considered.\n",
      "acknowledgements\n",
      "this data was collected via the fcc ecfs public api (docs).\n",
      "--\n",
      "todo: expand description\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "safebooru (safebooru.org) is a tag-based image archive maintained by anime enthusiasts. it allows users to post images and add tags, annotation, translations and comments. it's derived from danbooru, and differs from it in that it disallows explicit content. it's quite popular, and there are more than 2.3 million posts as of january 24, 2018.\n",
      "content\n",
      "the data was scraped via safebooru's online api, then converted from xml to csv (some attributes were discarded during the conversion to make the whole csv a little smaller). there are 1,934,214 rows of the metadata. contains images uploaded to safebooru.org in the time range of 2010-01-29 through 2016-11-20.\n",
      "acknowledgements\n",
      "banner image taken from https://safebooru.org/index.php?page=post&s=view&id=1514244\n",
      "inspiration\n",
      "what tags are highly correlated? can you predict missing tags? can you predict the score of an image based on its tags?\n",
      "complete kaggle datasets collection\n",
      "a dataset of kaggle datasets, so you can explore while you explore\n",
      "summary\n",
      "> observations: 8,036 unique datasets\n",
      "> variables: 14\n",
      "> current as: 16/01/2018\n",
      "description\n",
      "for a bit of fun i thought i'd write a quick script to retrieve all of the kaggle datasets and do a bit of analysis on it.\n",
      "the dataset contains all the unique datasets hosted on kaggle since existence, and each one links off to it.\n",
      "future temptations\n",
      "if the community is interested i am tempted to scrape over each one and retrieve each datasets metadata, consolidate a huge kaggle data dictionary?\n",
      "data structure\n",
      "observations: 8,036 \n",
      "variables: 14 \n",
      " $ title          <chr> \"trending youtube video statistics (updated)\", \"7ecb8f4fe2ece9f4c8ffd2... \n",
      " $ description    <chr> \"daily statistics (views, likes, category, tags+) for trending youtube... \n",
      " $ url            <chr> \"https://www.kaggle.com/datasnaek/youtube-new\", \"https://www.kaggle.co.. \n",
      " $ owner          <chr> \"mitchell j\", \"vera lei\", \"chfly2000\", \"snow2011\", \"tjb5670\", \"gabro\",... \n",
      " $ kernels        <int> 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... \n",
      " $ discussions    <int> 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... \n",
      " $ views          <int> 9484, 55, 26, 12, 7, 6, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3... \n",
      " $ downloads      <int> 1668, 2, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0... \n",
      " $ last_updated   <date> 2018-01-16, 2018-01-16, 2018-01-16, 2018-01-16, 2018-01-16, 2018-01-1... \n",
      " $ license        <chr> \"cc0\", \"other\", \"other\", \"cc0\", \"cc0\", \"other\", \"other\", \"cc0\", \"other... \n",
      " $ size           <dbl> 35087677, 127264365, 0, 1635900, 18, 777566, 404381, 137847611, 807171... \n",
      " $ featured       <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... \n",
      " $ super_featured <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... \n",
      " $ upvotes        <int> 46, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... \n",
      "authors\n",
      "jesse vent - author - jessevent\n",
      "acknowledgments\n",
      "github - crypto r-package\n",
      "kaggle - kaggle; need i say more?\n",
      "community acknowledgements\n",
      "mkffl\n",
      "u.s. government publishing office\n",
      "_fkih younes\n",
      "_ilab\n",
      "????\n",
      "\">\n",
      "<\"xss'\n",
      "0rangutan\n",
      "1251\n",
      "173050055\n",
      "30crmnsia\n",
      "361online\n",
      "4d4stra\n",
      "4quant\n",
      "73805\n",
      "7grandpa\n",
      "a grillo\n",
      "a.c.vanderlinde\n",
      "a.rocamora.terres\n",
      "å§œä¸šï¼ˆintegï¼‰\n",
      "a^b\n",
      "å°¹é½ç‚œä¸­å—\n",
      "å¤§æ¸…è¦å®œ\n",
      "aadrika singh\n",
      "aak\n",
      "aakaash jois\n",
      "aakash agrawal\n",
      "aalborg university\n",
      "aamir soni\n",
      "aariyan panchal\n",
      "aaron miles\n",
      "aaron\n",
      "aaron7sun\n",
      "aaronmckisic\n",
      "aashaysachdeva\n",
      "aashutoshagrawal\n",
      "abanil\n",
      "abbyasov marat\n",
      "abdalla g bakheet\n",
      "abdelhadi kerfa\n",
      "abdelhaq el aibi\n",
      "abdelkader laraichi\n",
      "abderrahman (abdou) ait ali\n",
      "abdhm\n",
      "abdul basit\n",
      "abdul qureshi\n",
      "abdul somat budiaji\n",
      "abdullah karimi\n",
      "abhijeet khandelwal\n",
      "abhilash reddy\n",
      "abhilash\n",
      "abhinandan\n",
      "abhinav ankit\n",
      "abhinav maurya\n",
      "abhinav moudgil\n",
      "abhinav ralhan\n",
      "abhinav walia\n",
      "abhinav\n",
      "abhinavagarwal\n",
      "abhishek bera\n",
      "abhishek jha\n",
      "abhishek kumar\n",
      "abhishek sharma\n",
      "abhisheksharma\n",
      "abhrajyoti pal\n",
      "abida aslam\n",
      "abidemi lorain grace\n",
      "abien fred agarap\n",
      "abineshkumar k\n",
      "abiodun bayowa\n",
      "abir\n",
      "abiyug\n",
      "abliamitabliamitov\n",
      "abo sol\n",
      "academy of motion picture arts and sciences\n",
      "achinta\n",
      "achmad wildan al aziz\n",
      "acruve15\n",
      "activegalaxy\n",
      "acutesharpness\n",
      "ada guo\n",
      "ãdã¡m markã³ja\n",
      "adam kolodny\n",
      "adam mathias bittlingmayer\n",
      "adam schroeder\n",
      "adam\n",
      "adamskafi\n",
      "adarsh chavakula\n",
      "adarshashrivastava\n",
      "addy naik\n",
      "ade ihsan hidayatullah\n",
      "adhokshajapradeep\n",
      "adign\n",
      "adithya ganesh\n",
      "adithya\n",
      "aditi garg\n",
      "aditi\n",
      "aditisingh\n",
      "aditya bhati\n",
      "aditya chetan\n",
      "aditya gupta\n",
      "aditya kirloskar\n",
      "aditya mehndiratta\n",
      "aditya pratap singh\n",
      "aditya rajuladevi\n",
      "aditya soni\n",
      "aditya tandon\n",
      "aditya\n",
      "adityadivakaruni\n",
      "adityalodha\n",
      "adityavamsikiran\n",
      "adivarma\n",
      "adm2752836\n",
      "admin admin\n",
      "adnan rasheed\n",
      "adong\n",
      "adriano pylro\n",
      "adrianulbona\n",
      "adrien chevrier\n",
      "adrien\n",
      "adu47249\n",
      "adult survey company\n",
      "adway s. wadekar\n",
      "ady1\n",
      "æž—æ¹§æ£® (dyson lin)\n",
      "æžç«‹å³°\n",
      "afeef k k\n",
      "afri\n",
      "agsantos\n",
      "agustin montero\n",
      "aguy\n",
      "ahiale darlington\n",
      "ahmad delforouzi\n",
      "ahmad obiedat\n",
      "ahmadzaenal\n",
      "ahmed abdelaal\n",
      "ahmed nader\n",
      "ahmed\n",
      "ahmedeveloper\n",
      "ahmet erkan\n",
      "ahmet hamza emra\n",
      "ahmetaksoy\n",
      "ahn kwang\n",
      "ahsan\n",
      "ahsan\n",
      "aifirst\n",
      "aimeshangula\n",
      "airbnb\n",
      "airly\n",
      "aishwarya deshpande\n",
      "aivar annamaa\n",
      "aj_2017\n",
      "ajana\n",
      "ajaxfb\n",
      "ajaykumarmanimala\n",
      "ajayrana\n",
      "ajinkya jumbad\n",
      "ajinkya kolhe\n",
      "ajinkya rasane\n",
      "ajitbrar\n",
      "ajmartinezm\n",
      "ajs\n",
      "akash gupta\n",
      "akash jaiswal\n",
      "akash kumar\n",
      "akash\n",
      "akashpatel\n",
      "akhil anto\n",
      "akhil jain\n",
      "akhilesh\n",
      "akhileshwarreddychennu\n",
      "akil elkamel\n",
      "akira.y\n",
      "akis zervas\n",
      "akshatuppal\n",
      "akshay babbar\n",
      "akshay kumar vikram\n",
      "akshay sharma\n",
      "akshay\n",
      "akshayaradhya\n",
      "akson\n",
      "alan \"aj\" pryor\n",
      "alan du\n",
      "albert costas\n",
      "albertjiang\n",
      "alberto almuiã±a\n",
      "alberto artasanchez\n",
      "alberto barradas\n",
      "alberto martinho\n",
      "albyati\n",
      "alec\n",
      "aleenahkhan\n",
      "alejandro taboada\n",
      "alejandro\n",
      "aleksandr ivanov\n",
      "aleksandr shevchenko\n",
      "aleksey bilogur\n",
      "alessandro de vito\n",
      "alex acosta\n",
      "alex k\n",
      "alex klibisz\n",
      "alex korablev\n",
      "alex lee\n",
      "alex miasoedov\n",
      "alex xiaotong gui\n",
      "alex_deng\n",
      "alex\n",
      "alexander kireev\n",
      "alexander konshin\n",
      "alexander long\n",
      "alexander mamaev\n",
      "alexander minushkin\n",
      "alexander raboin\n",
      "alexander shakhov\n",
      "alexander\n",
      "alexanderglulkhovtsev\n",
      "alexattia\n",
      "alexey filimonchuk\n",
      "alexey rozhnev\n",
      "alexis carrillo\n",
      "alexis fossart\n",
      "alexisglennespina\n",
      "alexlight\n",
      "alexnavarrete\n",
      "alexstrasza\n",
      "alexzhang\n",
      "alfonsoreyes\n",
      "alfredoquintana\n",
      "ali ghafour\n",
      "ali hussain\n",
      "alifarsi\n",
      "alifatemy\n",
      "aliia salakheeva\n",
      "alimbekovkz\n",
      "alin secareanu\n",
      "alishan kaisani\n",
      "allan scott\n",
      "allan\n",
      "allen institute for artificial intelligence\n",
      "allhailsammy\n",
      "allsmiles\n",
      "alok nimrani\n",
      "alon\n",
      "alopez247\n",
      "alp koã§\n",
      "alphahaxor\n",
      "alphajuliet\n",
      "altonlu\n",
      "alukosayo\n",
      "alvaro flores\n",
      "ãlvaro lã³pez garcã­a\n",
      "alvaro soares\n",
      "alvaro trancon\n",
      "alvin mbabazi\n",
      "alwayschacha\n",
      "alyssa\n",
      "aman agarwal\n",
      "aman ajmera\n",
      "aman mahendra\n",
      "aman shrivastava\n",
      "amandeep rathee\n",
      "amar basic\n",
      "amber song\n",
      "amer\n",
      "amey goel\n",
      "amil khare\n",
      "amin ghaderi\n",
      "amine gherbi\n",
      "aminer\n",
      "amins\n",
      "amir aharon\n",
      "amir rezaei\n",
      "amit maurya\n",
      "amit\n",
      "amita dhainje\n",
      "amitaashokdhainje\n",
      "amitani\n",
      "amlan praharaj\n",
      "amol naik\n",
      "amro\n",
      "amrrs\n",
      "anand jeyahar\n",
      "anand\n",
      "anantbhardwaj\n",
      "ananya nayan\n",
      "anas aboureada\n",
      "anastasios zouzias\n",
      "ancient one\n",
      "andersketelsen\n",
      "anderson chaves\n",
      "andi fauzi firdaus\n",
      "andieminogue\n",
      "andre holzner\n",
      "andre sionek\n",
      "andrea cesarini\n",
      "andrea girardi\n",
      "andrea leo\n",
      "andrea\n",
      "andreas kappl\n",
      "andreas klintberg\n",
      "andrei dukhounik\n",
      "andres c\n",
      "andres hernandez\n",
      "andresfelipebayonachinchilla\n",
      "andressa coelho\n",
      "andrew dacenko\n",
      "andrew gross\n",
      "andrew kirk\n",
      "andrew kreimer\n",
      "andrew thompson\n",
      "andrew truman\n",
      "andrew wang\n",
      "andrew yue xie\n",
      "andrew.chen\n",
      "andrew\n",
      "andrew\n",
      "andrewehsaei\n",
      "andrewmalinow, phd\n",
      "andrewnachtigal\n",
      "andrey dotsenko\n",
      "andrey\n",
      "andrey\n",
      "andrey\n",
      "andriy gudziy\n",
      "andry ml\n",
      "andy friedman\n",
      "andy harless\n",
      "andy levitskyy\n",
      "andyklyman\n",
      "angela houston\n",
      "angelalocoro\n",
      "angeline pld\n",
      "angga purnama\n",
      "anil\n",
      "anilkumarpallekonda\n",
      "animatronbot\n",
      "aniruddha achar\n",
      "aniruddha ghosh\n",
      "anirudh k. muralidhar\n",
      "anish n sharma\n",
      "anjali reddy\n",
      "anji\n",
      "ankit agarwal\n",
      "ankit akash jha\n",
      "ankit biradar crixus\n",
      "ankit chaubal\n",
      "ankit jindal\n",
      "ankit\n",
      "ankita\n",
      "ankur joshi\n",
      "ankur\n",
      "ankursaikia\n",
      "ankushanshuman\n",
      "anmol\n",
      "anna montoya\n",
      "anna montoya\n",
      "annamongillo\n",
      "annanya pratap\n",
      "annecool37\n",
      "annie pi\n",
      "anokas\n",
      "ansh.g\n",
      "anshul jain\n",
      "anshul kwatra\n",
      "anthony deluca\n",
      "anthony goldbloom\n",
      "anthony nguyen\n",
      "anthonyallen\n",
      "anton bobanev\n",
      "anton dmitriev\n",
      "anton lytyakov\n",
      "anton prokopyev\n",
      "anton savchenko\n",
      "antonio coelho\n",
      "antonio domenzain\n",
      "antonio guimarey marã³n\n",
      "antonio javier gonzã¡lez ferrer\n",
      "antonioferegrinobolaã±os\n",
      "antonioivanovski\n",
      "antonyj\n",
      "antti-paladin\n",
      "anttip\n",
      "anu\n",
      "anubhav dhiman\n",
      "anuj anand gagrai\n",
      "anuj goyal\n",
      "anujay saraf\n",
      "anupama jha\n",
      "anurag gothwal\n",
      "anurag k\n",
      "anurag maurya\n",
      "anurag sharma\n",
      "anurag\n",
      "anuragpuri\n",
      "anuraj\n",
      "anvesh tummala\n",
      "aparajita tiwari\n",
      "apollonius\n",
      "apoorv agnihotri\n",
      "apoorvajha\n",
      "applecrazy\n",
      "apratim bhattacharya\n",
      "arasaraja\n",
      "arash\n",
      "aravindhan s\n",
      "arcgis open data\n",
      "archana khanal\n",
      "archangell\n",
      "arda mavi\n",
      "arden tran\n",
      "areeves87\n",
      "ariful ambia\n",
      "arihant jain\n",
      "arijit mukherjee\n",
      "arion ai\n",
      "arion\n",
      "aritrasen\n",
      "arizona secretary of state\n",
      "arjoonnsharma\n",
      "arjun\n",
      "arkz\n",
      "armin talic\n",
      "armineh nourbakhsh\n",
      "arnaudlievin\n",
      "arnoud\n",
      "arokkones\n",
      "arooj anwar khan\n",
      "arpan dhatt\n",
      "arpi sinanyan\n",
      "arsenland\n",
      "arshadsiddhiqui\n",
      "arslan zulfiqar\n",
      "artem larionov\n",
      "artemzraev\n",
      "arthur stsepanenka\n",
      "arthur163\n",
      "artlee\n",
      "arun joseph\n",
      "arun kumar\n",
      "arun menon\n",
      "arun\n",
      "arvidzt\n",
      "arvind bhatt\n",
      "arvindhan rameshbabu\n",
      "arwin neil baichoo\n",
      "asadmahmood\n",
      "asado23\n",
      "asfdafae\n",
      "ashish bansal\n",
      "ashish chauhan\n",
      "ashish gupta\n",
      "ashish khanna\n",
      "ashish sonavane\n",
      "ashita gupta\n",
      "ashleysmith\n",
      "ashok kumar pant\n",
      "ashok lathwal\n",
      "ashutosh kumar\n",
      "ashvinking\n",
      "ashwani\n",
      "asim irshad\n",
      "askarnurbekov\n",
      "asma belhaoua\n",
      "asper\n",
      "asso pavic - angers smart city\n",
      "astandri k\n",
      "atanas atanasov\n",
      "atefeh goodarzi\n",
      "athabascaai\n",
      "athni\n",
      "athontz\n",
      "atikur rahman\n",
      "atul a\n",
      "aty rachmawati\n",
      "aubert sigouin\n",
      "august\n",
      "augustinpottier\n",
      "augusto pertence\n",
      "aumas\n",
      "aurelian\n",
      "aurelio agundez\n",
      "auriml\n",
      "austinsonger\n",
      "australian bureau of statistics\n",
      "austro\n",
      "autuanliu\n",
      "avani gupta\n",
      "avirudhtheraja\n",
      "avkash\n",
      "awesome\n",
      "awright\n",
      "axa_fossouo\n",
      "ãÿã‹ã¨ã‚‚\n",
      "ayan maity\n",
      "ayantiwari\n",
      "aydin ayanzadeh\n",
      "aymanfawzy\n",
      "aysun\n",
      "ayush sharma\n",
      "ayush\n",
      "ayushdewan\n",
      "ayushthada\n",
      "azeem bootwala\n",
      "babu priyavrat\n",
      "babuloseo\n",
      "babybear\n",
      "bachi\n",
      "backblaze\n",
      "bacon\n",
      "badari vishal madduluri\n",
      "bader\n",
      "badri adhikari\n",
      "bagmanas\n",
      "bahadir60\n",
      "bai li\n",
      "baking pi\n",
      "baligh mnassri\n",
      "bank of england\n",
      "baptisteamato\n",
      "bargava\n",
      "baris simsek\n",
      "barney farrell\n",
      "baronchen\n",
      "barton.news\n",
      "bas hilgers\n",
      "basil\n",
      "bastien javaux\n",
      "batangas\n",
      "batzig\n",
      "bayarjargal\n",
      "bazinga\n",
      "bb\n",
      "beavis butthead\n",
      "bec14\n",
      "bedy\n",
      "behzadgolshan\n",
      "beili zheng\n",
      "bello gbadebo\n",
      "beluga\n",
      "belvederethecat\n",
      "ben dilday\n",
      "ben hamner\n",
      "ben ho\n",
      "ben rudolph\n",
      "ben\n",
      "benben zhang\n",
      "benf\n",
      "benjamin taylor\n",
      "benjamin visser\n",
      "benjaminswedlove\n",
      "berhane\n",
      "berkeley earth\n",
      "bernardo lares\n",
      "bert carremans\n",
      "bethtseng\n",
      "bhamin patel\n",
      "bharadwaj srigiriraju\n",
      "bharani\n",
      "bharath nr\n",
      "bharath posa\n",
      "bhargav\n",
      "bhaskar voleti\n",
      "bhatnasir\n",
      "bhavesh\n",
      "bhavna chawla\n",
      "bhupen\n",
      "bhushan sonawane\n",
      "bhuwan pandeya\n",
      "bianca kramer\n",
      "bibin paul\n",
      "bielrv\n",
      "bigblesslee\n",
      "bigdatachennai\n",
      "bigzhao\n",
      "bilalmahmood\n",
      "bill s\n",
      "billurengin\n",
      "bin ury\n",
      "bingli\n",
      "binks\n",
      "binroot\n",
      "biosense @ uc berkeley school of information\n",
      "birdie\n",
      "biswa\n",
      "bitroy\n",
      "bkkaggle\n",
      "blacklee1994\n",
      "blairjennings\n",
      "blazej\n",
      "blissoft\n",
      "blitzer\n",
      "bo ju\n",
      "bob zhang\n",
      "bob-li\n",
      "bobbob\n",
      "bobitasingha\n",
      "bogdan puida\n",
      "bojan tunguz\n",
      "boltzmannbrain\n",
      "bongo\n",
      "boon p\n",
      "borapajo\n",
      "boris marjanovic\n",
      "bostjan mrak\n",
      "botao_deng\n",
      "botshot\n",
      "boyofans\n",
      "bpali26\n",
      "bqlearner\n",
      "brahanyaasomasundaram\n",
      "brandon lawrence\n",
      "brandon trabuco\n",
      "brandtcowan\n",
      "brandy chang\n",
      "brave\n",
      "breadsh\n",
      "breanamurphy\n",
      "breandan\n",
      "brendan finan\n",
      "brendan murphy\n",
      "brendaso\n",
      "breyonce bugg\n",
      "brian gonzalez\n",
      "brian ho\n",
      "brian j\n",
      "brian liao\n",
      "brian mcgarry\n",
      "brian roach\n",
      "brian rouse\n",
      "brian rushton\n",
      "brian w. shreeves\n",
      "brian\n",
      "briane paul samson\n",
      "brianon99\n",
      "brickettaswiss\n",
      "brihi joshi\n",
      "brij nanda\n",
      "brijesh singh\n",
      "brnt\n",
      "bronson\n",
      "brontosaur\n",
      "brouillette\n",
      "brucelees\n",
      "brucerowan\n",
      "bruno flores\n",
      "bryan arnold\n",
      "bryan chen\n",
      "bryan park\n",
      "bryandrive\n",
      "bryanmaloney\n",
      "bryant trombly\n",
      "bryce freshcorn\n",
      "bryn humphreys\n",
      "bshivaani\n",
      "bssasikanth\n",
      "bth project\n",
      "btolar1\n",
      "buggs23\n",
      "bughunter atgoogle\n",
      "buket konuk hirst\n",
      "bukun\n",
      "bulblight\n",
      "burakh\n",
      "buryburyzymon\n",
      "buzz zhang\n",
      "ç”³å°è™ž\n",
      "ç§‹ä¹‹çµç¾½\n",
      "caio correia\n",
      "caio lente\n",
      "caio moreno\n",
      "caiquecassemiro\n",
      "caitlin furby\n",
      "caleb willms\n",
      "calebfackler\n",
      "california environmental protection agency\n",
      "calvin chan\n",
      "cam nugent\n",
      "cameron chandler\n",
      "cameron\n",
      "camilasampaio\n",
      "camille debrun\n",
      "campbell mcgrouther\n",
      "canuto\n",
      "caparrini\n",
      "caramba donkey\n",
      "cards against humanity\n",
      "carl jackson\n",
      "carl thomã©\n",
      "carlesbalsach\n",
      "carlos aguayo\n",
      "carlos beltrã¡n villamizar\n",
      "carlos brioso\n",
      "carlos paradis\n",
      "carlos rafael\n",
      "carlos vouking\n",
      "carlosmoncayo\n",
      "carly wright\n",
      "caroline cypranowska\n",
      "carrie\n",
      "carsten behring\n",
      "cataras\n",
      "cathie so\n",
      "cauim\n",
      "cccheung\n",
      "cclark\n",
      "cecil kim\n",
      "cedrikfd\n",
      "celio larcher\n",
      "cem karabulut\n",
      "cenk bircanoäÿlu\n",
      "center for medicare and medicaid\n",
      "centers for disease control and prevention\n",
      "centers for medicare & medicaid services\n",
      "central bureau of statistics\n",
      "ceshine lee\n",
      "cgaete\n",
      "chad schirmer\n",
      "chaitanya bapat\n",
      "chamberunderground\n",
      "chandan singh\n",
      "chandlervan\n",
      "chandra bhushan roy\n",
      "chansh\n",
      "chanwoo kim\n",
      "chaochana siparitat\n",
      "chara remoundou\n",
      "charles jansen\n",
      "charlesyang\n",
      "charlie h.\n",
      "charlie monk\n",
      "charlie\n",
      "charmi\n",
      "chase bank\n",
      "chase willden\n",
      "chaton\n",
      "chekos\n",
      "chella priyadharshini\n",
      "chen chen\n",
      "chen shuyao\n",
      "chen\n",
      "cheng zhang\n",
      "cheng\n",
      "chengzhan\n",
      "chennai kaggler's forum\n",
      "chenxi_ge\n",
      "cheshire\n",
      "chester cheng\n",
      "chetan malhotra\n",
      "chetan\n",
      "chewable\n",
      "chfly2000\n",
      "chi\n",
      "chia sáº½ kinh nghiá»‡m äi du lá»‹ch há»™i an äã  náºµng\n",
      "chia yi\n",
      "chicago police department\n",
      "chickgod\n",
      "chidi\n",
      "chinelo okpalaonwuka\n",
      "ching\n",
      "chinkirai\n",
      "chinkitpatel\n",
      "chip0001\n",
      "chippy\n",
      "chiragbalakrishna\n",
      "chithra ms\n",
      "chloesh\n",
      "chnaveen\n",
      "chonlapat patanajirasit\n",
      "chris bartel\n",
      "chris brent\n",
      "chris buetti\n",
      "chris crawford\n",
      "chris cross\n",
      "chris evi-parker\n",
      "chris formey\n",
      "chris g\n",
      "chris h.\n",
      "chris murphy\n",
      "chris pierse\n",
      "chris roth\n",
      "chris scott\n",
      "chris\n",
      "chris\n",
      "chrisaddy\n",
      "chrisb\n",
      "chrisclark\n",
      "chrisdoil\n",
      "chrism!\n",
      "christ\n",
      "christenlucido\n",
      "christian nygaard\n",
      "christian safka\n",
      "christian urcuqui\n",
      "christian vorhemus\n",
      "christiantrachsel\n",
      "christina mak\n",
      "christophe chabreuil\n",
      "christopher clayford\n",
      "christopher lambert\n",
      "christopher\n",
      "christopherzerafa\n",
      "christophes\n",
      "chrisy1001\n",
      "chtholly\n",
      "chuck ephron\n",
      "chuck-yin\n",
      "churandy\n",
      "cigil achenkunju\n",
      "cindyyyyyy\n",
      "cities\n",
      "citrahsagala\n",
      "city of chicago\n",
      "city of los angeles\n",
      "city of new york\n",
      "citylines.co\n",
      "ckeller\n",
      "clalby\n",
      "claudio sanhueza\n",
      "clayd\n",
      "clement gauchy\n",
      "clemetine\n",
      "cleuton sampaio\n",
      "cliff saito\n",
      "clim\n",
      "coconup\n",
      "code_thief\n",
      "codingvangogh\n",
      "cogs\n",
      "colacole\n",
      "colemaclean\n",
      "colinmorris\n",
      "college board\n",
      "colliaux rã©mi\n",
      "colt bauman\n",
      "committee to protect journalists\n",
      "connecticut open data\n",
      "conobrodel\n",
      "conor macbride\n",
      "conostabile\n",
      "consumer financial protection bureau\n",
      "cooperunion\n",
      "coplin\n",
      "coredesign\n",
      "corentin rdn\n",
      "corneliavanderwalt\n",
      "cornell university\n",
      "cosmikalpha\n",
      "cosmin stamate\n",
      "costalaether\n",
      "costas voglis\n",
      "coulet.simon\n",
      "courtney wanson\n",
      "cpossehl\n",
      "cricketsavant\n",
      "cristhianboujon\n",
      "cristiano\n",
      "cristina\n",
      "criticalhits\n",
      "cro-magnon\n",
      "crowdflower\n",
      "csbond007\n",
      "csgwon\n",
      "csungroup67\n",
      "currie32\n",
      "curtis chong\n",
      "cutechick\n",
      "cwiloc\n",
      "cyphers\n",
      "cyril ma\n",
      "cyzhao0709\n",
      "d pc\n",
      "ð–ñƒð»ð´ñ‹ð·ð¶ð°ð½ð¡ð°ð³ð¸ð¼ð±ð°ðµð²\n",
      "daan sterk\n",
      "dada123\n",
      "dahee\n",
      "dai guanyu\n",
      "daia alexandru\n",
      "daigo miyoshi\n",
      "daisuke ishii\n",
      "daisuke\n",
      "dale matthews\n",
      "dalgacik\n",
      "dalia research\n",
      "dalx555\n",
      "damian denesha\n",
      "damian eliel aleman\n",
      "damiano\n",
      "damien beneschi\n",
      "dan chrispine\n",
      "dan emery\n",
      "dan ofer\n",
      "dan van der meulen\n",
      "dan wilden\n",
      "dan winchester\n",
      "dan xu\n",
      "dan_lo\n",
      "dan\n",
      "danai avgerinou\n",
      "dananos\n",
      "danb\n",
      "danerbland\n",
      "daniel esteves\n",
      "daniel franch\n",
      "daniel grijalva\n",
      "daniel labbe\n",
      "daniel pye\n",
      "daniel s. panizzo\n",
      "daniel sã¡nchez\n",
      "daniel silion\n",
      "daniel sobrado\n",
      "daniele\n",
      "danielhkl\n",
      "danielvargas\n",
      "danielviray\n",
      "danielwatabe\n",
      "danil zherebtsov\n",
      "danishxavier\n",
      "dano\n",
      "danyakosmin\n",
      "daoduyson\n",
      "darcy\n",
      "daria glebova\n",
      "darklord\n",
      "data hunter\n",
      "data quantum\n",
      "data to information to knowledge to wisdom\n",
      "data-refinement\n",
      "datacanary\n",
      "datadopeboy\n",
      "datafiniti\n",
      "datagraver\n",
      "dataist\n",
      "datamin2017\n",
      "datap\n",
      "datasf\n",
      "dataspartan\n",
      "datastreamer\n",
      "datatest84\n",
      "datathã¨que\n",
      "dave d harsh\n",
      "dave fisher-hickey\n",
      "daverosenman\n",
      "david azria\n",
      "david baker\n",
      "david bialer\n",
      "david calloway\n",
      "david chudzicki\n",
      "david cohen\n",
      "david cooperberg\n",
      "david de la iglesia castro\n",
      "david havera\n",
      "david odhiambo\n",
      "david prakash\n",
      "david rubal\n",
      "david schwertfeger\n",
      "david skipper everling\n",
      "david strã¶m\n",
      "david_becks\n",
      "david\n",
      "david\n",
      "david\n",
      "david\n",
      "davide andreazzini\n",
      "davidparr\n",
      "davidshahrestani\n",
      "davidwesley-james\n",
      "davidyang\n",
      "dayana moncada\n",
      "dazhangyu\n",
      "ðð»ð¸ñð° ðÿñƒð³ð°ñ‡ðµð²ð°\n",
      "ðð»ðµðºñð°ð½ð´ñ€ (cmf da)\n",
      "ðð½ð´ñ€ðµð¹ð¢ð¸ð¼ð¾ñ„ðµðµð²\n",
      "dddhiraj\n",
      "death penalty information center\n",
      "deathmood\n",
      "debanjan\n",
      "debashish dalal\n",
      "debayandasgupta\n",
      "debdootsheet\n",
      "debirath\n",
      "dechavez005\n",
      "deeley\n",
      "deena liz john\n",
      "deep\n",
      "deep\n",
      "deepak gupta\n",
      "deepak\n",
      "deepakgupta\n",
      "deepakkandasamy\n",
      "deepakmittal\n",
      "deepanalytics\n",
      "delepp\n",
      "deltoix\n",
      "deluxe hotel in dalhousie\n",
      "deluxe\n",
      "demetri pananos\n",
      "democracy fund\n",
      "denisafonin\n",
      "dennys mallqui\n",
      "department of defense\n",
      "department of homeland security\n",
      "department of justice\n",
      "department of transportation\n",
      "derek chia\n",
      "derek li\n",
      "derek y\n",
      "derek zhi\n",
      "derek\n",
      "derrick m\n",
      "derrine\n",
      "destin\n",
      "devansh besain\n",
      "devashismohapatra\n",
      "developers area\n",
      "deveshmaheshwari\n",
      "devin anderson\n",
      "devious dus\n",
      "devji chhanga\n",
      "devjyotichandra\n",
      "dexteritas\n",
      "dgoke1\n",
      "dhafermalouche\n",
      "dhananjay shembekar\n",
      "dhruv desai\n",
      "dhruvmangtani\n",
      "dian purnama\n",
      "diego villacreses\n",
      "digitalcowboy\n",
      "dileep pandey\n",
      "dilzeem\n",
      "dimitrif\n",
      "ding\n",
      "diogo cortez\n",
      "dipanjan\n",
      "dipika baad\n",
      "dish\n",
      "divyajain\n",
      "divyam soni\n",
      "divyansh\n",
      "divyanshkumar\n",
      "divyojyoti sinha\n",
      "dmi3kno\n",
      "dmitrii petukhov\n",
      "dmitrijsc\n",
      "dmitriy sakharov\n",
      "dmitry\n",
      "dmpierre\n",
      "ðñ€ñ‚ðµð¼ ð›ñð½\n",
      "dobroezlo\n",
      "documenting the american south (docsouth)\n",
      "doe jhon\n",
      "ðœð¸ñ…ð°ð¸ð»ðœð°ðºññžñ‚ðµð½ðºð¾\n",
      "dom hall\n",
      "domenico delle side\n",
      "dominic\n",
      "dominik gawlik\n",
      "don browning\n",
      "donfuzius\n",
      "donggeun oh\n",
      "dongwoo kim\n",
      "donyoe\n",
      "dor oppenheim\n",
      "doran wu\n",
      "doug friedman\n",
      "doug hersak\n",
      "dr. ahmad al sallab\n",
      "dr. rich\n",
      "dr.d.lakshmi\n",
      "dr.priskott\n",
      "dragon\n",
      "drbenlyons\n",
      "drew pope\n",
      "drguillermo\n",
      "driss ait labsir\n",
      "drjkuo\n",
      "dromosys\n",
      "drop-out\n",
      "dryad digital repository\n",
      "dsafonov\n",
      "dseverything\n",
      "dsloet\n",
      "duaanasif\n",
      "ducky\n",
      "ducthanhnguyen\n",
      "dust\n",
      "dversteele\n",
      "dyadya bogdan\n",
      "dylan amelot\n",
      "dylan willow\n",
      "dylan\n",
      "dzoulouvincisavitridvsinformatique_ma_passion\n",
      "e.nikumanesh.germany\n",
      "ë²¤ìž ë¯¼\n",
      "ê²½ë¦¼ ê³½\n",
      "eagle\n",
      "eagles2f\n",
      "earless abdul\n",
      "ebrahimi\n",
      "eccc\n",
      "ecerulm\n",
      "ecodan\n",
      "ed king\n",
      "eden\n",
      "edern haumont\n",
      "edgano\n",
      "edi mala\n",
      "edilson augusto\n",
      "edit osikovicz\n",
      "edith\n",
      "edmon\n",
      "edo miyazaki\n",
      "edoardopiccari\n",
      "eduardo\n",
      "eduardomagalhã£esoliveira\n",
      "edward turner\n",
      "edwin kestler\n",
      "edx\n",
      "efrain guzman\n",
      "eibriel\n",
      "eidan cohen\n",
      "eigenlaw\n",
      "ekansg garg\n",
      "ekianjo\n",
      "eldar tinjic\n",
      "eleanorblum\n",
      "eleanorxu\n",
      "electoral commission\n",
      "electronic frontier foundation\n",
      "elemente n\n",
      "elen vardanyan\n",
      "elena cuoco\n",
      "elenacall\n",
      "eli cerdan\n",
      "elias barba moral\n",
      "elias8888\n",
      "eliezer bourchardt\n",
      "elitcenkalp\n",
      "eliud kagema\n",
      "elizabeth sam\n",
      "elkana rosenblatt\n",
      "ellarabinovich\n",
      "elliptic to quantum\n",
      "ema\n",
      "email365\n",
      "emersonpereirabertolo\n",
      "emil andreas lund\n",
      "emil nikolov\n",
      "emil.p\n",
      "emiliomc\n",
      "emily\n",
      "emirozbir\n",
      "emma\n",
      "emmanuel kens\n",
      "en kim biokpc\n",
      "éœœé›ªåƒå¹´\n",
      "eoveson\n",
      "epattaro\n",
      "epiphany\n",
      "eran machlev\n",
      "eric grinstein\n",
      "eric mccracken\n",
      "eric oakley\n",
      "eric vos\n",
      "eric you\n",
      "ericfeng\n",
      "erich rodrigues\n",
      "erik van de ven\n",
      "erik\n",
      "erikhambardzumyan\n",
      "eros\n",
      "erun noid\n",
      "esha somavarapu\n",
      "especuloide\n",
      "espen sonneland\n",
      "espen\n",
      "esperto\n",
      "eswar\n",
      "etherqua\n",
      "etienne lq\n",
      "eugene\n",
      "european centre for medium-range weather forecasts\n",
      "european space agency\n",
      "eurostat\n",
      "evan jung\n",
      "evanpayne\n",
      "everton seiei arakaki\n",
      "everypolitician\n",
      "evgeniy malishev\n",
      "evgeniy vasilev\n",
      "evil.com\n",
      "evren leet\n",
      "eyyã¼b sari\n",
      "ezequiel bequet\n",
      "fabia\n",
      "fabiano bizarro\n",
      "fabio correa cordeiro\n",
      "fabiola\n",
      "fabiolux\n",
      "facebook\n",
      "fad2018\n",
      "faguilar-v\n",
      "faisal\n",
      "faizal abd kadir\n",
      "fan fei chong\n",
      "fandang\n",
      "fangda\n",
      "far east group\n",
      "faraz\n",
      "farhan karim\n",
      "faronfeng\n",
      "fatihah ulya\n",
      "fatima-ezzahra elaamraoui\n",
      "fatimalidia\n",
      "fauzi\n",
      "fayomi\n",
      "fciceri\n",
      "federal aviation administration\n",
      "federal bureau of investigation\n",
      "federal communications commission\n",
      "federal deposit insurance corporation\n",
      "federal election commission\n",
      "federal emergency management agency\n",
      "federal reserve\n",
      "federico baylã©\n",
      "federico soldo\n",
      "federicosarrocco\n",
      "felipe hoffa\n",
      "felipeargolo\n",
      "felipeleiteantunes\n",
      "felix gutierrez\n",
      "felixzhao\n",
      "felypebastos\n",
      "femo\n",
      "fenilsuchak\n",
      "fernanda castro\n",
      "fernando lopez\n",
      "fernandobecerra\n",
      "festa78\n",
      "fifth tribe\n",
      "figshare\n",
      "filemide\n",
      "filipe morandi\n",
      "filippo\n",
      "finintelligence.com\n",
      "firdha amelia\n",
      "fivethirtyeight\n",
      "fizmath\n",
      "flaredown\n",
      "flashthunder\n",
      "flaviengelineau\n",
      "florian pydde\n",
      "florianthaunay\n",
      "florin langer\n",
      "flx1\n",
      "foenix\n",
      "folaraz\n",
      "food and drug administration\n",
      "foolius\n",
      "fordeletion\n",
      "fornax.ai\n",
      "fortune\n",
      "foxtrot\n",
      "frã©dã©ric girod\n",
      "fracking analysis\n",
      "fran\n",
      "francis paul flores\n",
      "francisco glez\n",
      "francisco mendez\n",
      "francisco penovi\n",
      "francisgeek\n",
      "frank he\n",
      "frank pac\n",
      "frank\n",
      "frankfernandes\n",
      "frankie\n",
      "franklin bradfield\n",
      "freddie\n",
      "fredrik jonsson\n",
      "free code camp\n",
      "freecodecamp\n",
      "freedom house\n",
      "french dude\n",
      "fungyuehoi\n",
      "funnymango\n",
      "fuzzyfroghunter\n",
      "g1ng0\n",
      "gabriel forsythe y korzeniewicz\n",
      "gabriel gutierrez corral\n",
      "gabriel joshua miguel\n",
      "gabriel moreira\n",
      "gabriel preda\n",
      "gabrielacaesar\n",
      "gabrielavellaneda\n",
      "gabriele angeletti\n",
      "gabriele baldassarre\n",
      "gabro\n",
      "gael kngm\n",
      "gagan\n",
      "gaganbhatia\n",
      "gajendrabadwal\n",
      "ganesh\n",
      "gaoweiwang\n",
      "garrykevin\n",
      "gary ramah\n",
      "gasimov aydin\n",
      "gaspare\n",
      "gaurav arora\n",
      "gaurav sharma\n",
      "gauravjain\n",
      "gautam doshi\n",
      "gauthamsenthil\n",
      "gavin cheng\n",
      "gavinarmstrong\n",
      "gaz113\n",
      "geco\n",
      "gecodavide\n",
      "gellowmellow\n",
      "geneburin\n",
      "genexpres\n",
      "gennadii\n",
      "geoffnoble\n",
      "geonames\n",
      "geonsoo kim\n",
      "george b\n",
      "georgemcintire\n",
      "georgii vyshnia\n",
      "georginarose\n",
      "gerardo suarez\n",
      "gerardosegura\n",
      "getthedata\n",
      "getting_started\n",
      "gevault\n",
      "gevorg aghekyan\n",
      "gfan\n",
      "ggzet\n",
      "giant: machine learning for smart environments\n",
      "gibs\n",
      "gift\n",
      "giginim\n",
      "giim\n",
      "gilad\n",
      "gilsousa\n",
      "gilvolpe\n",
      "gin04kg\n",
      "giologicx\n",
      "giorgioroffo\n",
      "giovanni gonzalez\n",
      "girish bansal\n",
      "girish murthy\n",
      "github\n",
      "giulia carra\n",
      "giuseppe\n",
      "gkhi\n",
      "gl_li\n",
      "global footprint network\n",
      "gmadevs\n",
      "gnana prasath\n",
      "gnanesh\n",
      "gnania527\n",
      "godeater\n",
      "gokagglers\n",
      "gokul alex\n",
      "gokul alex\n",
      "golden oak research group\n",
      "gomteshhatgine\n",
      "goneee\n",
      "gonzalo falloux\n",
      "google brain\n",
      "google natural language understanding research\n",
      "google news lab\n",
      "googleboy\n",
      "gopal\n",
      "gopal chettri\n",
      "gopaljaiswal\n",
      "gopi vasudevan\n",
      "gor khachatryan\n",
      "gorodec\n",
      "gourabbhattacharyya\n",
      "goutham\n",
      "government of france\n",
      "govlab\n",
      "gowtham\n",
      "gpoudel\n",
      "graham daley\n",
      "grainsan\n",
      "greatimposter\n",
      "greekygeek\n",
      "greenet09\n",
      "greg\n",
      "gregkondla\n",
      "gregory\n",
      "gregorysmith\n",
      "gregv\n",
      "greycop\n",
      "grishasizov\n",
      "group09\n",
      "grouplens\n",
      "grubenm\n",
      "gudang\n",
      "guik\n",
      "guilherme diaz-bã©rrio\n",
      "guilherme diego\n",
      "guilherme folego\n",
      "guillaume vinet\n",
      "guillaumetouzin\n",
      "gummula srikanth\n",
      "gun violence archive\n",
      "gunheepark\n",
      "gunner38\n",
      "gurpreet singh\n",
      "gurpreetsingh\n",
      "gus segura\n",
      "gus\n",
      "gustavo bonesso\n",
      "gustavo felhberg\n",
      "gustavo palacios\n",
      "gustavo torres\n",
      "gustavofelhberg\n",
      "gutsyrobot\n",
      "guy t.\n",
      "gyan\n",
      "gyanendramishra\n",
      "h1kkigakki\n",
      "h3mant\n",
      "hacker news\n",
      "hacker1\n",
      "haemin jeong\n",
      "haho\n",
      "haibo\n",
      "haitam abdoullah\n",
      "haitao chen\n",
      "hakan eren\n",
      "hakan toguc\n",
      "hakeem frank\n",
      "hakky\n",
      "hakmesyo\n",
      "hakob sukiasyan\n",
      "hamachi\n",
      "hamad42\n",
      "hammad a. usmani\n",
      "hamza el karoui\n",
      "hamza zafar\n",
      "hani ramadhan\n",
      "hanna\n",
      "hansel d'souza\n",
      "hansmaulwurf\n",
      "hao\n",
      "haohan wang\n",
      "haoyuzhao\n",
      "haozhengni\n",
      "hard_core\n",
      "hari krishna k\n",
      "hari prasath\n",
      "haris21gr\n",
      "harlfoxem\n",
      "harmanpreetsingh\n",
      "harold almon\n",
      "haroon ahmed\n",
      "harpiecrispi\n",
      "harry peter\n",
      "harry\n",
      "harry\n",
      "harryquake\n",
      "harrytan\n",
      "harsh b. gupta\n",
      "harsh mehta\n",
      "harsha\n",
      "harshavardhan\n",
      "harshit joshi\n",
      "harshit sinha\n",
      "harshitagupta\n",
      "harshitmehta\n",
      "harshitsrivastava\n",
      "harshoday\n",
      "harshpandya\n",
      "harshvardhan\n",
      "harvard university\n",
      "hashus\n",
      "hasil sharma\n",
      "hassanaftabmughal\n",
      "hassankhanyusufzai\n",
      "hatem\n",
      "hax s\n",
      "hazrat ali\n",
      "hdkim\n",
      "heatingsmoke\n",
      "hectopascal\n",
      "hedi ho\n",
      "hefen zhou\n",
      "heidogsdf\n",
      "heihei\n",
      "heiko\n",
      "heitor tomaz\n",
      "hellenandreea\n",
      "hello ml world\n",
      "hellrider\n",
      "hemant sain\n",
      "hemanth k\n",
      "hemanth kumar veeranki\n",
      "hemanthgowda\n",
      "hena\n",
      "hendrik å uvalov\n",
      "hendrik hilleckes\n",
      "henrikheggland\n",
      "henry\n",
      "henrywconklin\n",
      "heraldo reis\n",
      "herimanitra\n",
      "hervind\n",
      "heuristic\n",
      "heymeredith\n",
      "hhl028\n",
      "hidark\n",
      "hidehisa arai\n",
      "hieuvt\n",
      "hill yu\n",
      "hillary dawkins\n",
      "himanshu chaudhary\n",
      "himanshu garg\n",
      "himanshu shekhar\n",
      "himanshu0113\n",
      "himanshurai\n",
      "hioki ryuji\n",
      "hiro ari\n",
      "hiroyukishinoda\n",
      "hit_cs_li bo\n",
      "hitcs_1150310416\n",
      "hitcs_jiangzhenfei\n",
      "hitcs_yuhong_zhong\n",
      "hitesh desai\n",
      "hlnaima\n",
      "hm land registry\n",
      "homo deus\n",
      "hong\n",
      "honggu\n",
      "honlamlai\n",
      "hossein banki koshki\n",
      "how toai\n",
      "howard smith\n",
      "hrfm\n",
      "hssan driss\n",
      "htan\n",
      "hu yao\n",
      "huang xuan\n",
      "huang, peng-hsuan\n",
      "huangkai yuãƒ¾(â°ð´â°)ãƒž\n",
      "huayuantu\n",
      "hubert wassner\n",
      "hugh\n",
      "hugo mathien\n",
      "hugodarwood\n",
      "hugues talbot (esiee)\n",
      "huijun zhao\n",
      "huimin\n",
      "huiyu ye\n",
      "human computation\n",
      "humberto brandã£o\n",
      "hungdo\n",
      "hunter anderson\n",
      "hunter mcgushion\n",
      "hunterr\n",
      "husein zolkepli\n",
      "huseyinkilic\n",
      "husnain wajid\n",
      "hussien el-sawy\n",
      "husttiger\n",
      "huynguyen\n",
      "hwhy\n",
      "hyuan\n",
      "hyun ook ryu\n",
      "hyunjungbyeon\n",
      "i,coder\n",
      "i2i2i2\n",
      "iagodã­az\n",
      "ian chu te\n",
      "ian nanez\n",
      "ianmobbs\n",
      "iary joseph\n",
      "ibrahimaljarah\n",
      "ibrahimkhaleelullah\n",
      "ibrarhussain\n",
      "icebear\n",
      "idiosyncraticee\n",
      "iditarod trail committee\n",
      "idris kuti\n",
      "ifechide monyei\n",
      "ifeoluwaakande\n",
      "ignacio chavarria\n",
      "igor alexeev\n",
      "igor lemes\n",
      "igor nikolskiy\n",
      "igun\n",
      "ihme\n",
      "ilenia\n",
      "ilias sekkaf\n",
      "ilknuricke\n",
      "ilko masaldzhiyski\n",
      "imad khan\n",
      "imam digmi\n",
      "imran arif\n",
      "inã¨s potier\n",
      "inaba\n",
      "indicium\n",
      "infinitewing\n",
      "infinitylabs\n",
      "inooooooovation\n",
      "inquisitor\n",
      "institute for computing education at georgia tech\n",
      "institute for public policy and social research\n",
      "institute of museum and library services\n",
      "interaction engineering laboratory\n",
      "interface\n",
      "internal revenue service\n",
      "internet association\n",
      "internet association\n",
      "intest\n",
      "invinoveritas\n",
      "îœî±ïî¹î¿ï‚ îœî¹ï‡î±î·î»î¹î´î·ï‚ kazanova\n",
      "iplee\n",
      "irfan\n",
      "irfanwahyudin\n",
      "irina kalatskaya\n",
      "irinaachkasova\n",
      "irio musskopf\n",
      "iryna\n",
      "isaac a.\n",
      "isaac blinder\n",
      "isaac34\n",
      "isaacsim\n",
      "isabella plonk\n",
      "ishaan\n",
      "ishank saxena\n",
      "ishigen\n",
      "ishiryish\n",
      "ishnoor\n",
      "isildaaa\n",
      "ismail turkmen\n",
      "itamar mushkin\n",
      "itest\n",
      "itzik yohanan\n",
      "ivan jakovcevic\n",
      "ivan mazharov\n",
      "ivan tsy\n",
      "ivan\n",
      "ivanloginov\n",
      "ivo penkov\n",
      "iwase yuya\n",
      "izabella\n",
      "izzie toren\n",
      "izzuddin\n",
      "izzy\n",
      "j from the riverside\n",
      "j.davidcorrea\n",
      "jã¶rg eitner\n",
      "jã¶rgen sinka\n",
      "jaak ungro\n",
      "jacco jurg\n",
      "jack blarr\n",
      "jack cook\n",
      "jack cosgrove\n",
      "jack ho\n",
      "jack miller\n",
      "jack sunny\n",
      "jack\n",
      "jack\n",
      "jack\n",
      "jackliu\n",
      "jackson harper\n",
      "jackson raja\n",
      "jacky wong\n",
      "jackyd\n",
      "jaclyn a\n",
      "jaco de groot\n",
      "jacob boysen\n",
      "jacobgoozner\n",
      "jaehyeon yu\n",
      "jaewonk\n",
      "jaffer syed\n",
      "jagan\n",
      "jagannath neupane\n",
      "jaime valero\n",
      "jaish k\n",
      "jake gnieser\n",
      "jake rohrer\n",
      "jake toffler\n",
      "jake waitze\n",
      "jakub pubrat\n",
      "jalaz kumar\n",
      "james ahn\n",
      "james clavin\n",
      "james condon\n",
      "james d.\n",
      "james littiebrant\n",
      "james mathews\n",
      "james tollefson\n",
      "james\n",
      "jamesbasker\n",
      "jamesg\n",
      "jamess\n",
      "jamesyang96\n",
      "jamesyuan\n",
      "jan bodnar\n",
      "jan charles maghirang adona\n",
      "jan christian blaise cruz\n",
      "jan nordin\n",
      "jana\n",
      "janani damodaran gantal\n",
      "janek\n",
      "janice\n",
      "janzen liu\n",
      "jason a. hatton\n",
      "jason a\n",
      "jason benner\n",
      "jason liu\n",
      "jason mcneill\n",
      "jason nguyen\n",
      "jason schenck\n",
      "jason.f_cn\n",
      "jason\n",
      "jason\n",
      "jasonhuang\n",
      "jasonzhang\n",
      "jatin raina\n",
      "jatin shah\n",
      "jaturong kongmanee\n",
      "javascript:alert(8007);\n",
      "javier villanueva-valle\n",
      "javier\n",
      "jay i\n",
      "jay kulshreshtha\n",
      "jay ravaliya\n",
      "jay333\n",
      "jaya gupta\n",
      "jayanth yetukuri\n",
      "jayanth\n",
      "jayavardhan reddy\n",
      "jayjay\n",
      "jaylee\n",
      "jbd\n",
      "jbfields\n",
      "jd torres\n",
      "jean pierre rukundo\n",
      "jean-marcbouvier\n",
      "jean-michel d.\n",
      "jean-nicholashould\n",
      "jean-phillipe\n",
      "jeanpat\n",
      "jeevannagaraj\n",
      "jeff kao\n",
      "jeff ussing\n",
      "jeff\n",
      "jefferyt\n",
      "jefftennis\n",
      "jegs\n",
      "jegyeongkim\n",
      "jekaterina kokatjuhha\n",
      "jekwon\n",
      "jemilu mohammed\n",
      "jenkins ruban\n",
      "jens laufer\n",
      "jenvo\n",
      "jeongmin ha\n",
      "jerad rose\n",
      "jeremy seibert\n",
      "jeremy wang\n",
      "jeremymiles\n",
      "jeremywickman\n",
      "jerrin joe varghese\n",
      "jerryg\n",
      "jerrywang\n",
      "jesse montgomery\n",
      "jessica yung\n",
      "jessie-raye bauer\n",
      "jesus jara lã³pez\n",
      "jesus santander\n",
      "jguerreiro\n",
      "jhonatanzubieta\n",
      "jiachuandeng\n",
      "jiajane\n",
      "jiaming huang\n",
      "jian w\n",
      "jian zhang\n",
      "jiang yu\n",
      "jiangzuo\n",
      "jianshefeng\n",
      "jibsgrl\n",
      "jigarkumar patel\n",
      "jihane hammout\n",
      "jihye sofia seo\n",
      "jiji\n",
      "jill_m\n",
      "jimmymarguerite\n",
      "jin liu\n",
      "jin-hwachiu\n",
      "jindong wang\n",
      "jindra lacko\n",
      "jinesh john\n",
      "jing zhang\n",
      "jing\n",
      "jingdazhou\n",
      "jingjuewang\n",
      "jingli\n",
      "jingwang\n",
      "jinner\n",
      "jinsoo yeo\n",
      "jinze he\n",
      "jiri roznovjak\n",
      "jirka vrany\n",
      "jitendra rajpurohit\n",
      "jitendrakumarbansal\n",
      "jiuzhang\n",
      "jjjooo1\n",
      "jlucas\n",
      "jmataya\n",
      "jo-team\n",
      "joã£o pedro peinado\n",
      "joao januario\n",
      "joao pedro evangelista\n",
      "jobspikr\n",
      "joe kim\n",
      "joe philleo\n",
      "joe ramir\n",
      "joe young\n",
      "joejoe\n",
      "joel jacobsen\n",
      "joel lee\n",
      "joel wilson\n",
      "joeland209\n",
      "joerg simon wicker\n",
      "joeymeyer\n",
      "joffles\n",
      "johannes plambeck\n",
      "johannesbuchner\n",
      "johanneslapoutre\n",
      "john bourassa\n",
      "john doe\n",
      "john doe\n",
      "john doe\n",
      "john doe\n",
      "john joe\n",
      "john jones\n",
      "john lin\n",
      "john mark\n",
      "john olafenwa\n",
      "john ostrowski\n",
      "john ruth\n",
      "john sumerel\n",
      "john traavis\n",
      "john wu\n",
      "john\n",
      "john\n",
      "john\n",
      "john2\n",
      "johncurcio\n",
      "johnd\n",
      "johndebugger\n",
      "johnheyrich\n",
      "johnjaychou&michellezhuang\n",
      "johnnyha\n",
      "johnworne\n",
      "johnx\n",
      "jolhe006\n",
      "jomendes\n",
      "jon b\n",
      "jon hong\n",
      "jon.bill\n",
      "jonah mary17\n",
      "jonahelisio\n",
      "jonatan cisneros\n",
      "jonathan\n",
      "jonathan\n",
      "jonathanphoon\n",
      "jones\n",
      "jonh doe\n",
      "jonihoppen\n",
      "joostlubach\n",
      "jordan goblet\n",
      "jordan meta\n",
      "jordan tremoureux\n",
      "jorgezazueta\n",
      "josä—andrä—salvarezcabrera\n",
      "josã© vicente\n",
      "josã©prado\n",
      "jose berengueres\n",
      "jose fco morales\n",
      "jose lery nunes\n",
      "jose luis juarez ruelas\n",
      "jose manuel vera\n",
      "jose toro\n",
      "jose\n",
      "josep a.\n",
      "joseph leichter\n",
      "joseph\n",
      "josephbae\n",
      "josh haimson\n",
      "josh wheeler\n",
      "josh woulfe\n",
      "josh777\n",
      "joshkyh\n",
      "joshmckenney\n",
      "joshua schnessl\n",
      "joshuaherman\n",
      "joss\n",
      "jossssss\n",
      "jpss\n",
      "jr91\n",
      "jruots\n",
      "jruvika\n",
      "jrvalentin\n",
      "jscharbach\n",
      "juan corporan\n",
      "juan r\n",
      "juan soler-company\n",
      "juanrodriguez\n",
      "juanu\n",
      "jujuuu\n",
      "julian christov\n",
      "julian simon de castro\n",
      "julie\n",
      "julien frisch\n",
      "jun zhu\n",
      "juncheng zhou\n",
      "junfeng zhang\n",
      "juniageorge\n",
      "junki cho\n",
      "juran\n",
      "just try\n",
      "justin pan\n",
      "justinmoore\n",
      "juturupavan\n",
      "jvent\n",
      "jvm56\n",
      "jwuthrich\n",
      "jyothi kamakshi\n",
      "jyoti sharma\n",
      "jyun-ting\n",
      "jyzaguirre\n",
      "jz2771\n",
      "k6box\n",
      "kã¤rt\n",
      "kã¢zä±m anä±l eren\n",
      "kaan can\n",
      "kaan ulgen\n",
      "kaffes\n",
      "kagami\n",
      "kaggle\n",
      "kaggleray\n",
      "kaguser\n",
      "kaho\n",
      "kai wang\n",
      "kaique da silva\n",
      "kairit\n",
      "kajot\n",
      "kalcal\n",
      "kalyanyerra\n",
      "kamal raj\n",
      "kamau john\n",
      "kambarakun\n",
      "kamesh s\n",
      "kamil jurek\n",
      "kamil kaczmarek\n",
      "kamlesh\n",
      "kamran\n",
      "kande bonfim\n",
      "kane\n",
      "kanika narang\n",
      "kanikachopra\n",
      "kanishka misra\n",
      "kanishkpratapsingh\n",
      "kannanpiedy\n",
      "karamveer\n",
      "karan thakkar\n",
      "karansharma\n",
      "kardopaska\n",
      "karelverhoeven\n",
      "karim ardi\n",
      "karimbelayati\n",
      "karimnahas\n",
      "karmanya aggarwal\n",
      "karmot\n",
      "karolina wullum\n",
      "kartheek\n",
      "karthickveerakumar\n",
      "karthickvel\n",
      "karthik\n",
      "karthiks\n",
      "karthikziffer\n",
      "kartik\n",
      "kartikpatnaik\n",
      "kashif kaleem\n",
      "kashish suneja\n",
      "kashyap\n",
      "kasper nielsen\n",
      "kate\n",
      "katrina ni\n",
      "katzwigmore\n",
      "kaus\n",
      "kaushik s\n",
      "kaveti naveen kumar\n",
      "kaylan foster\n",
      "kayode emmanuel oluwatobi\n",
      "kazuki\n",
      "kedanli\n",
      "keelan robinson\n",
      "keheira\n",
      "keik@\n",
      "keisei\n",
      "keita shimizu\n",
      "kelvin wellington\n",
      "kelvin xiao\n",
      "kemal yilmaz\n",
      "kemical\n",
      "ken yamaji\n",
      "kendallgillies\n",
      "kenichinakatani\n",
      "kenji kondo\n",
      "kenneth benavides\n",
      "kenneth chua\n",
      "kenomaru\n",
      "kentarotakemoto\n",
      "kenton w. murray\n",
      "keras\n",
      "keval m\n",
      "kevin\n",
      "kevin chow\n",
      "kevin mader\n",
      "kevin mario gerard\n",
      "kevin moodley\n",
      "kevin pertsovsky\n",
      "kevin ree\n",
      "kevin soucy\n",
      "kevin\n",
      "kevin\n",
      "kevin\n",
      "kevinh\n",
      "kevv\n",
      "khac bao anh nguyen\n",
      "khai xiang\n",
      "khaled salah\n",
      "khashayar baghizadeh hosseini\n",
      "kheirallah samaha\n",
      "khepry quixote\n",
      "khushboo\n",
      "kiefer smith\n",
      "kilian batzner\n",
      "kilian. o\n",
      "kim schreier\n",
      "kimos\n",
      "kimura\n",
      "kingsley samuel\n",
      "kiran ganji\n",
      "kiran gutha\n",
      "kirankarri\n",
      "kirthikababu\n",
      "kishan p\n",
      "kishore\n",
      "kittisak\n",
      "kiweee\n",
      "kiyonariharigae\n",
      "kk\n",
      "kk\n",
      "kk16\n",
      "kkddall\n",
      "kktestin2'\"\n",
      "kleber bernardo\n",
      "km1west\n",
      "kmmr\n",
      "kmuvunyi\n",
      "kola adebayo\n",
      "kondalarao vonteru\n",
      "konstantin lopuhin\n",
      "konstantin\n",
      "konstantinos bazakos\n",
      "korakot chaovavanich\n",
      "kory becker\n",
      "kosiewmm\n",
      "kostiantyn isaienkov\n",
      "kostya\n",
      "kote42\n",
      "kotobotov\n",
      "kouassi konan jean-claude\n",
      "kozlova\n",
      "kp\n",
      "kpapamih\n",
      "kravdiy\n",
      "krishna agarwal\n",
      "krishna bharadwaj\n",
      "krishnadheeraj\n",
      "krishnan\n",
      "krishnapraveen\n",
      "krishnathiyagarajan\n",
      "krismurphy\n",
      "kristian h\n",
      "kristjan pã¤rn\n",
      "kristofferhess\n",
      "kristopher sheets, phd\n",
      "krithel\n",
      "krizsã³ gergely\n",
      "krsimons\n",
      "ksayantani\n",
      "ksenia sukhova\n",
      "kso.\n",
      "kumar abhishek\n",
      "kumar nityan suman\n",
      "kumar\n",
      "kumaran k\n",
      "kumarbhrgv\n",
      "kumarhalake\n",
      "kunal kotian\n",
      "kunal singh\n",
      "kunal vaishnavi\n",
      "kunalkumawat\n",
      "kunimune\n",
      "kuntal sardar\n",
      "kushal\n",
      "kushneryk pavel\n",
      "kutsalbaranã–zkurt\n",
      "kveykva\n",
      "kvpratama\n",
      "kwan lowe\n",
      "kwangrok lee\n",
      "kwtcut\n",
      "kyle mcclurg\n",
      "kyle moon\n",
      "l sun\n",
      "la sul\n",
      "la times data desk\n",
      "lacie\n",
      "lacksonmundira\n",
      "ladams\n",
      "lahouarami\n",
      "laiyilin\n",
      "lakshadvani\n",
      "lakshya khandelwal\n",
      "lalit khandelwal\n",
      "lalit parihar\n",
      "lalitsomnathe\n",
      "lalthan\n",
      "lamda-dev\n",
      "langzi\n",
      "lantana camara\n",
      "lanvukuå¡iä\n",
      "lasteg\n",
      "lastjedi76\n",
      "laura\n",
      "laurae\n",
      "lauramoen\n",
      "lauren bk\n",
      "laurenstc\n",
      "laurentberder\n",
      "lavi\n",
      "lavishgulati\n",
      "lazkol\n",
      "le pallec clã©ment\n",
      "leandro dos santos coelho\n",
      "leandro silva\n",
      "learner\n",
      "lee worthington\n",
      "leeyun\n",
      "lefant\n",
      "lehmaudar\n",
      "lei ding\n",
      "leigh\n",
      "leo arruda\n",
      "leo\n",
      "leon martin\n",
      "leon\n",
      "leonardo ferreira\n",
      "leonidas\n",
      "leonpaul\n",
      "leroberge\n",
      "lesoler\n",
      "lespaulcustom\n",
      "leticiafilgueiras\n",
      "levima\n",
      "lewis\n",
      "lexie dempsey\n",
      "lgpatel\n",
      "liam cusack\n",
      "liamlarsen\n",
      "librahu\n",
      "lieven23\n",
      "light-boat\n",
      "lihan\n",
      "lihaoyang\n",
      "liisi\n",
      "lili\n",
      "liling tan\n",
      "lilit janjughazyan\n",
      "limi44\n",
      "liming\n",
      "limmen\n",
      "limon m\n",
      "lin gao\n",
      "lin ying lung\n",
      "lincoln\n",
      "lindada\n",
      "lingzhi\n",
      "linkanray\n",
      "lisa\n",
      "lisjin\n",
      "lislejoem\n",
      "lissette guzman\n",
      "litianyi\n",
      "little boat\n",
      "litu rout\n",
      "liuenda\n",
      "liuxiaoliu\n",
      "liuyang\n",
      "liuyongqi\n",
      "liuzhe0125\n",
      "livi\n",
      "liwste\n",
      "liza bolton\n",
      "ljhuang\n",
      "lkytal\n",
      "lnicalo\n",
      "loghorizon\n",
      "logwinner\n",
      "lohith\n",
      "lomungo\n",
      "looo\n",
      "lorna maria\n",
      "louis marmet\n",
      "louis\n",
      "louis\n",
      "louweal\n",
      "loyf\n",
      "lpitre\n",
      "luanho\n",
      "luã­s gustavo modelli\n",
      "lubaroli\n",
      "lucas astorian\n",
      "lucas dixon\n",
      "lucas erring\n",
      "lucas venezian povoa\n",
      "lucas vergeest\n",
      "lucas\n",
      "lucasvinze\n",
      "lucio lã³pez lecube\n",
      "ludovic benistant\n",
      "lugark\n",
      "luigi\n",
      "luis andre dutra e silva\n",
      "luis bronchal\n",
      "luis moneda\n",
      "luisaapf\n",
      "luistelmocosta\n",
      "luiz gerosa\n",
      "luiz gustavo schiller\n",
      "luiz henrique amorim\n",
      "luiza fontana\n",
      "luke bunge\n",
      "luke godwin-jones\n",
      "lukebyrne\n",
      "lukelee\n",
      "lumin\n",
      "lunarllama\n",
      "luu\n",
      "lyahmedtidiane\n",
      "lyh19970409\n",
      "lynn dai\n",
      "lynnpan\n",
      "m baddar\n",
      "m ganiyu\n",
      "m.f.\n",
      "maarten\n",
      "mabs\n",
      "machine learning datasets\n",
      "maciej witkowiak\n",
      "mad hab\n",
      "madhan varadhodiyil\n",
      "madhav iyengar\n",
      "madhavi burra\n",
      "madhur inani\n",
      "madis_lemsalu\n",
      "madison curtis\n",
      "madscientist\n",
      "maghilnan\n",
      "magick\n",
      "magsgiust\n",
      "mahadevan\n",
      "mahdijavid\n",
      "mahdy nabaee\n",
      "mahek hooda\n",
      "mahesh sinha\n",
      "mahesh_prs\n",
      "mahirkukreja\n",
      "mahmoud aljabary\n",
      "mahreenahmed\n",
      "maik3141\n",
      "mainak kundu\n",
      "maitree priyadarsini\n",
      "makarandvelankar\n",
      "maksim mikhotov\n",
      "maksym\n",
      "malathi arumugam\n",
      "malek trabelsi\n",
      "malinee fawcett\n",
      "malini\n",
      "mamun\n",
      "manan jain\n",
      "manan manwani\n",
      "manas\n",
      "manav sehgal\n",
      "mancml\n",
      "manfredi federico pivetta\n",
      "mani\n",
      "manikanta\n",
      "manikhossain\n",
      "manimala\n",
      "manish jain\n",
      "manish kumar\n",
      "manjeet singh\n",
      "manoj kumar\n",
      "manoj2891\n",
      "manojhariharan\n",
      "manojkumar parmar\n",
      "manojkumar\n",
      "manqiong\n",
      "manshubh singh rihal\n",
      "mansoor iqbal\n",
      "mansour movahhedinia\n",
      "mantas zimnickas\n",
      "manuel barrena\n",
      "mapik88\n",
      "marã­a otero\n",
      "marc kossa\n",
      "marc moreaux\n",
      "marc robert\n",
      "marc slaughter\n",
      "marc velmer\n",
      "marc\n",
      "marcel\n",
      "marcell \"mazuh\" guilherme costa da silva\n",
      "marcelo santos\n",
      "marco boaretto\n",
      "marco de nadai\n",
      "marco molina\n",
      "marco zanchi\n",
      "marcocarnini\n",
      "marcos boaglio\n",
      "marcschroeder\n",
      "marctorrellas\n",
      "marcus lin\n",
      "maria bile\n",
      "maria luiza\n",
      "mariakatosvich\n",
      "mariehane\n",
      "marielen ferreira\n",
      "mario navas\n",
      "mario pasquato\n",
      "marius\n",
      "mark dimarco\n",
      "mark eldridge\n",
      "mark\n",
      "markarchiegamayan\n",
      "marketing as is\n",
      "marko k\n",
      "markschultz\n",
      "markus lang\n",
      "marlesson\n",
      "marouane benmeida\n",
      "martin enzinger\n",
      "martin pereira\n",
      "martinboyanov\n",
      "martj\n",
      "marty\n",
      "marvin\n",
      "marwa saied\n",
      "masahito429\n",
      "masakt\n",
      "masato hagiwara\n",
      "masato\n",
      "masood hussain\n",
      "massachusetts institute of technology\n",
      "masseycre\n",
      "mateus\n",
      "mathew savage\n",
      "mathias meldgaard pedersen\n",
      "mathiasedman\n",
      "mathieu goutay\n",
      "mathijs waegemakers\n",
      "mathishammel\n",
      "mathurin achã©\n",
      "matiasfeld\n",
      "matsueushi\n",
      "matt hixon\n",
      "matt rose\n",
      "matt snell\n",
      "matt\n",
      "matteo casadei\n",
      "matteo_mazzola\n",
      "matthew allbee\n",
      "matthew anderson\n",
      "matthew carter\n",
      "matthew\n",
      "matthewhonnibal\n",
      "matthieu c\n",
      "mattia gigliotti\n",
      "mattilgale\n",
      "maurice_f\n",
      "mauro reverter\n",
      "mavez dabas\n",
      "max candocia\n",
      "max halford\n",
      "max horowitz\n",
      "max mind\n",
      "max stanford-taylor\n",
      "max.liu\n",
      "maxime fuccellaro\n",
      "maximilian hahn\n",
      "maximilian kapsecker\n",
      "mayank singla\n",
      "mayanksiddharth\n",
      "mayanktiwari\n",
      "maykon ravy\n",
      "mcdonald's\n",
      "mcrescenzo\n",
      "md irfan ali\n",
      "mearafat\n",
      "medicare\n",
      "meep\n",
      "meetika sharma\n",
      "meg shields\n",
      "megan risdal\n",
      "mehdi\n",
      "mehedi shafi\n",
      "mehrdad\n",
      "mehrdadz007\n",
      "mehta\n",
      "meigang gu\n",
      "meinertsen\n",
      "melody z\n",
      "melvincheung\n",
      "melvyn drag\n",
      "mengfei li\n",
      "mengmengyong\n",
      "mengyan\n",
      "mengye\n",
      "meow\n",
      "mepotts\n",
      "merilin kãµrnas\n",
      "mesum raza hemani\n",
      "mgkmgk\n",
      "mgn\n",
      "mharrys\n",
      "mhouellemont\n",
      "miaomiao\n",
      "michaå‚ jamry\n",
      "michaå‚puchalski\n",
      "michael clouting\n",
      "michael ibrahim\n",
      "michael ks\n",
      "michael nation\n",
      "michael pang\n",
      "michael pavlukhin\n",
      "michael plohhotnichenko\n",
      "michael skrzypiec\n",
      "michael\n",
      "michael\n",
      "michaelkirk\n",
      "michaelklear\n",
      "michaelstone\n",
      "michal januszewski\n",
      "michelle hy\n",
      "miemie kurisu\n",
      "miguel lladã³\n",
      "miguel\n",
      "miguel\n",
      "miguelsalazar\n",
      "mihai oltean\n",
      "mihir garg\n",
      "mihkel gering\n",
      "mihwahan\n",
      "miinooo\n",
      "mijim\n",
      "mike chirico\n",
      "mike johnson jr\n",
      "mike kim\n",
      "mike mekilo\n",
      "mike pastore\n",
      "mike sebel\n",
      "mikhail chesnokov\n",
      "miki112\n",
      "mikiokubo\n",
      "mikr\n",
      "milindparadkar\n",
      "miljenko bartulovic\n",
      "mimic1\n",
      "mina\n",
      "minat verma\n",
      "mindaugasmejeras\n",
      "minerwa min\n",
      "mingming\n",
      "minmind\n",
      "minso\n",
      "minx\n",
      "minxuan\n",
      "minyao\n",
      "mir ali\n",
      "miranda\n",
      "mircea stanciu\n",
      "mirko mã¤licke\n",
      "miro karpis\n",
      "miroslav sabo\n",
      "miroslav zoricak\n",
      "miroslavtyurin\n",
      "mirrorlu\n",
      "mission san jose ai club\n",
      "mistymoo\n",
      "mitchell j\n",
      "mithileshwaribhade\n",
      "mitillo\n",
      "mitsu\n",
      "mitusha\n",
      "miza'\n",
      "mizosalah\n",
      "mkmk\n",
      "ml coder\n",
      "ml_cx\n",
      "mlagunas\n",
      "mlane\n",
      "mls\n",
      "mlxd\n",
      "mnakajima\n",
      "modeling online auctions\n",
      "moghazy\n",
      "mohamed abul danish\n",
      "mohamed elsayed\n",
      "mohamed loey\n",
      "mohamed ramadan\n",
      "mohamed shawky dg\n",
      "mohamedsaiddaw\n",
      "mohamedshawky\n",
      "mohamedwasim\n",
      "mohammad ali\n",
      "mohammad ghahramani\n",
      "mohammad kachuee\n",
      "mohammadamir\n",
      "mohammadaseemurrehman\n",
      "mohammed alnemari\n",
      "mohit balani\n",
      "mohit sainani\n",
      "moi\n",
      "moko sharma\n",
      "monika munjal\n",
      "monishc\n",
      "monkeyking\n",
      "moon soo lee\n",
      "morcinim\n",
      "morganmazer\n",
      "moses salifu\n",
      "moshfiqur rahman\n",
      "motaz saad\n",
      "moufid\n",
      "mouli\n",
      "moxious\n",
      "mozilla\n",
      "mphogodfreynkadimeng\n",
      "mr. analytics\n",
      "mrdeeds\n",
      "mridulsharma\n",
      "mritunjaymohitesh\n",
      "mrjazz\n",
      "mrnasalhazel\n",
      "mrpantherson\n",
      "mrsantos\n",
      "mrverde\n",
      "mrzzheng\n",
      "ms brown\n",
      "msiebold\n",
      "msjass\n",
      "mszombie\n",
      "mt\n",
      "mudit choraria\n",
      "mufti mubarak\n",
      "muhamad nady\n",
      "muhammad abdul rehman\n",
      "muhammad alfiansyah\n",
      "muhammad ali\n",
      "muhammad aseem ur rehman\n",
      "muhammad asif khan\n",
      "muhammad jamil moughal\n",
      "muhammadmahadtariq\n",
      "muhammadyasiradnan\n",
      "mukarram pasha\n",
      "mukesh kumar\n",
      "muneeb ul hassan\n",
      "muonneutrino\n",
      "murali_munna\n",
      "muralidhar anumula\n",
      "murder accountability project\n",
      "mureren\n",
      "murilo siqueira\n",
      "murilo viviani\n",
      "muskanbararia\n",
      "mustakim\n",
      "muthukumar.j\n",
      "muttaqi ismail\n",
      "my khe nguyen\n",
      "myles o'neill\n",
      "mypapit\n",
      "n&n student\n",
      "n01z3\n",
      "nabeel raza\n",
      "nada fathallah\n",
      "nadin tamer\n",
      "nagabhushan s b\n",
      "nagaraj ramakrishna\n",
      "nagendra yadav\n",
      "nailo\n",
      "nami\n",
      "namory koulibaly\n",
      "namsraijav dugersuren\n",
      "nan ji\n",
      "nancy lubalo\n",
      "nandagopal m\n",
      "naominguyen\n",
      "narmeen\n",
      "nasa\n",
      "nasir mushtaq\n",
      "naszy\n",
      "nat t\n",
      "natalia\n",
      "natalia\n",
      "natalie ha\n",
      "natasha zope\n",
      "natasha\n",
      "natasha\n",
      "nate\n",
      "nathan burns\n",
      "nathan cohen\n",
      "nathan zhang\n",
      "nathan\n",
      "nathangeorge\n",
      "nathaniel see\n",
      "national archives\n",
      "national health service\n",
      "national institutes of health chest x-ray dataset\n",
      "national library of medicine\n",
      "national park service\n",
      "national snow and ice data center\n",
      "national ufo reporting center (nuforc)\n",
      "navdeep pal\n",
      "naveen holla\n",
      "naveen kumar\n",
      "naveen pandian\n",
      "navneethc\n",
      "navyashrees\n",
      "nayan bhattacharya\n",
      "nayan solanki\n",
      "nazimamzz\n",
      "ncaa\n",
      "ncls byr\n",
      "neel shah\n",
      "neeraj kasturi\n",
      "neerav kharche\n",
      "neha singh\n",
      "neha\n",
      "neils\n",
      "neinei\n",
      "neksdrawkcab\n",
      "nelson chu\n",
      "nelson\n",
      "nema\n",
      "nerdiholic\n",
      "netanelmalka\n",
      "netflix\n",
      "neuroguy\n",
      "never_die\n",
      "new america\n",
      "new york philharmonic\n",
      "new york public library\n",
      "newman\n",
      "nguyen tang tri duc\n",
      "nhtsa\n",
      "nic\n",
      "nicholas zufelt\n",
      "nick digiulio\n",
      "nick rose\n",
      "nick schroeder\n",
      "nick spadafora\n",
      "nick torsky\n",
      "nick wagner\n",
      "nick wong\n",
      "nickachin\n",
      "nicksehy\n",
      "niclas kjã¤ll-ohlsson\n",
      "nico belov\n",
      "nicolã¡s\n",
      "nicolabernini\n",
      "nicolas p\n",
      "nigel dalziel\n",
      "nihal88\n",
      "nika ioramishvili\n",
      "nikhil akki\n",
      "nikhil gargeya\n",
      "nikhil gupta\n",
      "nikhil jain\n",
      "nikhil parihar\n",
      "nikhil reddy\n",
      "nikhil\n",
      "nikhil\n",
      "nikita malyshev\n",
      "nikunj\n",
      "nilesh sakpal\n",
      "nilesh\n",
      "nils ponomarchuk\n",
      "nilzone\n",
      "ning zhou\n",
      "niniyan\n",
      "nirajk18\n",
      "niranjan nakkala\n",
      "niranjandeshpande\n",
      "nirav nikunj patel\n",
      "nirmalelumalai\n",
      "nirmalyakumarmohanty\n",
      "nishant\n",
      "nishant arora\n",
      "nishant bhadauria\n",
      "nishant k\n",
      "nishant kumar\n",
      "nishantjain\n",
      "nishio hirokazu\n",
      "nishit sehgal\n",
      "nitesh tiwari\n",
      "nitesh yadav\n",
      "niteshsurana\n",
      "nitin bisht\n",
      "nitin venkateswaran\n",
      "nitishaadhikari\n",
      "niwech harnkham\n",
      "niyamat ullah\n",
      "nlspdx\n",
      "nltk data\n",
      "nmin\n",
      "no more overfitting\n",
      "no re\n",
      "noaa\n",
      "noah gift\n",
      "noah schwartz\n",
      "noah wang\n",
      "nodes\n",
      "noel yoo\n",
      "nolan conaway\n",
      "nooh\n",
      "norbertbudincsevity\n",
      "norc.org\n",
      "nosbielcs\n",
      "nowshin nawar arony\n",
      "npo 2799\n",
      "nuggs\n",
      "numerai\n",
      "nupur warke\n",
      "nuraddin\n",
      "nuråÿenã–äÿã¼tveren\n",
      "nv27\n",
      "nyc open data\n",
      "nyc parks and recreation\n",
      "nypd\n",
      "ø¹ø¨ø¯ø§ù„ù„ø·ùšùø£ø­ù…ø¯øºù„ø§ø¨\n",
      "obadiahjeshurennaidoo\n",
      "obandoruben\n",
      "obey ismael\n",
      "ocelot\n",
      "octaviog\n",
      "ofaymailey\n",
      "oh inqueue\n",
      "ohhm prakash k i\n",
      "okus\n",
      "ole krã¶ger\n",
      "oleg brizhatiy\n",
      "oleg o\n",
      "olegsolomka\n",
      "oleksii nidzelskyi\n",
      "olga belitskaya\n",
      "olga ivanova\n",
      "oliveira, l. o. v. b.\n",
      "oliver collins\n",
      "olivermoraleslopez\n",
      "olivia\n",
      "olivier richard\n",
      "olivier\n",
      "ololo\n",
      "omajaykarthik\n",
      "omar\n",
      "omer gozuacik\n",
      "omicron\n",
      "omkarp\n",
      "onkarkadam\n",
      "onno eberhard\n",
      "onofrio_biscience\n",
      "open food facts\n",
      "open knowledge international\n",
      "open source sports\n",
      "open sourcing mental illness, ltd\n",
      "openaddresses\n",
      "openflights\n",
      "ophelia1234\n",
      "orco\n",
      "orges leka\n",
      "orgodoldawaasuren\n",
      "orgrimm9\n",
      "oscar takeshita\n",
      "oscar zamora\n",
      "oscarleo\n",
      "ostrokach\n",
      "osubmi\n",
      "oswin rahadiyan hartono\n",
      "ouissa souliman\n",
      "ouyangxuan\n",
      "owais\n",
      "ozan aygun\n",
      "ozgur\n",
      "p111110\n",
      "pablo\n",
      "pablo escobar\n",
      "pablo tabales\n",
      "pablomonleon\n",
      "padmavathi r\n",
      "paesibassi\n",
      "painkiller\n",
      "pakshal jain\n",
      "palak sharma\n",
      "palashshah\n",
      "pallav routh\n",
      "pallavi ramicetty\n",
      "panagiotis g. togias\n",
      "panchicore\n",
      "pancho\n",
      "panda974\n",
      "pandatadelta\n",
      "pandey nilesh prasad\n",
      "panos kostakos\n",
      "panos\n",
      "panos\n",
      "paolo campanelli\n",
      "paolo\n",
      "paosheng\n",
      "parallax\n",
      "paras jindal\n",
      "paresh\n",
      "parichart\n",
      "parindsheel singh\n",
      "park thirty-two\n",
      "parmanand sahu\n",
      "parole hearing data project\n",
      "parseltung\n",
      "parth gupta\n",
      "parth iramani\n",
      "parthmaheshwari\n",
      "pascal brenner\n",
      "patatae\n",
      "patit pawan karmakar\n",
      "patrick hyland\n",
      "patrick j\n",
      "patrick murphy\n",
      "patryk niedåºwiedziå„ski\n",
      "paul abramshe\n",
      "paul curry\n",
      "paul larmuseau\n",
      "paul magda\n",
      "paul rossotti\n",
      "paul schale\n",
      "paul tracey\n",
      "paul watt\n",
      "paul yang\n",
      "paul-louis hery\n",
      "paul\n",
      "paula ceccon\n",
      "paulo henrique vasconcellos\n",
      "paultimothymooney\n",
      "paulzh\n",
      "pavansubhash\n",
      "paveltroshenkov\n",
      "pavlin bakalov\n",
      "pavlos zitis\n",
      "pawan\n",
      "pazookii\n",
      "pbcquoc\n",
      "pcminers\n",
      "pedro lima\n",
      "pedro velez\n",
      "pedrofrantz\n",
      "pengm(mysaturdayself)\n",
      "pengzha\n",
      "people hr analytics repository\n",
      "peppermintshake\n",
      "perastikos\n",
      "perfectfit\n",
      "peter joseph arienza\n",
      "peter klauke\n",
      "peter ostroukhov\n",
      "peter wittek\n",
      "peter yang\n",
      "peter\n",
      "petit ours\n",
      "pguptha\n",
      "phalaris\n",
      "phatgamer\n",
      "philip corr\n",
      "philipharmuth\n",
      "philipjames11\n",
      "philipp schmidt\n",
      "phillipchin\n",
      "phillipliu\n",
      "phung van hoa\n",
      "picklechu\n",
      "pickou\n",
      "pierre sardin\n",
      "piks ral\n",
      "pipergragg\n",
      "pistachio_overlord\n",
      "piyushgoyal443\n",
      "piyushgupta\n",
      "pjmonti\n",
      "pkugoodspeed\n",
      "pkylas\n",
      "pmohun\n",
      "poetri heriningtyas\n",
      "polina vakhrusheva\n",
      "poorna\n",
      "poornima ravishankar\n",
      "poornimashanbhag\n",
      "poquilia\n",
      "portia brat\n",
      "pourmehrab\n",
      "pradeep.narayanan\n",
      "pradeep\n",
      "pradeepkumar\n",
      "pragya goyal\n",
      "prajit datta\n",
      "prajwal\n",
      "prakash tiwary\n",
      "prakashgawas\n",
      "prakhar srivastava\n",
      "prakriti iyengar\n",
      "pramit\n",
      "pramod kumar\n",
      "pramud\n",
      "pranav\n",
      "pranay aryal\n",
      "pranesh kumar palanisamy padmavathy\n",
      "pranjalgandhi\n",
      "pranstar\n",
      "prasanna nadimpalli\n",
      "prasanna steed\n",
      "prashant singh chauhan\n",
      "prashanth poojary\n",
      "prashanth sekar\n",
      "prashanthsreepuram\n",
      "prateek gupta\n",
      "prateek joshi\n",
      "prateik\n",
      "pratibha sharma\n",
      "pratibhasharma\n",
      "pratik agrawal\n",
      "pratik k\n",
      "pratik singh\n",
      "pratiksha salimath\n",
      "pratiush prasunn\n",
      "pravallika\n",
      "pravesh_ghorawat\n",
      "preeth kumar\n",
      "preetsinghkhalsa\n",
      "prem patrick\n",
      "premon\n",
      "premtewari\n",
      "prince grover\n",
      "priscilla\n",
      "priya_ds\n",
      "priyachowdary\n",
      "priyaljain\n",
      "priyank shah\n",
      "priyanka gagneja\n",
      "priyanka kolli\n",
      "priyanka kukunuru\n",
      "priyanshjain\n",
      "progress queens\n",
      "project jupyter\n",
      "proland\n",
      "promphongbandhuvara\n",
      "promptcloud\n",
      "pronto cycle share\n",
      "properati data\n",
      "prvns\n",
      "ps\n",
      "psparks\n",
      "pulkit jha\n",
      "pulkit khandelwal\n",
      "puneet\n",
      "puneeth019\n",
      "punxsutawney groundhog club\n",
      "purvank\n",
      "pushkar jain\n",
      "pushpendrapratap\n",
      "pylyfe\n",
      "pythonmython\n",
      "pytorch\n",
      "q82 capital\n",
      "qadeemkhan\n",
      "qishen ha\n",
      "qixiang109\n",
      "qizheng\n",
      "quan do\n",
      "quan nguyen\n",
      "quang nguyen\n",
      "quantscientist\n",
      "quentin garnier\n",
      "quentin mouton\n",
      "quinncarver\n",
      "quoc thang nguyen\n",
      "quoniammm\n",
      "quora\n",
      "r.venkatesh\n",
      "r1q3\n",
      "råå©kä©ä…\n",
      "raam\n",
      "raaz\n",
      "rachael tatman\n",
      "rachit sapra\n",
      "rachit srivastava\n",
      "radociechbubusierakowski\n",
      "radu stoicescu\n",
      "rafael novello\n",
      "rafal cycon (blaine)\n",
      "rafflesiakhan\n",
      "raghavi\n",
      "raghureddy\n",
      "rahi\n",
      "rahul bagga\n",
      "rahul batham\n",
      "rahul chaudhary\n",
      "rahul kumar\n",
      "rahul patil\n",
      "rahul sathyajit\n",
      "rahul\n",
      "rahul\n",
      "rahulbhambri\n",
      "rahulmayuranath\n",
      "rahulverma\n",
      "raihan kibria\n",
      "rainey\n",
      "raj sharma\n",
      "rajaganapathy\n",
      "rajanand ilangovan / à®‡à®°à®¾à®œà¯à®†à®©à®¨à¯à®¤à¯ à®‡à®³à®™à¯à®•à¯‹à®µà®©à¯\n",
      "rajasankar viswanathan\n",
      "rajat arora\n",
      "rajat sharma\n",
      "rajeev kumar\n",
      "rajeev\n",
      "rajesh kumar\n",
      "rajesh purwar\n",
      "rajeshm\n",
      "rajiv jeeva\n",
      "rajmund mokso\n",
      "rajorshi chaudhuri\n",
      "rajsekhar\n",
      "raju alluri\n",
      "rakannimer\n",
      "rakesh raushan\n",
      "rakeshsk\n",
      "rakuraku\n",
      "raliclo\n",
      "ram ramrakhya\n",
      "ramakrishnan srinivasan\n",
      "ramamet\n",
      "ramanujam allam\n",
      "ramesh\n",
      "ramiro\n",
      "ramirobentes\n",
      "ramnemani\n",
      "randy betancourt\n",
      "ranjan kumar\n",
      "ranjit kumar\n",
      "ranjithakorrapati\n",
      "ranjithkumar m\n",
      "raphaã«lmontaud\n",
      "raphael\n",
      "raquel aoki\n",
      "rashid ali\n",
      "rashid khan\n",
      "rashmi singh chauhan\n",
      "ratnachowdary\n",
      "raul\n",
      "ravali\n",
      "ravi rokhade\n",
      "ravi verma\n",
      "ravi\n",
      "ravi\n",
      "ravibhalala\n",
      "ravichandra malapati\n",
      "ravijain\n",
      "ravikiran\n",
      "ravin\n",
      "ravindra kompella\n",
      "ray\n",
      "rayen\n",
      "raymond delord\n",
      "raymondmak\n",
      "raysar\n",
      "razib mustafiz\n",
      "rbakes\n",
      "rcaer\n",
      "rdayala\n",
      "rdizzl3\n",
      "reason foundation\n",
      "rechards\n",
      "recruit institute of technology\n",
      "reddit\n",
      "redregressor\n",
      "ree_\n",
      "regis nunes vargas\n",
      "reinhard\n",
      "remi myers\n",
      "renata barros\n",
      "renzoramirez\n",
      "retailrocket\n",
      "reynald riviere\n",
      "reza agung pambudi\n",
      "reza javidi\n",
      "reza katebi\n",
      "reza\n",
      "rhammell\n",
      "rhishikesh nepal\n",
      "rhitamjeetsaharia\n",
      "rhostam\n",
      "ri_nandiya\n",
      "ricardo moya\n",
      "ricardo suarez\n",
      "ricardo zuccolo\n",
      "riccardo bongiovanni\n",
      "riccardo miccini\n",
      "riccardo nizzolo\n",
      "richa gautam\n",
      "richard churchill\n",
      "richard gu\n",
      "richard nagyfi\n",
      "richard\n",
      "richardbj\n",
      "richardnguyen\n",
      "rick chen\n",
      "rickvenadata\n",
      "ricky\n",
      "rickymak\n",
      "rickysaurav\n",
      "ridhi adyanthaya\n",
      "rini\n",
      "rio 2016\n",
      "rippon\n",
      "riri\n",
      "rishab gargeya\n",
      "rishabh kumar jha\n",
      "rishabh mishra\n",
      "rishi anand\n",
      "rishi sankineni\n",
      "rishibarath\n",
      "riti\n",
      "ritikajain\n",
      "ritusharma15bce1347\n",
      "river\n",
      "riyas\n",
      "rjcampa\n",
      "rjl2155\n",
      "rm\n",
      "rmsda2\n",
      "roam analytics\n",
      "rob harrand\n",
      "rob wishart\n",
      "robbert manders\n",
      "robbies\n",
      "robert hargraves\n",
      "robert nolan\n",
      "robert wexler\n",
      "roberto sousa\n",
      "roberto spadim\n",
      "roberto williams\n",
      "robin e. masliah\n",
      "robin nicole\n",
      "robin praet\n",
      "robinreni\n",
      "robotcator\n",
      "rocha\n",
      "rock pereira\n",
      "rockbottom\n",
      "rodrigo ancavil\n",
      "rodrigo domingos\n",
      "rodrigo ramele\n",
      "rodrigo salas\n",
      "rodrigo\n",
      "roel van den boom\n",
      "roger\n",
      "rogerio lopes\n",
      "rogiermonshouwer\n",
      "rohan kale\n",
      "rohan kayan\n",
      "rohan patel\n",
      "rohit sharma\n",
      "rohit singh\n",
      "rohithrpai\n",
      "rohitmathur\n",
      "rohk\n",
      "roi shikler\n",
      "rojour\n",
      "rolandas å imkus\n",
      "rolando p. aguirre\n",
      "romain loiseau\n",
      "romain loury\n",
      "romainvincent\n",
      "roman akhunov\n",
      "roman semenyk\n",
      "roman\n",
      "romitdhamija\n",
      "romy\n",
      "ron graf\n",
      "ron leplae\n",
      "ronald troncoso\n",
      "rongruosong\n",
      "ronnie\n",
      "ronny kimathi kaimenyi\n",
      "rony lussari\n",
      "roopalikaujalgi\n",
      "rosanaider\n",
      "rosegao\n",
      "roselyn kinuthia\n",
      "roshaankhan\n",
      "roshan\n",
      "rounak banik\n",
      "roundedup\n",
      "rovilayjnr\n",
      "roy garrard\n",
      "roy kiran\n",
      "roy klaasse bos\n",
      "roywwilson\n",
      "royxss\n",
      "rpygamer\n",
      "rshorty30\n",
      "rudd fawcett\n",
      "ruhshan\n",
      "rui romanini\n",
      "ruijie li\n",
      "ruishen lyu\n",
      "rumen manev\n",
      "rupali\n",
      "rush kirubi\n",
      "ruslan khalitov\n",
      "ruslan\n",
      "rutuj gavankar\n",
      "ryan bain\n",
      "ryan buck\n",
      "ryan chang\n",
      "ryan cushen\n",
      "ryan epp\n",
      "ryan harrison\n",
      "ryan li\n",
      "ryan sloot\n",
      "ryan\n",
      "ryanhuang\n",
      "ryanlott\n",
      "ryoogata\n",
      "ryujiseung\n",
      "ryvolum\n",
      "s sakarin\n",
      "s. zotos\n",
      "s.ayadi\n",
      "s.s. tarek\n",
      "s1m0n38\n",
      "sã©bastien aroulanda\n",
      "sã©bastien mathieu\n",
      "sã©bastien pouilly\n",
      "saagie_anthony\n",
      "sab30226\n",
      "sabber ahamed\n",
      "sabyasachi\n",
      "sachgupta\n",
      "sachiemon\n",
      "sachin kalsi\n",
      "sachin patel\n",
      "sachinumrao\n",
      "sadhanasingh\n",
      "safecast\n",
      "sagar sarkar\n",
      "sagarnil das\n",
      "sagarsen\n",
      "sahil gandhi\n",
      "sai c\n",
      "sai pranav\n",
      "saida antonyan\n",
      "saigonapps\n",
      "saikiran\n",
      "saikumar\n",
      "saimagesh r\n",
      "sainath\n",
      "saiprasad\n",
      "sajal\n",
      "sajid\n",
      "sakinadas\n",
      "sakthisiva\n",
      "sakti prasad\n",
      "salil gautam\n",
      "salim dohri\n",
      "salimchouai\n",
      "salmanpathan\n",
      "salomon\n",
      "salvadordali\n",
      "sam edelstein\n",
      "sam harris\n",
      "sam komo\n",
      "sam shideler\n",
      "sam stonesifer\n",
      "sam wong\n",
      "samael\n",
      "sambitsekhar\n",
      "samdeeplearning\n",
      "samdotson\n",
      "sameer mahajan\n",
      "sameer\n",
      "sami rahman\n",
      "samiraklaylat\n",
      "samitabet\n",
      "sammy klasfeld\n",
      "sammy123\n",
      "samrat\n",
      "samriddhi sinha\n",
      "samuel longwell\n",
      "samuel\n",
      "samyak jain\n",
      "sand\n",
      "sandeep kumar\n",
      "sandeep\n",
      "sandeepramesh\n",
      "sandeepyadav\n",
      "sandhya raghavan\n",
      "sandra cristina bustos galvis\n",
      "sandrarivera\n",
      "sandro marcelo peirano gozalvez\n",
      "sandsp\n",
      "sandy he\n",
      "sangamverma\n",
      "sangeetha sasikumar\n",
      "sanjay kushwah\n",
      "sanjaya wijeratne\n",
      "sanjeet kumar yadav\n",
      "sanjeev upreti\n",
      "sanket kumar\n",
      "santa meilisa\n",
      "santhoshmurali\n",
      "santiagovazquezgomez\n",
      "santosh boina\n",
      "sanyam\n",
      "saqib mujtaba\n",
      "sara g. mille\n",
      "sarah adsit\n",
      "sarah vch\n",
      "sarahz\n",
      "sarai rosenberg\n",
      "saravanan b\n",
      "saravanan jaichandar\n",
      "sariya\n",
      "sarra zammit chatti\n",
      "sarthak nautiyal\n",
      "sarubhava\n",
      "sasan jafarnejad\n",
      "sash\n",
      "sasi\n",
      "saswata das\n",
      "satadru5\n",
      "satavisha mitra\n",
      "satheeshperepu\n",
      "sathu79\n",
      "satish karivedha\n",
      "satish tiwari\n",
      "satya patel\n",
      "satyaki banik\n",
      "satyasai\n",
      "saudal-zakwani\n",
      "saurabh singh\n",
      "saurabh singh\n",
      "saurabh\n",
      "saurabhbhagvatula\n",
      "saurav ghosh\n",
      "saurav kumar\n",
      "saurav suman\n",
      "sauro grandi\n",
      "savannahlogan\n",
      "savasyä±ldä±rä±m\n",
      "savioz\n",
      "sawayaka\n",
      "saxinou\n",
      "sazidurrahman\n",
      "scielo\n",
      "scott a. miller\n",
      "scott cole\n",
      "scott\n",
      "scott\n",
      "scottfree analytics llc\n",
      "scotthendrickson\n",
      "sdorius\n",
      "seagoat\n",
      "seagullbird\n",
      "sean kelley\n",
      "sean marjason\n",
      "sean saito\n",
      "sean\n",
      "seankim\n",
      "seanlahman\n",
      "seattle public library\n",
      "sebastian mantey\n",
      "sebastian\n",
      "sebastianmarkow\n",
      "sebastianzanabria\n",
      "securities and exchange commission\n",
      "security3test\n",
      "securityteamvictim4\n",
      "seetharam indurti\n",
      "sekar m g\n",
      "selah\n",
      "selfish gene\n",
      "selvakumar\n",
      "semin\n",
      "semionkorchevskiy\n",
      "seong-jae chu\n",
      "septa\n",
      "serena chen\n",
      "sergei fironov\n",
      "sergey kosterin\n",
      "sergey kuznetsov\n",
      "sergey\n",
      "sergeya\n",
      "sergio gq\n",
      "sergiogonzalez\n",
      "sergioperez\n",
      "sergiy chumachenko\n",
      "serhiy subota\n",
      "serigne\n",
      "seunghyun jeon\n",
      "sevaspb\n",
      "seyvar\n",
      "sgde\n",
      "sgdysregulation\n",
      "sh lee\n",
      "shabeer\n",
      "shabu kc\n",
      "shahebaz\n",
      "shahumangkamleshbhai15bce1303\n",
      "shaik kamran\n",
      "shakaed subin\n",
      "shakti sharma\n",
      "shakti\n",
      "shalvarai16mcb0025\n",
      "shams ul arfeen\n",
      "shan\n",
      "shan\n",
      "shane smith\n",
      "shang pengxu\n",
      "shanger lin\n",
      "shanigershtein\n",
      "shankar\n",
      "shantamvijayputra\n",
      "shantanu acharya\n",
      "shantanu\n",
      "shanth\n",
      "sharadhi v\n",
      "sharan naribole\n",
      "sharddha\n",
      "sharon lin\n",
      "shashank\n",
      "shashank kumar\n",
      "shashank shekhar shukla\n",
      "shashank yadav\n",
      "shashanknainwal\n",
      "shatiel\n",
      "shaunakchadha\n",
      "shaurya munshi\n",
      "shaurya munshi\n",
      "shauryachawla\n",
      "shawn tian\n",
      "shayenne moura\n",
      "shazad udwadia\n",
      "sheik mohamed imran\n",
      "sheikh asif imran shouborno\n",
      "sheil naik\n",
      "sheng guo\n",
      "shengwei\n",
      "shenjiawei\n",
      "sherry_cs\n",
      "shihao\n",
      "shikhar\n",
      "shilpibhattacharyya\n",
      "shilpitha\n",
      "shimu\n",
      "shiny\n",
      "shirley\n",
      "shisancd\n",
      "shishir\n",
      "shitao zeng\n",
      "shiv gehlot\n",
      "shiv santosh\n",
      "shiva manhar\n",
      "shivajialaparthi\n",
      "shivam panchal\n",
      "shivam patel\n",
      "shivam patel\n",
      "shivamagrawal\n",
      "shivamgoel\n",
      "shivamnijhawan\n",
      "shivinderkapil\n",
      "shodiq\n",
      "shradhajoshi\n",
      "shreeya bhosale\n",
      "shreyams jain\n",
      "shreyassomashekara\n",
      "shrihans giriraj meena\n",
      "shruthishankar\n",
      "shruti bhargava\n",
      "shubham\n",
      "shubham barudwale\n",
      "shubham deshmukh\n",
      "shubham karande\n",
      "shubhamagarwal\n",
      "shubhammaurya\n",
      "shubhampawar\n",
      "shubhamthakur\n",
      "shubhangi\n",
      "shuchi\n",
      "shuhei fujiwara\n",
      "shunpoco\n",
      "shunya\n",
      "shuwenz\n",
      "shwet prakash\n",
      "shweta\n",
      "shwetabh123\n",
      "sibappa\n",
      "sichunlam\n",
      "sid shetty\n",
      "siddartha\n",
      "siddhanth vinay\n",
      "siddhartha sharan\n",
      "siddhartha\n",
      "sidg\n",
      "sidhant\n",
      "sidhantdeka\n",
      "siero\n",
      "siewkamonn\n",
      "siim m\n",
      "sijo vm\n",
      "silicon99\n",
      "silogram\n",
      "silvio santana\n",
      "simo\n",
      "simon asiimwe\n",
      "simon fraser university - summit\n",
      "simon gurcke\n",
      "simon plovyt\n",
      "simon tse\n",
      "simon\n",
      "simon\n",
      "simon\n",
      "simone seregni\n",
      "simonedalessio\n",
      "simonrazniewski\n",
      "sindhu rao\n",
      "sirish\n",
      "siva kumar\n",
      "siva swaminathan\n",
      "siyuanh\n",
      "sizzle\n",
      "skakki\n",
      "skalldihor\n",
      "skalskip\n",
      "skiddie\n",
      "skylord\n",
      "sleight82\n",
      "smart revolution\n",
      "smeschke\n",
      "smota\n",
      "sna\n",
      "snehaa ganesan\n",
      "snehanshusengupta\n",
      "snehareddy\n",
      "snow dog\n",
      "snow2011\n",
      "sofiya\n",
      "sohaib ali\n",
      "sohaibomar\n",
      "soham patel\n",
      "sohel\n",
      "sohier dane\n",
      "sohinibhattacharya\n",
      "somasundaram sankaranaraynan\n",
      "somesh\n",
      "sommenoob\n",
      "somnath roy\n",
      "son genacrys\n",
      "sonali chawla\n",
      "sonamsrivastava\n",
      "song wang\n",
      "soojung\n",
      "soorajms\n",
      "soroosh\n",
      "sotopia\n",
      "soufiane fhiyil\n",
      "soufianeorama\n",
      "souhaiel\n",
      "souhail toumdi\n",
      "soukaina\n",
      "souman roy\n",
      "soumitra agarwal\n",
      "sourabhmittal\n",
      "sourav nandi\n",
      "sourav roy\n",
      "sourav verma\n",
      "souravmaharana\n",
      "sovboc2018\n",
      "sowhit\n",
      "sowmiya nagarajan\n",
      "soywu\n",
      "spacex\n",
      "spencer buja\n",
      "spider pig\n",
      "sprabakar\n",
      "sreeram reddy kasarla (srk16113)\n",
      "sreyansh jain\n",
      "sri charan\n",
      "sri kamma\n",
      "sri kanth\n",
      "sri manjusha\n",
      "sri santhosh hari\n",
      "sridhar narasaiahgari\n",
      "sridhar\n",
      "srihari vasudevan\n",
      "sriharirao\n",
      "srilakshmi\n",
      "srilakshminandamuri\n",
      "srilbg\n",
      "srinath sridharan\n",
      "srinivasrao\n",
      "srinivinnakota\n",
      "srk\n",
      "ssvitian\n",
      "stack overflow\n",
      "stan\n",
      "stanford network analysis project\n",
      "stanford open policing project\n",
      "stanford university\n",
      "starbucks\n",
      "starconf\n",
      "starmine.ai\n",
      "start consortium\n",
      "startup policy lab\n",
      "stawary\n",
      "steal\n",
      "steevehuang\n",
      "stefanie04736\n",
      "stephan andre\n",
      "stephan wessels\n",
      "stephane bernadac\n",
      "stephanerappeneau\n",
      "stephanie le grange\n",
      "stephen cranney\n",
      "stephen huan\n",
      "stephen mcglennon\n",
      "stephen thompson\n",
      "stephen\n",
      "stephrouen\n",
      "steve ahn\n",
      "steve joly\n",
      "steve palley\n",
      "steven venezie\n",
      "steven\n",
      "steven\n",
      "stoddy\n",
      "stonepurple\n",
      "streichholz\n",
      "stuart chan\n",
      "stuart colianni\n",
      "stytch\n",
      "styven ponnusamy\n",
      "subarnarana\n",
      "subham das\n",
      "subhransu sekhar sahoo\n",
      "submarineering\n",
      "subra\n",
      "suchit gupta\n",
      "sudarshan\n",
      "sudeepta kkr\n",
      "sudheej sudhakaran\n",
      "sudheer sankar\n",
      "sudhir thuppale\n",
      "sudip das\n",
      "suhel\n",
      "sujan ghimire\n",
      "sujan\n",
      "sujay khandagale\n",
      "sujith\n",
      "sujithramkotagiri\n",
      "sulata patra\n",
      "sultan\n",
      "sumanth\n",
      "sumanthsrao\n",
      "sumendar\n",
      "sumit bhongale\n",
      "sumit kant\n",
      "sumit kothari\n",
      "sumit kumar\n",
      "sumit\n",
      "sundai\n",
      "suneetsawant\n",
      "sungpil han\n",
      "sunil kumar sv\n",
      "sunil neurgaonkar\n",
      "sunil sethi\n",
      "sunilp\n",
      "sunmarkil\n",
      "superdave\n",
      "suprabhat tiwari\n",
      "suprabhat tiwari\n",
      "supriyadubey\n",
      "surabhi\n",
      "suraj\n",
      "surajpathak\n",
      "suresh bhusare\n",
      "sureshsrinivas\n",
      "suryasista\n",
      "susan noboa\n",
      "susan wang\n",
      "susanna\n",
      "sushant\n",
      "sushant jha\n",
      "susmitha\n",
      "sustainable development solutions network\n",
      "svidon\n",
      "swami krishnamurthy\n",
      "swapnil\n",
      "swapnilkale\n",
      "swaroopvenigalla\n",
      "swathi priyadarsini\n",
      "swati\n",
      "swatish swaminathan\n",
      "swayam mittal\n",
      "sweety\n",
      "swetaagrawal\n",
      "sylas\n",
      "sylvia mittal\n",
      "szery\n",
      "szkript\n",
      "szrlee\n",
      "t byrnes\n",
      "t mcketterick\n",
      "t michaels\n",
      "t peng\n",
      "t. scharf\n",
      "t\n",
      "t7 - pokemon challenge\n",
      "tadashinagao\n",
      "taffey lewis\n",
      "taha zerrouki\n",
      "tahsin mayeesha\n",
      "taichiwang\n",
      "taimur khan\n",
      "taiwoo.adetiloye\n",
      "taka\n",
      "takuoko\n",
      "tamber\n",
      "tamil dhoni\n",
      "tamilselvan sudalai\n",
      "tammy rotem\n",
      "tan kinh bui\n",
      "tang yiming\n",
      "tanishk parihar\n",
      "tankeestka\n",
      "tanya makkar\n",
      "tãº anh hoã ng\n",
      "tara rutkowski\n",
      "taraprasanna saha babu\n",
      "taras\n",
      "tarek benkhelif\n",
      "tarun khanna\n",
      "tarun kumar\n",
      "tdougherty223\n",
      "team ai\n",
      "team puppygogo\n",
      "techmn\n",
      "teck44\"><\n",
      "tecperson\n",
      "tehreemansari\n",
      "tejasvagarwal\n",
      "temilade adefioye aina\n",
      "teng lei\n",
      "teodosiy\n",
      "teraflops\n",
      "terenceliu\n",
      "terminal security agency\n",
      "test \">\n",
      "test\">\n",
      "test\n",
      "test\n",
      "test2\"><\n",
      "testaccountkagglee\n",
      "testbugmasooddd\n",
      "tester\n",
      "testimon @ ntnu\n",
      "testingshi\n",
      "tetianamyronivska\n",
      "tetyanaloskutova\n",
      "tetyanayatsenko\n",
      "tevec systems\n",
      "thais rodrigues neubauer\n",
      "thaisalmeida\n",
      "thanakom sangnetra\n",
      "thanuj\n",
      "thao\n",
      "tharini padmagirisan\n",
      "the bear\n",
      "the bgu cyber security research center\n",
      "the fellow\n",
      "the flying munkey\n",
      "the guardian\n",
      "the huffington post\n",
      "the marshall project\n",
      "the metropolitan museum of art\n",
      "the movie database (tmdb)\n",
      "the museum of modern art\n",
      "the nobel foundation\n",
      "the smithsonian institution\n",
      "the wall street journal\n",
      "the washington post\n",
      "the1owl\n",
      "theo ioa\n",
      "thescientistbr\n",
      "thetraderrr\n",
      "theudas\n",
      "thiago balbo\n",
      "thiago oliveira\n",
      "thinh uy quang\n",
      "thiru maalavan\n",
      "thobani hlophe\n",
      "thomas de jonghe\n",
      "thomas nelson\n",
      "thomas pappas\n",
      "thomas ranvier\n",
      "thomas wade culbertson\n",
      "thomas\n",
      "thomasluby\n",
      "thomasvoreyer\n",
      "thorodinovich\n",
      "thought vector\n",
      "throne1032\n",
      "thuanhieu\n",
      "thulani tembo\n",
      "tiago v. melo\n",
      "tiago vinhoza\n",
      "tiantian\n",
      "tianyi wang\n",
      "tigran davtyan\n",
      "tilak\n",
      "tim hradil\n",
      "tim kartawijaya\n",
      "tim pearce\n",
      "time magazine\n",
      "timo bozsolik\n",
      "timothy leung\n",
      "timru\n",
      "timsyang\n",
      "ting zhou\n",
      "tiredgeek\n",
      "tirthgajjar\n",
      "tito maraca\n",
      "tivoli2\n",
      "tiziano teso\n",
      "tjb5670\n",
      "tk\n",
      "tmthyjames\n",
      "tobeystrauch\n",
      "toby jolly\n",
      "tolu toluhi\n",
      "tom bombadil\n",
      "tom hill\n",
      "tomã¡s accini\n",
      "tomã¡s bustamante\n",
      "tomasz bartczak\n",
      "tomato\n",
      "tomer eldor\n",
      "tomi-andre\n",
      "tommert\n",
      "tommy pompo\n",
      "tomneeld\n",
      "tomo\n",
      "tony pino\n",
      "tony xie\n",
      "tonychan\n",
      "tophatsteve\n",
      "torr\n",
      "toshnoue\n",
      "tp\n",
      "traceyvanp\n",
      "tranndo\n",
      "transparency international\n",
      "trent baur\n",
      "trey kollmer\n",
      "triplefireyan\n",
      "trond magne lamprecht haaland\n",
      "trumedicines\n",
      "truong an\n",
      "truth lover\n",
      "tsimins\n",
      "tuhin saha\n",
      "tung thanh le\n",
      "tusha kutusha\n",
      "tushar dhyani\n",
      "tushar gupta\n",
      "tushar mahendra patil\n",
      "tushar makkar\n",
      "tushar yadav\n",
      "tvscitechtalk\n",
      "twistfateboy\n",
      "ty\n",
      "tyjzhong\n",
      "tylerfuller\n",
      "tylertuschhoff\n",
      "u_kag\n",
      "u.s. national archives and records administration\n",
      "uc san diego\n",
      "uci machine learning\n",
      "udacity\n",
      "udas\n",
      "uday\n",
      "uddeshya singh\n",
      "udeme udofia\n",
      "ugocupcic\n",
      "ujjwal kr gupta\n",
      "ujjwal\n",
      "ujjwal\n",
      "ultra-jack\n",
      "umakant\n",
      "umang dhiman\n",
      "umberto\n",
      "umut\n",
      "union of concerned scientists\n",
      "united nations development program\n",
      "united nations\n",
      "united states air force\n",
      "united states department of agriculture\n",
      "united states drought monitor\n",
      "university of connecticut\n",
      "university of copenhagen\n",
      "university of michigan\n",
      "university of north texas\n",
      "university of pittsburgh\n",
      "university of virginia\n",
      "uranio255\n",
      "urvangpatel\n",
      "us bureau of labor statistics\n",
      "us census bureau\n",
      "us customs and border protection\n",
      "us department of agriculture\n",
      "us department of energy\n",
      "us department of health and human services\n",
      "us environmental protection agency\n",
      "us geological survey\n",
      "us patent and trademark office\n",
      "us senate\n",
      "usb\n",
      "usfundamentals\n",
      "ushchent\n",
      "utagh\n",
      "utkarsh aggarwal\n",
      "utmhikari\n",
      "uttahjazz\n",
      "v.a. freeman\n",
      "v81msk\n",
      "vadim shmelev\n",
      "vahe andonians\n",
      "vahik95\n",
      "vaibhav_varshney\n",
      "vaibhavgeek\n",
      "vaibhavi singh\n",
      "vaibs\n",
      "valentina c\n",
      "valeria barã³n\n",
      "valeriesalazar\n",
      "valerio luciani\n",
      "valerio luzzi\n",
      "valeriovaccaro\n",
      "vanamsen\n",
      "vanessa\n",
      "vardan\n",
      "vardial\n",
      "varun belliappa\n",
      "varun bhargava\n",
      "varun kashyap.k.s.\n",
      "varunagarwal\n",
      "vasilisnikolaou\n",
      "vasyavologdin\n",
      "vedant ruparelia\n",
      "vedapragnareddy\n",
      "vein\n",
      "venkat ramakrishnan\n",
      "venkataduvvuri\n",
      "venkatasivaabhishek\n",
      "venkatesh madhava\n",
      "venkateshgopal\n",
      "vera lei\n",
      "vered shwartz\n",
      "verginer\n",
      "veysel kocaman\n",
      "vicc alexander\n",
      "vicky1\n",
      "vickylee\n",
      "victor dos santos\n",
      "victor genin\n",
      "victor hugo\n",
      "victor paslay\n",
      "victor\n",
      "victor7246\n",
      "victorelie\n",
      "victorgrobberio\n",
      "vidhu shekhar tripathi\n",
      "viditjain\n",
      "vignesh varadarajan\n",
      "vihan\n",
      "vijay dhameliya\n",
      "vijay\n",
      "vijaykumar ummadisetty\n",
      "vijayn\n",
      "vikas kamath m\n",
      "vikas pandey\n",
      "vikassangwan\n",
      "vikassrivastava\n",
      "vikrant yadav\n",
      "vikrantthakur\n",
      "viktor malyi\n",
      "viktoriasuponenko\n",
      "vinay shanbhag\n",
      "vinayagam.d.ganesh\n",
      "vinceallenvince\n",
      "vincent assoun\n",
      "vincentla\n",
      "vineetkothari\n",
      "vinodkumar\n",
      "viraj bhambri\n",
      "virginie do\n",
      "virgodata\n",
      "viru\n",
      "vish chekuri\n",
      "vish vishal\n",
      "vishakhhegde\n",
      "vishal modagekar\n",
      "vishnuraghavan\n",
      "vishwas shrikhande\n",
      "vishwesh s\n",
      "viswateja gajulavarthy\n",
      "vitalii peretiatko\n",
      "vitaly burachyonok\n",
      "vitaly korchagin\n",
      "vitor r. f.\n",
      "vivek chutke\n",
      "vivek kumar\n",
      "vivek pandey\n",
      "vivek singh\n",
      "vivekgopinathlal\n",
      "vivekmangipudi\n",
      "vivian l\n",
      "vivin abraham\n",
      "vl\n",
      "vlad golubev\n",
      "vlad.pambucol\n",
      "vladb\n",
      "vladifidchuk\n",
      "vladimir alencar\n",
      "vladimir belyaev\n",
      "vladimir gmyzin\n",
      "vladimir kiselev\n",
      "vladimir kuznetsov\n",
      "vladislav zavadskyy\n",
      "volodymyr sadovyy\n",
      "vonage garage\n",
      "voronwe2007\n",
      "voteview\n",
      "vrushali patel\n",
      "vsmolyakov\n",
      "vyas\n",
      "w. yifan\n",
      "wal8800\n",
      "walla2ae\n",
      "wally atkins\n",
      "walter_sam\n",
      "wanglaiqi\n",
      "wangqiucheng\n",
      "wangtianju\n",
      "wanqiwang\n",
      "waqas malik\n",
      "ward bradt\n",
      "warren elder\n",
      "warrentnt\n",
      "washim ahmed\n",
      "washington university\n",
      "watts\n",
      "wayne haubner\n",
      "waynec\n",
      "wazeed\n",
      "web ir\n",
      "webdev\n",
      "weeliangng\n",
      "wei chun chang\n",
      "wei ouyang\n",
      "weibo\n",
      "weisinhong\n",
      "wellll\n",
      "wenbocao\n",
      "wenchen\n",
      "wenchi\n",
      "wendy kan\n",
      "wenlong\n",
      "wesduckett\n",
      "wh0801\n",
      "whosonit 1\n",
      "wî”\n",
      "wijdan aljumiah\n",
      "wil o c ward\n",
      "wildgrok\n",
      "wilian osaku\n",
      "will gao\n",
      "will hunt\n",
      "willamgreen\n",
      "william cao\n",
      "william cukierski\n",
      "william hyde\n",
      "william straus\n",
      "william walter\n",
      "williamnowak\n",
      "willie liao\n",
      "willinghorse\n",
      "winastwan gora\n",
      "windson\n",
      "windy torgerud\n",
      "winnie\n",
      "wnyc\n",
      "wojciechwå‚odarczyk\n",
      "wol4ara_vio\n",
      "work1810\n",
      "world bank\n",
      "world bank\n",
      "world economic forum\n",
      "worldvaluesurvey\n",
      "woutervh88\n",
      "wrackshipparty\n",
      "wrong\n",
      "wu wuhui\n",
      "wuzzuf\n",
      "xachi\n",
      "xai nano\n",
      "xaliap\n",
      "xavierbays\n",
      "xaviermartinezbartra\n",
      "xavya\n",
      "xgan\n",
      "xiang zhang\n",
      "xiaocongsonia\n",
      "xiaojingli\n",
      "xiaoxiao wu\n",
      "xiaozhou yang\n",
      "ximing\n",
      "xin\n",
      "xingzhangren\n",
      "xiong songsong\n",
      "xjtushilei\n",
      "xss\n",
      "xtyscut\n",
      "xuetao shi\n",
      "xuleiyang\n",
      "xuseniayu\n",
      "xuy2\n",
      "xwang\n",
      "xx\n",
      "yabir canario\n",
      "yachuncheng\n",
      "yagana sheriff-hussaini\n",
      "yagnesh badiyani\n",
      "yahyacivelek\n",
      "yalitsai\n",
      "yamuuu\n",
      "yan ramos da silva\n",
      "yan zhu\n",
      "yang lin\n",
      "yang yunfan\n",
      "yanir\n",
      "yannis pappas\n",
      "yannmallegol\n",
      "yannsar\n",
      "yao hu\n",
      "yao lu\n",
      "yaohsiao\n",
      "yaosenyou\n",
      "yaoxiang li\n",
      "yap wei yih\n",
      "yapi donatien achou\n",
      "yarden sharon\n",
      "yasar kocal\n",
      "yaser ahmed\n",
      "yash pradhan\n",
      "yash\n",
      "yashjain\n",
      "yashna shravani\n",
      "yashu\n",
      "yasmeenw\n",
      "yassine marzougui\n",
      "yassine morakkam\n",
      "yassineameur\n",
      "yasuhiro_121\n",
      "yaswanth gosula\n",
      "yatishbn\n",
      "yazi\n",
      "ye huangjie\n",
      "yeomyungro\n",
      "yeongchan\n",
      "yeongseok\n",
      "yevgeniya migranova\n",
      "yexiaofeng\n",
      "yi cao\n",
      "yi jingyuan-é™è¿œ\n",
      "yi su\n",
      "yichenâ€œeddieâ€shen\n",
      "yifan xie\n",
      "yijiezhuang\n",
      "yin zhang\n",
      "yinghan\n",
      "yingzhu\n",
      "yiqizhang\n",
      "yiweihuang\n",
      "yixin sun\n",
      "ykamikawa\n",
      "ykatayama\n",
      "yl\n",
      "ylan kazi\n",
      "yliu\n",
      "ymlai87416\n",
      "ymtoo\n",
      "yoann pradat\n",
      "yochanan scharf\n",
      "yogesh gupta\n",
      "yogeshsingh\n",
      "yogi\n",
      "yoka\n",
      "yonatan vaizman\n",
      "yongho choi\n",
      "young and dumb\n",
      "yourkingdomcome\n",
      "ysaz\n",
      "yu sheng lu\n",
      "yu_chih\n",
      "yuanjie li\n",
      "yuansaijie0604\n",
      "yueming\n",
      "yuesu\n",
      "yuhaowang\n",
      "yuhuaxiong\n",
      "yujack\n",
      "yukarin\n",
      "yulia g\n",
      "yuncheng li\n",
      "yunguan fu\n",
      "yuqing01\n",
      "yura shakhnazaryan\n",
      "yuranan\n",
      "yurii biurher\n",
      "yury kashnitsky\n",
      "yusuf\n",
      "yuwenjin\n",
      "yuzie yu\n",
      "yvon\n",
      "yvonhk\n",
      "zach barillaro\n",
      "zach\n",
      "zack\n",
      "zackcode\n",
      "zackentonasi\n",
      "zackthoutt\n",
      "zagarsurensukhbaatar\n",
      "zain baig\n",
      "zain rizvi\n",
      "zakar h.\n",
      "zalando research\n",
      "zan huang\n",
      "zaruhi avagyan\n",
      "zaur begiev\n",
      "zedd\n",
      "zeeshan-ul-hassan usmani\n",
      "zelhassn\n",
      "zeta\n",
      "zhai kun\n",
      "zhangchengwei\n",
      "zhanglanqing\n",
      "zhaofengli\n",
      "zhaojingnan\n",
      "zhe lin\n",
      "zhecj\n",
      "zhengyi zhu\n",
      "zhenyubo\n",
      "zheye yuan\n",
      "zhijin\n",
      "zhiliang\n",
      "zhixing\n",
      "zhousheng\n",
      "zielak\n",
      "zillow\n",
      "zinuo jia\n",
      "ziyuanzhong\n",
      "zjf\n",
      "zluckyh\n",
      "zoerenwick\n",
      "zoey\n",
      "ztyh0121\n",
      "zuhaibali\n",
      "zuoyu miao\n",
      "zurda\n",
      "zuswi\n",
      "zwidofhelangani gabara\n",
      "zyaj\n",
      "context\n",
      "this is the dataset used in the second chapter of aurélien géron's recent book 'hands-on machine learning with scikit-learn and tensorflow'. it serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n",
      "the data contains information from the 1990 california census. so although it may not help you with predicting current housing prices like the zillow zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n",
      "content\n",
      "the data pertains to the houses found in a given california district and some summary stats about them based on the 1990 census data. be warned the data aren't cleaned so there are some preprocessing steps required! the columns are as follows, their names are pretty self explanitory:\n",
      "longitude\n",
      "latitude\n",
      "housing_median_age\n",
      "total_rooms\n",
      "total_bedrooms\n",
      "population\n",
      "households\n",
      "median_income\n",
      "median_house_value\n",
      "ocean_proximity\n",
      "acknowledgements\n",
      "this data was initially featured in the following paper: pace, r. kelley, and ronald barry. \"sparse spatial autoregressions.\" statistics & probability letters 33.3 (1997): 291-297.\n",
      "and i encountered it in 'hands-on machine learning with scikit-learn and tensorflow' by aurélien géron. aurélien géron wrote: this dataset is a modified version of the california housing dataset available from: luís torgo's page (university of porto)\n",
      "inspiration\n",
      "see my kernel on machine learning basics in r using this dataset, or venture over to the following link for a python based introductory tutorial: https://github.com/ageron/handson-ml/tree/master/datasets/housing\n",
      "context\n",
      "what feature affects on housing price(specifically apartment price). figuring out what patterns hidden in real estate market (apartment).\n",
      "content\n",
      "for time period 10years traded apartment detailed data using api provided from data.go.kr\n",
      "this data dealt only one specific district.\n",
      "acknowledgements\n",
      "inspiration\n",
      "what patterns are hidden in data? if you can not find patterns or generalize, what feature should be added/consider?\n",
      "context\n",
      "health, safety, and environment (hse) is a dicspline centered on implementing practices for environmental protection and safety in a workplace. energy companies place a strong emphasis on hse when conducting day to day operations, whether it is on the field or in an office. a major challenge with hse, however, is monitoring and managing hse incidents across an enterprise. the common practice for incident management is analyzing detailed incident reports. this can be cumbersome and time-consuming, because in most cases, these reports contain unstructured text. to increase efficiency, companies are seeking technologies that allow them to derive valuable insights from unstructured hse data efficiently.\n",
      "content\n",
      "this dataset contains abstracts of the accidents and injuries of construction workers from 2015-2017. there is some structured data around the unstructured text abstracts, such as degree of injury, body part(s) affected, and construction end use.\n",
      "acknowledgements\n",
      "this is osha data which is publicly available.\n",
      "inspiration\n",
      "what are the most buildings/structures to build? what trends do we see in injuries in terms of time of day, time of year, etc.? what is the reason injuries are occurring? where do we need more training and safety measures in place?\n",
      "context\n",
      "the usda plant database extraction from the natural resources conservation service.\n",
      "content\n",
      "it contains a wide variety of varieties in raw format.\n",
      "inspiration\n",
      "there is currently no usda plant information available via api. using this data set i'm hoping that i can extract needed information for improved plant growth in controlled environments.\n",
      "data description\n",
      "dataset is from one of the leading travel site containing hotel reviews provided by customers.\n",
      "variable | description\n",
      "--- | ---\n",
      "user_id | unique id of the customer\n",
      "description | description of the review posted\n",
      "browser_used | browser used to post the review\n",
      "device_used | device used to post the review\n",
      "is_response | target variable\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "pretty simple and not huge dataset of png images from motherboard, lead legs of chipset. created for training ml/dp models on detection and classification of defected parts. contain 30 images with defect, and 323 of standard one.\n",
      "content\n",
      "collected at the august of 2017, by me. for it used photo microscope and 2 boards, one with defect parts, another is standard. as you can see, here is just 30 photos with problem parts. if someone need more, i can make and crop them by a short time, just msg me here or at the my email.\n",
      "acknowledgements\n",
      "this is totally my dataset, used it for model training on detection and classification of lead parts at the motherboard.\n",
      "inspiration\n",
      "this dataset can be useful for people, who trying to solve same problems with me, and cant create their own dataset. soon, i will try to upload another dataset with solder parts! not for commercial use, just for science~\n",
      "context\n",
      "this is a data dump of the statsguru section of cricinfo's searchable cricket statistics database. i have only uploaded test match, and odi data for now, and will upload t20i data soon.\n",
      "content\n",
      "i have pulled details data on the following disciplines:\n",
      "team stats\n",
      "batting stats\n",
      "bowling stats\n",
      "fielding stats\n",
      "all-rounders stats\n",
      "batting partnerships\n",
      "acknowledgements\n",
      "all data pulled can be found on the cricinfo website: http://stats.espncricinfo.com/ci/engine/stats/index.html\n",
      "inspiration\n",
      "for anyone who enjoys cricket, and analyzing cricket stats.\n",
      "context\n",
      "this data includes daily open, high, low, close values for all crypto currencies (since april 2013) as well as daily lunar geocentric data (distance, declination, brightness, illumination %, and constellation). please note that i have consolidated this data from the below two sources (originally submitted by mcrescenzo and jvent) after my mother asked me if there's a correlation between the lunar status and the financial markets.\n",
      "content\n",
      "this data includes daily open, high, low, close values for all crypto currencies (since april 2013 until january 2018) as well as daily lunar geocentric data (distance, declination, brightness, illumination %, and constellation) for same timeframe.\n",
      "acknowledgements\n",
      "lunar daily distance and declination : 1800-2020. click for original data submitted by mcrescenzo every cryptocurrency daily market price. click for original data submitted by jvent\n",
      "inspiration\n",
      "is there any correlation between cryptocurrencies and lunar phases?\n",
      "can we predict cryptocurrency movements by the lunar phases?\n",
      "context\n",
      "the headlines, with links in most cases, were harvested for a quick viewing of the kinds of election talk the online news media were carrying as the campaign picked up steam ahead of nepal's first federal and provincial elections less than a month away.\n",
      "content\n",
      "the headlines, with links preceding them, were scraped from 24 news websites of nepal on 11/14/2017. they comprise 510 lines of nepali texts in utf-8, after removing the links in english.\n",
      "acknowledgements\n",
      "the dataset is part of a personal hobby of the author to take stock of the election talk going on in nepali media at the moment. this was possible thanks to the python libraries, requests and bs4, available with the jupyter notebooks on anaconda.\n",
      "inspiration\n",
      "media headlines tend to be a succinct gist of the election talk going on in the campaign period, with their potential to throw light on the kind of rhetoric and quality of arguments used for election gains. what can a text analysis of the headlines show?\n",
      "### context\n",
      "call test measurements for mobile network monitoring and optimization.\n",
      "### content\n",
      "the measurements were performed with smartphones and collected on proprietary databases.\n",
      "### inspiration\n",
      "the scope is to predict a level of \"quality of experience\" (e.g. mean opinion score - mos) with other measured kpis (classification task). at the moment the best accuracy is about 60% obtained with a random forest classifier (implemented with sci-kit learn on python)\n",
      "context\n",
      "what is the tsca chemical substances control inventory?\n",
      "section 8 (b) of the toxic substances control act (tsca) requires epa to compile, keep current and publish a list of each chemical substance that is manufactured or processed, including imports, in the united states for uses under tsca. also called the “tsca inventory” or simply “the inventory,” it plays a central role in the regulation of most industrial chemicals in the united states.\n",
      "the initial reporting period by manufacturers, processors and importers was january to may of 1978 for chemical substances that had been in commerce since january of 1975. the inventory was initially published in 1979, and a second version, containing about 62,000 chemical substances, was published in 1982. the tsca inventory has continued to grow since then, and now lists about 85,000 chemicals.\n",
      "epa’s compilation of the public tsca inventory information is updated twice a year to include new and corrected tsca inventory chemical listings, and it contains none of the chemical identities claimed as confidential. thus it is not as complete nor current as the information contained in epa's tsca master inventory file, which includes the chemical identities claimed as confidential and is updated continuously as new and corrected information is received by epa. consequently, for the purposes of tsca compliance, the tsca master inventory file maintained by epa's office of pollution prevention and toxics is the only complete and accurate source that can provide authoritative and conclusive information about which chemical substances are currently included in the tsca inventory.\n",
      "content\n",
      "tscainv_062017.csv\n",
      "id: record id number\n",
      "rn: chemical abstracts service (cas) registry number\n",
      "casregno: cas registry number without \"-\" [dashes]\n",
      "in: index name (chemical name)\n",
      "df: chemical substance definition\n",
      "fl: epa tsca regulatory flag\n",
      "uv: uvcb flag\n",
      "cs: commercial status designation\n",
      "pmnacc_062017.csv\n",
      "id: record number id\n",
      "pmnno: pmn number/form number\n",
      "accno: epa accession number\n",
      "gn: generic name\n",
      "fl: epa tsca regulatory flag\n",
      "cs: commercial status designation\n",
      "acknowledgements\n",
      "the epa updates this registry is twice per year. the version here was downloaded on oct 18th, 2017. check the epa website for updated versions: https://www.epa.gov/tsca-inventory/how-access-tsca-inventory.\n",
      "inspiration\n",
      "there are lots of air quality and pollution datasets that you can use in conjunction with this tsca registry to learn more about contaminants and chemicals in general.\n",
      "context\n",
      "dataset is provided to take a look and train model on historical crypto-currency data including extra financial indicators.\n",
      "content\n",
      "dataset contains 200 days of 5 minute ticker dataset for crypto currency btc (bitcoin) and vrc (vericoin) together w around 70 financial indicators. additionally each row contains label, which describes buy, sell and hold action.\n",
      "small sample of labels:\n",
      "acknowledgements\n",
      "dataset is generated with open source project mosquito-blueprint module.\n",
      "inspiration\n",
      "main point is give public (including me) the possibility to answer on following questions:\n",
      "how well can we predict labeled price of given crypto-currency pair?\n",
      "does the dataset contain enough information to give good enough results?\n",
      "context\n",
      "about whether the horse will be in the next 5 years, whether the occurrence of diabetes data set\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "blue plaques are a well-known and very popular permanent historical marker scheme administered in the united kingdom that has since spread to many other countries in europe and the world. according to wikipedia:\n",
      "a blue plaque is a permanent sign installed in a public place in the united kingdom and elsewhere to commemorate a link between that location and a famous person or event, serving as a historical marker. the brainchild of british politician william ewart in 1863, it is the oldest such scheme in the world.\n",
      "the world's first blue plaques were erected in london in the 19th century to mark the homes and workplaces of famous people. this scheme continues to the present day...the term \"blue plaque\" may be used narrowly to refer to the official english heritage scheme, but is often used informally to encompass all similar schemes.\"\n",
      "here's what a model blue plaque looks like:\n",
      "(image via wikimedia commons)\n",
      "this dataset contains data about most of the blue plaques installed in europe as of june 2017, as reported by open plaques.\n",
      "content\n",
      "this dataset contains information on the location of each plaque, who the subject is, and metadata about the person or organization being recognized.\n",
      "acknowledgements\n",
      "this dataset is republished as-is from the original on open plauqes.\n",
      "inspiration\n",
      "where are the blue plaques located?\n",
      "what kinds of people and places get awarded a plaque?\n",
      "what is the geospatial distribution of these plaques throughout the uk? worldwide?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this data set contains data regarding the allocation of funding from the department of developmental services to developmentally-disabled individuals in california in 2014.\n",
      "content\n",
      "the variables included are:\n",
      "id [int]\n",
      "age cohort (age group) [factor]\n",
      "age [int]\n",
      "gender [factor]\n",
      "expenditures [int]\n",
      "ethnicity [factor]\n",
      "this data set is well suited for exploring the effects of simpson's paradox and confounding variables.\n",
      "acknowledgements\n",
      "the data was originally retrieved from the california department of developmental services (http://www.dds.ca.gov) by stanley taylor and amy mickel from california state university, sacremento. the names associated with each record have been removed to protect anonymity.\n",
      "taylor and mickel explored simpson's paradox using this data set after a discrimination lawsuit was filed against the california dds. the lawsuit claimed that white non-hispanics were receiving more funding than hispanics. to learn more about the analysis and findings of taylor and mickel, read the paper they published together by following this link: www.amstat.org/publications/jse/v22n1/mickel.pdf\n",
      "inspiration\n",
      "is there any basis to the claim of discrimination? what are the confounding variables? what are other ways to organize this data to gain an alternate perspective?\n",
      "context\n",
      "this data collected via survey. with this data, user or items will be cluster. based on cluster new minimalist applicatin will be designed.\n",
      "content\n",
      "there are descripted data like gender, phone type etc. main reason for that data is it possible to make cluster based on descripted data.\n",
      "inspiration\n",
      "is it possible to make cluster from this data ? clustering items together. i am not expecting seperate diffrent cluster. it should be overlap some feauters.\n",
      "this dataset does not have a description yet.\n",
      "context:\n",
      "it can difficult to identify the language that a tweet is written in. in addition to being very short, they often include code-switching, where the user uses two or more languages together, or names borrowed from a different language.\n",
      "this dataset contains tweets from a variety of languages, tagged for whether they are in english or not, whether they contain code-switching, whether they includes names from a different language and whether they were generated automatically.\n",
      "content:\n",
      "this dataset contains 10,502 tweets, randomly sampled from all publicly available geotagged twitter messages, annotated for being in english, non-english, or having code switching, language ambiguity or having been automatically generated. it includes messages sent from 130 different countries.\n",
      "the file all_annotated.tsv contains the dataset of 10,502 tweets used in the paper. text is encoded as utf-8.\n",
      "the column headings (also given in the .tsv file) are: tweet id, iso country code, tweet date, tweet text, definitely english, ambiguous, definitely not english, code-switched, ambiguous due to named entities, and automatically generated tweets.\n",
      "all annotations are binary; the definitely english, ambiguous, and definitely not english annotations are mutually exclusive.\n",
      "acknowledgements:\n",
      "this dataset was collected by su lin blodgett, johnny tian-zheng wei and brendan o'connor. it is redistributed here under the creative commons attribution 4.0 international license. if you use this data in your work, please cite the following paper:\n",
      "blodgett, su lin, johnny wei, and brendan o'connor. \"a dataset and classifier for recognizing social media english.\" proceedings of the 3rd workshop on noisy user-generated text. 2017.\n",
      "you can find more information on this dataset and related work on this website.\n",
      "inspiration:\n",
      "can you use this dataset to build a classifier that identifies whether a tweet is in english or not?\n",
      "can you use this dataset to build a language identifier? (you can check out the authors’ language identifier here.)\n",
      "this dataset does not have a description yet.\n",
      "content\n",
      "this is a dataset from the uci datasets repository. this dataset contains the final scores of students at the end of a math programs with several features that might or might not impact the future outcome of these students.\n",
      "citation:\n",
      "please include this citation if you plan to use this database:\n",
      "p. cortez and a. silva. using data mining to predict secondary school student performance. in a. brito and j. teixeira eds., proceedings of 5th future business technology conference (fubutec 2008) pp. 5-12, porto, portugal, april, 2008, eurosis, isbn 978-9077381-39-7. [web link]\n",
      "attribute information:\n",
      "attributes for both student-mat.csv (math course) and student-por.csv (portuguese language course) datasets:\n",
      "1 school - student's school (binary: 'gp' - gabriel pereira or 'ms' - mousinho da silveira)\n",
      "2 sex - student's sex (binary: 'f' - female or 'm' - male)\n",
      "3 age - student's age (numeric: from 15 to 22)\n",
      "4 address - student's home address type (binary: 'u' - urban or 'r' - rural)\n",
      "5 famsize - family size (binary: 'le3' - less or equal to 3 or 'gt3' - greater than 3)\n",
      "6 pstatus - parent's cohabitation status (binary: 't' - living together or 'a' - apart)\n",
      "7 medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n",
      "8 fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n",
      "9 mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
      "10 fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
      "11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
      "12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n",
      "13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
      "14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
      "15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
      "16 schoolsup - extra educational support (binary: yes or no)\n",
      "17 famsup - family educational support (binary: yes or no)\n",
      "18 paid - extra paid classes within the course subject (math or portuguese) (binary: yes or no)\n",
      "19 activities - extra-curricular activities (binary: yes or no)\n",
      "20 nursery - attended nursery school (binary: yes or no)\n",
      "21 higher - wants to take higher education (binary: yes or no)\n",
      "22 internet - internet access at home (binary: yes or no)\n",
      "23 romantic - with a romantic relationship (binary: yes or no)\n",
      "24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
      "25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
      "26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
      "27 dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
      "28 walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
      "29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
      "30 absences - number of school absences (numeric: from 0 to 93)\n",
      "these grades are related with the course subject, math:\n",
      "31 g1 - first period grade (numeric: from 0 to 20)\n",
      "31 g2 - second period grade (numeric: from 0 to 20)\n",
      "32 g3 - final grade (numeric: from 0 to 20, output target)\n",
      "contain 95k rows and 11 columns\n",
      "content\n",
      "this dataset is about the causes of death in 2014 release by un for public.\n",
      "acknowledgements\n",
      "credit to united nation\n",
      "inspiration\n",
      "data to make a world a better place\n",
      "context\n",
      "these 19th century works on nepal were downloaded from project gutenberg for a quick viewing of how the line graphs of their page-by-page sentiment compared with one another in applying the the text mining, analysis and visualization capabilities of r, inspired by the work on janeaustenr or gutenbergr.\n",
      "content\n",
      "the books and collection of journals on nepal of about 200 years ago are in text files.\n",
      "acknowledgements\n",
      "these works have been available in machine readable format thanks to project gutenberg.\n",
      "inspiration\n",
      "although the volumes of books and journals are growing in the online repositories of project gutenberg, only a few works in english are about nepal. how are their portrayals of nepal similar or different in word-clouds and sentiments? which r packages can be useful to make these comparisons?\n",
      "context\n",
      "this dataset contains a pickle file and csv files that contain data of bank stocks of 6 top banks throughout the financial crisis all the way to early 2016.\n",
      "content\n",
      "the stock information is from the following banks:\n",
      "bank of america\n",
      "citigroup\n",
      "goldman sachs\n",
      "jpmorgan chase\n",
      "morgan stanley\n",
      "wells fargo\n",
      "in the dataset, each bank is represented by its ticker symbol. for example, bank of america is represented by bac and wells fargo is represented by its ticker symbol wfc.\n",
      "acknowledgements\n",
      "this dataset has been collected from google finance as part of my data capstone project for my udemy datascience and ml bootcamp.\n",
      "context\n",
      "like most things in my life, it all started with the bee movie. i wanted to know what the most popular words were in the film. so i wrote a python script that found the words used in the movie and the number of times they appear in the script.\n",
      "content\n",
      "each movie has an associated .csv file where the first column represents the words found in the script for that move and the second column is the number of times that word appears.\n",
      "my python code can be found at https://github.com/emmabel96/wordoccurrences\n",
      "acknowledgements\n",
      "most of the scripts used were collected by alberto acerbi and can be found at https://figshare.com/projects/imsdb_movie_scripts/18907\n",
      "inspiration\n",
      "i'm looking forward to seeing the creative ways this dataset is used!\n",
      "context\n",
      "since amazon echo dot 2 has been the best selling alexa product, we decided to extract the reviews posted on amazon for this device. this particular dataset contains reviews posted in september and october 2017. the complete dataset with all the reviews from 2016 can be downloaded from datastock - a repository of clean and structured web datasets with historical records.\n",
      "content\n",
      "given below are the data fields:\n",
      "pageurl\n",
      "title\n",
      "review text\n",
      "device color\n",
      "user verified\n",
      "review date\n",
      "review useful count\n",
      "configuration\n",
      "rating\n",
      "declaration text (example: vine voice, top 100 reviewer, etc.)\n",
      "acknowledgements\n",
      "this dataset has been created via promptcloud's in-house web data extraction solution.\n",
      "inspiration\n",
      "the initial set of analyses can be access here - https://goo.gl/xhve9b.\n",
      "this dataset contains results of the third development round of nips 2017 adversarial learning competition.\n",
      "content\n",
      "matrices with intermediate results\n",
      "following matrices with intermediate results are provided:\n",
      "accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense\n",
      "error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense\n",
      "hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense\n",
      "in each of these matrices, rows correspond to defenses, columns correspond to attack. also first row and column are headers with kaggle team ids (or baseline id).\n",
      "scores and run time statistics of submissions\n",
      "following files contain scores and run time stats of the submissions:\n",
      "non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks\n",
      "targeted_attack_results.csv - scores and run time statistics of all targeted attacks\n",
      "defense_results.csv - scores and run time statistics of all defenses\n",
      "each row of these files correspond to one submission. columns have following meaning:\n",
      "kaggleteamid - either kaggle team id or id of the baseline.\n",
      "teamname - human readable team name\n",
      "score - raw score of the submission\n",
      "normalizedscore - normalized (to be between 0 and 1) score of the submission\n",
      "minevaltime - minimum evaluation time of 100 images\n",
      "maxevaltime - maximum evaluation time of 100 images\n",
      "medianevaltime - median evaluation time of 100 images\n",
      "meanevaltime - average evaluation time of 100 images\n",
      "notes about the data\n",
      "due to team mergers, team name in these files might be different from the leaderboard.\n",
      "not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).\n",
      "few targeted and non-targeted attacks exceeded 500 seconds time limit on all batches of images. these submissions received score 0 in the official leaderboard. we still were able to compute \"real\" score for these submissions and include it into non_targeted_attack_results.csv and targeted_attack_results.csv files. however these scores are negated in the provided files to emphasize that these submissions violate the time limit.\n",
      "google news articles tagged under hate crimes in the us, feb. 13-oct. 28, 2017\n",
      "rankings for halloween costumes by state in october 2017\n",
      "context\n",
      "safecast is a volunteer driven non-profit organization whose goal is to create useful, accessible, and granular environmental data for public information and research. begun in response to the nuclear disaster in japan in march, 2011, safecast collects radiation and other environmental data from all over the world. all safecast data is published, free of charge, under a cc0 designation.\n",
      "the official “safecast data” published for others to use is collected by safecast volunteers using professional quality devices. a combination of off the shelf commercial radiation monitors and devices are used in the collection process. most devices are standardized on the same sensor, the lnd7317 which is commonly referred to as the 2″ pancake. this is a highly sensitive piece of equipment that is used by nuclear professionals all over the world.\n",
      "content\n",
      "\"presently we assume the radiation comes from cesium-137 which is the most prevalent isotope still around from nuclear weapons testing and from the accidents at chernobyl and fukushima. based on calibration tests of multiple device designs using the same detector, we've settled on a conversion factor of 334. that is, 334cpm from a bgeigie equates to 1usv/h.\n",
      "it would be more accurate to have conversion factors tuned to each locale based on the spectrum of radiation present, but we don't have much of that data and in practice the error is estimated to be relatively small. \"\n",
      "- joe moross from safecast\n",
      "captured time\n",
      "time data was captured\n",
      "latitude\n",
      "longitude\n",
      "value\n",
      "actual data, in whatever units are in the \"unit\" field. about 130k out of over 80 million measurements are not in cpm (from a 2-inch pancake geiger tube)\n",
      "unit\n",
      "describes the raw form of the data. the vast majority of the measurements in the database are from safecast-designed bgeigies, which record radiation levels in cpm (counts per minute)\n",
      "location name\n",
      "device id\n",
      "md5sum\n",
      "height\n",
      "height from the ground in meters\n",
      "surface\n",
      "denotes data recorded very close to a surface such as pavement. this is typically done at 1cm height so should be regarded as contamination density and displayed or analyzed in units such as becquerels, not as dose data.\n",
      "radiation\n",
      "uploaded time\n",
      "loader id\n",
      "acknowledgements\n",
      "thank you to safecast for collecting and sharing this dataset. the source files were downloaded from safecast.org and have not been modified.\n",
      "inspiration\n",
      "safecast shared this datset for the public to have an un-biased source of radiation measurements. use this dataset to see where safecast volunteers have recorded data.\n",
      "this dataset was downloaded from data.gov.au here in june 2016. it contains monthly aggregated data of flights between australian cities. the csv file has the following fields: city1, city2, month, passenger_trips, aircraft_trips, passenger_load_factor, distance_gc_(km), rpks, asks, seats, year, month_num. see the link for full information.\n",
      "it it released under creative commons attribution 3.0 australia.\n",
      "this is a dataset scraped from global peace index wikipedia page and presents \"relative position of nations' and regions' peacefulness\".\n",
      "it was created using this script: https://gist.github.com/kretes/2c191dddd78f8b5dcf20f3841eda24db\n",
      "this can be used in various geopolitical analysis\n",
      "the dataset is a collection of images of selfies with sunglasses and images with sunglasses with the scope for improving the accuracy of face recognition. the dataset contains 2768 unannotated images and a total of 5536 images. this dataset was created with the motive of removing reflection on sunglasses in a selfie and reconstruct the closest possible shade of the sunglasses. the images in the dataset are of varying dimensions and have to be resized for your research. currently, the ownership of the dataset is shared between me and dr. boqing gong, who is an assistant professor at the centre for research in computer vision at the university of central florida. more images will be added to the dataset in the future and i request fellow kagglers to send images to the author to increase the size of the dataset which will help the machine learning and computer vision engineers community.\n",
      "about the dataset: the dataset is a collection of images of selfies with sunglasses and images with sunglasses to improve the accuracy of face recognition. the dataset contains 2768 unannotated images and a total of 5536 images. the repository also has the code for annotation which is ready to use.\n",
      "before executing the program: 1. make sure you have entered the correct address of the folder which contains the images and the saved_images folder.\n",
      "once everything works, a window should pop-up showing the image to annotate the image, 1. left-click and drag across the image where you wish to draw the box. 2. to set a dot on the image, double right-click on the image. 3. the coordinates of the red-dot will be stored in a .txt file and will also be displayed in the console. 4. to undo or redraw the rectangle press 'r' key. 5. to show the cropped image, press 'n' key. 6. to show the next image, press 'n' key again. 7. the output file saves the coordinates of the red-dot as a python dictionary so that it is easy to read and manipulate.\n",
      "since kaggle won't support uploading more than 1000 images in a compressed format, you can get the complete repository of the images from https://github.com/shreyas0906/dataset if you have any question, please email me at shreyas0906@gmail.com\n",
      "this data set shows different types of polls and methods concerning the 2012 election.\n",
      "given data contains jan-2014 to aug-2016 daily tv sales quantity. there are total 124 models. this data is collected from one of the leading brand of bangladesh. annually there are two big festivals (eid) which follows islamic lunar calendar. provided data are in csv format. it contains only three columns.\n",
      "date: date of sale\n",
      "model: tv model (not original)\n",
      "count: sale quantity\n",
      "i like to throw a challenge to kagglers to device a effective sales forecasting model.\n",
      "the dataset contains the results of all the students studying under anna university, chennnai, tamil nadu. it contains the name, registernumber and department.\n",
      "the dataset was scraped from the anna university result site, the script to download it is here.\n",
      "the data was collected to analyse the performance of colleges, subject performances.\n",
      "structure\n",
      "the csv file has two columns, one is res, a dictionary containing the results and studentdetail, a list containing the student details.\n",
      "context and acknowledgements: this dataset contains information about location of public wifi aps in buenos aires city. it is an official dataset produced by ministerio de modernización - unidad de sistemas de información geográfica. the source of the data is agencia de sistemas de información - gerencia operativa de redes. the last update is from april 2015 despite being tagged as monthly updated.\n",
      "important: it is csv file type but it is separated by semi-colons \";\"\n",
      "fields: - wkt: geolocation coordinates. type: text. - id: unique identifier. type: numeric - nombre: name of the ap. type: text. - tipo: category of location (e.g. museum, subway, etc). type: text. - etapa: stage of inauguration of the ap. type: text - etapa_obse: stage of secondary observations. type: date. - estado: availability. type: text. - calle: main street. type: text. - altura: number on main street. type: numeric - calle2: intersection street. type: text. - direccion: address in one line. type: text. - observacio: observations. type: text. - observa_01: private observations. type: text. - publicable: publishable. type: numeric. - verificado : verified. type: numeric. - distrito: district of location. type: text.\n",
      "this dataset contains information about the streets and street parking signals in the city of montreal. they are obtained from the city's portal at http://donnees.ville.montreal.qc.ca/group/transport.\n",
      "in this database, you will see three different files, relevant to our problem of interest. gbdouble.json: this is a geo-json file which contains the geographical coordinates for each side of the streets in the city. each street side is described by a number of line segments (the coordinates of a number of points). to open and read the coordinates of the street segments, you can have a look at https://www.kaggle.com/mnabaee/d/mnabaee/mtlstreetparking/load-street-side-coordinates/\n",
      "signalisation.csv: this csv file includes all of the parking signals in the city. each row will have the latitude, longitude, a text description as well as a number of other fields. one may need to parse/digest the text information field to understand what the signal means.\n",
      "signalisation.pdf: this pdf contains all of the picture of each signal and their corresponding signal code (which can be matched with the codes provided in signalisation.csv).\n",
      "the main goal in analyzing this dataset is to create an interactive map of the street parking spots at a given time (interval). the available parking spots should be found by analyzing the signals in a street and digesting their meaning.\n",
      "a preliminary work on the data set is done on the data where you can see at github.\n",
      "the material is a industry list. and we provide 10 sample description text for most of them. the text is about some business for sale or investment projects. we need some one to use machine learning and natural language to build a classification script. but we hope the script can classify some other kind of text, such as news, summary, etc. so nlp might be as import as the machine learning.\n",
      "we know the amount is far from enough to do some machine learning with good results. that's why we need some help.\n",
      "for some of the industries, we didn't find enough text (red-color area), just do it with less sample. and we will try fill them later.\n",
      "some description will appear in multiple industry, such as a plastic company, their products are for automotive industry. so they will be both \"plastic\" and \"automotive general parts\"\n",
      "we need some one to train the machine with the material, and result we need is that after we input a \"description\" and it can tell us which industry(industries) it is.\n",
      "and you might needs to deal with the following situation: \"a furniture manufacturer, they are selling themselves with the real estate and they have a rental vehicle.\" their industry should be only \"household light industry\" instead of \"household light industry, real estate, automobile service and leasing\"\n",
      "we have more text material, but they are not classified. if you think they are also helpful. just let us know.\n",
      "crashes 2014 dataset describes whether conditions, number of vehicles included in the crash, number of fatalities etc. dataset has 80 columns and 292020 rows. currently dataset is raw. i would like to work on it if you accept.\n",
      "donald trump tweets\n",
      "one of the most compelling trends in technology today is the open data and open governance movement. it's not without reason that no less than tim berners-lee himself, the creator of the worldwide web and one of the most preeminent scholars of the internet, is doing his latest work in getting more government data on the web: in an interview with the new york times a few years ago he spoke to how even records as mundane as traffic statistics or weather data could drive tinkerers to \"make government run better\".\n",
      "new york city has been at the forefront of this movement: mayor bloomberg formalized a citywide analytics team as the mayor's office for data analytics in 2013, and the effort has continued under mayor de blasio, with the city cementing its first open data plan in july 2015. the resultant nyc open data portal is populated with over 1500 datasets. it was, and is, the largest citywide open data portal in the world.\n",
      "nevertheless, a good open data platform is more than a count; it's a function also of all of the maintenance and structure that goes into it. what's a \"dataset\", who's publishing them, and how well-maintained are they?\n",
      "this dataset contains the publicly available metadata about the datasets in the nyc open data portal, provided in a json format.\n",
      "for an initial exploration of its contents see this blog post.\n",
      "this is a historical data of hangseng futures index based in hong kong.\n",
      "for non traders, the data is a time-series (sequential flow of numbers) describing the hangseng futures index of hongkong. every minute one line of data is created.\n",
      "each line has : open : first price at start of that minute high : highest price during that minute low : lowest price during that minute close : last price for that minute time frames volume : total number of units traded\n",
      "this is can be called 'raw data' on a 1 minute time frame.\n",
      "context\n",
      "i used to play banco imobiliário a lot in my childhood and i just realized how interesting this game can be for statisticians. my main idea was to analyze the game data to try some reverse engineering and discover what is happening inside the game mathematically. also, i was curious to see if there's any correlation between banco imobiliário and monopoly's logic.\n",
      "content\n",
      "i started listing all the properties cards in a spreadsheet (which is available in the properties.csv). then, i created another file for the transportation companies (properties.csv) and finally the \"sorte ou revés\" cards (sorte-ou-reves.csv) (huge work to type the cards' descriptions).\n",
      "acknowledgements\n",
      "i owe special thanks to my wife that dictate all the cards stats to me even falling asleep sometimes.\n",
      "inspiration\n",
      "what's the most efficient way to give a property a score including all its stats like rent value, rent with multiple houses and hotel built, price per building, sell value and its color-neighbor properties? also, now that you can see the game's big picture, what's the most effective strategy to beat everyone in this game?\n",
      "medium sized song list\n",
      "this dataset consists of a list of songs arranged as follow:\n",
      "id_user1,id_song,rating\n",
      "id_user1,id_song,rating\n",
      "...\n",
      "id_user2,id_song,rating\n",
      "id_user2,id_song,rating\n",
      "...\n",
      "id_usern,id_song,rating\n",
      "id_usern,id_song,rating\n",
      "the main idea for this dataset is to implement recommendation algorithms based on collaborative filters. in addition to grouping data, reduce and compress lists. it is distributed under the cc 4.0 license. it's educational purpose.\n",
      "in fact, as the music is coded in an id, the dataset could be for anything else like, movies, places, etc. use it for training your collaborative filters. (the data truly represent songs)\n",
      "context\n",
      "this data set concerns data in team histories of mlb.\n",
      "content\n",
      "this data set is 2594*23 in dimensions. it mainly keeps track of the existing 30 teams, with respect of winning records, managers and players chronically from 1870s to 2016.\n",
      "acknowledgements\n",
      "we hereby appreciate professor miles chen at ucla, for introducing us getting this dataset using nodes extraction from the mlb website \"baseball-reference.com\". this dataset is for solving homework 3.\n",
      "inspiration\n",
      "this dataset is for an analysis on coaching records of managers, and for figuring out the reason why managers switches jobs. moreover, we are supposed to find out the big picture of mlb over the past one century and forty years. the dataset is expected to receive feedbacks on details of manager ratings and player ratings.\n",
      "this dataset does not have a description yet.\n",
      "note: we're having some trouble uploading the actual images of the handwritten names. stay tuned.\n",
      "this dataset contains links to images of handwritten names along with human contributors’ transcription of these written names. over 125,000 examples of first or last names. most names are french, making this dataset of particular interest for work on dealing with accent marks in handwritten character recognition.\n",
      "acknowledgments\n",
      "data was provided by the data for everyone library on crowdflower.\n",
      "our data for everyone library is a collection of our favorite open data jobs that have come through our platform. they're available free of charge for the community, forever.\n",
      "the data\n",
      "a file handwritten_names.csv that contains the following fields:\n",
      "_unit_id: a unique id for the image\n",
      "image_url: the path to the image; begins with \"images/\"\n",
      "transcription: the (typed) name\n",
      "first_or_last: whether it's a first name or a last name\n",
      "a folder images that contains each of the image files.\n",
      "monthly adjusted era-interim 2m temperature anomalies relative to 1981-2010.\n",
      "see https://climate.copernicus.eu/resources/data-analysis/average-surface-air-temperature-analysis/monthly-maps/october-2016 for more details of the data.\n",
      "see also: copernicus climate change service\n",
      "era-interim data license\n",
      "context\n",
      "uk pubs as open data, including pub name, address, position and local authority.\n",
      "content\n",
      "fsa_id\n",
      "name\n",
      "address\n",
      "postcode\n",
      "easting\n",
      "northing\n",
      "latitude\n",
      "longitude\n",
      "local_authority\n",
      "fsa_id is the fsa's id for the premises and allows you to link the pub to their food hygiene ratings.\n",
      "acknowledgements\n",
      "for latest version and documentation see the open pubs homepage.\n",
      "derived from the food standard agency food hygiene ratings database and licensed under their terms and conditions.\n",
      "local authority field derived from the ons postcode directory licensed under the ogl.\n",
      "contains os data © crown copyright and database right 2016\n",
      "contains royal mail data © royal mail copyright and database right 2016\n",
      "contains national statistics data © crown copyright and database right 2016\n",
      "published and maintained by getthedata.\n",
      "inspiration\n",
      "create mashups with other geocoded open datasets: pubs/bus stop mashup\n",
      "optimise pubcrawls: world's longest pub crawl: maths team plots route between 25,000 uk boozers\n",
      "overview\n",
      "the data is taken from the epfl cvlab's library of tree-reconstruction examples (http://cvlab.epfl.ch/data/delin)\n",
      "the data are as images and some kind of swc file which is basically a text file with columns for index, position and width. the kernels show how it can be loaded.\n",
      "tasks\n",
      "the initial task is to segment the streets and their connectivity from the images. ideally creating a list of points in a similar order to the swc provided and with a minimal distance between the two center positions.\n",
      "secondary tasks include\n",
      "automatically identifying intersections (useful for self-driving cars and robots that need to navigate streets)\n",
      "representing the map as a graph with nodes and edges\n",
      "dataset: the files on your computer.\n",
      "crab is a command line tool for mac and windows that scans file data into a sqlite database, so you can run sql queries over it.\n",
      "e.g. (win)       c:> crab c:\\some\\path\\myproject\n",
      "or  (mac)        $ crab /some/path/myproject\n",
      "you get a crab> prompt where you can enter sql queries on the data, e.g. count files by extension\n",
      "select extension, count(*) \n",
      "from files \n",
      "group by extension;\n",
      "e.g. list the 5 biggest directories\n",
      "select parentpath, sum(bytes)/1e9 as gb \n",
      "from files \n",
      "group by parentpath \n",
      "order by sum(bytes) desc limit 5;\n",
      "crab provides a virtual table, fileslines, which exposes file contents to sql\n",
      "e.g. count todo and fixme entries in any .c files, recursively\n",
      "select fullpath, count(*) from fileslines \n",
      "where parentpath like '/users/gn/hl3/%' and extension = '.c'\n",
      "    and (data like '%todo%' or data like '%fixme%')\n",
      "group by fullpath;\n",
      "as well there are functions to run programs or shell commands on any subset of files, or lines within files e.g. (mac) unzip all the .zip files, recursively\n",
      "select exec('unzip', '-n', fullpath, '-d', '/users/johnsmith/target dir/') \n",
      "from files \n",
      "where parentpath like '/users/johnsmith/source dir/%'  and extension = '.zip';\n",
      "(here -n tells unzip not to overwrite anything, and -d specifies target directory)\n",
      "there is also a function to write query output to file, e.g. (win) sort the lines of all the .txt files in a directory and write them to a new file\n",
      "select writeln('c:\\users\\sjohnson\\dictionary2.txt', data) \n",
      "from fileslines \n",
      "where parentpath = 'c:\\users\\sjohnson\\' and extension = '.txt'\n",
      "order by data;\n",
      "in place of the interactive prompt you can run queries in batch mode. e.g. here is a one-liner that returns the full path all the files in the current directory\n",
      "c:>  crab -batch -maxdepth 1 . \"select fullpath from files\"\n",
      "crab sql can also be used in windows batch files, or bash scripts, e.g. for etl processing.\n",
      "crab is free for personal use, $5/mo commercial\n",
      "see more details here (mac): http://etia.co.uk/ or here (win): http://etia.co.uk/win/about/\n",
      "an example sqlite database (mac data) has been uploaded for you to play with. it includes an example files table for the directory tree you get when downloading the project gutenberg corpus, which contains 95k directories and 123k files.\n",
      "to scan your own files, and get access to the virtual tables and support functions you have to use the crab sqlite shell, available for download from this page (mac): http://etia.co.uk/download/ or this page (win): http://etia.co.uk/win/download/\n",
      "content\n",
      "files table\n",
      "the files table contains details of every item scanned, file or directory. all columns are indexed except 'mode'\n",
      "columns\n",
      "  fileid (int) primary key  -- files table row number, a unique id for each item\n",
      "  name (text)               -- item name e.g. 'hei.ttf'\n",
      "  bytes (int)               -- item size in bytes e.g. 7502752\n",
      "  depth (int)               -- how far scan recursed to find the item, starts at 0\n",
      "  accessed (text)           -- datetime item was accessed\n",
      "  modified (text)           -- datetime item was modified\n",
      "  basename (text)           -- item name without path or extension, e.g. 'hei'\n",
      "  extension (text)          -- item extension including the dot, e.g. '.ttf'\n",
      "  type (text)               -- item type, 'f' for file or 'd' for directory\n",
      "  mode (text)               -- further type info and permissions, e.g. 'drwxr-xr-x'\n",
      "  parentpath (text)         -- absolute path of directory containing the item, e.g. '/library/fonts/'\n",
      "  fullpath (text) unique    -- parentpath of the item concatenated with its name, e.g. '/library/fonts/hei.ttf'\n",
      "\n",
      "paths\n",
      "1) parentpath and fullpath don't support abbreviations such as ~ . or ..  they're just strings.\n",
      "2) directory paths all have a '/' on the end.\n",
      "fileslines table\n",
      "the fileslines table is for querying data content of files. it has line number and data columns, with one row for each line of data in each file scanned by crab.\n",
      "this table isn't available in the example dataset, because it's a virtual table and doesn't physically contain data.\n",
      "columns\n",
      "  linenumber (int)  -- line number within file, restarts count from 1 at the first line of each file\n",
      "  data (text)       -- data content of the files, one entry for each line\n",
      "fileslines also duplicates the columns of the files table: fileid, name, bytes, depth, accessed, modified, basename, extension, type, mode, parentpath, and fullpath. this way you can restrict which files are searched without having to join tables.\n",
      "example gutenberg data\n",
      "an example sqlite database (mac data), database.sqlite, has been uploaded for you to play with. it includes an example files table for the directory tree you get when downloading the project gutenberg corpus, which contains 95k directories and 123k files.\n",
      "you can open it with any sqlite shell, or query it with any sqlite query tools, but the virtual tables such as fileslines and support functions such as exec() and writeln() only work from the crab shell that you have to download from etia.co.uk.\n",
      "uses\n",
      "reporting and analysis of filesystem contents\n",
      "finding files and directories\n",
      "filesystem operations such moving, copying, deleting, unzipping files\n",
      "etl processing\n",
      "introduction\n",
      "each year schools in pennsylvania are required to report weapons violations, substance abuse, cyber harassment, and other crimes committed during school, at school events, or on a bus (or waiting at a bus stop) to and from school.\n",
      "the raw data can be found at www.safeschools.state.pa.us\n",
      "important: (lea types)\n",
      "the rows in this dataset include several \"lea types\" (legal entity types): \"school\", \"school district\", \"county\"... etc. please note that several schools may fall under a single \"school district\", and there maybe several \"school districts\" in a single county. hence, if you include both \"lea types\", the counts could be off.\n",
      "key pennsylvania safe schools legislation\n",
      "the safe schools act of 1995 (act 26) was amended in 1997 (act 30) to mandate annual reporting of all incidents of violence, weapons, alcohol, drugs and tobacco possession to the department of education. local education agencies also are required to develop a memorandum of understanding with local law enforcement agencies and provide for other procedural safeguards to enhance school safety. another amendment to act 26 (act 36 of 1999) empowers schools to acquire the tools and resources needed to develop and enhance safe learning environments.\n",
      "how this data was collected for kaggle.\n",
      "see the following gist.\n",
      "context\n",
      "mlagunas asked a question here: \"what to do when data has less rows than columns?\". the real question is: \"can we accurately predict the right answers to all the questions, based on the answers given and score received for only a few students\". (hint: no one student answered all questions correctly).\n",
      "content\n",
      "23 students took the same test, answering 25 multiple choice questions. each question has 5 possible answers (a-e). the last column shows the score the students received.\n",
      "acknowledgements\n",
      "data originated from mlagunas.\n",
      "inspiration\n",
      "so, can you find out what the right answers are for each question?\n",
      "context\n",
      "the federal city of moscow, russia is divided into twelve administrative okrugs, which are in turn subdivided into districts (raions). (source wikipedia)\n",
      "content\n",
      "okato: russian classification on objects of administrative division.\n",
      "oktmo: russian classification on territories of municipal division.\n",
      "raion: raion's name.\n",
      "okrugs: okrugs' name.\n",
      "acknowledgements\n",
      "the shapefile data has been found here: http://gis-lab.info/qa/moscow-atd.html.\n",
      "but i have translated the okrugs and raions names in english.\n",
      "inspiration\n",
      "i hope you can use this shapefile to make interesting visualisations!\n",
      "you can use okato and/or oktmo codes to join your data to this shapefile.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "this dataset was released by the obama white house consisting of about 5.9 million visitor logs. the more recent administration does not plan on releasing this dataset, so i thought it would be nice to move the obama dataset to kaggle to have this platform serve as an alternate home for this data.\n",
      "challenges\n",
      "note that the total dataset (5.9 million rows) is a total of 1.1 gb, so i split it into 6 files of 1 million rows each.\n",
      "more info\n",
      "source: https://obamawhitehouse.archives.gov/briefing-room/disclosures/visitor-records\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "points scored by each player in each formula 1 race for years 2000 to 2016 according to new points system. more data will be uploaded soon.\n",
      "filename format: 'year'_points_new.csv\n",
      "data obtained from ergast api link: [click here][1]http://ergast.com/mrd/\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "i like to compensate my frustration with sports analyzing sports data so i put together every data i could from the first season of this netflix series and here we are.\n",
      "context\n",
      "simple analysis of how money affects the decisions of policy makers.\n",
      "acknowledgements\n",
      "the data was taken from the website https://www.followthemoney.org/\n",
      "inspiration\n",
      "mainly i was frustrated with the lack of consumer protection, so i wanted to see how much money isps donated to congress.\n",
      "context\n",
      "i gethered this data while providing some investigation about used car market. this dataset helps to transfer from annual mileage and engine volume/type to fuel consumption and cost.\n",
      "content\n",
      "represents the weighted average specific fuel consumption of new private cars first registered in the years 2000 to 2011 in ireland.\n",
      "acknowledgements\n",
      "the core data was collected by sustainable energy authority of ireland and freely available on its web page (http://www.seai.ie/)\n",
      "inspiration\n",
      "though there are plenty of notes and news about car fuel consumption it is not so easy to find clear and simple data to use in fuel spendings estimations.\n",
      "context\n",
      "i was wandering the kaggle datasets and then i came across the dataset of ipl. so i just thought that why not have a dataset for psl as well.\n",
      "content\n",
      "it has 3 basic csv files for every match in both the psl edition 1 and psl edition 2. first file is the file that has the ball by ball commentary of inning1 second file is the file that has the ball by ball commentary of inning2 and then finally the third file contains the scorecard of the whole match.\n",
      "acknowledgements\n",
      "i would like to acknowledge espncricinfo for having such a detailed data for every match that has been played on earth for quite a while.\n",
      "inspiration\n",
      "i want people to find out one specific thing from the data . that is to use nlp and determine the reasons behind batsman getting out and the remedy that what might have been done differently to avoid the wicket.\n",
      "context\n",
      "when i was working on twitter sentiment analysis, i found it very difficult to look for a dataset. many that i found were too complicated to process or could not be used. this inspired me to create my own dataset and pre-process it .\n",
      "i have provided pre-processed tweets divided into positive, negative and neutral categories . the positive and negative tweets were retrieved using emoticons. neutral tweets are the timeline tweets of the telegraph.\n",
      "future works\n",
      "i would be expanding the dataset in future.\n",
      "imdb dataset for 5000 movies\n",
      "context\n",
      "this is just my archived search history . this actually took me a solid 3 hours to put together because i had no idea how to deal with json files (and i still dont)\n",
      "content\n",
      "json timestamp\n",
      "text\n",
      "the most recent is at index 0 then 1, 2, and so on\n",
      "for what?\n",
      "this dataset is for basic data analysis. student statisticians or data-analysists (like myself) could use this as a basic learning point. even ml students could predict future prices and speeds of computers.\n",
      "unfortunately, this dataset doesn't come with dates. (which are a pain to work with anyway), but the computers are in order from earliest to latest.\n",
      "i will be uploading another version with this and a more detailed csv that has the computer name, date, and other stats. this dataset is free to use for any purpose.\n",
      "this is simply to gain understanding in analyzing data. at least for me.\n",
      "content\n",
      "price, speed, hd, ram, screen, cd, multi, premium, ads, trend\n",
      "something glorious is coming\n",
      "the largest computer csv? maybe? maybe im scrapping it right now? who knows? ;)\n",
      "context\n",
      "the main aim of this data analysis is to identify the ongoing research in indian universities and indian industry. it gives a basic answer about research source and trend with top authors and publication. it also shows the participation of industry and universities in research.\n",
      "content\n",
      "it is a collection of 1387 paper dataset from scopus journal between 2001 to 2016 published by indian universities or india based research center of any industry.\n",
      "if a paper has multiple authors from industry and indian university, we count that paper as university paper.\n",
      "if a paper published by industry and non-indian university, we count that paper as industry paper.\n",
      "during cleaning of data, we consider the different name of institute as single institute. for example iit-madras, indian institute of technology and iit-m count as the same institute.\n",
      "we also consider the different name of same industry as single industry, for example, tcs and tata consultancy service count as the same industry.\n",
      "acknowledgements\n",
      "this dataset is available as open source on scopus journal. we took only indian researcher's detail from it.\n",
      "detail of analysis and blog : scopus journal blog\n",
      "is a dataset that contains a list of items sorted into a set of shopping carts.\n",
      "context\n",
      "for an undergraduate project at the university of michigan, my team collected this data to see if we could classify gender using images of people. then to try to improve performance, we used machine learning and computer vision methods to track the person over time, automatically and used the additional tracking data to boost the performance of our convolutional neural network. it turned out that tracking improved our performance on our gender classifier.\n",
      "content\n",
      "70 sequences of people walking with tight bounding boxes and gender labels.\n",
      "current status\n",
      "can't upload data due, since the site detects the contents of my .zip files as uncompressed.\n",
      "context\n",
      "collection of results from running events in porto, portugal.\n",
      "content\n",
      "each row corresponds to the race name and year, runner name, official time (clock time), net time, place, age class, sex and country. data collected from the website runporto.com.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "this is a small dataset new york personalized license plate applications received from the new york dmv in response to a july 2014 freedom of information law (foil) request. covers applications from 10/1/2010 to 9/26/2014.\n",
      "content\n",
      "accepted-plates.csv: csv of plate applications that were accepted and issued, with order date and plate configuration.\n",
      "rejected-plates.csv: csv of plate applications that were rejected by the department, with order date and plate configuration.\n",
      "red-guide.csv: a copy of the red guide, the list of \"inappropriate\" plate configurations that are automatically disallowed by the new york dmv as of july 2015. procedure.pdf: a document listing the dmv's plate review and cancellation procedures as of june 2014.\n",
      "acknowledgements\n",
      "this dataset is from a foil request by noah veltman at data news. here are some notes about the process, but check the original source for more information. thanks to noah for letting us share this dataset with the kaggle community!\n",
      "https://github.com/datanews/license-plates\n",
      "this data may contain explicit or offensive language.\n",
      "plate configurations in accepted-plates.csv may have since been revoked by the dmv.\n",
      "plate configurations in rejected-plates.csv were rejected by the department. it does not include plates that were reserved, banned by the red guide, or cancelled for administrative reasons.\n",
      "some plate configurations may exist in multiple applications.\n",
      "a small number of rows may contain erroneous data because of excel cell formatting in the original prepared files.\n",
      "although the dmv collects an explanation of the requested combination from each online applicant, that information is not preserved.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?https://github.com/datanews/license-plateshttps://github.com/datanews/license-plates\n",
      "context\n",
      "i ran into the american presidency project and was inspired by the incredible amount of data that the founders of this project had accumulated. further, i ran into a few key projects such as the wordy words of hillary clinton and donald trump and ibm watson compares trump's inauguration speech to obama's that used this data.\n",
      "the site itself, however, simply has a php page that individually returns every one of the 120,000+ documents in an html format. my goal was to extract the almost 60,000 documents released by the offices of all of the presidents of the united states, starting with george washington, in an effort to make this data available to anyone interested in diving into this data set for unique studies and experimentation.\n",
      "content\n",
      "the data is normalized using two key properties of a document: president and document category. document categories can include, but are not limited to: oral, written, etc.\n",
      "each document has a variety of properties:\n",
      "category - this category field is a further detailed categorial assignment, such as address, memo, etc.\n",
      "subcategory - inaugural, etc.\n",
      "document_date - format: 1861-03-04 00:00:00\n",
      "title - title of the released document.\n",
      "pid - this value, stored as an integer, can be used to access the original document at the following url: http://www.presidency.ucsb.edu/ws/index.php?pid={}. where {} can be replaced with the value in this field.\n",
      "content - this is the full text of the released document.\n",
      "a markdown version of this json structure can be found on github.\n",
      "acknowledgements\n",
      "a huge thank you for the data and inspiration to the american presidency project.\n",
      "today, we are excited to announce the first in what we plan to be a series of public dataset releases. our dataset releases will be oriented around various problems of relevance to quora and will give researchers in diverse areas such as machine learning, natural language processing, network science, etc. the opportunity to try their hand at some of the challenges that arise in building a scalable online knowledge-sharing platform. our first dataset is related to the problem of identifying duplicate questions.\n",
      "an important product principle for quora is that there should be a single question page for each logically distinct question. as a simple example, the queries “what is the most populous state in the usa?” and “which state in the united states has the most people?” should not exist separately on quora because the intent behind both is identical. having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.\n",
      "to mitigate the inefficiencies of having duplicate question pages at scale, we need an automated way of detecting if pairs of question text actually correspond to semantically equivalent queries. this is a challenging problem in natural language processing and machine learning, and it is a problem for which we are always searching for a better solution.\n",
      "the dataset that we are releasing today will give anyone the opportunity to train and test models of semantic equivalence, based on actual quora data. we are eager to see how diverse approaches fare on this problem.\n",
      "our dataset consists of over 400,000 lines of potential question duplicate pairs. each line contains ids for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair. here are a few sample lines of the dataset:\n",
      "here are a few important things to keep in mind about this dataset:\n",
      "our original sampling method returned an imbalanced dataset with many more true examples of duplicate pairs than non-duplicates. therefore, we supplemented the dataset with negative examples. one source of negative examples were pairs of “related questions” which, although pertaining to similar topics, are not truly semantically equivalent. the distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on quora. this is, in part, because of the combination of sampling procedures and also due to some sanitization measures that have been applied to the final dataset (e.g., removal of questions with extremely long question details).\n",
      "links for download data: http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv\n",
      "source: https://data.quora.com/first-quora-dataset-release-question-pairs\n",
      "content\n",
      "data scraped from the famous big data conference site:\n",
      "https://conferences.oreilly.com/strata/strata-eu/public/schedule/grid/public/2017-05-24\n",
      "acknowledgements\n",
      "kiyoto tamura did a similar analysis back in 2015:\n",
      "https://blog.treasuredata.com/blog/2015/03/10/sponsored-talks-at-strata/\n",
      "inspiration\n",
      "what topics do people prefer at the conference?\n",
      "do ratings change depending on time of day?\n",
      "this data was stripped manually from onthesnow.com as part of work to understand how closely related snowfall is to daily temperature.\n",
      "the set contains the following:\n",
      "date (dd-mon-yy)\n",
      "daily snowfall (24 hr new snow)\n",
      "cumulative seasonal snowfall (season snowfall total)\n",
      "base depth for the resort being measured (base depth)\n",
      "each resort measured has been loaded in its own isolated csv file.\n",
      "note: the accuracy and validity of the data contained has not been verified in any way.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "with the events unfolding within the new administration, i was curious if any president had been this active this early. i set out in search of a data set for executive orders, but didn't have a whole lot of easy success. pulling from the american presidency project (which doesn't really make pulling mass amounts of data easy with my skill level), and adding \"memorandums\" from wikipedia, i compiled this csv to analyze and share. if anyone has datasets including full text of executive orders, that would be fun to dive into!\n",
      "content\n",
      "the data attached has the president's name, date signed, executive order id, and title.\n",
      "acknowledgements\n",
      "data pulled from the american presidency project : http://www.presidency.ucsb.edu/executive_orders.php?year=2017&submit=display\n",
      "and wikipedia\n",
      "cabinet confirmations pulled from https://www.senate.gov/reference/resources/pdf/cabinettable.pdf and wikipedia\n",
      "inspiration\n",
      "one last thing i'd like to have added to this data set: classification! would be nice to analyze topics. which president had more orders concering economy? gov't? war? international relations?\n",
      "context\n",
      "this is an example dataset of straits times index.\n",
      "this is a time series data and could be used to explore the trends, index and other things.\n",
      "content\n",
      "straits index is a financial time series of a basket of top stocks in singapore, the equivalent of the american dow jones or s&p 500.\n",
      "you will explore and notice that volume is missing for a notable time period. what do you, should we discard volume in our model. sholud we impute values?\n",
      "acknowledgements\n",
      "this dataset is provided by the university to explore nnet\n",
      "inspiration\n",
      "this is a practice data set if someone wants to explore and understand the time series based data which is usually non-linear. some of the answers we could try to predict will be: 1. what is going to be tomorrow closing price 2. in general what is going to be trend tomorrow 3. imagine a person in test data set placing bet 1 dollar for each row. do you think he is going to make money if he follow the prediction of your model.\n",
      "the dataset contains the to-from name of between distance and the distance in imperial miles. there are 1000 cities it is referencing so it contains 1 million lines in database. thank you!\n",
      "context\n",
      "movielens data sets were collected by the grouplens research project at the university of minnesota.\n",
      "this data set consists of: * 100,000 ratings (1-5) from 943 users on 1682 movies. * each user has rated at least 20 movies. * simple demographic info for the users (age, gender, occupation, zip)\n",
      "the data was collected through the movielens web site (movielens.umn.edu) during the seven-month period from september 19th, 1997 through april 22nd, 1998. this data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. detailed descriptions of the data file can be found at the end of this file.\n",
      "neither the university of minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "this data set contains collision data for car accidents in canada from 1999-2014 as provided by transport canada. this dataset provides various features such as time of day, whether or not there were fatalities, driver gender, etc. the codes for the different categories can be found in 'drivinglegend.pdf'. the original csv file is no longer available, however it can be downloaded in portions by selecting the various features using this portal.\n",
      "content\n",
      "each feature is 100% categorical data, with some features having 2 categories, while others can have 30+. the data is not completely imputed appropriately (you can thank stats canada), so some data preprocessing is required. for instance, categories may have duplicates in the form of '01' and '1', or some data may be formatted as integers while others are formatted as strings. some data is not known and is marked accordingly in 'drivinglegend.pdf'. unfortunately, features such as location and impaired driving are not a part of this feature set, however there are plenty of others to work with.\n",
      "acknowledgements\n",
      "this data is provided by transport canada and statistics canada. this data is provided under the statistics canada open license agreement.\n",
      "inspiration\n",
      "questions of particular interest: - what are the main contributing factors to accident fatalities? - can a machine learning classifier be used to predict fatalities? note: if attempting to predict fatalities, the data is highly skewed towards non-fatalities.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "all us presidential candidates are required to fill out form 278e, a general disclosure of their assets, debts, and sources of income. this is an unpacked version of the pdf of trump's form that was made available by the federal election commission in mid 2016. it contains some information about his financial interests, but not enough to paint a complete picture of his net worth. it may be possible to use some of these forms to identify his foreign business partners.\n",
      "acknowledgements\n",
      "this dataset unpacked from the original pdf and kindly made available by quartz. please see their original article for the full background on what this form does and does not contain.\n",
      "you might also like\n",
      "trump's world\n",
      "trump's tweets\n",
      "trump campaign expenditures\n",
      "context\n",
      "trumedicines has trained a deep convolutional neural network to autoencode and retrieve a saved image, from a large image dataset based on the random pattern of dots on the surface of the pharmaceutical tablet (pill). using a mobile phone app a user can query the image datebase and verify the query pill is not counterfeit and is authentic, additional meta data can be displayed to the user: manf date, manf location, drug expiration date, drug strength, adverse reactions etc.\n",
      "content\n",
      "trumedicines pharmaceutical images of 252 speckled pill images. we have convoluted the images to create 20,000 training database by: rotations, grey scale, black and white, added noise, non-pill images, images are 292px x 292px in jpeg format\n",
      "in this playground competition, kagglers are challenged to develop deep convolutional neural network and hash codes to accurately identify images of pills and quickly retrieved from our database. jpeg images of pills can be autoencoded using a cnn and retrieved using a cnn hashing code index. our android app takes a phone of a pill and sends a query to the image database for a match, then returns meta data abut the pill: manf date, expiration date, ingredients, adverse reactions etc. techniques from computer vision alongside other current technologies can make recognition of non-counterfeit, medications cheaper, faster, and more reliable.\n",
      "acknowledgements\n",
      "special thanks to microsoft paul debaun and steve borg and nwcadence, bellevue wa for their assistance\n",
      "inspiration\n",
      "trumedicines is using machine learning on a mobile app to stop the spread of counterfeit medicines around the world. every year the world health organization who estimates 1 million people die or become disabled due to counterfeit medicine.\n",
      "context:\n",
      "faspell dataset was developed for the evaluation of spell checking algorithms. it contains a set of pairs of misspelled persian (farsi) words and their corresponding corrected forms similar to the aspell dataset used for english.\n",
      "content:\n",
      "the dataset consists of two parts:\n",
      "faspell_main: list of 5050 pairs collected from errors made by elementary school pupils and professional typists.\n",
      "faspell_ocr: list of 800 pairs collected from the output of a farsi ocr system.\n",
      "acknowledgements:\n",
      "based on a work at http://pars.ie/lr/faspell_dataset. please acknowledge the use of this dataset by referencing one of the following papers:\n",
      "barari, l., & qasemizadeh, b. (2005). clonizer spell checker adaptive language independent spell checker. in aiml 2005 conference cicc, cairo, egypt (pp. 65-71).\n",
      "qasemizadeh, b., ilkhani, a., & ganjeii, a. (2006, june). adaptive language independent spell checking using intelligent traverse on a tree. in cybernetics and intelligent systems, 2006 ieee conference on (pp. 1-6). ieee.\n",
      "license\n",
      "faspell by behrang qasemizadeh is licensed under a creative commons attribution 4.0 international license. based on a work at http://pars.ie/lr/faspell_dataset.\n",
      "inspiration:\n",
      "which kinds of misspellings occurs more often?\n",
      "are certain characters more likely to be misspelled? certain words?\n",
      "can you construct a finite state automaton spell checker for persian based on this data?\n",
      "context\n",
      "i found this dataset after reading a five thirty eight article. the author got a tally of every death and cuss word in tarantino's movies. that's no small feat considering the content of tarantino flicks! such endurance!\n",
      "content\n",
      "movie: film title\n",
      "type: whether the event was a profane word or a death\n",
      "word: the specific profane word, if the event was a word\n",
      "minutes_in: the number of minutes into the film the event occurred\n",
      "acknowledgements\n",
      "thanks to fivethirtyeight for throwing this dataset up on github and sharing with everyone.\n",
      "the original article can be found on fivethirtyeight's website here: https://fivethirtyeight.com/features/complete-catalog-curses-deaths-quentin-tarantino-films/\n",
      "and the dataset is can be found here: https://github.com/fivethirtyeight/data/tree/master/tarantino\n",
      "inspiration\n",
      "try some word counting and see how tarantino's murder:death:cuss ratios have changed over time. what are his favorite cuss words? which movies have the most deaths?\n",
      "shared under mit license\n",
      "context:\n",
      "on a typical day in the united states, police officers make more than 50,000 traffic stops. the stanford open policing project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\n",
      "if you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.\n",
      "content:\n",
      "this dataset includes 1.7 gb of stop data from south carolina, covering all of 2010 onwards. please see the data readme for the full details of the available fields.\n",
      "acknowledgements:\n",
      "this dataset was kindly made available by the stanford open policing project. if you use it for a research publication, please cite their working paper: e. pierson, c. simoiu, j. overgoor, s. corbett-davies, v. ramachandran, c. phillips, s. goel. (2017) “a large-scale analysis of racial disparities in police stops across the united states”.\n",
      "inspiration:\n",
      "how predictable are the stop rates? are there times and places that reliably generate stops?\n",
      "concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. can you identify any jurisdictions that may be exhibiting this behavior?\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "introduction\n",
      "this image dataset was collected as a part of my final year undergraduate project vehicle detection and road traffic congestion mapping using image processing. a total of 30 traffic videos, each of approx. 4 mins, from different streets of kathmandu were taken and images of vehicles were manually cropped out from the video frames.\n",
      "content\n",
      "total images: 4800 , no. of two-wheeler vehicles: 1811 , no. of four-wheeler vehicles: 2989 , size of each image: varies , resolution of each image: varies , image format: \".jpg\"\n",
      "acknowledgement\n",
      "this dataset wouldn't be here without the help of my project mates anup adhikari, binish koirala and sparsha bhattarai. thank you lads for your wonderful contribution.\n",
      "note\n",
      "since kaggle won't support uploading more than 1000 images in a compressed format, you can get the complete repository of the images from https://github.com/sdevkota007/vehicles-nepal-dataset if you have any question, please email me at sdevkota007@gmail.com\n",
      "the new york city bike share enables quick, easy, and affordable bike trips around the new york city boroughs. they make regular open data releases (this dataset is a transformed version of the data from this link). the dataset contains 735502 anonymised trips information made from jan 2015 to june 2017.\n",
      "acknowledgements -\n",
      "this dataset is the property of nyc bike share, llc and jersey city bike share, llc (“bikeshare”) operates new york city’s citi bike bicycle sharing service for t&c click here\n",
      "objectives -\n",
      "eda\n",
      "feature engineering\n",
      "predict gender of the riders\n",
      "context\n",
      "this dataset describes the monthly number of sales of shampoo over a 3-year period.\n",
      "content\n",
      "the units are a sales count and there are 36 observations. the original dataset is credited to makridakis, wheelwright, and hyndman (1998).\n",
      "acknowledgements\n",
      "source: time series data library (citing: makridakis, wheelwright and hyndman (1998))\n",
      "context\n",
      "between the years of 1982 and 2017, the state of texas has executed approximately 543 inmates. during this time, the tdcj(texas department of criminal justice) recorded data regarding each execution.\n",
      "content\n",
      "each row in the data set includes the executed inmate's age, last statement, date of his/her execution, first and last name, race and county. the data was scraped from the tdcj 's website: here\n",
      "acknowledgements\n",
      "thank you to the tdcj for recording this dataset\n",
      "inspiration\n",
      "i would like to see some analysis on the demographics of the prisoner and their last statement(or lack of one). is age associated with the length of the last statement? do the demographics of the prisoner have an association with whether or not the prisoner left a last statement? how many times, on average is the word \"sorry\" used?\n",
      "context\n",
      "north carolina school report cards provide an efficient method for comparing and reviewing student academic performance across all public schools in north carolina. the following data set combines school report card (spg) grades and scores with other school metadata gathered by the state and other local organizations.\n",
      "content\n",
      "school report card grades and scores for three consecutive school years of state testing (2013/14, 2014/15, and 2015/16). additional school metadata is also included (addresses, geo-codes, poverty indicators, transportation and budget information, etc...)\n",
      "acknowledgements\n",
      "all data sourced from public resources: https://ncreportcards.ondemand.sas.com/src/#/?_k=mpdibp\n",
      "for additional information, please consult a full data dictionary here: http://www.ncpublicschools.org/docs/src/researchers/data-dictionary.pdf\n",
      "special thanks to gmaps d. kahle and h. wickham. ggmap: spatial visualization with ggplot2. the r journal, 5(1), 144-161. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf\n",
      "inspiration\n",
      "this data set was constructed as part of a data science project related to my master's degree studies. some of my findings and basic information can be found here: https://ncschoolreportcard.wordpress.com/. additional ideas include:\n",
      "better visualizations for counties, cities, etc...\n",
      "prediction of future school performance\n",
      "feature engineering for school performance\n",
      "context\n",
      "this is the oil price and the share price of a few companies.\n",
      "content\n",
      "the share price contains the following columns: date,open,high,low,close,adj close,volume the oil price contains the following columns: date and brent oil price\n",
      "acknowledgements\n",
      "the oil price is obtained from the website of the u.s energy information administration: https://www.eia.gov/dnav/pet/hist/leafhandler.ashx?n=pet&s=rbrte&f=d\n",
      "share price dataset in a daily frequency from a few companies:\n",
      "to download the dataset i use yahoo finance. the following link is an example to download the share price dataset: https://uk.finance.yahoo.com/quote/rdsb.l/history?period1=946684800&period2=1499122800&interval=1d&filter=history&frequency=1d\n",
      "inspiration\n",
      "context\n",
      "this dataset contains all reported traffic accidents in new york 2016. the data is from new york open data. i have preprocessed the data to work well with the new york city taxi trip duration and the new york city taxi with osrm dataset. i have made made sure that the street names have the same format so that the datasets can be used together.\n",
      "context\n",
      "in order to optimize my estimate for the nyc taxi trip duration competition, i thought it would be helpful to include historical traffic volume data in various parts of nyc. this dataset was compiled from 2011 -2013, and consists of over 100 days in which traffic counts were recorded on an hourly basis for numerous locations (anywhere from a few to hundreds of locations).\n",
      "content\n",
      "the traffic counts are measured on a particular road (roadway name) from one intersection to another (from and to). the data set also specifies a direction of traffic that is being measured (direction - nb, sb, wb, eb).\n",
      "acknowledgements\n",
      "i found this dataset on the nyc open data(https://opendata.cityofnewyork.us/).\n",
      "inspiration\n",
      "my hope is that with these data one can better predict the patterns of traffic in nyc on hourly and daily basis.\n",
      "context\n",
      "census data is essential in gaining insights into how the population is distributed within a certain geographic area.\n",
      "content\n",
      "this census dataset contains details related to the housing of the population in california, including the number of people in a certain area, geographic area, their proximity to the bay and kind of house that the population is living in currently.\n",
      "acknowledgements\n",
      "i have taken this data from the link mentioned below https://github.com/ageron/handson-ml/tree/master/datasets/housing\n",
      "inspiration\n",
      "through this data i want to gain insight in to the housing prices in california.\n",
      "context\n",
      "this dataset contains data from a list of indian stocks in nse. it includes a collection of well performing stocks with all the data necessary to predict which stocks to buy, hold, or exit.\n",
      "acknowledgements\n",
      "i work in a stock research firm. this stock data is for all kaggle users to play and experiment with in order to learn more about stock research.\n",
      "inspiration\n",
      "the second column, \"category\", gives a list of all the stocks that a user needs to buy, hold, or exit . we challenge you to develop an algorithm to see if your result matches ours.\n",
      "question\n",
      "as per the 1992 ergonomics study, what is the optimum length of chopsticks usable by adults & children?\n",
      "acknowledgements\n",
      "an investigation for determining the optimum length of chopsticks. hsu sh, wu sp. appl ergon. 1991 dec;22(6):395-400. pmid: 15676839\n",
      "inspiration\n",
      "data sets shared by a beginner, for beginners :) thanks to david venturi and udacity, that i was able to get started off on this. more details here : link\n",
      "context\n",
      "this dataset was uploadedto be able to link the countries iso codes to any data in a better way than just names. this dataset can give the opportunity to improve current and new notebooks as well as other datasets. libraries like plotly use country codes to easily identify the data linked to the country. this dataset can help with that task.\n",
      "content\n",
      "the dataset contains a list of all the states and their codes. columns: - alpha-2 code: the alpha-2 code of the country (2 characters) - alpha-3 code: the alpha-3 code of the country (3 characters) - numeric code: the numeric code of the country (int) - iso 3166-2: the iso 3166-2 code. formatted as: iso 3166-2:[2 characters]\n",
      "acknowledgements\n",
      "https://gist.github.com/radcliff/f09c0f88344a7fcef373\n",
      "inspiration\n",
      "any dataset that contains a country column, can be linked to this dataset and be used to link other data, as well as plotting maps. libraries like plotly use country codes to easily identify the data linked to the country. this dataset can help with that task.\n",
      "data are acquired from mercedes benz monthly sales report. the price is the average msrp (aka sticker price) stands for the manufacturer suddested retail price.\n",
      "content\n",
      "over 10 years of historical exchange rate data of bric countries currencies/ u.s. dollar\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "online\n",
      "website\n",
      "github\n",
      "datahub\n",
      "sparql endpoint\n",
      "you can query some of the data online there. there is also the download link. of course you can download it here.\n",
      "context\n",
      "electronic medical records contain multi-format electronic medical data that consist of an abundance of medical knowledge. facing with patients symptoms, experienced caregivers make right medical decisions based on their professional knowledge that accurately grasps relationships between symptoms, diagnosis, and treatments. we aim to capture these relationships by constructing a large and high-quality heterogeneous graph linking patients, diseases, and drugs (pdd) in emrs.\n",
      "content\n",
      "specifically, we extract important medical entities from mimic-iii (medical information mart for intensive care iii) and automatically link them with the existing biomedical knowledge graphs, including icd-9 ontology and drugbank. the pdd graph presented is accessible on the web via the sparql endpoint, and provides a pathway for medical discovery and applications, such as effective treatment recommendations.\n",
      "a subgraph of pdd is illustrated in the followng figure to betterunderstand the pdd graph.\n",
      "acknowledgements\n",
      "author\n",
      "data set belongs to meng wang, jiaheng zhang, jun liu,wei hu, sen wang, , wenqiang liu and lei shi\n",
      "they come from： 1. moeklinns lab, xi’an jiaotong university, xi’an, china 2. state key laboratory for novel software technology, nanjing university, nanjing, china 3. griffith universtiy, gold coast campus, australia\n",
      "some email： - meng wang：wangmengsd@stu.xjtu.edu.cn - lei shi：xjtushilei@foxmail.com - jun liu：liukeen@xjtu.edu.cn\n",
      "research\n",
      "the paper is being reviewed and is not easily disclosed.so it can't be linked here.\n",
      "inspiration\n",
      "if you have any questions, please contact the email address above.\n",
      "do you have any suggestions ? and send them to an e-mail address above.\n",
      "license\n",
      "this work is licensed under a creative commons attribution 4.0 international license.\n",
      "if your article needs to be reference our work , you can reference our github.\n",
      "context\n",
      "based on fisher's linear discriminant model, this data set became a typical test case for many statistical classification techniques in machine learning such as support vector machines.\n",
      "content\n",
      "the iris flower data set or fisher's iris data set is a multivariate data set introduced by the british statistician and biologist ronald fisher in his 1936 paper the use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.[1] it is sometimes called anderson's iris data set because edgar anderson collected the data to quantify the morphologic variation of iris flowers of three related species.[2] two of the three species were collected in the gaspé peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".[3]\n",
      "the data set consists of 50 samples from each of three species of iris (iris setosa, iris virginica and iris versicolor). four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. based on the combination of these four features, fisher developed a linear discriminant model to distinguish the species from each other.\n",
      "acknowledgements\n",
      "description taken from wiki would like to thank dr. jason brownlee who has explained all the examples very nicely and clearly!\n",
      "context\n",
      "since july 1, 2016, connecticut has updated this nightly dataset of every inmate held in jail while awaiting trial. at the time of download, this dataset contains just over one year of data with 1132352 rows of data, where one row is one inmate.\n",
      "content\n",
      "field descriptions: * download date: date in which the data were extracted and reflecting the population for that day.\n",
      "idenitifier: individual inmate identifier\n",
      "latest admission date: most recent date in which the inmate has been admitted. in some instances, this may reflect an original date of admission to a correctional facility. generally, if a date is more than one year old, an inmate should not be considered to have been held for the entire duration of that time.\n",
      "race: race of inmate\n",
      "age: age of inmate\n",
      "bond amount: amount of bond for which the inmate is being held. in some instances, for particularly low (less than $100), this bond amount may be considered a place holder value\n",
      "offense: controlling offense for which the bond amount has been set.\n",
      "facility: department of correction facility where the inmate is currently held.\n",
      "detainer: denotes whether inmate is being held at the request of another criminal justice agency, or if another agency is to be notified upon release.\n",
      "acknowledgements\n",
      "thanks to [http://dataispluralc.om] for the tip on this dataset! this dataset was downloaded on july 26, 2017 - chekc the original source for more up-to-date data (updated nightly) [https://data.ct.gov/public-safety/accused-pre-trial-inmates-in-correctional-faciliti/b674-jy6w]\n",
      "inspiration\n",
      "this dataset contains information about inmate's race and the nature of the crimes. the minority report is a sci-fi story pretty well known for predicting crimes and arresting people before they happen. can you do the same? would you dare do the same?\n",
      "context\n",
      "the correlates of state policy project aims to compile, disseminate, and encourage the use of data relevant to u.s. state policy research, tracking policy differences across and change over time in the 50 states. we have gathered more than nine-hundred variables from various sources and assembled them into one large, useful dataset. we hope this project will become a “one-stop shop” for academics, policy analysts, students, and researchers looking for variables germane to the study of state policies and politics.\n",
      "content\n",
      "the correlates of state policy project includes more than nine-hundred variables, with observations across the u.s. 50 states and time (1900 – 2016). these variables represent policy outputs or political, social, or economic factors that may influence policy differences across the states. the codebook includes the variable name, a short description of the variable, the variable time frame, a longer description of the variable, and the variable source(s) and notes.\n",
      "take a look at the codebook pdf to get more information about each column\n",
      "acknowledgements\n",
      "this aggregated data set is only possible because many scholars and students have spent tireless hours creating, collecting, cleaning, and making data publicly available. thus if you use the dataset, please cite the original data sources.\n",
      "jordan, marty p. and matt grossmann. 2016. the correlates of state policy project v.1.10. east lansing, mi: institute for public policy and social research (ippsr).\n",
      "this dataset was originally downloaded from\n",
      "http://ippsr.msu.edu/public-policy/correlates-state-policy\n",
      "this data set contains concentrations of phytoplankton, protozoa, total bacteria and metabolically active bacteria assessed by flow cytometry on transects 12, 1, 3, 5, 7, 9 and 11 of the broke-west survey of the southern ocean between january and march 2006. only total bacterial concentrations were assesed for transect 11.\n",
      "between 4 and 12 depths were sampled for marine microbes and concentations were assesed using facscan flowcytometer. phytoplankton were identified and counted based on the autofluorescense of chlorophyll a when excited by the 488 nm laser of the facscan. protozoa were identified and counted after staining with the acid vacuole stain lysotracker green. total bacteria were identified and counted using the cell permeant syto 13 nucleic stain. metabolically active bacteria were identified and counted after staining for intracellular esterases with the esterase stain 6cfda.\n",
      "data collected by the australian antarctic division. sourced from: http://data.gov.au/dataset/broke-west-microbial-concentrations-voyage-3-of-the-aurora-australis-2005-2006\n",
      "context\n",
      "test, discover, suggest, and repeat.\n",
      "inspiration\n",
      "you inspire me. yes, you. no not behind you, i mean you. yes you. thats it. take credit. enjoy the feeling. relax. create a kernel now.\n",
      "acknowledgements\n",
      "licenses intact for due credit. enjoy the ride.\n",
      "this dataset does not have a description yet.\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "context\n",
      "basic habitation information: the data refers to the list of habitations, its population in different caste category (sc, st and general) and status of availability of potable drinking water (covered or partially covered) all over india.\n",
      "water quality affected habitations: the data refers to the list of drinking water quality affected habitations all over india due to contamination such as fluoride, arsenic, iron, salinity and nitrate.\n",
      "content\n",
      "basic habitation information from 2009 and 2012. water quality affected habitations from 2009 and 2012.\n",
      "acknowledgements\n",
      "ministry of drinking water and sanitation (mdws), govt of india has shared this data in open govt data platform india portal under govt. open data license - india.\n",
      "context\n",
      "as many others i have asked myself if it is possible to use machine learning in order to create valid predictions for football (soccer) match outcomes. hence i created a dataset consisting of historic match data for the german bundesliga (1st and 2nd division) as well as the english premier league reaching back as far as 1993 up to 2016. besides the mere information concerning goals scored and home/draw/away win the dataset also includes per site (team) data such as transfer value per team (pre-season), the squad strength, etc. unfortunately i was only able to find sources for these advanced attributes going back to the 2005 season.\n",
      "i have used this dataset with different machine learning algorithms including random forests, xgboost as well as different recurrent neural network architectures (in order to potentially identify recurring patterns in winning streaks, etc.). i'd like to share the approaches i used as separate kernels here as well. so far i did not manage to exceed an accuracy of 53% consistently on a validation set using 2016 season of bundesliga 1 (no information rate = 49%).\n",
      "although i have done some visual exploration before implementing the different machine learning approaches using tableau, i think a visual exploration kernel would be very beneficial.\n",
      "content\n",
      "the data comes as an sqlite file containing the following tables and fields:\n",
      "table: matches\n",
      "match_id (int): unique id per match\n",
      "div (str): identifies the division the match was played in (d1 = bundesliga, d2 = bundesliga 2, e0 = english premier league)\n",
      "season (int): season the match took place in (usually covering the period of august till may of the following year)\n",
      "date (str): date of the match\n",
      "hometeam (str): name of the home team\n",
      "awayteam (str): name of the away team\n",
      "fthg (int) (full time home goals): number of goals scored by the home team\n",
      "ftag (int) (full time away goals): number of goals scored by the away team\n",
      "ftr (str) (full time result): 3-way result of the match (h = home win, d = draw, a = away win)\n",
      "table: teams\n",
      "season (str): football season for which the data is valid\n",
      "teamname (str): name of the team the data concerns\n",
      "kaderhome (str): number of players in the squad\n",
      "avgagehome (str): average age of players\n",
      "foreignplayershome (str): number of foreign players (non-german, non-english respectively) playing for the team\n",
      "overallmarketvaluehome (str): overall market value of the team pre-season in eur (based on data from transfermarkt.de)\n",
      "avgmarketvaluehome (str): average market value (per player) of the team pre-season in eur (based on data from transfermarkt.de)\n",
      "stadiumcapacity (str): maximum stadium capacity of the team's home stadium\n",
      "table: unique teams\n",
      "teamname (str): name of a team\n",
      "unique_team_id (int): unique identifier for each team\n",
      "table: teams_in_matches\n",
      "match_id (int): unique match id\n",
      "unique_team_id (int): unique team id (this table is used to easily retrieve each match a given team has played in)\n",
      "based on these tables i created a couple of views which i used as input for my machine learning models:\n",
      "view: flatview\n",
      "combination of all matches with the respective additional data from teams table for both home and away team.\n",
      "view: flatview_advanced\n",
      "same as flatview but also includes unique_team_id and unique_team in order to easily retrieve all matches played by a team in chronological order.\n",
      "view: flatview_chrono_teamorder_reduced\n",
      "similar to flatview_advanced, however missing the additional attributes from team in order to have a longer history including years 1993 - 2004. especially interesting if one is only interested in analyzing winning/loosing streaks.\n",
      "acknowledgements\n",
      "thanks to football-data.co.uk and transfermarkt.de for providing the raw data used in this dataset.\n",
      "inspiration\n",
      "please feel free to use the humble dataset provided here for any purpose you want. to me it would be most interesting if others think that recurrent neural networks could in fact be of help (and even maybe outperform classical feature engineering) in identifying streaks of losses and wins. in the literature i mostly only found example of rnn application where the data were time series in a very narrow sense (e.g. temperature measurements over time) hence it would be interesting to get your input on this question.\n",
      "maybe someone also finds additional attributes per team or match which have substantial impact on match outcome. so far i have found the \"market value\" of a team to be by far the best predictor when two teams face each other, which makes sense as the market value usually tends to correlate closely with the strength of a team and it's propects at winning\n",
      "context\n",
      "crimes reported in baton rouge and handled by the baton rouge police department. crimes include burglaries (vehicle, residential and non-residential), robberies (individual and business), theft, narcotics, vice crimes, assault, nuisance, battery, firearm, homicides, criminal damage to property, sexual assaults and juvenile.\n",
      "content\n",
      "dataset only includes records through september 21st, 2017\n",
      "columns included: file number, offense date, offense time, crime, committed, offense, offense desc, address, st number, st dir, st name, st type, city, state, zip, district, zone, subzone, complete district, geolocation\n",
      "acknowledgements\n",
      "this public domain data is provided by open data br through socrata. see this dataset's official page for more information. public domain licensed banner image provided by goodfreephotos.com.\n",
      "data set name: hepatocellular carcinoma dataset (hcc dataset)\n",
      "abstract: hepatocellular carcinoma dataset (hcc dataset) was collected at a university hospital in portugal. it contains real clinical data of 165 patients diagnosed with hcc.\n",
      "donors: miriam seoane santos (miriams@student.dei.uc.pt) and pedro henriques abreu (pha@dei.uc.pt), department of informatics engineering, faculty of sciences and technology, university of coimbra armando carvalho (aspcarvalho@gmail.com) and adélia simão (adeliasimao@gmail.com), internal medicine service, hospital and university centre of coimbra\n",
      "data type: multivariate task: classification, regression, clustering, casual discovery attribute type: categorical, integer and real\n",
      "area: life sciences format type: matrix missing values: yes\n",
      "instances and attributes: number of instances (records in your data set): 165 number of attributes (fields within each record): 49\n",
      "relevant information: hcc dataset was obtained at a university hospital in portugal and contais several demographic, risk factors, laboratory and overall survival features of 165 real patients diagnosed with hcc. the dataset contains 49 features selected according to the easl-eortc (european association for the study of the liver - european organisation for research and treatment of cancer) clinical practice guidelines, which are the current state-of-the-art on the management of hcc.\n",
      "this is an heterogeneous dataset, with 23 quantitative variables, and 26 qualitative variables. overall, missing data represents 10.22% of the whole dataset and only eight patients have complete information in all fields (4.85%). the target variables is the survival at 1 year, and was encoded as a binary variable: 0 (dies) and 1 (lives). a certain degree of class-imbalance is also present (63 cases labeled as “dies” and 102 as “lives”).\n",
      "a detailed description of the hcc dataset (feature’s type/scale, range, mean/mode and missing data percentages) is provided in santos et al. “a new cluster-based oversampling method for improving survival prediction of hepatocellular carcinoma patients”, journal of biomedical informatics, 58, 49-59, 2015.\n",
      "source:\n",
      "the dataset was created by angeliki xifara (angxifara '@' gmail.com, civil/structural engineer) and was processed by athanasios tsanas (tsanasthanasis '@' gmail.com, oxford centre for industrial and applied mathematics, university of oxford, uk).\n",
      "data set information:\n",
      "we perform energy analysis using 12 different building shapes simulated in ecotect. the buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. we simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. the dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. it can also be used as a multi-class classification problem if the response is rounded to the nearest integer.\n",
      "attribute information:\n",
      "the dataset contains eight attributes (or features, denoted by x1...x8) and two responses (or outcomes, denoted by y1 and y2). the aim is to use the eight features to predict each of the two responses.\n",
      "specifically: x1 relative compactness x2 surface area x3 wall area x4 roof area x5 overall height x6 orientation x7 glazing area x8 glazing area distribution y1 heating load y2 cooling load\n",
      "relevant papers:\n",
      "a. tsanas, a. xifara: 'accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools', energy and buildings, vol. 49, pp. 560-567, 2012\n",
      "citation request:\n",
      "a. tsanas, a. xifara: 'accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools', energy and buildings, vol. 49, pp. 560-567, 2012 (the paper can be accessed from [web link])\n",
      "for further details on the data analysis methodology: a. tsanas, 'accurate telemonitoring of parkinsonâ€™s disease symptom severity using nonlinear speech signal processing and statistical machine learning', d.phil. thesis, university of oxford, 2012 (which can be accessed from [web link])\n",
      "context:\n",
      "\"tulip mania, tulipmania, or tulipomania (dutch names include: tulpenmanie, tulpomanie, tulpenwoede, tulpengekte and bollengekte) was a period in the dutch golden age during which contract prices for bulbs of the recently introduced tulip reached extraordinarily high levels and then dramatically collapsed in february 1637. it is generally considered the first recorded speculative bubble (or economic bubble).\" -- from wikipedia, cc by-sa\n",
      "market forecasting is difficult. there are many factors that may affect the market, and a high degree of uncertainty. one thing that some researchers have been investigating is whether natural language processing (nlp) of news texts can help with market forecasting. recent publications suggest that it can be.\n",
      "peng, y., & jiang, h. (2016). leverage financial news to predict stock price movements using word embeddings and deep neural networks. in proceedings of naacl-hlt (pp. 374-379).\n",
      "fraiberger, s. p. (2016). news sentiment and cross-country fluctuations. nlp+ css 2016, 125.\n",
      "this dataset an interesting test case for these methodologies. it contains dutch-language newspapers from the years immediately preceding and following tulip mania. can you use nlp techniques to model the tulip market over time?\n",
      "content:\n",
      "this dataset contains the texts of 8,559 newspaper deliveries from the 17th century, from june 14th, 1618 to december 31, 1699. the text is in dutch. since the text was scraped from old newspapers using ocr (optical character recognition), there are some errors in the text.\n",
      "acknowledgments:\n",
      "this dataset was compiled by delpher, an archive service provided by the national library of the netherlands. it is provided under a cc-by 4.0 license. for more information, and newspapers from other years, please visit their website (in dutch). if you use this dataset in your work, please include this citation:\n",
      "delpher open newspaper archive (1.0). creative commons attribution 4.0 , the hague, 2017 .\n",
      "context:\n",
      "each year, after the president's state of the union address, the office of management and budget (omb) releases the administration's budget, offering proposals on key priorities and newly announced initiatives. in 2016 & 2017 obama’s omb released all of the data included in the president's budget in a machine-readable format here on github. “the budget process should be a reflection of our values as a country, so we think it's important that members of the public have as many tools as possible to see the data behind the president's proposals. and, if people are motivated to create their own visualizations or products from the data, they should have that chance as well.”\n",
      "content:\n",
      "this branch includes three data files that contain an extract of the office of management and budget (omb) budget database. these files can be used to reproduce many of the totals published in the budget and examine unpublished details below the levels of aggregation published in the budget. the user guide file contains detailed information about this data, its format, and its limitations.\n",
      "acknowledgements:\n",
      "datasets were compiled by obama white house officials and released at this github repo.\n",
      "inspiration:\n",
      "what significant changes were there between 2016 and 2017 proposals?\n",
      "how was the federal budget distributed across agencies?\n",
      "where there any interesting changes in federal receipts?\n",
      "上证综合指数前复权日线数据\n",
      "context\n",
      "this dataset has been uploaded primarily to help me, a novice learning python, to practice coding before attempting the tutorials on kaggle. by all means, anyone may make use of it. for my part, i'm trying to code a simple program that will print all prayers applicable to whatever sets of rosary mysteries someone wishes to pray.\n",
      "content\n",
      "a traditional dominican crown rosary comprises 15 'decades' of 10 hail mary prayers with a few others. in addition, there are variable introductory prayers ('the drop') and concluding prayers. these 15 decades are made up of three groups of 'mysteries' - joyful, sorrowful and glorious. usually, one prays a five decade rosary focusing on just one group, according to the day of the week. one csv file contains the prayers, and the other contains the mysteries.\n",
      "acknowledgements\n",
      "i cobbled this public domain prayers from various sites; a few good ones are:\n",
      "http://www.preces-latinae.org/thesaurus/bvm/rosarium.html https://www.fisheaters.com/rosary.html\n",
      "inspiration\n",
      "i've been having difficulties making simple input scripts work in python (repeated eof errors), asking the user to indicate how many decades he/she wishes to pray, and which set of mysteries should be prayed first. i have some private code uploaded, but would be interested in how others do it.\n",
      "this file contains \"24-hour\" and \"48-hour\" reports of independent expenditures filed during the current election cycle and for election cycles through 2010. the file contains detailed information about independent expenditures, including who was paid, the purpose of the disbursement, date and amount of the expenditure and the candidate for or against whom the expenditure was made.\n",
      "independent expenditures represent spending by individual people, groups, political committees, corporations or unions expressly advocating the election or defeat of clearly identified federal candidates. these expenditures may not be made in concert or cooperation with or at the request or suggestion of a candidate, the candidate's campaign or a political party.\n",
      "any time up to 20 days before an election, if these independent expenditures by a person or organization aggregate more than $10,000 in a race they must be reported to the commission before the end of the second day after the communication is publicly distributed. if the communications are distributed within the last 19 days before the election, the expenditure must be reported within one day if they aggregate more than $1,000 in any race.\n",
      "acknowledgements\n",
      "this data comes from the us federal election commission. you can find the original dataset here.\n",
      "if you like...\n",
      "if you enjoyed this dataset, you might also like the congressional election disbursements dataset.\n",
      "context:\n",
      "thor is a painstakingly cultivated database of historic aerial bombings from world war i through vietnam. thor has already proven useful in finding unexploded ordinance in southeast asia and improving air force combat tactics. our goal is to see where public discourse and innovation takes this data. each theater of warfare has a separate data file, in addition to a thor overview.\n",
      "content:\n",
      "by june 1950, the u.s. air force had constructed a comprehensive historical program. over half the records in the air force historical archives consisted of world war ii artifacts, including unit histories and combat reports compiled by field historians as they received a steady flow of documents from operational squadrons and wings. the archives team developed experience pouring through intelligence reports, target folders, bomb damage assessments, and statistics to develop hard earned lessons on modern warfare. so from the first day of combat, 25 june, historians embedded within operational commands in korea knew recording events from the start would be important. in particular, albert f. simpson, the archives' director, picked up the phone and directly called the headquarters of the far east air forces (feaf) to request they begin collecting data on all sorties generated in theater. their statistical services agreed, and began regularly sending typed reports on 20 essential data items:\n",
      "group and squadron designations\n",
      "operating base location\n",
      "type and model of aircraft\n",
      "aborted, airborne, and effective sorties\n",
      "number of aircraft lost or damaged to enemy ground, aircraft, or other action\n",
      "personnel killed, wounded, or missing in action\n",
      "number of enemy aircraft destroyed or damaged\n",
      "number of bombs, rockets, and bullets expended\n",
      "read more here on the exteter database and consult the data dictionary here.\n",
      "acknowledgements:\n",
      "thor is a dataset project initiated by lt col jenns robertson and continued in partnership with data.mil, an experimental project, created by the defense digital service in collaboration with the deputy chief management officer and data owners throughout the u.s. military.\n",
      "inspiration:\n",
      "which campaigns saw the heaviest bombings?\n",
      "which months saw the most runs?\n",
      "this dataset contains run time statistics and details about scores for the second development round of nips 2017 adversarial learning competition\n",
      "content\n",
      "matrices with intermediate results\n",
      "following matrices with intermediate results are provided:\n",
      "accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense\n",
      "error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense\n",
      "hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense\n",
      "in each of these matrices, rows correspond to defenses, columns correspond to attack. also first row and column are headers with kaggle team ids (or baseline id).\n",
      "scores and run time statistics of submissions\n",
      "following files contain scores and run time stats of the submissions:\n",
      "non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks\n",
      "targeted_attack_results.csv - scores and run time statistics of all targeted attacks\n",
      "defense_results.csv - scores and run time statistics of all defenses\n",
      "each row of these files correspond to one submission. columns have following meaning:\n",
      "kaggleteamid - either kaggle team id or id of the baseline.\n",
      "teamname - human readable team name\n",
      "score - raw score of the submission\n",
      "normalizedscore - normalized (to be between 0 and 1) score of the submission\n",
      "minevaltime - minimum evaluation time of 100 images\n",
      "maxevaltime - maximum evaluation time of 100 images\n",
      "medianevaltime - median evaluation time of 100 images\n",
      "meanevaltime - average evaluation time of 100 images\n",
      "notes about the data\n",
      "due to team mergers, team name in these files might be different from the leaderboard.\n",
      "not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).\n",
      "few targeted and non-targeted attacks exceeded 500 seconds time limit on all batches of images. these submissions received score 0 in the official leaderboard. we still were able to compute \"real\" score for these submissions and include it into non_targeted_attack_results.csv and targeted_attack_results.csv files. however these scores are negated in the provided files to emphasize that these submissions violate the time limit.\n",
      "\"north american slave narratives\" collects books and articles that document the individual and collective story of african americans struggling for freedom and human rights in the eighteenth, nineteenth, and early twentieth centuries. this collection includes all the existing autobiographical narratives of fugitive and former slaves published as broadsides, pamphlets, or books in english up to 1920. also included are many of the biographies of fugitive and former slaves and some significant fictionalized slave narratives published in english before 1920.\n",
      "context\n",
      "the north american slave narratives collection at the university of north carolina contains 344 items and is the most extensive collection of such documents in the world.\n",
      "the physical collection was digitized and transcribed by students and library employees. this means that the text is far more reliable than uncorrected ocr output which is common in digitized archives.\n",
      "more information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh\n",
      "the plain text files have been optimized for use in voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. you may wish to delete these in order to focus your analysis on just the narratives.\n",
      "the .csv file acts as a table of contents for the collection and includes title, author, publication date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with voyant: http://voyant-tools.org/).\n",
      "copyright statement and acknowledgements\n",
      "with the exception of \"fields's observation: the slave narrative of a nineteenth-century virginian,\" which has no known rights, the texts, encoding, and metadata available in open docsouth are made available for use under the terms of a creative commons attribution license (cc by 4.0:http://creativecommons.org/licenses/by/4.0/). users are free to copy, share, adapt, and re-publish any of the content in open docsouth as long as they credit the university library at the university of north carolina at chapel hill for making this material available.\n",
      "if you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. send any feedback to wilsonlibrary@unc.edu.\n",
      "about the docsouth data project\n",
      "doc south data provides access to some of the documenting the american south collections in formats that work well with common text mining and data analysis tools.\n",
      "documenting the american south is one of the longest running digital publishing initiatives at the university of north carolina. it was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.\n",
      "doc south data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. we have made it easy to use tools such as voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis.\n",
      "\"the church in the southern black community\" collects autobiographies, biographies, church documents, sermons, histories, encyclopedias, and other published materials. these texts present a collected history of the way southern african americans experienced and transformed protestant christianity into the central institution of community life. coverage begins with white churches' conversion efforts, especially in the post-revolutionary period, and depicts the tensions and contradictions between the egalitarian potential of evangelical christianity and the realities of slavery. it focuses, through slave narratives and observations by other african american authors, on how the black community adapted evangelical christianity, making it a metaphor for freedom, community, and personal survival.\n",
      "context\n",
      "the north american slave narratives collection at the university of north carolina contains 344 items and is the most extensive collection of such documents in the world.\n",
      "the physical collection was digitized and transcribed by students and library employees. this means that the text is far more reliable than uncorrected ocr output which is common in digitized archives.\n",
      "more information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh\n",
      "the plain text files have been optimized for use in voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. you may wish to delete these in order to focus your analysis on just the narratives.\n",
      "the .csv file acts as a table of contents for the collection and includes title, author, publication date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with voyant: http://voyant-tools.org/).\n",
      "copyright statement and acknowledgements\n",
      "with the exception of \"fields's observation: the slave narrative of a nineteenth-century virginian,\" which has no known rights, the texts, encoding, and metadata available in open docsouth are made available for use under the terms of a creative commons attribution license (cc by 4.0:http://creativecommons.org/licenses/by/4.0/). users are free to copy, share, adapt, and re-publish any of the content in open docsouth as long as they credit the university library at the university of north carolina at chapel hill for making this material available.\n",
      "if you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. send any feedback to wilsonlibrary@unc.edu.\n",
      "about the docsouth data project\n",
      "doc south data provides access to some of the documenting the american south collections in formats that work well with common text mining and data analysis tools.\n",
      "documenting the american south is one of the longest running digital publishing initiatives at the university of north carolina. it was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.\n",
      "doc south data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. we have made it easy to use tools such as voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis.\n",
      "context:\n",
      "the city of new york issues certificates of occupancy to newly constructed (and newly reconstructed, e.g. “gut renovated”) buildings in new york city. these documents assert that the city has deemed the building habitable and safe to move into.\n",
      "content:\n",
      "this dataset includes all temporary (expirable) and final (permanent) certificates of occupancies issues to newly habitable buildings in new york city, split between new (job type: nb) and reconstructed (job type: a1) buildings, issued between july 12, 2012 and august 29, 2017.\n",
      "acknowledgements:\n",
      "this data is published as-is by the new york city department of buildings.\n",
      "inspiration:\n",
      "in what areas of new york city are the newly constructed buildings concentrated?\n",
      "what is the difference in distribution between buildings that are newly built and ones that are newly rebuilt?\n",
      "in combination with the new york city buildings database dataset, what are notable differences in physical characteristics between recently constructed buildings and existing ones?\n",
      "context\n",
      "the vast majority of food and food ingredients eaten today is processed in some way before they arrived at the kitchen or dinner table. food processing equipment may leave trace amounts of various industrial chemical compounds in the foods we eat, and these chemicals, classed indirect food additives, are regulated by the united states food and drug administration. this dataset is a list of indirect food additives approved by the fda.\n",
      "content\n",
      "this dataset contains the names of chemical compounds and references to the federal government regulatory code approving and controlling their usage.\n",
      "acknowledgements\n",
      "this dataset is published by the fda and available online as a for-excel csv file. a few errant header columns have been cleaned up prior to upload to kaggle, but otherwise the dataset is published as-is.\n",
      "inspiration\n",
      "what tokens most commonly appear amongst the names contained in this list?\n",
      "any identifiable elements or compounds?\n",
      "context\n",
      "industry contractors that work for or with the united states department of defense and comes into contact with secret or privileged information must submit to a background check by the government as a part of their contractual obligations. any employee who fails to get the necessary clearance will be unable to work.\n",
      "employees may however appeal their decision; in this case the decision will be reviewed and finalized (or reversed) by the department of defense office of hearings and appeals (doha). this dataset contains summaries of the deliberations and results of such hearings, and provides a window into getting security clearance to work as a defense contractor in the united states.\n",
      "content\n",
      "this data contains dates, case numbers, decisions, and decisions summaries for over 20,000 cases submitted for review between late 1996 and early 2016.\n",
      "acknowledgements\n",
      "this data was published in an html format by the us department of defense. it has been converted into a csv format before upload to kaggle.\n",
      "inspiration\n",
      "what percentage of appeals were declined or upheld?\n",
      "what were the dominant reasons decisions were made? have the factors behind decisions changed over times?\n",
      "what kinds of words appear in decision texts?\n",
      "a sample of betfair data, normally available to those who spend a lot of money wagering. all sports except horse racing (for horse racing, there is a twin dataset at https://www.kaggle.com/zygmunt/betfair-horses).\n",
      "see http://data.betfair.com/ for a description.\n",
      "the file has 1306731 data rows. it is 321 mb uncompressed.\n",
      "full list of sport ids is available at http://data.betfair.com/sportids.htm. sports present in this file are:\n",
      "1 - soccer\n",
      "2 - tennis\n",
      "3 - golf\n",
      "4 - cricket\n",
      "5 - rugby union\n",
      "6 - boxing\n",
      "8 - motor sport\n",
      "10 - special bets\n",
      "11 - cycling\n",
      "1477 - rugby league\n",
      "3503 - darts\n",
      "3988 - athletics\n",
      "4339 - greyhound racing\n",
      "6231 - financial bets\n",
      "6422 - snooker\n",
      "6423 - american football\n",
      "7511 - baseball\n",
      "7522 - basketball\n",
      "7524 - ice hockey\n",
      "61420 - australian rules\n",
      "104049 - ? [1 row]\n",
      "468328 - handball\n",
      "998917 - volleyball\n",
      "2152880 - gaelic games\n",
      "26420387 - ufc\n",
      "columns:\n",
      "event_id\n",
      "full_description\n",
      "scheduled_off\n",
      "event\n",
      "actual_off\n",
      "selection\n",
      "settled_date\n",
      "odds\n",
      "latest_taken (when these odds were last matched on the selection)\n",
      "first_taken (when these odds were first matched on the selection)\n",
      "in_play (ip - in-play, pe - pre-event, ni - event did not go in-play)\n",
      "number_bets (number of individual bets placed)\n",
      "volume_matched (sums the stakes of both back and lay bets)\n",
      "sports_id\n",
      "selection_id\n",
      "win_flag (1 if the selection was paid out as a full or partial winner, 0 otherwise)\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "the daikon corpus was created during the diachronic text evaluation task in semeval-2015. the task was to create a system that can date a piece of text.\n",
      "for example, given a text snippet:\n",
      "“dictator saddam hussein ordered his troops to march into kuwait. after the invasion is condemned by the un security council, the us has forged a coalition with allies. today american troops are sent to saudi arabia in operation desert shield, protecting saudi arabia from possible attack.”\n",
      "the text has clear temporal evidence with reference to a\n",
      "historical figure (“saddam hussein”),\n",
      "notable organization (“un security council”)\n",
      "factual event (“operation desert shield”).\n",
      "historically, we know that\n",
      "saddam hussein lived between 1937 to 2006,\n",
      "un security council has existed since 1946\n",
      "operation desert shield (i.e. the gulf war) occurred between 1990-1991\n",
      "given the specific chronic deicticity (“today”) that indicates that the text is published during the gulf war, we can conceive that the text snippet should be dated 1990-1991.\n",
      "content\n",
      "the daikon corpus is made up of articles from the british spectator news magazine from year 828 to 2008.\n",
      "the corpus contains 24,280 articles with 19 million tokens; the token count is calculated by summing the number of whitespaces plus 1 for each paragraph.\n",
      "the daikon corpus is saved in the json format, where the outer most-structure is a list and the inner data structure is a key-value dictionary/hashmap that contains the:\n",
      "url: url where the original article resides\n",
      "date: date of the article\n",
      "body: a list of paragraphs\n",
      "title: title of the text\n",
      "note: if the url is broken, try removing the .html suffix of the url. e.g. change\n",
      "http://archive.spectator.co.uk/article/24th-september-2005/57/doctor-in-the-house.html \n",
      "to\n",
      "http://archive.spectator.co.uk/article/24th-september-2005/57/doctor-in-the-house\n",
      "citations\n",
      "liling tan and noam ordan. 2015. \n",
      "usaar-chronos:  crawling the web for temporal annotations. \n",
      "in proceedings of ninth international workshop on \n",
      "semantic evaluation (semeval 2015). denver, usa.\n",
      "task reference:\n",
      "octavian popescu and carlo strapparava. \n",
      "semeval 2015, task 7: diachronic text evaluation. \n",
      "in proceedings of ninth international workshop on \n",
      "semantic evaluation (semeval 2015). denver, usa.\n",
      "dataset image comes from jonathan pielmayer\n",
      "inspiration\n",
      "let's make an artificially intelligent \"flynn carsen\" !!\n",
      "context\n",
      "this is data on reg cf. reg cf is a form of crowdfunding that enables start-ups and small businesses to seek money directly from both accredited and non-accredited investors.\n",
      "content\n",
      "the data is taken directly from the edgar website. the data is from august 3rd, 2017.\n",
      "the data is in the reg\n",
      "inspiration\n",
      "although some sites have sections catered to reg cf data, i have not yet seen any place that also includes the financial information of all the companies that have filed. this dataset is an attempt to aggregate both to see if any insights can be drawn.\n",
      "context\n",
      "hey everyone out there! wikipedia is a publicly available encyclopedia which can be modified by anyone. some of these modifications are useful whereas some are not. this data set captures all the edits done to english wikipedia by anyone across the globe. as there are two edits per second, the data which i have collected is for just 20 minutes.\n",
      "content\n",
      "i have revised the original data set, removed the duplicates and included only the relevant and useful columns. this data set has below mentioned columns: a) action : only edits action is captured. other actions maybe talk, etc. b) change_size : the number of characters added or deleted. positive size means the change was added and negative means the change was deleted. c) geo_ip : this is null if the user is registered in wikipedia otherwise it is a json object containing city, latitude, country_name, region_name and longitude d) is_anonymous : this is a flag/boolean value(true/false) that notifies whether the user is registered or unregistered(anonymous) e) is_bot : this flag/boolean value(true/false) determines if the user is a bot(robot) or a human. f) is_minor: thus flag/boolean value(true/false) identifies whether the change made to wikipedia article was minor or major one. g) page_title : this is the title of the wikipedia article edited by the user. h) url : this field has the url or link which compares the wikipedia article before and after the change. i) user : if the user is unregistered, this field will have ip address either in ipv4 or ipv6 format and if the user is register it will contain the username used when registering on wikipedia.\n",
      "acknowledgements\n",
      "i would like to thank hatnote.com from which i could get this data. if you need the original data you may visit www.hatnote.com or directly connect this websocket - ws://wikimon.hatnote.com/en/\n",
      "context\n",
      "there are 4933 pharmacies in belgium, and each pharmacy (in groups) are obliged to create a network of night-guard pharmacies covering complete belgium. compare it with a hospital that has 2000 nurses and want's to distribute the burden of the 'night' shift or 'weekend' shifts over the 2000 nurses on an 'equal foot' basis, but here we add a geographical aspect. so its a maximal covering location problem combined with an typical 'personel' planning problem\n",
      "the challenge...\n",
      "the distribution of the pharmacies follows certain rules: 11million inhabitants having access to 5000 pharmacies, you can estimate that each pharmacy serves 2200 inhabitants. this is approximately true. you see a glimpse of the guard kalender : blackpoints, feast days, day/night guard (sun/moon)\n",
      "each pharmacy is equal and has to do equal number of guards. that is in this description rounded 12 days guard. we give each pharmacy a guard-capital. meaning when at the end of the year one pharmacy has done 10 days guard, the next year the pharmacy has to do two days guard more. on average each pharmacy is doing 12 days per year. so starting with an equal guard capital. we try to minimize the difference from the mean (mse).\n",
      "the guard is divided in a day part from 9:00u-22:00 and from 22:00pm to 9:00am as night guard. each pharmacy can choose to do guard during 1 day, having 12 days and 6 nights distributed over the year. with at least 2 sunday guards per year and one sunday night. or each pharmacy can choose to do his guard in blocks of 4 weekdays (mo-tu-we-th /// fr-sa-so) doing at least two midweek blocks and two weekendblocks ending up with 2 day's too much guard capital. from those blocks he get alternating fe the mo and we a nightshift. or by example the fr/so or sa as nightshift . the nightshifts are also equally distributed. the choice for midweek/weekend or day guard is a freedom indicated in the database. we filled in a random example. usually the freedom collides with regions. so a dayblock and a nightblock each get one guard point.\n",
      "each customer has to find a pharmacy within 20 minutes from his home. on average this rule is easily obeyed, since its possible to find 3-5 pharmacies within 20 minutes in 'city' zones. its only in very rural zones this rule can be violated. we use a giss database to correctly calculate the distance and travelling time between each pharmacy. you can use google-api or haversine, internally we have exact data. but within this proof of concept this doesn't matter. actually highway's are draining more people to a pharmacy, and the algorithm shows the pharmacy as a faster alternative than geographic haversine closer pharmacies. so a very fine tuned model takes this driving speed into account in function of that 20 minutes rule. but here the haversine distance between the closest clusters should give a good approximation. it actually counts only for the case where the 20 minutes rule is 'violated'\n",
      "if we divide belgium in 165 clusters there are 30 pharmacies per cluster. each cluster has 1 pharmacy available for 66000 inhabitants within 20 minutes. this during daytime. (daytime is defined until 22:00u) at night the scheme halves. 82 clusters, with 60 pharmacies per cluster. each night cluster has then 132.000 inhabitants. the same rule each cluster has 1 pharmacy available. we search to maximize the distance between each guard-pharmacy , so that there is an maximal spread for the guard. this guarantees that customers find very fast a pharmacy. if you think about it,on the belgium card you can superimpose a 'grid' that is shifting each day and each night selecting a pharmacy in the intersections of the grid. the only interfering element here is that 50% of the pharmacies chooses to have guard in weekend/midweek scheme, and 50% wants day/night guards, hence you have to swap the guard between neighbouring pharmacy's, so the distance rule remains respected.\n",
      "each pharmacy can block 3 weeks of vacation, that is typical during school vacations periods that pharmacy's tend to block periods. we call it black-points; you can generate random 3 weeks school/holiday vacation weeks that are blocked for each pharmacy. the database is filled in with a manual created sample.. actually the pharmacist can block 3weeks, or 6 weekend and mid/week blocks. here i simplified to three week (number of week , week of year)\n",
      "doing guard on a holiday like christmas, new year, eastern, sinksen, national feast day, is rewarded with an extra guard capital point. those pharmacies can do as such 1 day less guard. this as a last twitch\n",
      "what do we need at the end\n",
      "1° a database of all pharmacy's and their guard capital points. we tend to minimize the difference with the mean capital points. and usually the current algorithm selects the first the pharmacies in a cluster with the lowest capital points as prime candidate.\n",
      "2° a list of guards for all the 165clusters for all 365days, or 60225 guards per year. and a measurement that estimates the distance between all pharmacies. for that day.. the mean square error of the distance between the pharmacies has to be minimized\n",
      "context\n",
      "the production of steel has shifted from a just few primary countries to many countries all over the world. this dataset provides statistics and insight into the locations and volumes of steel production for years 2011-2016 with specific ranking in 2015 and 2016.\n",
      "acknowledgements\n",
      "the data source is worldsteel association. www.worldsteel.org\n",
      "inspiration\n",
      "some questions are trends, predictive analytics/forecasting/consolidation possibilities and global supply chain. while the steel industry is 'flat' where steel is produced and shipped globally. there may an opportunity for a new model where steel is increasingly produced locally to save shipping and logistics costs.\n",
      "this dataset includes the applicable tariff rates and statistical categories for all merchandise imported into the united states. it is based on the international harmonized system, the global system of nomenclature that is used to describe most world trade in goods.\n",
      "although the usitc publishes and maintains the htsa in its various forms, customs and border protection is the only agency that can provide legally binding advice or rulings on classification of imports. contact your nearest customs office with questions about how potential imports should be classified. for a binding ruling on classification, contact the bureau of customs and border protection.\n",
      "content\n",
      "the csv is a somewhat condensed version of a series of pdf documents. the row by row contents are generally comprehensive, but the pdf chapters often contain general information that is not included here.\n",
      "acknowledgements\n",
      "this dataset was made available by the united states international trade commission. you can find the original dataset, updated regularly, here.\n",
      "waroftherebellion is an annotated corpus of data from war of the rebellion (a large set of american civil war archives). it was built using geoannotate.\n",
      "it consists of two parts: a toponym corpus and a document-geolocation corpus.\n",
      "document geolocation corpus\n",
      "the document geolocation corpus is found in two json files.\n",
      "wotr-docgeo-jan-5-2016-625pm-by-vol.json gives the spans by volume.\n",
      "wotr-docgeo-jan-5-2016-625pm-80-0-20-by-split.json gives the spans by split, with an 80-20 training/test split.\n",
      "in both cases, the json data for an individual span consists of the following information:\n",
      "the volume number, from war of the rebellion.\n",
      "the span character offsets, from corrected ocr'd text.\n",
      "the text of the span in question.\n",
      "the counts of individual words, using the tokenization algorithm followed in the paper (fixme, name of paper). they are stored in a string, with a space separating word-count pairs and a colon separating the word from the count. the word itself is url-encoded, i.e. a colon is represented as %3a and a percent character as %25.\n",
      "the date (if available), extracted from the text using regular expressions.\n",
      "the full geojson of the points and polygons annotated for the span.\n",
      "the centroid of the points and polygons, computed first by taking the centroid of each polygon and then taking the centroid of the resulting set of annotated points and polygon-centroid points. the centroid is in the form of a size-2 array of longitude and latitude (the same as how points are stored in geojson).\n",
      "toponym corpus\n",
      "the toponym corpus, otherwise known as wotr-topo, is given in two different formats. the first format is json format files, split into train and test. geographic information for toponyms is given by the geojson standard, with annotations done in a stand-off style.\n",
      "not everything that has been annotated is guaranteed to be correct. the creators encourage others to correct errors that they find in a branched repository and submit pull requests when corrections are made.\n",
      "for questions regarding the corpus, please contact its creators ben wing (ben@benwing.com) and grant delozier (grantdelozier@gmail.com). this data is reproduced here under the mit license. please see the file “license” for more information.\n",
      "the acl law paper describing the corpus and performing benchmark evaluation\n",
      "content:\n",
      "kwici is a 4m-word corpus drawn from the welsh wikipedia as it was on 30 december 2013.\n",
      "the final pages and articles dump for 2013 was downloaded from the wikimedia dump page. the wikiextractor tool written by giuseppe attardi and antonio fuschetto was then used to extract plain text (discarding markup etc) from the 165mb dump, resulting in a 33mb output file. this was tidied by removing remaining xml, blank lines, and blocks of english text.\n",
      "the text was then split to give into a total of 360,477 sentences, and these were imported into a postgresql database table. the sentences were pruned by removing all items less than 50 characters long, all items containing numbers only (eg timelines), and all duplicates, to give a final total of 204,789 sentences in the corpus.\n",
      "the file contains the following fields:\n",
      "id: unique identifier for the sentence;\n",
      "welsh: the sentence in welsh;\n",
      "word_w: the number of words in the welsh sentence.\n",
      "acknowledgements:\n",
      "this dictionary was created by kevin donnell. if using kwici in research, please use the following citation\n",
      "kevin donnelly (2014). \"kwici: a 4m-word corpus drawn from the welsh wikipedia.\" http://cymraeg.org.uk/kwici. (bibtex)\n",
      "inspiration:\n",
      "can you use this corpus to add frequency information to this welsh dictionary?\n",
      "can you use this corpus to create a stemmer for welsh?\n",
      "context:\n",
      "this database of terrorism prosecutions and sentencing information was created using public records including three lists of prosecutions from the u.s. department of justice (from 2010, 2014, and 2015), court files available through the federal judiciary’s case management system, doj press releases, and inmate data from the bureau of prisons.\n",
      "content:\n",
      "trevor aaronson created the first iteration of this database as part of a project funded by the investigative reporting program at the university of california, berkeley. mother jones magazine published that data in 2011, along with accompanying articles, in a package that is still available online. beginning in 2016, aaronson and margot williams collaborated to update and expand the database, with a new emphasis to include bureau of prisons data because so many post-9/11 terrorism defendants had been released. the cases include any prosecutions after september 11, 2001, that the u.s. government labeled as related to international terrorism. the intercept first published this database on april 20, 2017. for each defendant in the database, u.s. criminal code data related to charges has been categorized according to this legend\n",
      "acknowledgements:\n",
      "this database is licensed under creative commons for noncommercial uses with appropriate attribution. if you publish this database, in part or whole, you must credit trevor aaronson and margot williams.\n",
      "inspiration:\n",
      "what are the most common charges?\n",
      "are the sentence lengths similar?\n",
      "context:\n",
      "mapping the klan is a rough timeline of the rise of the second ku klux klan between 1915 and 1940. each red dot shows a local unit or \"klavern.\" the official numbers for each klavern indicate a basic chronology for the chartering of the klaverns, and they also reveal patterns of klan organizing.\n",
      "content:\n",
      "the data for mapping the klan is based on a variety of sources, mostly newspapers sponsored by or sympathetic to the ku klux klan. these publications reported on the activities of local units, known officially as klaverns. data includes approximate date of charter, location(lat/lon), nickname, source for data, and related notes.\n",
      "dates: the dates for each klavern come from the publication listed for that entry. so, it is likely that the klaverns identified were established even earlier than the date indicated. the klan’s recruitment methods make it harder to accurately date the beginning of a klavern. each local group had to recruit a set number of members before it could get its charter and number.\n",
      "numbers: the klaverns in each state were numbered in chronological order of their chartering. so we can assume that if a klan number 40 is dated october 1923, klans 1 to 39 were established before 1923.\n",
      "as historians agree, the busiest years of klan expansion were 1922-1924, with big declines thereafter. the large number of klaverns established after 1925, when the ku klux klan largely disappeared from the national news media, is intriguing. the continued organizing of klaverns after 1925 is more difficult to study, for lack of sources. that history remains to be explored. learn more.\n",
      "acknowledgements:\n",
      "source data here available through the vcu library site. data was compiled by:\n",
      "john kneebone, lead author and professor of history, vcu\n",
      "shariq torres, lead web developer and data co-author, vcu libraries\n",
      "erin white, project manager, vcu libraries\n",
      "lauren work, digital collections, vcu libraries\n",
      "alison tinker, web designer, vcu libraries\n",
      "john glover, digital humanities consultant, vcu libraries\n",
      "inspiration:\n",
      "where was the densest concentrations of kkk?\n",
      "what years saw the biggest rises?\n",
      "context\n",
      "the wmt15_eval dataset contains the files to machine translation evaluation output from workshop on machine translation (wmt15).\n",
      "nltk uses this dataset to validate the machine translation bleu score implementations.\n",
      "content\n",
      "the wmt15_eval directory contains the files to evaluate mt evaluation metrics, it's not production standards data, neither will it be helpful in shared task participation but it provides a good testbed for new metrics implementation and comparison against metrics already available in nltk.translate.*_score.py to validate the numbers.\n",
      "it includes the first 100 sentences from the newstest 2015 development set for the english-russian language part, made available at workshop for machine translation 2016 (wmt16) and the google translate of the english source sentences.\n",
      "[plaintext]\n",
      "newstest-2015-100sents.en-ru.src.en\n",
      "newstest-2015-100sents.en-ru.ref.ru\n",
      "newstest-2015-100sents.en-ru.google.ru\n",
      "[sgm]\n",
      "newstest2015-100sents-enru-ref.ru.sgm\n",
      "newstest2015-100sents-enru-src.en.sgm\n",
      "newstest2015-100sents-enru-google.ru.sgm\n",
      "and the original ,sgm files from wmt16:\n",
      "newstest2015-enru-ref.ru.sgm\n",
      "newstest2015-enru-src.en.sgm\n",
      "the plaintext are converted from the .sgm files from the development sets in wmt with the following command:\n",
      "sed -e 's/<[^>]*>//g; /^\\s*$/d' newstest-2015.enru.src.en.sgm | head -n100 > newstest-2015-100sents.en-ru.src.en\n",
      "sed -e 's/<[^>]*>//g; /^\\s*$/d' newstest-2015.enru.ref.ru.sgm | head -n100 > newstest-2015-100sents.en-ru.ref.en\n",
      "the tokenized versions of the natural text files above are processed using moses tokenizer.perl:\n",
      "~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru < newstest-2015-100sents.en-ru.ref.ru > ref.ru\n",
      "~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru < newstest-2015-100sents.en-ru.google.ru > google.ru\n",
      "~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < newstest-2015-100sents.en-ru.src.en > src.en\n",
      "the google translate outputs are created on 25 oct 2016 10am. using the english source sentences.\n",
      "the newstest2015-100sents-enru-google.ru.sgm is created using the wrap-xml.perl tool in moses:\n",
      "~/mosesdecoder/scripts/ems/support/wrap-xml.perl ru newstest2015-100sents-enru-src.en.sgm google < google.ru > newstest2015-100sents-enru-google.ru.sgm\n",
      "the bleu scores output from multi-bleu.perl is as such:\n",
      "~/mosesdecoder/scripts/generic/multi-bleu.perl ref.ru < google.ru \n",
      "bleu = 23.17, 53.8/29.6/17.6/10.3 (bp=1.000, ratio=1.074, hyp_len=1989, ref_len=1852)\n",
      "the mteval-13a.output file is produced using the mteval-v13a.pl\n",
      "~/mosesdecoder/scripts/generic/mteval-v13a.pl -r newstest2015-100sents-enru-ref.ru.sgm -s newstest2015-100sents-enru-src.en.sgm -t newstest2015-100sents-enru-google.ru.sgm  > mteval-13a.output\n",
      "acknowledgements\n",
      "credits go to the organizers of wmt15 and wmt16.\n",
      "context\n",
      "the perluniprops dataset in nltk is a subset of the index of unicode version 7.0.0 character properties in perl\n",
      "the pythonic equivalence of the perl uniprops is created primarily to ease the porting of regex related perl code to nltk, inspired by this stackoverflow question.\n",
      "content\n",
      "the nltk port of the perl uniprops contains the following character sets:\n",
      "close_punctuation.txt\n",
      "currency_symbol.txt\n",
      "isalnum.txt\n",
      "isalpha.txt\n",
      "islower.txt\n",
      "isn.txt\n",
      "issc.txt\n",
      "isso.txt\n",
      "isupper.txt\n",
      "line_separator.txt\n",
      "number.txt\n",
      "open_punctuation.txt\n",
      "punctuation.txt\n",
      "separator.txt\n",
      "symbol.txt\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"conll2000\" name=\"conll 2000 chunking corpus\"\n",
      "         webpage=\"http://www.cnts.ua.ac.be/conll2000/chunking/\"\n",
      "         contact=\"erik tjong kim sang (erikt@uia.ua.ac.be)\"\n",
      "         unzip=\"1\"\n",
      "         />\n",
      "\n",
      "<package id=\"conll2002\" name=\"conll 2002 named entity recognition corpus\"\n",
      "         webpage=\"http://www.cnts.ua.ac.be/conll2002/ner/\"\n",
      "         unzip=\"1\"\n",
      "         />\n",
      "\n",
      "\n",
      "<package id=\"conll2007\" name=\"dependency treebanks from conll 2007 (catalan and basque subset)\"\n",
      "         webpage=\"http://nextens.uvt.nl/depparse-wiki/datadownload\"\n",
      "         contact=\"kepa sarasola\"\n",
      "         copyright=\"copyright (c) 2007 the university of the basque country\"\n",
      "         license=\"creative commons attribution-noncommercial-noderivativeworks license\"\n",
      "         unzip=\"0\"\n",
      "         />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"genesis\" name=\"genesis corpus\"\n",
      "     copyright=\"public domain\"\n",
      "     license=\"public domain\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"twitter_samples\" name=\"twitter samples\"\n",
      "     copyright=\"copyright (c) 2015 twitter, inc\"\n",
      "     license=\"must be used subject to twitter developer agreement\n",
      "      (https://dev.twitter.com/overview/terms/agreement)\"\n",
      " note=\"sample of tweets collected from the twitter apis,\n",
      "       observing the 50k limit required by https://dev.twitter.com/overview/terms/policy#6._be_a_good_partner_to_twitter \"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      " <package id=\"webtext\"\n",
      "     name=\"web text corpus\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"ptb\" name=\"penn treebank\"\n",
      "     copyright=\"copyright (c) 1995 university of pennsylvania\"\n",
      "     license=\"this is a stub for the full penn treebank corpus version 3.\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"reuters\" \n",
      "     name=\"the reuters-21578 benchmark corpus, aptemod version\"\n",
      "     webpage=\"http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\"\n",
      "     license=\"the copyright for the text of newswire articles and reuters \n",
      "     annotations in the reuters-21578 collection resides with reuters ltd. \n",
      "     reuters ltd. and carnegie group, inc. have agreed to allow the free \n",
      "     distribution of this data *for research purposes only*.  \n",
      "     if you publish results based on this data set, please acknowledge its use, \n",
      "     refer to the data set by the name 'reuters-21578, distribution 1.0', and \n",
      "     inform your readers of the current location of the data set.\"\n",
      "     unzip=\"0\"\n",
      "     />\n",
      "overview\n",
      "glove is an unsupervised learning algorithm for obtaining vector representations for words. training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
      "content\n",
      "pre-trained word vectors. this data is made available under the public domain dedication and license v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/\n",
      "twitter (2b tweets, 27b tokens, 1.2m vocab, uncased, 25d, 50d, 100d, & 200d vectors)\n",
      "acknowledgements\n",
      "jeffrey pennington, richard socher, and christopher d. manning. 2014. glove: global vectors for word representation https://nlp.stanford.edu/pubs/glove.pdf\n",
      "https://nlp.stanford.edu/projects/glove/\n",
      "inspiration\n",
      "nlp endeavors\n",
      "usage in kaggle kernels\n",
      "context\n",
      "first, what is asbestos? well it is a mineral that can be pulled into fine fibres with high resistance to heat, electricity and chemical corrosion. in the past it was a common ingredient in construction materials (caveat: this is at least true for the european union). why in the past? asbestos is a threat to health due to its very fibre structure. those microscopic fibers can become trapped in the respiratory system, causing cancer and other disease decades after exposure.\n",
      "second, where is poland?! the answer depends on how grumpy the internet is on that day. my home country, poland, is located in the eastern or central europe. poland joined the european union in 2004 and suddenly stuff was required of her. strangely enough, poland is the only european country that plans to be free of asbestos by 2032. the national asbestos cleaning program program was initiated in 2009 with one of the aims to create a complete database of asbestos contamination by 2012. in this blog post i’m hoping to shed some light on the progress of this ambitious plan.\n",
      "content\n",
      "the database is run by the ministry of development and should be updated yearly. it was originally uploaded on march 21st 2016 and then updated 8 months later. as far as i can see they don’t keep older versions. the spreadsheet contains columns with the total number of asbestos in the given location (in kilograms), how much of that has been utilised (also in kilograms) and how much still needs to be utilised (not kidding). there is also name of the place and its code teryt. teryt translates as the national official register of the territorial division of the country. it is a very useful thing in identifying cities and regions, especially for languages that include certain letters with diacritics, the overdot, the tail and the stroke. as a side note, teryt code for asbestos dataset was incomplete i.e. missing the last digit (!). in addition, there was no metadata that describes the data collection process or time when it was taken.\n",
      "acknowledgements\n",
      "this dataset was downloaded from the polish public data and is considered public data and can be used under following restrictions: - one should inform about the source of this data and the creation time of reused information as well\n",
      "inspiration\n",
      "is poland on track to be free of asbestos by 20132?\n",
      "context\n",
      "car theft numbers in brazil are ridiculously high. sao paulo reports numbers for the state in two different datasets: robbery and theft, every month.\n",
      "content\n",
      "i will describe each column in next update\n",
      "acknowledgements\n",
      "all data is available at ssp website: http://www.ssp.sp.gov.br/transparenciassp/ when reading the csv files, it helps using encoding='utf-16le', sep=\"\\t\", dayfirst=true as arguments in pd.read_csv()\n",
      "inspiration\n",
      "i compiled all data from 2017 to study patterns, models of cars that are robbed the most, which brands are robbers favourites, cities and neighborhoods that are the most dangerous, etc. feel free to explore it.\n",
      "context-\n",
      "this is a dataset containing records from the new crime incident report system, which includes a reduced set of fields focused on capturing the type of incident as well as when and where it occurred.\n",
      "content-\n",
      "this dataset has 2,60,760 rows and 17 columns.\n",
      "incident_number:\n",
      "offense_code:\n",
      "offense_code_group:\n",
      "offense_description:\n",
      "district:\n",
      "reporting_area:\n",
      "shooting:\n",
      "occurred_on_date:\n",
      "year:\n",
      "month:\n",
      "day_of_week:\n",
      "hour:\n",
      "ucr_part:\n",
      "street:\n",
      "latitude:\n",
      "longitude:\n",
      "location:\n",
      "acknowledgements-\n",
      "i would like to thank the boston police department for making this dataset available to everyone.\n",
      "inspiration\n",
      "how has crime changed over the years?\n",
      "is it possible to predict where or when a crime will be committed?\n",
      "which areas of the city have evolved over this time span?\n",
      "in which area most crimes are committed?\n",
      "i needed to create this dataset to scrape pricing info from xfinity. i didn't find anything like this anywhere, so i decided to host it on kaggle in case someone else needs it.\n",
      "here is the script used to scrape this data: https://gist.github.com/theriley106/b4fdb027c6bdc36e9f7109547348e147\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"unicode_samples\" \n",
      "     name=\"unicode samples\"\n",
      "     note=\"a very small corpus used to demonstrate unicode encoding in chapter 10 of the book\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      " <package id=\"words\" name=\"word lists\"\n",
      "     webpage=\"http://en.wikipedia.org/wiki/words_(unix)\"\n",
      "     license=\"public domain\"\n",
      "     copyright=\"public domain\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "the nltk.corpus.words are words a list of words from http://en.wikipedia.org/wiki/words_(unix)\n",
      "which in unix, you can do:\n",
      " ls /usr/share/dict/\n",
      "see also:\n",
      "https://unix.stackexchange.com/questions/286787/who-or-what-compiled-usr-share-dict-words\n",
      "https://stackoverflow.com/questions/44449284/nltk-words-corpus-does-not-contain-okay\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "violent crime rates by us state\n",
      "content\n",
      "this data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 us states in 1973. also given is the percent of the population living in urban areas.\n",
      "acknowledgements\n",
      "world almanac and book of facts 1975. (crime rates).\n",
      "statistical abstracts of the united states 1975. (urban rates).\n",
      "references\n",
      "mcneil, d. r. (1977) interactive data analysis. new york: wiley.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this dataset does not have a description yet.\n",
      "this dataset contains exercises for the advanced pandas learn tutorial. it is not meant to be consumed separately.\n",
      "context\n",
      "osm russia. central district is a real-world dataset with geo points.\n",
      "content\n",
      "files are in the .osm, . geojson and .pbf formats.\n",
      "the files map.osm and map.geojson have a small size for quick training and preprocessing.\n",
      "acknowledgements\n",
      "the database contains files from open internet sources:\n",
      "openstreetmap;\n",
      "openstreetmap data extracts.\n",
      "all license conditions are the same with the original data.\n",
      "inspiration\n",
      "map preprocessing and analyzing are really important in data science and machine learning practice.\n",
      "philippine stock exchange data\n",
      "daily ohlcv stock data from the philippines\n",
      "why not?\n",
      "here is a link to the script used to scrape this dataset\n",
      "700 annotations of kumar sangakkara's face\n",
      "context\n",
      "recently i have been working on some object localization problems using convolutional nets and i wanted to try train the model on a new dataset other than the very common coco or pascal voc datasets. while pondering on what object to compile a small dataset around, i thought of pushing the challenge a bit more to see if the same model can be trained to localize faces. having this in mind i wanted a dataset of a person's face annotations.\n",
      "as you may know with deep learning models, the more data you have the more accuracy you reach. so considering the challenge to detect a face i wanted a considerable number of images of the same face that the model should be trained on.\n",
      "hence, i needed many pictures of the same person. so the person had to be famous so i could easily find many pictures. so being in sri lanka where else to look other than our cricket stars. so i chose the living legend in sri lankan cricket, kumar sangakkara.\n",
      "content\n",
      "i downloaded around 1000 images from google images and after manual cleaning ended up with 704, which are contained here. i manually annotated all the pictures using a python script to generate the xml files. (yeah, i couldn't find a better thing to do in that 2 hours.) now here is the dataset for anyone to make use of.\n",
      "the zip file attached contains two folders, images and annotations.\n",
      "inspiration\n",
      "so as i mentioned in the above description, my goal with this dataset was to see if an object localization model can be used to detect a face of a person. even though i have the pipeline, i couldn't still thoroughly test its performance using a gpu. so anyone whose interested can use this dataset to test those results. also if these annotations can be useful for any other application, feel free to use it and share it. have fun!\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "data breaches. incidents in the world, that compromised more than 30000 records, between 2004 and 2017. english version. i wanted to visualize the data including the possibility to compare numbers between variable levels. i did some improvements in levels of variables as well as data, and i did a visualization. i also uploaded this version of the dataset in spanish. i did the visualization with tableau software.\n",
      "in this post in my blog, you can read more about it: spanish version and english version. you can also see the visualization in this link: spanish version and english version.\n",
      "content\n",
      "the dataset has 270 observations and 11 variables. most of them, are categorical variables. incidents happened between 2004 and 2017. last updated: february 2018. format: csv2.\n",
      "variables (columns) [en]:\n",
      "entity: name of the organization (public or private) that had the breach. string\n",
      "alternative name: other known names of the entity. string\n",
      "story: tells a summary of what happened. string\n",
      "year: year of the breach. date\n",
      "records lost: number of records that the breach compromised.integer\n",
      "sector: organization's main sector (or field of business). string\n",
      "method of leak: main cause of the breach. string\n",
      "1st source (link): 1st. url with more info about the breach. string\n",
      "2nd source (link): 2nd. url with more info about the breach. string\n",
      "3rd source (link): 3rd. url with more info about the breach. string\n",
      "source name: name of the source of news, official reports, blog, etc. included. note that some of them have changed after i replaced some previous broken links that the original dataset had. string\n",
      "acknowledgements\n",
      "informationisbeautiful.net. before the improvements, a first dataset was downloaded from this site, by the end of 2017.\n",
      "inspiration\n",
      "the main question to be answered with the data visualization was \"what quantities of records were compromised by important data breaches, in organizations and sectors, between 2004 and 2017, and what was the reason?\". i wanted to have a visual answer that allows to compare numbers between year, sector, and method of leak. it would be great to improve the dataset adding new variables for data mining in the future. achieving a complete and exhaustive \"data breaches 2004-2017\" dataset, would help to an in-depth analysis of incidents in this period. 2017 has been the worst year in the history.\n",
      "first of all, this dataset is not mine!\n",
      "i just want to use this dataset to approve my machine learning skills.\n",
      "so i upload this one! :-)\n",
      "hope you like it!\n",
      "========================================================================================\n",
      "this dataset classifies people described by a set of attributes as good or bad credit risks. comes in two formats (one all numeric). also comes with a cost matrix.\n",
      "source: professor dr. hans hofmann institut f\"ur statistik und \"okonometrie universit\"at hamburg fb wirtschaftswissenschaften von-melle-park 5 2000 hamburg 13\n",
      "============================================================================================\n",
      "data set information:\n",
      "two datasets are provided. the original dataset, in the form provided by prof. hofmann, contains categorical/symbolic attributes and is in the file \"german.data\".\n",
      "for algorithms that need numerical attributes, strathclyde university produced the file \"german.data-numeric\". this file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables. several attributes that are ordered categorical (such as attribute 17) have been coded as integer. this was the form used by statlog.\n",
      "this dataset requires use of a cost matrix (see below)\n",
      "..... 1 2\n",
      "1 0 1\n",
      "2 5 0\n",
      "(1 = good, 2 = bad)\n",
      "the rows represent the actual classification and the columns the predicted classification.\n",
      "it is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\n",
      "========================================================================================\n",
      "uci german credit data original version\n",
      "you can find more details about variance, etc. > https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n",
      "uci german credit data modify version\n",
      "here is an modify version of the original one. > https://github.com/stedy/machine-learning-with-r-datasets/blob/master/credit.csv\n",
      "context\n",
      "using measurements from an indoor temperature sensor mounted over a cooktop, can you help monitor the welfare of an elderly person who wants to live at home later in life?\n",
      "content\n",
      "indoor temperature over the cooktop\n",
      "the dataset contains temperature measurements from an indoor sensor mounted 30cm over a combination cooktop-oven. data were collected over a 100 day period. measurements were recorded only when the temperature changed by 1 degree c or more and at a minimum of every 15 minutes.\n",
      "outside temperature and relative humidity\n",
      "also included are outside air temperatures and relative humidity collected in the same region during the same period.\n",
      "other factors include:\n",
      "the data include round-the-clock measurements including typical meal preparation times and long periods (overnight) when the kitchen is not being used.\n",
      "ambient temperature in the house varies between 15c - 30c.\n",
      "the home may be under air conditioning or with the windows and sliding doors open when outside temperature and humidity allow it; use the associated climate data to help determine which state the house is in.\n",
      "when the air conditioning is in use, the thermostat keeps the house between 24c - 25c.\n",
      "there is an incandescent light bulb about 20cm over the sensor that may be on or off for long periods.\n",
      "first use of the cooktop is typically to prepare coffee between 05:30 and 07:00; any absence of this event is likely an indicator of trouble.\n",
      "acknowledgements\n",
      "indoor temperatures were collected in a home setting by the poster. outdoor temperature and humidity are accessed from the national climatic data center, u.s. climate reference network (uscrn/usrcrn), via anonymous ftp at: ftp://ftp.ncdc.noaa.gov/pub/data/uscrn/products/hourly02\n",
      "inspiration\n",
      "can you help monitor the welfare of elderly residents of a home so they can safely live independently later in life? can you establish a profile of typical use of the oven and cooktop and then detect anomalies that signal the occupant(s) are in trouble and may need help? if so, a simple monitoring system can alert family members to check on their elderly parents or grand parents.\n",
      "can you detect these trouble events, that exist in this dataset and possibly in future data? - a saucepan was left on the cooktop too long creating a fire hazard - normal meal preparation patterns have been interrupted requiring a check on the occupant(s)\n",
      "context\n",
      "another week, sadly another school shooting.\n",
      "to better understand the facts i went looking for data and found it difficult to come by - often embedded in other datasets or fragmented and unusable. i decided to create my own compilation based on a mashup of the pah/amaral/hagan research on school shootings with the wikipedia article from 1990 to present.\n",
      "content\n",
      "pah_wikp file: a list of all school shooting incidents from 1990 to present.\n",
      "fields:\n",
      "date: date of incident\n",
      "city: location of incident\n",
      "state: location of incident\n",
      "area type: urban or suburban (only in pah dataset)\n",
      "school: c = college, hs = high school, ms = middle school, es = elementary school, - = unknown\n",
      "fatalities: # killed\n",
      "wounded: # wounded (only in wikipedia dataset)\n",
      "dupe: whether this incident appears in both datasets. note: only the \"pah\" version of the incident is marked.\n",
      "source: pah or wikp\n",
      "desc: text description of incident (only in wikipedia dataset)\n",
      "cps file: us census data on school populations. fields should be fairly self explanatory.\n",
      "acknowledgements\n",
      "thanks to the authors referenced above as well as the wikipedia contributors!\n",
      "inspiration\n",
      "why are school shootings (and death counts) increasing over time?\n",
      "how does the risk of being killed in a school shooting compare with other risks?\n",
      "are some schools / cities / states at higher risk?\n",
      "is there a correlation between countermeasures and a decrease in fatalities?\n",
      "what else correlates with school shooting risks? in addition to firearms and the people who wield them, is there any clear causality?\n",
      "context\n",
      "the austin animal center is the largest no-kill animal shelter in the united states that provides care and shelter to over 18,000 animals each year and is involved in a range of county, city, and state-wide initiatives for the protection and care of abandoned, at-risk, and surrendered animals.\n",
      "as part of the city of austin open data initiative, the austin animal center makes available its collected dataset that contains statistics and outcomes of animals entering the austin animal services system.\n",
      "the dataset was explored in a series of jupyter notebooks that end with a pipeline prediction model written in scikit-learn. the series of notebooks can be viewed here:\n",
      "part one - downloading, cleaning and feature engineering the aac shelter outcome dataset\n",
      "part two - exploratory data analysis of shelter cat outcomes with pandas and seaborn\n",
      "content\n",
      "the dataset contains shelter outcomes of several types of animals and breeds from 10/1/2013 to the present with a hourly time frequency. the data is updated daily.\n",
      "the austin animal center's original dataset has the following column entries:\n",
      "animal id\n",
      "name\n",
      "datetime\n",
      "monthyear\n",
      "date of birth\n",
      "outcome type\n",
      "outcome subtype\n",
      "animal type\n",
      "sex upon outcome\n",
      "age upon outcome\n",
      "breed\n",
      "color\n",
      "the additional data set filtered for only shelter cat outcomes has the additional columns added as part of the cleaning and feature engineering steps.\n",
      "sex\n",
      "spay/neuter\n",
      "periods\n",
      "period range\n",
      "outcome_age_(days)\n",
      "outcome_age_(years)\n",
      "cat/kitten (outcome)\n",
      "sex_age_outcome\n",
      "age_group\n",
      "dob_year\n",
      "dob_month\n",
      "dob_monthyear\n",
      "outcome_month\n",
      "outcome_year\n",
      "outcome_weekday\n",
      "outcome_hour\n",
      "breed1\n",
      "breed2\n",
      "cfa_breed\n",
      "domestic_breed\n",
      "coat_pattern\n",
      "color1\n",
      "color2\n",
      "coat\n",
      "acknowledgements\n",
      "the dataset is provided by the wonderful folks at the austin animal center, the largest no-kill animal shelter in the united states. the aac makes available the data on the austin open data portal. more information on the dataset can be found one the shelter outcomes page.\n",
      "inspiration\n",
      "the inspiration for sharing this dataset and the associated notebooks is to spread awareness and provide another set of data to help support and care for the animals who need it most. by increasing the amount of data and knowledge around best practices and data analysis, those in the animal welfare community can more effectively respond and identify animals that need more support to avoid unwanted outcomes.\n",
      "context\n",
      "this dataset comes from a study into the movement of bats by researchers at the university of bristol, uk. i found it whilst exploring the open datasets at the movebank data repository, a site dedicated to animal tracking data.\n",
      "content\n",
      "the datasets contain information on the position and timestamps for multiple bats. the type of movement (individual or paired) is also included. see the readme.txt file for much more information.\n",
      "acknowledgements\n",
      "i did not create this data. full citations are below,\n",
      "data, holderied m, giuggioli l, mcketterick tj (2015) data from: delayed response and biosonar perception explain movement coordination in trawling bats. movebank data repository. doi:10.5441/001/1.62h1f7k9\n",
      "associated paper (open access), giuggioli l, mcketterick tj, holderied m (2015) delayed response and biosonar perception explain movement coordination in trawling bats. plos computational biology. doi:10.1371/journal.pcbi.1004089.t001\n",
      "connect/follow me on linkedin for more updates on interesting dataset like this. thanks.\n",
      "content\n",
      "this dataset contains the various finance detail of india.\n",
      "aggregate expenditure.\n",
      "capital expenditure.\n",
      "social sector expenditure.\n",
      "revenue expenditure.\n",
      "revenue deficit.\n",
      "gross fiscal deficit.\n",
      "own tax revenues.\n",
      "nominal gsdp series.\n",
      "granularity: annual time period: 1980-81 to 2015-16. amount: in crore rupees (i.e, 1 crore = 10 million)\n",
      "acknowledgements\n",
      "national institution for transforming india (niti aayog)/planning commission, govt of india has published this data on their website.\n",
      "content\n",
      "the dataset was scraped from ebay's site on august 21st. it shows all listings available at that time.\n",
      "inspiration\n",
      "what are the most popular bikes? what are the most popular types of bikes? any trends with respect to sellers?\n",
      "context\n",
      "the basic unit of analysis for the ucdp ged dataset is the “event”, i.e. an individual incident (phenomenon) of lethal violence occurring at a given time and place. this version authored by: mihai croicu, ralph sundberg, ph. d.\n",
      "http://www.ucdp.uu.se/downloads/\n",
      "please check the attached pdf codebook\n",
      "content\n",
      "the dataset contains 135 181 events. ged 17.1 is a global dataset that covers the entirety of the globe (excluding syria) between 1989-01-01 and 2016-12-31. the maximum (best) spatial resolution of the dataset is the individual village or town. the dataset is fully geocoded. the maximum (best) temporal resolution of the dataset is the day.\n",
      "only events linkable to a ucdp/prio armed conflict, a ucdp non-state conflict or a ucdp one-sided violence instance are included. events are included for the entire period, i.e. both for the years when such conflicts were active and for the years when such conflicts where not active.\n",
      "ucdp ged 17.1 is compatible with the 17.1 series of ucdp datasets\n",
      "the ucdp ged 17.1 is (mostly) backwards compatible with ucdp ged versions 1.0-5.0. check the compatibility notes below for further details. significant changes have been made in the actor, dyad and actor/side id meaning these identifiers are no longer backwards compatible.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "the maximum (best) spatial resolution of the dataset is the individual village or town. the dataset is fully geocoded. the maximum (best) temporal resolution of the dataset is the day.\n",
      "inspiration\n",
      "hopefully, learn from wars.\n",
      "context\n",
      "data for sentiment analysis\n",
      "content\n",
      "the data has sentences from 3 sources imdb reviews yelp reviews amazon reviews each line in data set is tagged positive or negative\n",
      "acknowledgements\n",
      "https://archive.ics.uci.edu/ml/datasets/sentiment+labelled+sentences#\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "oecd countries full macroeconomic indicators\n",
      "context:\n",
      "welsh is a member of the brittonic branch of the celtic languages. it is spoken natively in wales, by some in england, and in y wladfa (the welsh colony in chubut province, argentina). historically, it has also been known in english as ‘cambrian’, ‘cambric’ and ‘cymric’. the current number of welsh speakers in wales is over 562,000.\n",
      "content:\n",
      "eurfa is the largest welsh dictionary under a free license, and it was the first dictionary of a celtic language to list verbal inflections and mutated forms. it also includes in-context citations for most words from a number of corpora:\n",
      "bilingual (welsh-english, welsh-spanish):\n",
      "the 18m-word kynulliad3 corpus (k3). this contains formal written welsh (the majority of it translated from english).\n",
      "the 450k-word siarad corpus (s). these transcribed conversations contain \"welsh as she is spoke\", including english codeswitches. for readability, the version here (download) removes much of the transcription marking.\n",
      "the 200k-word patagonia corpus (p). these transcribed conversations contain spoken welsh from patagonia. this has fewer codeswitches, and many of them are in spanish rather than english. for readability, the version here (download) removes much of the transcription marking.\n",
      "the 200k-word korrect/kywiro corpus (ko). this contains welsh translations of english text in free/open software programs.\n",
      "monolingual (welsh only)\n",
      "a 220k-word subset of the 300k-word cig1 child (18-30 months) language acquisition corpus (kig1), containing non-child utterances only. the version here removes much of the transcription marking.\n",
      "a 100k-word subset of the 560k-word cig2 child (3-7 years) language acquisition corpus (kig2), containing non-child utterances only. the version here removes much of the transcription marking.\n",
      "acknowledgements:\n",
      "this dictionary was created by kevin donnelly and is redistributed here under the gnu general public license. for more information, see the attached license file.\n",
      "you may also like:\n",
      "4 million word corpus of contemporary welsh\n",
      "context\n",
      "scraped and copied from http://www.wnba.com/stats/player-stats/#?season=2017&seasontype=regular%20season&permode=totals + http://www.wnba.com/ in general for the bio data.\n",
      "content\n",
      "stats from all games of season 2016-2017\n",
      "g = games played\n",
      "min = minutes played\n",
      "fgm = field goals made\n",
      "fga = field goals attempts\n",
      "fg% = field goals %\n",
      "3pm = 3points made\n",
      "3pa = 3points attempts\n",
      "3p% = 3points %\n",
      "ftm = free throws made\n",
      "fta = free throws attempts\n",
      "ft% = free throws %\n",
      "oreb = offensive rebounds\n",
      "dreb = defensive rebounds\n",
      "reb = total rebounds\n",
      "ast = assists\n",
      "stl = steals\n",
      "blk = blocks\n",
      "to = turnovers\n",
      "pts = total points\n",
      "dd2 = double doubles\n",
      "td3 = triple doubles\n",
      "inspiration\n",
      "compare wnba to nba in best players, average heights, ...\n",
      "context\n",
      "a download of the population data for different countries from open ei data sets.\n",
      "content\n",
      "pretty simple, the first column is country name and then follow the population figures (in millions) for the years 1980-2010\n",
      "acknowledgements\n",
      "i don't own anything. this is for pure exploration. source -> https://openei.org/datasets/dataset/population-by-country-1980-2010\n",
      "inspiration\n",
      "there is a great deal of information here. explore this data set exclusively or join with other data sets that need this information.\n",
      "context\n",
      "this dataset was created by yaroslav bulatov by taking some publicly available fonts and extracting glyphs from them to make a dataset similar to mnist. there are 10 classes, with letters a-j.\n",
      "content\n",
      "a set of training and test images of letters from a to j on various typefaces. the images size is 28x28 pixels.\n",
      "acknowledgements\n",
      "the dataset can be found on tensorflow github page as well as on the blog from yaroslav, here.\n",
      "inspiration\n",
      "this is a pretty good dataset to train classifiers! according to yaroslav:\n",
      "judging by the examples, one would expect this to be a harder task than mnist. this seems to be the case -- logistic regression on top of stacked auto-encoder with fine-tuning gets about 89% accuracy whereas same approach gives got 98% on mnist. dataset consists of small hand-cleaned part, about 19k instances, and large uncleaned dataset, 500k instances. two parts have approximately 0.5% and 6.5% label error rate. i got this by looking through glyphs and counting how often my guess of the letter didn't match it's unicode value in the font file.\n",
      "enjoy!\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "a set of preference judgements among generated random property pairs for 350 random wikidata persons. for each (entity, property1, property2) record, 10 annotators judged which of the two properties is more interesting for the respective entity.\n",
      "the goal is then to predict the annotator judgments as good as possible.\n",
      "current state-of-the-art methods (wikidata property suggester and others) achieve 61% precision in this task, while methods based on linguistic similarity get to 74%, still significantly below annotator agreement (87.5%).\n",
      "further details are in the paper \"doctoral advisor or medical condition: towards entity-specific rankings of knowledge base properties\", adma 2017, available at http://www.simonrazniewski.com/2017_adma.pdf\n",
      "context\n",
      "proceedings of the european parliament (http://statmt.org/europarl/) annotated for speaker gender and age at the sentence-level. the dataset was used for work on personalized machine-translation; specifically, preserving authorial gender traits during the automatic translation process. the corpus contains two parallel subcorpora: english-french and english-german. for further details about the dataset please refer to http://aclanthology.coli.uni-saarland.de/pdf/e/e17/e17-1101.pdf.\n",
      "context\n",
      "this is the data set of the 170.000+ global terrorist attacks during 1970-2016. i've removed many of the text columns, since these can be found as categorized data as well. the number of columns in this data set is 58. in the original source file it was 135.\n",
      "content\n",
      "the columns in this data set are the following: eventid iyear imonth iday extended country country_txt region region_txt provstate city latitude longitude specificity vicinity location summary crit1 crit2 crit3 doubtterr alternative alternative_txt multiple success suicide attacktype1 attacktype1_txt targtype1 targtype1_txt targsubtype1 targsubtype1_txt corp1 target1 natlty1 natlty1_txt gname motive guncertain1 individual nperps nperpcap claimed compclaim weaptype1 weaptype1_txt weapsubtype1 weapsubtype1_txt nkill nkillter nwound nwoundte property propextent propextent_txt ishostkid nhostkid dbsource\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"udhr2\" name=\"universal declaration of human rights corpus (unicode version)\"\n",
      "     webpage=\"http://unicode.org/udhr/\"\n",
      "     license=\"public domain\"\n",
      "     copyright=\"public domain\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"verbnet\"\n",
      "     name=\"verbnet lexicon, version 2.1\"\n",
      "     version=\"2.1\"\n",
      "     author=\"karin kipper-schuler\"\n",
      "     webpage=\"http://verbs.colorado.edu/~mpalmer/projects/verbnet.html\"\n",
      "     license=\"distributed with permission of the author.\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      " <package id=\"universal_treebanks_v20\" name=\"universal treebanks version 2.0\"\n",
      "     license=\"creative commons attribution-noncommercial-sharealike 3.0 united states\"\n",
      "     webpage=\"https://code.google.com/p/uni-dep-tb/\"\n",
      "     unzip=\"0\"\n",
      "     />\n",
      "context\n",
      " <package id=\"wordnet\" name=\"wordnet\"\n",
      "     version=\"3.0\"\n",
      "     license=\"permission to use, copy, modify and distribute this software and \n",
      "     database and its documentation for any purpose and without fee or royalty is \n",
      "     hereby granted, provided that you agree to comply with the following copyright \n",
      "     notice and statements, including the disclaimer, and that the same appear on all \n",
      "     copies of the software, database and documentation, including modifications that \n",
      "     you make for internal use or for distribution.... [see webpage for full license]\"\n",
      "     copyright=\"wordnet 3.0 copyright 2006 by princeton university.  all rights reserved.\"\n",
      "     webpage=\"http://wordnet.princeton.edu/\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "\n",
      "\n",
      "\n",
      " <package id='wordnet_ic' name='wordnet-infocontent'\n",
      "     version='3.0'\n",
      "     webpage='http://wn-similarity.sourceforge.net'\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context:\n",
      "youtube has introduced automatic generation of subtitles based on speech recognition of uploaded video. this dataset provides collection of subtitles robert phoenix the 11th house uploaded podcasts. it serves as database for an introduction to algorithmic analysis of spoken language.\n",
      "from the podcasts author description: “the eleventh house is the home of robert phoenix, a journalist, blogger, interviewer, astrologer and psychic medium with over 30 years experience in personal readings and coaching, and has been a media personality in tv and radio. the 11th house delves into the supernatural, geopolitics, exopolitics, conspiracy theories, and pop culture.”\n",
      "content:\n",
      "the 11th house speeches dataset consists of 543 subtitles (sets of words) retrieved from youtube playlists: https://www.youtube.com/user/freeassociationradio/videos\n",
      "this dataset consists of a single csv file robertphoenixthe11thhouse.csv. the columns are: 'id', 'playlist', 'upload_date', 'title', 'view_count', 'average_rating', 'like_count', 'dislike_count', 'subtitles', which are delimited with a comma.\n",
      "text data in columns 'subtitles' is not sentence based, there are not commas or dots. it is only stream of words being translated from speech into text by googlevoice (more here https://googleblog.blogspot.com.au/2009/11/automatic-captions-in-youtube.html).\n",
      "acknowledgements:\n",
      "the data was downloaded using youtube-dl package.\n",
      "inspiration:\n",
      "i'm interested in a deeper meaning behind current affairs. (for example see http://www.blogtalkradio.com/freeassociationradio)\n",
      "context\n",
      "this data set was created to help kaggle users in the new your city taxi trip duration competition. new features were generated using wolfram mathematica system.\n",
      "hope that this data set will help both young and experienced researchers in their data mastering path.\n",
      "all sources can be found here.\n",
      "content\n",
      "given dataset consists of both features from initial dataset and generated via wolfram mathematica computational system. thus, all features can be split into following groups:\n",
      "initial features (extracted from initial data),\n",
      "calendar features (contains of season, day name and day period),\n",
      "weather features (information about temperature, snow, and rain),\n",
      "travel features (geo distance with estimated driving distance and time).\n",
      "dataset contains the following columns:\n",
      "id - a unique identifier for each trip,\n",
      "vendorid - a code indicating the provider associated with the trip record,\n",
      "passengercount - the number of passengers in the vehicle (driver entered value),\n",
      "year,\n",
      "month,\n",
      "day,\n",
      "hour,\n",
      "minute,\n",
      "second,\n",
      "season,\n",
      "dayname,\n",
      "dayperiod - day period, e.g. late night, morning, and etc.,\n",
      "temperature,\n",
      "rain,\n",
      "snow,\n",
      "startlatitude,\n",
      "startlongitude,\n",
      "endlatitude,\n",
      "endlongitude,\n",
      "flag - this flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - y=store and forward; n=not a store and forward trip,\n",
      "drivingdistance - driving distance, estimated via wolfram mathematica system,\n",
      "drivingtime - driving time, estimated via wolfram mathematica system,\n",
      "geodistance - distance between starting and ending points,\n",
      "tripduration - duration of the trip in seconds (value -1 indicates test rows).\n",
      "from: http://kavita-ganesan.com/entity-ranking-data\n",
      "downloads: - [dataset] (https://code.google.com/p/dataset/downloads/detail?name=opinrankdataset.zip&can=2&q=) - only reviews (~98mb) [[ readme ] (http://kavita-ganesan.com/sites/default/files/opinrankdataset.pdf)]\n",
      "[dataset with judgments] (http://kavita-ganesan.com/modules/pubdlcnt/pubdlcnt.php?file=https://github.com/kavgan/opinrank/releases/download/opinrank/opinrankdatasetwithjudgments.zip&nid=141) - reviews and relevance judgments (~100mb) [[ readme ] (http://kavita-ganesan.com/sites/default/files/opinrankdatasetwithjudgments.pdf)]\n",
      "opinrank dataset - reviews from tripadvisor and edmunds dataset type: text format: full reviews from tripadvisor (~259,000 reviews) and edmunds (~42,230 reviews) domain: hotels, cars how to cite dataset: [ bib ]\n",
      "citing dataset [ bib ] if you use this dataset for your own research please cite the following to mark the dataset:\n",
      "ganesan, k. a., and c. x. zhai, \"opinion-based entity ranking\", information retrieval.\n",
      "@article{ganesan2012opinion, title={opinion-based entity ranking}, author={ganesan, kavita and zhai, chengxiang}, journal={information retrieval}, volume={15}, number={2}, pages={116--150}, year={2012}, publisher={springer} }\n",
      "dataset overview this data set contains full reviews for cars and and hotels collected from tripadvisor (~259,000 reviews) and edmunds (~42,230 reviews).\n",
      "car reviews dataset description\n",
      "full reviews of cars for model-years 2007, 2008, and 2009 there are about 140-250 cars for each model year extracted fields include dates, author names, favorites and the full textual review total number of reviews: ~42,230 year 2007 -18,903 reviews year 2008 -15,438 reviews year 2009 - 7,947 reviews format there are three different folders (2007,2008,2009) representing the three model years. each file (within these 3 folders) would contain all reviews for a particular car. the filename represents the name of the car. within each car file, you would see a set of reviews in the following format:\n",
      "06/15/2009 the author the review goes here.. what are my favorites about this car\n",
      "note that each review is enclosed within a element as shown above and all the extracted items are within this element.\n",
      "hotel reviews dataset description\n",
      "full reviews of hotels in 10 different cities (dubai, beijing, london, new york city, new delhi, san francisco, shanghai, montreal, las vegas, chicago) there are about 80-700 hotels in each city extracted fields include date, review title and the full review total number of reviews: ~259,000 format there should be 10 different folders representing the 10 cities mentioned earlier. each file (within these 10 folders) would contain all reviews related to a particular hotel. the filename represents the name of the hotel. within each file, you would see a set of reviews in the following format:\n",
      "date1review title1full review 1 date2review title2full review 2 ................ ................\n",
      "each line in the file represents a separate review entry. tabs are used to separate the different fields.\n",
      "story\n",
      "this is a set of 13,000 images from the site https://prnt.sc/. it is a site that enables users to easily upload images, either through the web interface, or, most commonly, through the downloadable screen cap tool which enables easy selection and uploading of an area of your screen. as you can see on their homepage, at the point of posting this, they have almost a billion images uploaded. the amount of information in there will be incredible, it’s an information enthusiast dream. around 2 years ago i discovered this, and i thought it was interesting to mass download these images with a tool i created, but i was manually looking at every single image. as i became more interested in machine learning, i figured experimenting with the 20,000 or so images i had downloaded at the time from the site would be interesting, especially since because of the nature of the site and its ease of access, it gets used for a few very specific purposes which is very useful for image categorisation. video games are an extremely popular use, pictures of people, animations, screenshots of chats and the most popular; debugging or technical help. anyone in this field knows people are not particularly conscious of where they put information. i’m sure you can imagine some of the interesting nuggets of info in here just waiting to be found. i was able to find a fair amount just using a retrained inception cnn and some ocr. i manually categorised just over 5,600 images into 6 categories:\n",
      "animations\n",
      "games\n",
      "objects\n",
      "people\n",
      "text\n",
      "a very specific kind of animated game character uploaded frequently enough to deserve its own category\n",
      "i was able to achieve around 85% accuracy for categorisation with the rest of the image set (i have 1,000,000 images downloaded from the site total) using just those manually categorised 5,609 images.\n",
      "image collection\n",
      "the way an image is assigned its link on their site is what made it easy to scan and scrape images from their site. the url codes are generated sequentially out of a combination letters and numbers of length 6, i.e. prnt.sc/jkl123 prnt.sc/jkl124 prnt.sc/jkl125 prnt.sc/jkl126 would represent images uploaded one after another. this is of course very easy to write a script to scrape and collect images from. so far i have collected 1,000,000 images in total, and i am now making as many of them as i could available here, but of course you can imagine how easy it would be to collect massive amounts of images from this site. my images were collected over the last 2 years, although the vast majority were collected within the last 4 months.\n",
      "copyright\n",
      "the lightshot terms of service, including their stance on copyright: https://app.prntscr.com/en/privacy.html\n",
      "inspiration\n",
      "this image set is interesting because it represents a balance between a completely random and chaotic set of images and a very structured set of images, due to the fact that there is a great amount of variance image to image, but essentially every image can be categorised into a set of very specific purposes that the site is used for. because of this, it is good for learning and testing out image processing models.\n",
      "it is essentially an information gatherers gold mine. there are 1 billion screenshots uploaded by everyday users to be collected and processed on this site, all available sequentially. i’m sure you can imagine the kinds of easily accessible (to a machine learning enthusiast) information waiting to be collected on this site.\n",
      "context\n",
      "coming soon\n",
      "content\n",
      "coming soon\n",
      "acknowledgements\n",
      "special thanks to; http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/\n",
      "inspiration\n",
      "coming soon\n",
      "context\n",
      "the universal tagset (petrov et al. 2012) was created to facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices. an up-to-date documentation can be found on http://universaldependencies.org/u/pos/\n",
      "content\n",
      "the files in this repository contain mappings from treebank specific tagsets\n",
      "to a set of 12 universal part-of-speech tags. the 12 universal tags are:\n",
      "\n",
      "verb - verbs (all tenses and modes)\n",
      "noun - nouns (common and proper)\n",
      "pron - pronouns \n",
      "adj - adjectives\n",
      "adv - adverbs\n",
      "adp - adpositions (prepositions and postpositions)\n",
      "conj - conjunctions\n",
      "det - determiners\n",
      "num - cardinal numbers\n",
      "prt - particles or other function words\n",
      "x - other: foreign words, typos, abbreviations\n",
      ". - punctuation\n",
      "\n",
      "see \"a universal part-of-speech tagset\"\n",
      "by slav petrov, dipanjan das and ryan mcdonald\n",
      "for more details:\n",
      "http://arxiv.org/abs/1104.2086\n",
      "the zipfile contains the <lang>-<tagset>.map files that maps the respective <tagset> pos tagsets in <lang> to the universal tagset, e.g. the en-ptb.map contains the mapping from the english penn tree bank tagset to the universal tagset.\n",
      "the list of mappings includes:\n",
      "ar-padt.map\n",
      "bg-btb.map\n",
      "ca-cat3lb.map\n",
      "cs-pdt.map\n",
      "da-ddt.map\n",
      "de-negra.map\n",
      "de-tiger.map\n",
      "el-gdt.map\n",
      "en-brown.map\n",
      "en-ptb.map\n",
      "en-tweet.map\n",
      "es-cast3lb.map\n",
      "es-eagles.map\n",
      "es-iula.map\n",
      "es-treetagger.map\n",
      "eu-eus3lb.map\n",
      "fi-tdt.map\n",
      "fr-paris.map\n",
      "hu-szeged.map\n",
      "it-isst.map\n",
      "iw-mila.map\n",
      "ja-kyoto.map\n",
      "ja-verbmobil.map\n",
      "ko-sejong.map\n",
      "nl-alpino.map\n",
      "pl-ipipan.map\n",
      "pt-bosque.map\n",
      "ru-rnc.map\n",
      "sl-sdt.map\n",
      "sv-talbanken.map\n",
      "tu-metusbanci.map\n",
      "zh-ctb6.map\n",
      "zh-sinica.map\n",
      "additionally, it contains:\n",
      "readme: a readme file\n",
      "universal_tags.py: a script use to convert tags to the universal tagset using the mappings by nathan schneider\n",
      "en-tweet.readme: a description of the tweeter tag mappings from (noah et al. 2011)\n",
      "citations\n",
      "slav petrov,  dipanjan das, and ryan mcdonald. \n",
      "a universal part-of-speech tagset.  in lrec 2012\n",
      "context\n",
      "this was the original pre-trained pos tagger that nltk.pos_tag used.\n",
      "this is the infamous maximum entropy pos tagger that gained a lot of heat when no one knew where exactly the model came from.\n",
      "acknowledge\n",
      "we would like to know who to acknowledge too ;p\n",
      "context\n",
      "vader sentiment analysis. vader (valence aware dictionary and sentiment reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.\n",
      "the vader lexicon is an empirically validated by multiple independent human judges, vader incorporates a \"gold-standard\" sentiment lexicon that is especially attuned to microblog-like contexts.\n",
      "the documentation of the lexicon and the algorithm can be found from the original implementation: https://github.com/cjhutto/vadersentiment\n",
      "the nltk port has slight modification to integrate with the nltk interfaces and it comes with better python3 support: https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py\n",
      "citation\n",
      "hutto, c.j. & gilbert, e.e. (2014). vader: a parsimonious rule-based model for\n",
      "sentiment analysis of social media text. eighth international conference on\n",
      "weblogs and social media (icwsm-14). ann arbor, mi, june 2014.\n",
      "inspiration\n",
      "we look forward to more sentiment analysis datasets, perhaps a jedi (joint entropy decay iteration) algorithm someday?\n",
      "context\n",
      "nltk provides an interface to the bllip reranking parser (aka charniak-johnson parser, charniak parser, brown reranking parser).\n",
      "nltk redistribute the pre-trained model trained on the wsj section of the penn treebank without auxillary.\n",
      "the full list of models can be found on https://github.com/bllip/bllip-parser/blob/master/models.rst\n",
      "acknowledgements\n",
      "parser and reranker:\n",
      "eugene charniak and mark johnson. 2005. \"coarse-to-fine n-best \n",
      "parsing and maxent discriminative reranking.\" in acl.\n",
      "\n",
      "eugene charniak. 2000. \"a maximum-entropy-inspired parser.\" in acl.\n",
      "self-training:\n",
      "david mcclosky, eugene charniak, and mark johnson. 2006. \n",
      "\"effective self-training for parsing.\" in hlt-naacl.\n",
      "syntactic fusion:\n",
      "do kook choe, david mcclosky, and eugene charniak. 2015. \n",
      "\"syntactic parse fusion.\" in emnlp.\n",
      "context\n",
      "nltk redistributes the moses machine translation models to test the nltk.translate functionalities, originally from http://www.statmt.org/moses/?n=development.getstarted\n",
      "content\n",
      "the moses sample contains the following subdirectories:\n",
      "lm: pre-trained n-gram language models using europarl and srilm\n",
      "phrase-model: pre-trained moses phrase-based model\n",
      "string-to-tree: pre-trained moses string-to-tree model\n",
      "tree-to-tree: pre-trained moses tree-to-tree model\n",
      "acknowledgements\n",
      "credit goes to the moses developers who distribute this as a regression test to check that moses statistical machine translation tool is successfully installed.\n",
      "context\n",
      "the maxent_ne_chunker contains two pre-trained english named entity chunkers trained on an ace corpus (perhaps ace ace 2004 multilingual training corpus?)\n",
      "it will load an nltk.chunk.named_entity.nechunkparser object and it is used by the nltk.ne_chunk() function.\n",
      "robert m. johnson has written an excellent expository of what the pre-trained model is doing under the hood\n",
      "from the relation extraction code function in nltk, it lists the following tags for the ace tagset:\n",
      "location\n",
      "organization\n",
      "person\n",
      "duration\n",
      "date\n",
      "cardinal\n",
      "percent\n",
      "money\n",
      "measure\n",
      "facility\n",
      "gpe\n",
      "content\n",
      "the maxent_ne_chunker.zip contains - english_ace_binary.pickle: chunks the input pos tagged sentence and labeled positive nes as ne. - english_ace_multiclass.pickle: chunks the input pos tagged sentence and outputs the repsective ne labels under the ace tagset. - py3: subdirectory that contains the python3 compatiable pickles as above\n",
      "acknowledgements\n",
      "we're not sure who exactly to credit for this pre-trained model, it'll be great if anyone who knows help to document this on https://github.com/nltk/nltk/issues/1783\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"gutenberg\" name=\"project gutenberg selections\"\n",
      "     webpage=\"http://gutenberg.net/\"\n",
      "     license=\"public domain\"\n",
      "     copyright=\"public domain\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"nps_chat\" name=\"nps chat\"\n",
      "     author=\"craig martell (cmartell@nps.edu)\"\n",
      "     webpage=\"http://faculty.nps.edu/cmartell/npschat.htm\"\n",
      "     license=\"this corpus is distributed solely for non-commercial, non-profit educational and research use. it is a derivative compilation work of multiple works whose copyrights are held by the respective original authors.\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"paradigms\" name=\"paradigm corpus\"\n",
      "     author=\"cathy bow, university of melbourne\"\n",
      "     license=\"distributed with the permission of the author\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"product_reviews_1\"\n",
      " name=\"product reviews (5 products)\"\n",
      " author=\"bing liu\"\n",
      "     copyright=\"copyright (c) 2004 bing liu\"\n",
      "     license=\"creative commons attribution 4.0 international\"\n",
      " licenseurl = \"http://creativecommons.org/licenses/by/4.0/\"\n",
      " webpage=\"http://www.cs.uic.edu/~liub/fbs/sentiment-analysis.html#datasets\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "\n",
      "<package id=\"product_reviews_2\"\n",
      " name=\"product reviews (9 products)\"\n",
      " author=\"bing liu\"\n",
      "     copyright=\"copyright (c) 2007 bing liu\"\n",
      "     license=\"creative commons attribution 4.0 international\"\n",
      " licenseurl = \"http://creativecommons.org/licenses/by/4.0/\"\n",
      " webpage=\"http://www.cs.uic.edu/~liub/fbs/sentiment-analysis.html#datasets\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"shakespeare\" \n",
      "     name=\"shakespeare xml corpus sample\"\n",
      "     license=\"public domain\"\n",
      "     copyright=\"public domain\"\n",
      "     webpage=\"http://www.andrew.cmu.edu/user/akj/shakespeare/\"\n",
      "     sample=\"true\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"stopwords\" name=\"stopwords corpus\"\n",
      "     webpage=\"ftp://ftp.cs.cornell.edu/pub/smart/english.stop and http://snowball.tartarus.org/ and others\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"toolbox\"\n",
      "     name=\"toolbox sample files\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "context\n",
      "the [sentiment polarity dataset version 2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/ ) is created by bo pang and lillian lee. this dataset is redistributed with nltk with permission from the authors.\n",
      "this corpus is also used in the document classification section of chapter 6.1.3 of the nltk book.\n",
      "content\n",
      "this dataset contains 1000 positive and 1000 negative processed reviews.\n",
      "citation\n",
      "bo pang and lillian lee. 2004. a sentimental education: sentiment analysis \n",
      "using subjectivity summarization based on minimum cuts. in acl.\n",
      "bibtex:\n",
      "@inproceedings{pang+lee:04a,\n",
      "  author =       {bo pang and lillian lee},\n",
      "  title =        {a sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts},\n",
      "  booktitle =    \"proceedings of the acl\",\n",
      "  year =         2004\n",
      "}\n",
      "context\n",
      "the corpus consists of one million words of american english texts printed in 1961.\n",
      "the canonical metadata on nltk:\n",
      "<package id=\"brown\" name=\"brown corpus\"\n",
      "         author=\"w. n. francis and h. kucera\"\n",
      "         license=\"may be used for non-commercial purposes.\"\n",
      "         webpage=\"http://www.hit.uib.no/icame/brown/bcm.html\"\n",
      "         unzip=\"1\"\n",
      "         />\n",
      "\n",
      "<package id=\"brown_tei\" name=\"brown corpus (tei xml version)\"\n",
      "     author=\"w. n. francis and h. kucera\"\n",
      "     license=\"may be used for non-commercial purposes.\"\n",
      "     webpage=\"http://www.hit.uib.no/icame/brown/bcm.html\"\n",
      "     contact=\"lou burnard -- lou.burnard@oucs.ox.ac.uk\"\n",
      "     unzip=\"1\"\n",
      "     />\n",
      "description\n",
      "this dataset has two parts, one is created using 100 million sentences from last 18 months of us patent data downloaded from uspto site. second is from 83 million sentences extracted from ncbi pmc open access articles.\n",
      "preparation of data\n",
      "text from original xml extracted using python lxml module. extracted text split into sentences using simple regex. resulting 83 million sentences from pmc articles, 120 million sentences from patents.\n",
      "clustering of sentences\n",
      "sentences with less than 5 words and more than 70 words were removed. stop words and unwanted characters were removed. extracted sentences were fed to the clustering algorithm to generate hierarchical clusters.\n",
      "sentences are clustered as if each sentence is compared with all other sentences, ie all against all match. however, all against all match for 100 million sentences computationally may not be possible, even extracting features would be costly, graph mining algorithms used to cluster the data.\n",
      "files description\n",
      "ncbi pmc\n",
      "file : ncbi_data.json ; mapping : {count: cluster_id, val:[list of line ids] } file : ncbi_lines_map.json ; mapping : {pmc: line id, line: } split pmc with _, first part is the doc id file : ncbi_doc_titles_map.json ; mapping : {pmc: line id, article-title: , journal-title: , article-id:}\n",
      "us patent data\n",
      "file : uspto_data.json ; mapping : {count: cluster_id, val:[list of line ids] } file : uspto_lines_map.json ; mapping : {num: line id, line : } split num with _, first part is the doc id file : uspto_doc_titles_map.json ; mapping : {num: patent id extracted from xml xpath //publication-reference//document-id//doc-number, title: invention title from patent xml }\n",
      "where this data can be used?\n",
      "this can be used in search ranking of similar documents, conceptual search, plagiarism detection etc. having similar sentences can improve the accuracy of the prediction models. machine translation and other areas where alignment is needed, running models on already aligned data can boost the accuracy while reducing the time.\n",
      "details about the uploaded data\n",
      "original data size uncompressed is 32 gb for ncbi pmc data, 30 gb for uspto data. clustered data size is slightly bigger than original, so it cant be uploaded in kaggle.\n",
      "only clusters with less than 5/8 lines were uploaded, consisting of 400k and 358 clusters for ncbi and uspto respectively. each has more than 800k lines in 250k and 135k documents.\n",
      "examples from data\n",
      "ncbi\n",
      "pmc : 3045422_8\n",
      " article-title : association of a-adducin and g-protein b3 genetic polymorphisms with hypertension: a meta-analysis of chinese populations\n",
      " journal-title : plos one\n",
      "line : as the genomic sequences of a-adducin and gnb3 genes are highly polymorphic, it is of added interest to identify which polymorphism(s) in these genes might have functional potentials of affecting their bioavailability\n",
      "\n",
      " pmc : 3142626_5  \n",
      " article-title :evaluation of transforming growth factor beta-1 gene 869t/c polymorphism with hypertension: a meta-analysis\n",
      "  journal-title : international journal of hypertension\n",
      "  line : since the genomic sequence of tgfb1 gene is highly polymorphic, it is of added interest to confirm which tgfb1 polymorphism(s) might have functional potentials to influence the final bioavailability of tgf- b 1, thus the development of hypertension\n",
      "\n",
      "pmc : 3166328_6\n",
      "article-title : an updated meta-analysis of endothelial nitric oxide synthase gene: three well-characterized polymorphisms with hypertension\n",
      "journal-title : plos one\n",
      "line : since the genomic sequence of enos is highly polymorphic, it is of added interest to confirm which polymorphism(s) at enos might have functional potentials to affect the final bioavailability of enos, and thus the development of hypertension\n",
      "pmc : 362850_30\n",
      "article-title : de-orphaning the structural proteome through reciprocal comparison of evolutionarily important structural features\n",
      "journal-title : plos one\n",
      "line : since a small root mean squared deviation (rmsd) alone is not sufficient to guarantee the functional relevance of a match , , a support vector machine (svm) trained on enzymes () considers in addition to rmsd whether the matches also fall on evolutionarily important regions of t i \n",
      "\n",
      "pmc : 4651773_184\n",
      "article-title : crystal structure of group ii intron domain 1 reveals a template for rna assembly\n",
      "journal-title : nature chemical biology\n",
      "line : all of the root mean square deviation (rmsd) values were calculated by pymol without allowing removal of non-fitting residues to minimize rmsd. for calculating the simulated annealing omit map, the region of interest was first deleted from the model, and then this partial model was subject to simulated annealing refinement in phenix.refine\n",
      "us patent data\n",
      "num : 09226555_77\n",
      "title: cane structure\n",
      "line : as such, when the negative terminal of a battery is located at the holder with the l-shaped blocking wall 2112 , the battery will be blocked from being electrically connected with the associated conductive sheet 213 therein to prevent an incorrect electrical connection that may cause a damage or failure to the circuit board 22 \n",
      "\n",
      "num : 09520536_507\n",
      "title : light emitting diode chip having electrode pad\n",
      "line : therefore, the second electrode extensions 39 a and the transparent conductive layer 33 are not electrically connected to each other in the positions in which the holes h are formed, thus, an electrical flow is blocked in the corresponding positions in which the holes h are formed, and the second electrode extensions 39 a are located on the current blocking layer 31 b\n",
      "num : 09233144_346\n",
      "title : tyrosine kinase receptor tyro3 as a therapeutic target in the treatment of cancer\n",
      "line : in order to investigate the role of tyro3 in cell growth and tumorigenic properties, tyro3 expression was blocked using rna interference technology or tyro3 activity was inhibited using a blocking antibody directed against the extracellular domain of tyro3 or a soluble receptor consisting of the recombinant extracellular domain of tyro3 produced in bacteria\n",
      "\n",
      "num : 09283245_194\n",
      "title : composition containing pias3 as an active ingredient for preventing or treating cancer or immune disease\n",
      "line : in order to measure the amount of produced il-17 cytokine, the supernatant of the cell culture medium was collected and level of il-17 expression was investigated using human il-17 and sandwich elisa. after reaction on a 96 well plate with 2 mg/ml of monoclonal anti-il-17 at 4deg c., overnight, non-specific binding was blocked with blocking solution (1% bsa/pbst)\n",
      "\n",
      "num : 09546210_330\n",
      "title : cripto antagonism of activin and tgf-b signaling\n",
      "line : in contrast, the cripto degf mutant blocked roughly half of the luciferase activity induced by activin-b ( fig. 10 ), indicating an independent role for the cfc domain in blocking activin-b signaling\n",
      "context\n",
      "this dataset was built as a supplementary to \"[european soccer database][1]\". it includes data dictionary, extraction of detailed match information previously contains in xml columns.\n",
      "content\n",
      "positionreference.csv: a reference of position x, y and map them to actual position in a play court.\n",
      "datadictionary.xlsx: data dictionary for all xml columns in \"match\" data table.\n",
      "card_detail.csv: detailed xml information extracted form \"card\" column in \"match\" data table.\n",
      "corner_detail.csv: detailed xml information extracted form \"corner\" column in \"match\" data table.\n",
      "cross_detail.csv: detailed xml information extracted form \"cross\" column in \"match\" data table.\n",
      "foulcommit_detail.csv: detailed xml information extracted form \"foulcommit\" column in \"match\" data table.\n",
      "goal_detail.csv: detailed xml information extracted form \"goal\" column in \"match\" data table.\n",
      "possession_detail.csv: detailed xml information extracted form \"possession\" column in \"match\" data table.\n",
      "shotoff_detail.csv: detailed xml information extracted form \"shotoffl\" column in \"match\" data table.\n",
      "shoton_detail.csv: detailed xml information extracted form \"shoton\" column in \"match\" data table.\n",
      "acknowledgements\n",
      "original data comes from [european soccer database][1] by hugo mathien. i personally thank him for all his efforts.\n",
      "inspiration\n",
      "since this is a open dataset with no specific goals / objectives, i would like to explore the following aspects by data analytics / data mining:\n",
      "team statistics including overall team ranking, team points, winning possibility, team lineup, etc. mostly descriptive analysis.\n",
      "team transferring track and study team players transferring in the market. study team's strength and weakness, construct models to suggest best fit players to the team.\n",
      "player statistics summarize player's performance (goal, assist, cross, corner, pass, block, etc). identify key factors of players by position. based on these factors, evaluate player's characteristics.\n",
      "player evolution construct model to predict player's rating of future.\n",
      "new player's template identify template and model player for young players cater to their positions and characteristics.\n",
      "market value prediction predict player's market value based on player's capacity and performance.\n",
      "the winning eleven given a season / league / other criteria, propose the best 11 players as a team based on their capacity and performance.\n",
      "context\n",
      "tailpipe emissions for bmw 3series sedan\n",
      "content\n",
      "analysis for tailpipe omissions data for bmw 3 series\n",
      "data is recorded for car travelling at every second with following variables: vehicle_speed (km/h), hc_tailpipe (g/s), co_tailpipe (g/s) and nox_tailpipe (g/s). it is expected that the value of theses emissions are close to zero, so the recommended emissions are speed = 39.9 , hc = 0.0005, co = 0.0200 𝑎𝑛𝑑 no = 0.0004\n",
      "context\n",
      "kannada basic word set for natural language processing\n",
      "content\n",
      "words which has meaning in kannada language\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "the state of the nation address of the president of south africa (abbreviated sona) is an annual event in the republic of south africa, in which the president of south africa reports on the status of the nation, normally to the resumption of a joint sitting of parliament (the national assembly and the national council of provinces).\n",
      "content\n",
      "full text of all the speeches, from 1990 through to 2018. in years that elections took place, a state of the nation address happens twice, once before and again after the election.\n",
      "title: protein localization sites\n",
      "creator and maintainer: kenta nakai institue of molecular and cellular biology osaka, university 1-3 yamada-oka, suita 565 japan nakai@imcb.osaka-u.ac.jp http://www.imcb.osaka-u.ac.jp/nakai/psort.html donor: paul horton (paulh@cs.berkeley.edu) date: september, 1996 see also: yeast database\n",
      "past usage. reference: \"a probablistic classification system for predicting the cellular localization sites of proteins\", paul horton & kenta nakai, intelligent systems in molecular biology, 109-115. st. louis, usa 1996. results: 81% for e.coli with an ad hoc structured probability model. also similar accuracy for binary decision tree and bayesian classifier methods applied by the same authors in unpublished results.\n",
      "predicted attribute: localization site of protein. ( non-numeric ).\n",
      "the references below describe a predecessor to this dataset and its development. they also give results (not cross-validated) for classification by a rule-based expert system with that version of the dataset.\n",
      "reference: \"expert sytem for predicting protein localization sites in gram-negative bacteria\", kenta nakai & minoru kanehisa,\n",
      "proteins: structure, function, and genetics 11:95-110, 1991.\n",
      "reference: \"a knowledge base for predicting protein localization sites in eukaryotic cells\", kenta nakai & minoru kanehisa, genomics 14:897-911, 1992.\n",
      "number of instances: 336 for the e.coli dataset and\n",
      "number of attributes. for e.coli dataset: 8 ( 7 predictive, 1 name )\n",
      "attribute information.\n",
      "sequence name: accession number for the swiss-prot database\n",
      "mcg: mcgeoch's method for signal sequence recognition.\n",
      "gvh: von heijne's method for signal sequence recognition.\n",
      "lip: von heijne's signal peptidase ii consensus sequence score. binary attribute.\n",
      "chg: presence of charge on n-terminus of predicted lipoproteins. binary attribute.\n",
      "aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
      "alm1: score of the alom membrane spanning region prediction program.\n",
      "alm2: score of alom program after excluding putative cleavable signal regions from the sequence.\n",
      "missing attribute values: none.\n",
      "class distribution. the class is the localization site. please see nakai & kanehisa referenced above for more details.\n",
      "cp (cytoplasm) 143 im (inner membrane without signal sequence) 77\n",
      "pp (perisplasm) 52 imu (inner membrane, uncleavable signal sequence) 35 om (outer membrane) 20 oml (outer membrane lipoprotein) 5 iml (inner membrane lipoprotein) 2 ims (inner membrane, cleavable signal sequence) 2\n",
      "source: https://archive.ics.uci.edu/ml/datasets/forest+fires\n",
      "citation request: this dataset is public available for research. the details are described in [cortez and morais, 2007]. please include this citation if you plan to use this database:\n",
      "p. cortez and a. morais. a data mining approach to predict forest fires using meteorological data. in j. neves, m. f. santos and j. machado eds., new trends in artificial intelligence, proceedings of the 13th epia 2007 - portuguese conference on artificial intelligence, december, guimaraes, portugal, pp. 512-523, 2007. appia, isbn-13 978-989-95618-0-9. available at: http://www.dsi.uminho.pt/~pcortez/fires.pdf\n",
      "title: forest fires\n",
      "sources created by: paulo cortez and an�bal morais (univ. minho) @ 2007\n",
      "past usage:\n",
      "p. cortez and a. morais. a data mining approach to predict forest fires using meteorological data. in proceedings of the 13th epia 2007 - portuguese conference on artificial intelligence, december, 2007. (http://www.dsi.uminho.pt/~pcortez/fires.pdf)\n",
      "in the above reference, the output \"area\" was first transformed with a ln(x+1) function. then, several data mining methods were applied. after fitting the models, the outputs were post-processed with the inverse of the ln(x+1) transform. four different input setups were used. the experiments were conducted using a 10-fold (cross-validation) x 30 runs. two regression metrics were measured: mad and rmse. a gaussian support vector machine (svm) fed with only 4 direct weather conditions (temp, rh, wind and rain) obtained the best mad value: 12.71 +- 0.01 (mean and confidence interval within 95% using a t-student distribution). the best rmse was attained by the naive mean predictor. an analysis to the regression error curve (rec) shows that the svm model predicts more examples within a lower admitted error. in effect, the svm model predicts better small fires, which are the majority.\n",
      "relevant information:\n",
      "this is a very difficult regression task. it can be used to test regression methods. also, it could be used to test outlier detection methods, since it is not clear how many outliers are there. yet, the number of examples of fires with a large burned area is very small.\n",
      "number of instances: 517\n",
      "number of attributes: 12 + output attribute\n",
      "note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.\n",
      "attribute information:\n",
      "for more information, read [cortez and morais, 2007].\n",
      "x - x-axis spatial coordinate within the montesinho park map: 1 to 9\n",
      "y - y-axis spatial coordinate within the montesinho park map: 2 to 9\n",
      "month - month of the year: \"jan\" to \"dec\"\n",
      "day - day of the week: \"mon\" to \"sun\"\n",
      "ffmc - ffmc index from the fwi system: 18.7 to 96.20\n",
      "dmc - dmc index from the fwi system: 1.1 to 291.3\n",
      "dc - dc index from the fwi system: 7.9 to 860.6\n",
      "isi - isi index from the fwi system: 0.0 to 56.10\n",
      "temp - temperature in celsius degrees: 2.2 to 33.30\n",
      "rh - relative humidity in %: 15.0 to 100\n",
      "wind - wind speed in km/h: 0.40 to 9.40\n",
      "rain - outside rain in mm/m2 : 0.0 to 6.4\n",
      "area - the burned area of the forest (in ha): 0.00 to 1090.84 (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n",
      "missing attribute values: none\n",
      "this dataset contains a collection of the private neighborhoods in argentina.\n",
      "context\n",
      "i created this dataset mainly for a challenge at school on which we had to predict house prices. houses that are located on a private neighborhood, have prices that are completely different than the price of houses that are on the same city, but outside the neighborhoods. i used this dataset to detect whether a house was inside or outside a private neighborhood.\n",
      "github project: https://github.com/harkdev/barrios_privados_argentina\n",
      "source: http://www.guiacountry.com/countries/imagenes/listado.php\n",
      "context\n",
      "these datasets contain estimated speed and estimated trip durations for all trips generated by the fastest route dataset from oscarleo. the estimated trip durations are a better approximation for the actual trip duration than the total_travel_time variable from the fastest route dataset is.\n",
      "how these features are generated in detail can be found in this kernel: https://www.kaggle.com/pepeeee/nyc-estimating-avg-speed-using-fastest-route/notebook\n",
      "content\n",
      "estimated_speed - this variable contains the estimated average speed of the respective trip. the unit of this variable is meter per second.\n",
      "estimated_trip_duration - this variable is generated by dividing the total_distance variable of the fastest route dataset by estimated_speed. the unit of the variable is seconds.\n",
      "acknowledgements\n",
      "i'm very thankful to oscarleo for sharing the great dataset about the fastest routes. without that dataset mine wouldn't have been possible. i would also like to thank saihttam for sharing the holidays package.\n",
      "inspiration\n",
      "the fastest route contains a lot of very valuable data for predicting the trip durations of taxis in new york city. just using the variables total_travel_time, total_distance and number_of_steps didn't feel like using its full potential to me.\n",
      "context\n",
      "simply list of iso_639-1_codes\n",
      "content\n",
      "needed to access through kaggle\n",
      "acknowledgements\n",
      "https://en.wikipedia.org/wiki/list_of_iso_639-1_codes\n",
      "image from goran ivos at unplash\n",
      "trying to name a child? looking for something a little different? something that will force a preschool's database to support unicode? look no farther! from aðdal to ösp to ben, this list has you covered.\n",
      "the icelandic naming committee maintains an official register of approved icelandic given names and is the governing body of introduction of new given names into the culture of iceland. in many cases parents use the database as a guide when choosing a name. if the name they have in mind is not in the register they can fill out a special form and request whether the name will be considered allowable by law. if the committee rules positively on a request the name will be added to the personal names register. if the committee denies the request, the child may not be allowed to get an icelandic passport with that name.\n",
      "the register is stored and maintained at registers iceland and is accessible through the national portal ísland.is.\n",
      "note on the column headers: drengir = boys, millinöfn = girls, stúlkur = either.\n",
      "winning numbers from the new york state lotto since 2001.\n",
      "acknowledgements\n",
      "this dataset was kindly made available by the state of new york. you can find the original dataset here.\n",
      "inspiration\n",
      "some other state lotteries have proven to be predictable and ended up being gamed. it's extremely unlikely that any real patterns exist in a large and long running lotto like new york's, but can you find any?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "data sourced from the uk government, and used in the car registration api http://www.regcheck.org.uk - correct as of august 2017\n",
      "context\n",
      "this dataset was downloaded from internet few years ago.\n",
      "content\n",
      "this dataset contains many individual word/ text files describing different places and their history/geography around the world.\n",
      "acknowledgements\n",
      "as this dataset is downloaded from internet few years ago and i don't remember the site/ particular authors of this file.but any how thanks to those people who created these files.\n",
      "inspiration\n",
      "this might be helpful for those who practise nlp/nltk, text mining. each individual file contains many paragraphs which describes about the place in a lengthy manner. with text mining one should be able to remove all stop words and describe the data in a brief way.\n",
      "context\n",
      "the file presents a listing of characters wearing powered armor / mini or giant meccha in movies, comics, animation etc.\n",
      "the purpose was to analyse our imaginaries in a specific field (i.e armors in this case) in order to see what are the macro elements, see how they evolve around time and if they are close to what is used in real life.\n",
      "content\n",
      "each armor is analyzed according to 13 characteristics (uses an ai or not, what kind or power, where is the weapon, its capacities (does is fly, gives enhanced strength etc.). being a social science professor and not a data analysts, i went on marvel wikia, dc wikia etc. to compile it. something like 80 heroes are fully presented, and a list of almost 300 been found.\n",
      "inspiration\n",
      "coming from social science i compiled that data during my free time, but i understand that it is highly limiting and that there must be a way to aggregate much more data, & faster. also, i am sure that it does not meet some of the standards for such work. being a newbie here, please tell me how to improve this & i will.\n",
      "the question after is to know if we can \"predict\" what future armors will look like : is there a trend showing that ai is used more and more ? that they all fly ? once this done, it would allow to \"delineate\" the ideal characteristics of a super hero and hence, where we could innovate if we do not want to reproduce things that already done while imagining them ?\n",
      "the last questions correlate to social trends : do some characteristics appear during a certain period ? if yes, is it correlated to some specific social context ? (new type of wars impacting how we imagine our heroes ?).\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this dataset compiles the titles, publication dates, and other data about all reports published in the official capacities of new york city government agency work are listed in the city hall library catalog. the catalog functions like a city-level equivalent of the national library of congress, and goes back very far --- at least to the 1800s.\n",
      "content\n",
      "columns are provided for the report name and report sub-header, the year the report was issued, the name of the publisher compiling the report, and some other smaller fields.\n",
      "acknowledgements\n",
      "this data was originally published in a pound (\"#\") delimited dataset on the new york city open data portal. it has been restructured as a csv and lightly cleaned up for formatting prior to being uploaded to kaggle.\n",
      "inspiration\n",
      "can you separate reporting publications by the city of new york into topics?\n",
      "who are the most common report issuers, and what causes do they represent?\n",
      "what are some common elements to report titles?\n",
      "context\n",
      "it would be fun to have an system which could generate new cocktails from just the name and if the cocktail already exists produce the right result\n",
      "content\n",
      "the data is a scraped collection of cocktails and their ingredients\n",
      "acknowledgements\n",
      "the idea was made at an ai-first meetup and the scraping idea came from the group who worked on it\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "dataset contains infromation about car accidents in 2013-2016\n",
      "main columns: date time\n",
      "borough latitude\n",
      "longitude\n",
      "location\n",
      "json file with nyc boroughs geoshapes\n",
      "context\n",
      "realtime gtfs data collected from 2017_5_29_11_30_15 till 2017_6_6_13_42_11 at saint-petersburg, russia. data requested from: http://transport.orgp.spb.ru/portal/transport/internalapi/gtfs\n",
      "content\n",
      "archive contain real-time gtfs files (https://developers.google.com/transit/gtfs-realtime/)\n",
      "acknowledgements\n",
      "thanks to http://transport.orgp.spb.ru for implementing api\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "there is no story behind this data.\n",
      "these are just supplementary datasets which i plan on using for plotting county wise data on maps.. (in particular for using with my kernel : https://www.kaggle.com/stansilas/maps-are-beautiful-unemployment-is-not/)\n",
      "as that data set didn't have the info i needed for plotting an interactive map using highcharter .\n",
      "content\n",
      "since i noticed that most demographic datasets here on kaggle, either have state code, state name, or county name + state name but not all of it i.e county name, fips code, state name + state code.\n",
      "using these two datasets one can get any combination of state county codes etc.\n",
      "states.csv has state name + code\n",
      "us counties.csv has county wise data.\n",
      "acknowledgements\n",
      "picture : https://unsplash.com/search/usa-states?photo=-ro2dfpl7we\n",
      "counties : https://www.census.gov/geo/reference/codes/cou.html\n",
      "state :\n",
      "inspiration\n",
      "not applicable.\n",
      "context\n",
      "taken from; http://www.internationalgenome.org/data\n",
      "content\n",
      "coming soon\n",
      "acknowledgements\n",
      "special thanks to international genome org.\n",
      "inspiration\n",
      "coming soon\n",
      "introduction\n",
      "if you love movies, and you love san francisco, you're bound to love this -- a listing of filming locations of movies shot in san francisco starting from 1924. you'll find the titles, locations, fun facts, names of the director, writer, actors, and studio for most of these films.\n",
      "inspiration\n",
      "combine with the popular imdb 5000 dataset on kaggle to see how movies filmed in san francisco are rated. click on \"new kernel\" and add this dataset source by clicking on \"input files\".\n",
      "start a new kernel\n",
      "context\n",
      "this dataset contains a subset of information, pertaining to the weather patterns during januaray 2016 - june 2016 in nyc. this may be important to those who are looking to add columns to their dataset.\n",
      "content\n",
      "preciptation, snowfall, temperatures, latitude and longitude, along with dates. all the important information we need to know. unforunately, this is not minute to minue information, but strictly daily information.\n",
      "acknowledgements\n",
      "i would like to thank mcrepy94 for sharing the link https://www.ncdc.noaa.gov/cdo-web/search?datasetid=ghcnd. please upvote his post if you upvote this.\n",
      "inspiration\n",
      "i want people to look at this dataset, and create more features for their uses.\n",
      "context\n",
      "this data was originally taken from titanic: machine learning from disaster .but its better refined and cleaned & some features have been self engineered typically for logistic regression . if you use this data for other models and benefit from it , i would be happy to receive your comments and improvements.\n",
      "content\n",
      "there are two files namely:- train_data.csv :- typically a data set of 792x16 . the survived column is your target variable (the output you want to predict).the parch & sibsb columns from the original data set has been replaced with a single column called family size.\n",
      "all categorical data like embarked , pclass have been re-encoded using the one hot encoding method .\n",
      "additionally, 4 more columns have been added , re-engineered from the name column to title_1 to title_4 signifying males & females depending on whether they were married or not .(mr , mrs ,master,miss). an additional analysis to see if married or in other words people with social responsibilities had more survival instincts/or not & is the trend similar for both genders.\n",
      "all missing values have been filled with a median of the column values . all real valued data columns have been normalized.\n",
      "test_data.csv :- a data of 100x16 , for testing your model , the arrangement of test_data exactly matches the train_data\n",
      "i am open to feedbacks & suggesstions\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "i found this dataset after exploring numerous datasets for my personal project.\n",
      "content\n",
      "it contains images(in pixel format) along with emotions .\n",
      "acknowledgements\n",
      "well, i found it on my own.\n",
      "inspiration\n",
      "context\n",
      "the college scorecard was created by the u.s. department of education in an attempt to better understand the efficacy of colleges in the united states. the scorecard reports information such as the cost of tuition, undergraduate enrollment size, and the rate of graduation. further details can be found in the file \"fulldatadescription.pdf\".\n",
      "content\n",
      "u.s. college statistics from 1996 to 2015, organized by year. data was pulled from the official website (https://collegescorecard.ed.gov/data/) in june of 2017. it was reportedly last updated in january 2017.\n",
      "context:\n",
      "some words, like “the” or “and” in english, are used a lot in speech and writing. for most natural language processing applications, you will want to remove these very frequent words. this is usually done using a list of “stopwords” which has been complied by hand.\n",
      "content:\n",
      "this project uses the source texts provided by the african storybook project as a corpus and provides a number of tools to extract frequency lists and lists of stopwords from this corpus for the 60+ languages covered by asp.\n",
      "included in this dataset are the following languages:\n",
      "afrikaans: stoplist and word frequency\n",
      "hausa: stoplist and word frequency\n",
      "lugbarati: word frequency only\n",
      "lugbarati (official): word frequency only\n",
      "somali: stoplist and word frequency\n",
      "sesotho: stoplist and word frequency\n",
      "kiswahili: stoplist and word frequency\n",
      "yoruba: stoplist and word frequency\n",
      "isizulu: stoplist and word frequency\n",
      "files are named using the language’s iso code. for each language, code.txt is the list of stopwords, and code_frequency_list.txt is word frequency information. a list of iso codes the the languages associated with them may be found in iso_codes.csv.\n",
      "acknowledgements:\n",
      "this project therefore attempts to fill in the gap in language coverage for african language stoplists by using the freely-available and open-licensed asp source project as a corpus. dual-licensed under cc-by and apache-2.0 license. compiled by liam doherty. more information and the scripts used to generate these files are available here.\n",
      "inspiration:\n",
      "this dataset is mainly helpful for use during nlp analysis, however there may some interesting insights in the data.\n",
      "what qualities do stopwords share across languages? given a novel language, could you predict what its stopwords should be?\n",
      "what stopwords are shared across languages?\n",
      "often, related languages will have words with the same meaning and similar spellings. can you automatically identify any of these pairs of words?\n",
      "you may also like:\n",
      "stopword lists for 19 languages (mainly european and south asian)\n",
      "context:\n",
      "the armed conflict location and event data project is designed for disaggregated conflict analysis and crisis mapping. this dataset codes the dates and locations of all reported political violence and protest events in developing asian countries in. political violence and protest includes events that occur within civil wars and periods of instability, public protest and regime breakdown. the project covers 2015 to the present.\n",
      "content:\n",
      "these data contain information on:\n",
      "dates and locations of conflict events;\n",
      "specific types of events including battles, civilian killings, riots, protests and recruitment activities;\n",
      "events by a range of actors, including rebels, governments, militias, armed groups, protesters and civilians;\n",
      "changes in territorial control; and\n",
      "reported fatalities.\n",
      "event data are derived from a variety of sources including reports from developing countries and local media, humanitarian agencies, and research publications. please review the codebook and user guide for additional information: the codebook is for coders and users of acled, whereas the brief guide for users reviews important information for downloading, reviewing and using acled data. a specific user guide for development and humanitarian practitioners is also available, as is a guide to our sourcing materials.\n",
      "acknowledgements:\n",
      "acled is directed by prof. clionadh raleigh (university of sussex). it is operated by senior research manager andrea carboni (university of sussex) for africa and hillary tanoff for south and south-east asia. the data collection involves several research analysts, including charles vannice, james moody, daniel wigmore-shepherd, andrea carboni, matt batten-carew, margaux pinaud, roudabeh kishi, helen morris, braden fuller, daniel moody and others. please cite:\n",
      "raleigh, clionadh, andrew linke, håvard hegre and joakim karlsen. 2010. introducing acled-armed conflict location and event data. journal of peace research 47(5) 651-660.\n",
      "inspiration:\n",
      "do conflicts in one region predict future flare-ups? how do the individual actors interact across time?\n",
      "context\n",
      "i want to create an app that could generate instrumentals of songs that we listen daily.\n",
      "content\n",
      "i have generated wav files of different notes using garageband. i will use this data to classify musical notes.\n",
      "acknowledgements\n",
      "i took stanford paper on sheet music from audio files by jan dlabal and richard wedeen\n",
      "inspiration\n",
      "can we even generate midi files of complicated melodies just using wav files of the song.\n",
      "this dataset contains run time statistics and details about scores for the first development round of nips 2017 adversarial learning competition\n",
      "content\n",
      "matrices with intermediate results\n",
      "following matrices with intermediate results are provided:\n",
      "accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense\n",
      "error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense\n",
      "hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense\n",
      "in each of these matrices, rows correspond to defenses, columns correspond to attack. also first row and column are headers with kaggle team ids (or baseline id).\n",
      "scores and run time statistics of submissions\n",
      "following files contain scores and run time stats of the submissions:\n",
      "non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks\n",
      "targeted_attack_results.csv - scores and run time statistics of all targeted attacks\n",
      "defense_results.csv - scores and run time statistics of all defenses\n",
      "each row of these files correspond to one submission. columns have following meaning:\n",
      "kaggleteamid - either kaggle team id or id of the baseline.\n",
      "teamname - human readable team name\n",
      "score - raw score of the submission\n",
      "normalizedscore - normalized (to be between 0 and 1) score of the submission\n",
      "minevaltime - minimum evaluation time of 100 images\n",
      "maxevaltime - maximum evaluation time of 100 images\n",
      "medianevaltime - median evaluation time of 100 images\n",
      "meanevaltime - average evaluation time of 100 images\n",
      "notes about the data\n",
      "due to team merging these files contain slightly more submissions than reflected in leaderboard.\n",
      "also not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one data file for each of these countries:\n",
      "bermuda - bermuda.csv\n",
      "canada - canada.csv\n",
      "curaçao - curaçao.csv\n",
      "jamaica - jamaica.csv\n",
      "mexico - mexico.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip.\n",
      "context\n",
      "openaddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\n",
      "content\n",
      "this dataset contains one data file for each of these countries:\n",
      "argentina - argentina.csv\n",
      "brazil - brazil.csv\n",
      "chile - chile.csv\n",
      "columbia - columbia.csv\n",
      "uraguay - uraguay.csv\n",
      "field descriptions:\n",
      "lon - longitude\n",
      "lat - latitude\n",
      "number - street number\n",
      "street - street name\n",
      "unit - unit or apartment number\n",
      "city - city name\n",
      "district - ?\n",
      "region - ?\n",
      "postcode - postcode or zipcode\n",
      "id - ?\n",
      "hash - ?\n",
      "acknowledgements\n",
      "data collected around 2017-07-25 by openaddresses (http://openaddresses.io).\n",
      "address data is essential infrastructure. street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\n",
      "data licenses can be found in license.txt.\n",
      "data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\n",
      "inspiration\n",
      "use this dataset to create maps in conjunction with other datasets to map weather, crime, or plan your next canoeing trip.\n",
      "context\n",
      "this dataset contains lot of historical sales data. it was extracted from a brazilian top retailer and has many skus and many stores. the data was transformed to protect the identity of the retailer.\n",
      "content\n",
      "[tbd]\n",
      "acknowledgements\n",
      "this data would not be available without the full collaboration from our customers who understand that sharing their core and strategical information has more advantages than possible hazards. they also support our continuos development of innovative ml systems across their value chain.\n",
      "inspiration\n",
      "every retail business in the world faces a fundamental question: how much inventory should i carry? in one hand to mush inventory means working capital costs, operational costs and a complex operation. on the other hand lack of inventory leads to lost sales, unhappy customers and a damaged brand.\n",
      "current inventory management models have many solutions to place the correct order, but they are all based in a single unknown factor: the demand for the next periods.\n",
      "this is why short-term forecasting is so important in retail and consumer goods industry.\n",
      "we encourage you to seek for the best demand forecasting model for the next 2-3 weeks. this valuable insight can help many supply chain practitioners to correctly manage their inventory levels.\n",
      "\"first-person narratives of the american south\" is a collection of diaries, autobiographies, memoirs, travel accounts, and ex-slave narratives written by southerners. the majority of materials in this collection are written by those southerners whose voices were less prominent in their time, including african americans, women, enlisted men, laborers, and native americans.\n",
      "the narratives available in this collection offer personal accounts of southern life between 1860 and 1920, a period of enormous change. at the end of the civil war, the south faced the enormous challenge of re-creating their society after their land had been ravaged by war, many of their men were dead or injured, and the economic and social system of slavery had been abolished. many farmers, confronted by periodic depressions and market turmoil, joined political and social protest movements. for african americans, the end of slavery brought hope for unprecedented control of their own lives, but whether they stayed in the south or moved north or west, they continued to face social and political oppression. most african americans in the south were pulled into a darwinistic sharecropper system and saw their lives circumscribed by the rise of segregation. as conservative views faced a growing challenge from modernist thought, southern arts, sciences, and religion also reflected the considerable tensions manifested throughout southern society. admidst these dramatic changes, southerners who had lived in the antebellum south and soldiers who had fought for the confederacy wrote memoirs that and strived to preserve a memory of many different experiences. southerners recorded their stories of these tumultuous times in print and in diaries and letters, but few first-person narratives, other than those written by the social and economic elite found their way into the national print culture. in this online collection, accounts of life on the farm or in the servants' quarters or in the cotton mill have priority over accounts of public lives and leading military battles. each narrative offers a unique perspective on life in the south, and serves as an important primary resource for the study of the american south. the original texts for \"first-person narratives of the american south\" come from the university library of the university of north carolina at chapel hill, which includes the southern historical collection, one of the largest collections of southern manuscripts in the country and the north carolina collection, the most complete printed documentation of a single state anywhere. the docsouth editorial board, composed of faculty and librarians at unc and staff from the unc press, oversees this collection and all other collections on documenting the american south.\n",
      "context\n",
      "the north american slave narratives collection at the university of north carolina contains 344 items and is the most extensive collection of such documents in the world.\n",
      "the physical collection was digitized and transcribed by students and library employees. this means that the text is far more reliable than uncorrected ocr output which is common in digitized archives.\n",
      "more information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh\n",
      "the plain text files have been optimized for use in voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. you may wish to delete these in order to focus your analysis on just the narratives.\n",
      "the .csv file acts as a table of contents for the collection and includes title, author, publication date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with voyant: http://voyant-tools.org/).\n",
      "copyright statement and acknowledgements\n",
      "with the exception of \"fields's observation: the slave narrative of a nineteenth-century virginian,\" which has no known rights, the texts, encoding, and metadata available in open docsouth are made available for use under the terms of a creative commons attribution license (cc by 4.0:http://creativecommons.org/licenses/by/4.0/). users are free to copy, share, adapt, and re-publish any of the content in open docsouth as long as they credit the university library at the university of north carolina at chapel hill for making this material available.\n",
      "if you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. send any feedback to wilsonlibrary@unc.edu.\n",
      "about the docsouth data project\n",
      "doc south data provides access to some of the documenting the american south collections in formats that work well with common text mining and data analysis tools.\n",
      "documenting the american south is one of the longest running digital publishing initiatives at the university of north carolina. it was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.\n",
      "doc south data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. we have made it easy to use tools such as voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis.\n",
      "feel free to use this data.\n",
      "thanks\n",
      "context\n",
      "per capita total expenditure on health at average exchange rate (us$)\n",
      "content\n",
      "per capita total expenditure on health expressed at average exchange rate for that year in us$. current prices.\n",
      "acknowledgements\n",
      "it is downloaded from who, gapminder.\n",
      "inspiration\n",
      "it seems good to me for forecasting the total spending of money on health around the world, it can be good use case for prediction of money spending on health spending.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "information reproduced from the national archives:\n",
      "the korean conflict extract data file of the defense casualty analysis system (dcas) extract files contains records of u.s. military fatal casualties of the korean war. these records were transferred into the custody of the national archives and records administration in 2008. the defense casualty analysis system extract files were created by the defense manpower data center (dmdc) of the office of the secretary of defense. the records correspond to the korean war conflict statistics on the dmdc web site, which is accessible online at https://www.dmdc.osd.mil/dcas/pages/main.xhtml .\n",
      "a full series description for the defense casualty analysis system (dcas) extract files is accessible online via the national archives catalog under the national archives identifier 2240988. the korean war conflict extract data file is also accessible for direct download via the national archives catalog file-level description, national archives identifier 2240988.\n",
      "content\n",
      "the raw data files have been cleaned and labelled as best as i can with reference to the accompanying supplemental code lists. names and id numbers have been removed out of respect and to provide anonymity.\n",
      "data fields: * service_type * service_code * enrollment * branch * rank * pay_grade * position * birth_year * sex * home_city * home_county * home_state * state_code * nationality * marital_status * ethnicity * ethnicity_1 * ethnicity_2 * division * fatality_year * fatality_date * hostility_conditions * fatality * burial_status\n",
      "acknowledgements\n",
      "data provided by the u.s. national archives and records administration.\n",
      "raw data can be accessed via the following link: https://catalog.archives.gov/id/2240988\n",
      "inspiration\n",
      "by cleaning the data i hope to give wider access to this resource.\n",
      "why?\n",
      "https://twitter.com/lindsaylee13/status/826298008824328192\n",
      "i expressed my interest in using my technical skills anywhere i could. one of the suggestions made was to have a running list of representation throughout the nation.\n",
      "my approach for this case was to retrieve a list of zip codes and return representation via google's civic information api.\n",
      "my source for zip codes is: https://www.aggdata.com/node/86\n",
      "this data set currently has just over 29,000 of the total 43,000 listed here. this is due to:\n",
      "1) rate limit 2) zip codes not found\n",
      "my goal is to continue to expand on the list of zip codes to get as comprehensive of a view of representation as possible.\n",
      "this is not possible without google's civic information api!\n",
      "content\n",
      "the bulletins were downloaded from the dane web platform and organized by products in cvs files.\n",
      "acknowledgements\n",
      "sipsa\n",
      "context\n",
      "wine recognition dataset from uc irvine. great for testing out different classifiers\n",
      "labels: \"name\" - number denoting a specific wine class\n",
      "number of instances of each wine class\n",
      "class 1 - 59\n",
      "class 2 - 71\n",
      "class 3 - 48\n",
      "features:\n",
      "alcohol\n",
      "malic acid\n",
      "ash\n",
      "alcalinity of ash\n",
      "magnesium\n",
      "total phenols\n",
      "flavanoids\n",
      "nonflavanoid phenols\n",
      "proanthocyanins\n",
      "color intensity\n",
      "hue\n",
      "od280/od315 of diluted wines\n",
      "proline\n",
      "content\n",
      "\"this data set is the result of a chemical analysis of wines grown in the same region in italy but derived from three different cultivars. the analysis determined the quantities of 13 constituents found in each of the three types of wines\"\n",
      "acknowledgements\n",
      "lichman, m. (2013). uci machine learning repository [http://archive.ics.uci.edu/ml]. irvine, ca: university of california, school of information and computer science.\n",
      "@misc{lichman:2013 , author = \"m. lichman\", year = \"2013\", title = \"{uci} machine learning repository\", url = \"http://archive.ics.uci.edu/ml\", institution = \"university of california, irvine, school of information and computer sciences\" }\n",
      "uc irvine data base: \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine\"\n",
      "sources: (a) forina, m. et al, parvus - an extendible package for data exploration, classification and correlation. institute of pharmaceutical and food analysis and technologies, via brigata salerno, 16147 genoa, italy. (b) stefan aeberhard, email: stefan@coral.cs.jcu.edu.au (c) july 1991 past usage: (1) s. aeberhard, d. coomans and o. de vel, comparison of classifiers in high dimensional settings, tech. rep. no. 92-02, (1992), dept. of computer science and dept. of mathematics and statistics, james cook university of north queensland. (also submitted to technometrics).\n",
      "the data was used with many others for comparing various classifiers. the classes are separable, though only rda has achieved 100% correct classification. (rda : 100%, qda 99.4%, lda 98.9%, 1nn 96.1% (z-transformed data)) (all results using the leave-one-out technique)\n",
      "(2) s. aeberhard, d. coomans and o. de vel, \"the classification performance of rda\" tech. rep. no. 92-01, (1992), dept. of computer science and dept. of mathematics and statistics, james cook university of north queensland. (also submitted to journal of chemometrics).\n",
      "inspiration\n",
      "this data set is great for drawing comparisons between algorithms and testing out classifications models when learning new techniques\n",
      "context\n",
      "human communication abilities have greatly evolved with time. speech/text/images/videos are the channels we often use to communicate, store/share information.\"text\" is one of the primary modes in formal communication and might continue to be so for quite some time.\n",
      "i wonder, how many words, a person would type in his lifetime, when he sends an email/text message or prepare some documents. the count might run into millions. we are accustomed to key-in words, without worrying much about the 'effort' involved in typing the word. we don't bother much about the origin of the word or the correlation between the meaning and the textual representation. 'big' is actually smaller than 'small' just going by the words' length.\n",
      "i had some questions, which, i thought, could be best answered by analyzing the big data we are surrounded with today. since the data volume growing at such high rates, can we bring about some kind of optimization or restructuring in the word usage, so that, we are benefited in terms of data storage, transmission, processing. can scanning more documents, would provide better automated suggestions in email / chats, based on what word usually follows a particular word, and assist in quicker sentence completion.\n",
      "what set of words, in the available text content globally, if we can identify and condense, would reduce the overall storage space required.\n",
      "what set of words in the regular usage, email/text/documents, if we condense, would reduce the total effort involved in typing (keying-in the text) and reduce the overall size of the text content, which eventually might lead to lesser transmission time, occupy less storage space, lesser processing time for applications which feed on these data for analysis/decision making.\n",
      "to answer these, we may have to parse the entire web and almost every email/message/blog post/tweet/machine generated content that is in or will be generated in every phone/laptop/computer/servers, data generated by every person/bot. considering tones of text lying around in databases across the world webpages/wikipedia/text archives/digital libraries, and the multiple versions/copies of these content. parsing all, would be a humongous task. fresh data is continually generated from various sources. the plate is never empty, if the data is cooked at a rate than the available processing capability.\n",
      "here is an attempt to analyze a tiny chunk of data, to see, if the outcome is significant enough to take a note of, if the finding is generalized and extrapolated to larger databases.\n",
      "content\n",
      "looking out for a reliable source, i could not think of anything better than the wikipedia database of webpages. wiki articles are available for download as html dumps, for any offline processing. https://dumps.wikimedia.org/other/static_html_dumps/, the dump which i downloaded is a ~40 gb compressed file (that turned in ~208 gb folder containing ~15 million files, upon extraction).\n",
      "with my newly acquired r skills, i tried to parse the html pages, extract the distinct words with their total count in the page paragraphs.i could consolidate the output from the \"first million\" of html files out of available 15 million. attached dataset \"wikiwords_firstmillion.csv\" is a comma separated file with the list of words and their count. there are two columns - word and count. \"word\" column contains distinct words as extracted from the paragraphs in the wiki pages and \"count\" column has the count of occurrence in one million wiki pages. non-alphanumeric characters have been removed at the time of text extraction.\n",
      "any array of characters separated by space are included in the list of words and the count has been presented as is without any filters. to get better estimates, it should be ok to make suitable assumptions, like considering root words, ignoring words if they appear more specific to wikipedia pages (welcome, wikipedia, articles, pages, edit, contribution.. ).\n",
      "acknowledgements\n",
      "wikimedia, for providing the offline dumps r community, for the software/packages/blog posts/articles/suggestions and solution on the q & a sites\n",
      "inspiration\n",
      "in case, the entire english language community across the world decides to designate every alphabet as a word [apart from 'a' and 'i' all other alphabets seem to be potential candidates to be a word, a one-lettered word],\n",
      "(a) which of the 24 words from the data set are most eligible to get upgraded as a one letter word. assuming, it is decided to replace the existing words with the newly designated one-lettered word, to achieve storage efficiency.\n",
      "(b) assuming, the word count in the data set is a fair estimate of the composition of the words available in the global text content, (say we do a \"find\" and \"replace\" on global text content). if the current big data size is 3 exabytes (10 ^ 18), and say 30% of it is text content, how much space reduction can be achieved with (a), assuming 1 character requires 1 byte of storage space.\n",
      "(c) suppose, it is decided to accommodate 10 short cut keys for 10 different words,one short cut key for each word. which 10 words would increase the speed of text documentation assuming, (1) same amount of time is taken to type any word (2) time required to type a word increases with its length\n",
      "curious to know the findings!!\n",
      "context\n",
      "publicly available data from the city of chicago. see more here.\n",
      "content\n",
      "data is from jan 1, 2001 - dec. 31, 2016. the original dataset has over 6,000,000 entries (1.4gb)...this is just a 1% random sample to make computation time quicker and because of kaggle's 500mb limit.\n",
      "please visit this dataset's official page for a more complete description and list of caveats.\n",
      "context\n",
      "this dataset is for showing how to visualize od datasets\n",
      "content\n",
      "this dataset contains all the cities where the british queen has visited in her lifetime.\n",
      "acknowledgements\n",
      "the dataset is obtained from the internet.\n",
      "past research\n",
      "no\n",
      "inspiration\n",
      "showing od dataset is very fun.\n",
      "82.558 human instructions in chinese extracted from wikihow\n",
      "step-by-step instructions in chinese extracted from wikihow and decomposed into a formal graph representation in rdf.\n",
      "this is one of multiple dataset repositories for different languages. for more information, additional resources, and other versions of these instructions in other languages see the main kaggle dataset page:\n",
      "https://www.kaggle.com/paolop/human-instructions-multilingual-wikihow\n",
      "to cite this dataset use: paolo pareti, benoit testu, ryutaro ichise, ewan klein and adam barker. integrating know-how into the linked data cloud. knowledge engineering and knowledge management, volume 8876 of lecture notes in computer science, pages 385-396. springer international publishing (2014) (pdf) (bibtex)\n",
      "this dataset is based on original instructions from wikihow accessed on the 3rd of march 2017.\n",
      "for any queries and requests contact: paolo pareti\n",
      "context\n",
      "i recently implemented an iphone app for my younger daughter to learn multiplication. it probably was as much of a learning exercise for me as it was for her. but now i want to tackle the next challenge and add some more smarts with a module to recognise her handwritten digits. instead of taking a pixel based approach, i took the opportunity of the touch-based input device to record the training data as vectorized paths.\n",
      "content\n",
      "the data contains 400 paths for digits along with their matching labels. each path is normalized to 20 vectors (2d) and the total length of the vectors is normalized to ~100. find an example record of the json-structure below:\n",
      "{ \"items\": [ {\"y\":1, \"id\": 1488036701781, \"p\": [[0,0],[1,-2],[7,-9],[3,-5],[6,-9],[2,-3],[3,-4],[1,-1],[1,-1],[0,0],[0,2],[0,10],[0,8],[0,17],[0,6],[0,10],[0,3],[0,3],[0,0],[-1,1]] }] }\n",
      "acknowledgements\n",
      "thanks to my daughter for the training data!\n",
      "inspiration\n",
      "i hope you have fun with learning and i very much welcome hints on how to better capture the data.\n",
      "mnist data from http://neuralnetworksanddeeplearning.com\n",
      "context\n",
      "condensation of data from world bank (gdp) and bacen - banco central do brasil (electricity consumption). gdp is expressed in usd trillion and electricity in terawatt hour year.\n",
      "content\n",
      "three columns : \"year\", \"tw/h\" and \"gdp\"\n",
      "acknowledgements\n",
      "world bank and bacen.\n",
      "context\n",
      "this will act as the base data for the investigation into the possible solutions for the uk energy requirements\n",
      "content\n",
      "a cleaned version of the uk statistics on renewable energy generation.\n",
      "acknowledgements\n",
      "https://www.gov.uk/government/statistics/regional-renewable-statistics7\n",
      "all content is available under the open government licence v3.0,\n",
      "context\n",
      "in progress...\n",
      "content\n",
      "in progress...\n",
      "acknowledgements\n",
      "in progress...\n",
      "inspiration\n",
      "in progress...\n",
      "the indieweb is a people-focused alternative to the \"corporate\" web. participants use their own personal web sites to post, reply, share, organize events and rsvp, and interact in online social networking in ways that have otherwise been limited to centralized silos like facebook and twitter.\n",
      "the indie map dataset is a social network of the 2300 most active indieweb sites, including all connections between sites and number of links in each direction, broken down by type. it includes:\n",
      "5.8m web pages, including raw html, parsed microformats2, and extracted links with metadata.\n",
      "631m links and 706k \"friend\" relationships between sites.\n",
      "380gb of html and http requests in warc format.\n",
      "the zip file here contains a json file for each site, which includes metadata, a list of other sites linked to and from, and the number of links of each type.\n",
      "the complete dataset of 5.8m html pages is available in a publicly accessible google bigquery dataset. the raw pages can also be downloaded as warc files. they're hosted on google cloud storage.\n",
      "more details in the full documentation.\n",
      "indie map is free, open source, and placed into the public domain via cc0. crawled content remains the property of each site's owner and author, and subject to their existing copyrights.\n",
      "you can find the brand, model and version information of almost all automobile manufacturers between 1970 and 2016 in this database.\n",
      "context\n",
      "classification calories and food types\n",
      "content\n",
      "it's useful to analyse the food related queries by finding calories level, preferences, veg type etc.,\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "want to know the food preference quickly ?\n",
      "context\n",
      "the united states constitution provides that the president \"shall nominate, and by and with the advice and consent of the senate, shall appoint ambassadors, other public ministers and consuls, judges of the supreme court, and all other officers of the united states, whose appointments are not herein otherwise provided for...\" (article ii, section 2). this provision, like many others in the constitution, was born of compromise, and, over the more than two centuries since its adoption, has inspired widely varying interpretations.\n",
      "the president nominates all federal judges in the judicial branch and specified officers in cabinet-level departments, independent agencies, the military services, the foreign service and uniformed civilian services, as well as u.s. attorneys and u.s. marshals. the importance of the position, the qualifications of the nominee, and the prevailing political climate influence the character of the senate's response to each nomination. views of the senate's role range from a narrow construction that the senate is obligated to confirm unless the nominee is manifestly lacking in character and competence, to a broad interpretation that accords the senate power to reject for any reason a majority of its members deems appropriate. just as the president is not required to explain why he selected a particular nominee, neither is the senate obligated to give reasons for rejecting a nominee.\n",
      "acknowledgements\n",
      "the confirmation vote records were recorded, compiled, and published by the office of the secretary of the senate.\n",
      "image enhancement\n",
      "the data accompanying the lecture about image enhancement from anders kaestner as part of the quantitative big imaging course.\n",
      "the slides for the lecture are here\n",
      "context\n",
      "this is a small data set of us presidents heights.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "content\n",
      "this is a slightly modified version of the openly available data set 'open data 500 companies - full list' provided by the od500 global network ('http://www.opendata500.com/').\n",
      "i am using this dataset for a kernel project series which will be investigating the value and worth of a company's choice of logo design. therefore, have removed columns such as \"description\" and \"short description\", as well as a few others. if you'd like the entire original dataset please download from the original source here --> 'http://www.opendata500.com/us/download/us_companies.csv'\n",
      "acknowledgements\n",
      "i take no credit for the collection, production, or presentation of this data set. i am simply using it for a person research study. the creators are: http://www.opendata500.com/us/list/\n",
      "context\n",
      "automotive crash data for 2014\n",
      "content\n",
      "data on weather conditions, people involved, locations etc where traffic accidents have occured.\n",
      "acknowledgements\n",
      "this dataset is simply a port to csv from an xslx dataset uploaded by aditi\n",
      "inspiration\n",
      "interested to know what are the factors involved that make auto accidents fatal\n",
      "this codded dataset is from marketing agency and reflact internet users activity at the site page.\n",
      "context\n",
      "calcium, dust and nitrate concentrations in monthly resolution in ice core dml94c07_38 (b38). doi:10.1594/pangaea.837875\n",
      "content\n",
      "acknowledgements\n",
      "schmidt, kerstin; wegner, anna; weller, rolf; leuenberger, daiana; tijm-reijmer, carleen h; fischer, hubertus (2014): calcium, dust and nitrate concentrations in monthly resolution in ice core dml94c07_38 (b38). doi:10.1594/pangaea.837875, in supplement to: schmidt, kerstin; wegner, anna; weller, rolf; leuenberger, daiana; fischer, hubertus (submitted): variability of aerosol tracers in ice cores from coastal dronning maud land, antarctica. the cryosphere\n",
      "inspiration\n",
      "test data\n",
      "context\n",
      "aggregate and visualize data how you want—from tabular to graphical and geographic forms—for more profitable underwriting.\n",
      "perform sophisticated, multi-dimensional analysis to supply any data combination and permutation.\n",
      "respond to claims quickly and improve customer satisfaction with real-time and historical access to catastrophe and hazard data\n",
      "content\n",
      "the sample insurance file contains 36,634 records in florida for 2012 from a sample company that implemented an agressive growth plan in 2012. there are total insured value (tiv) columns containing tiv from 2011 and 2012, so this dataset is great for testing out the comparison feature. this file has address information that you can choose to geocode, or you can use the existing latitude/longitude in the file.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this was the dataset used in the article 'sentiment analysis of portuguese comments from foursquare' (doi:10.1145/2976796.2988180).\n",
      "the dataset is composed of tips referring to localities of the city of são paulo/brazil. to the data collection, we use the foursquare api . the tips belong to the foursquare's categories: food, shop & service and nightlife spot. the database has a total of 179,181 tips.\n",
      "@inproceedings{almeida:2016:sap:2976796.2988180, author = {almeida, thais g. and souza, bruno a. and menezes, alice a.f. and figueiredo, carlos m.s. and nakamura, eduardo f.}, title = {sentiment analysis of portuguese comments from foursquare}, booktitle = {proceedings of the 22nd brazilian symposium on multimedia and the web}, series = {webmedia '16}, year = {2016}, isbn = {978-1-4503-4512-5}, location = {teresina, piau\\&#237; state, brazil}, pages = {355--358}, numpages = {4}, url = {http://doi.acm.org/10.1145/2976796.2988180}, doi = {10.1145/2976796.2988180}, acmid = {2988180}, publisher = {acm}, address = {new york, ny, usa}, keywords = {foursquare, sentiment analysis, supervised learning}, }\n",
      "context\n",
      "lexique v3.81 on www.lexique.org/\n",
      "content\n",
      "words of the french language with pronunciation, grouping and statistics.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "here i have data for last 10 year average indian rupee against usd. it will try to predict the value of a year.\n",
      "i am using leaner regression for this\n",
      "i was new to machine learning. i was thinking of how usd values made changes in india how that changes happen if we passed any year this program will try to predict\n",
      "context\n",
      "increase your earnings through analyzing the admob data set.\n",
      "content\n",
      "it has more than 12 column including earnings, impressions, and other attributes\n",
      "acknowledgements\n",
      "suleman bader owns this data set compiled by google admob.\n",
      "inspiration\n",
      "to increase your earnings\n",
      "council plan performance indicators by coventry city council\n",
      "the council plan sets out coventry city council's vision and priorities for the city.\n",
      "content\n",
      "these are the performance indicators in the council plan performance report in an open data format.\n",
      "acknowledgements\n",
      "coventry city council\n",
      "find out more at: www.coventry.gov.uk/performance/ visualisation: https://smarturl.it/covperformancedata\n",
      "the opennebula is an open source environment which provides cloud resources with the help of haizea as a lease manager. the haizea supports different types of leases from which deadline sensitive lease is one of them. in real time, most of the leases are deadline sensitive leases. these deadline sensitive leases are scheduled by using the backfilling based scheduling algorithms.\n",
      "please don't forget to cite:\n",
      "nayak, suvendu chandan, and chitaranjan tripathy. \"deadline sensitive lease scheduling in cloud computing environment using ahp.\" journal of king saud university-computer and information sciences (2016).\n",
      "import libraries\n",
      "    suppressmessages(library(xlconnect))\n",
      "    suppressmessages(library(xlsx))\n",
      "    suppressmessages(library(rvest))\n",
      "    suppressmessages(library(dplyr))\n",
      "    suppressmessages(library(lubridate))\n",
      "    suppressmessages(library(ggplot2))\n",
      "    suppressmessages(library(plotly))\n",
      "\n",
      "    setwd(\".../r dolartoday/\")\n",
      "\n",
      "    #download & import data from dolartoday.com | the \"url\" changes every 5 minutes aprox\n",
      "    url <- 'https://dolartoday.com/indicadores/'\n",
      "    url <- read_html(url) %>% html_nodes('a') %>% html_attrs()\n",
      "    url <- as.character(url[grep('xlsx',url)])\n",
      "    download.file(url,\"dolartoday.xlsx\",mode=\"wb\")\n",
      "    dt <- read.xlsx(\"dolartoday.xlsx\", sheetname=\"dolartoday\", colindex = 1:2)\n",
      "    dt$fecha <- as.date(dt$fecha, format = \"%m-%d-%y\")\n",
      "    dt$año <- as.numeric(format(dt$fecha,'%y'))\n",
      "    dt$dolartoday <- gsub(\",\",\".\",dt$dolartoday)\n",
      "    dt$dolartoday <- as.numeric(dt$dolartoday)\n",
      "\n",
      "    #add simadi data\n",
      "    simadi <- 'http://cambiobolivar.com/sistema-marginal-de-divisas/'\n",
      "    simadi <- read_html(simadi) %>% html_nodes('.wp-table-reloaded') %>% html_table()\n",
      "    simadi <- rbind(as.data.frame(simadi[1]),\n",
      "                    as.data.frame(simadi[2]),\n",
      "                    as.data.frame(simadi[3]),\n",
      "                    as.data.frame(simadi[4]))\n",
      "    simadi$fecha <- as.date(simadi$fecha,format=\"%d/%m/%y\")\n",
      "    simadi$tasa.simadi <- as.numeric(sub(\",\",\".\",sub(\",\",\".\",simadi$tasa.simadi)))\n",
      "    colnames(simadi) <- c(\"fecha\",\"simadi\")\n",
      "\n",
      "    #merge data\n",
      "    dt <- (merge(dt, simadi, all = true))\n",
      "    dt <- mutate(dt,relación=dolartoday/simadi)\n",
      "\n",
      "    # first plot\n",
      "    plot <- dt[dt$año>=2016,]\n",
      "    plot_ly(plot, x = ~fecha, y = ~dolartoday, name = 'dolartoday',type='scatter',connectgaps=true,mode='lines') %>%\n",
      "      add_trace(y = ~simadi, name='simadi',mode='lines', connectgaps=true) %>%\n",
      "      layout(title=\"dólar paralelo en venezuela\",\n",
      "             xaxis = list(title = \"fecha\"),\n",
      "             yaxis = list(title = \"bolívares por usd\")\n",
      "    )\n",
      "\n",
      "# second plot\n",
      "    dt$mes <- month(dt$fecha)\n",
      "    dt$año_mes <- paste(dt$año,dt$mes,sep=\"-\")\n",
      "    ggplot(filter(dt,\n",
      "                  año >= 2016), \n",
      "           aes(x = as.factor(mes), y = dolartoday)) +\n",
      "      geom_jitter(alpha = 0.7) +\n",
      "      geom_boxplot(fill = 'red', alpha = 0.7) +\n",
      "      ggtitle(\"boxplot: dolartoday por mes\") +\n",
      "      theme_bw() +\n",
      "      scale_y_continuous(name = \"dolartoday\") +\n",
      "      scale_x_discrete(name = \"meses\") +\n",
      "      facet_grid(año ~ .)\n",
      "    summary(filter(dt,año==2017)%>%select(dolartoday))\n",
      "this describe names of girls and boys, include the meaning of there names.\n",
      "the cells are: gender, name, meaning, origin\n",
      "this data can help to enrich other's data sets.\n",
      "connectionist bench (sonar, mines vs. rocks) data set\n",
      "content\n",
      "http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)\n",
      "content\n",
      "money supply m2 (broader money) bric economies in $ 1997-01-01 to 2015-12-01\n",
      "frequency quarterly\n",
      "context\n",
      "this is speeches scraped from unhcr.org website. it includes all speeches made by the high commissioner up until june 2014.\n",
      "content\n",
      "json array of speeches with the following keys: \"author\", \"by\", \"content\", \"id\", \"title\".\n",
      "acknowledgements\n",
      "www.unhcr.org\n",
      "inspiration\n",
      "what words are most used? which countries are mentioned? what do the high commissioners think about innovation? gay rights?\n",
      "this is filtered weather data that includes the date, precipitation, and average precipitation in syracuse, ny from june-september 2011-2016.\n",
      "us domestic flights delay: flight delays in the month of january,august, november and december of 2016\n",
      "summary\n",
      "this dataset (ml-20m) describes 5-star rating and free-text tagging activity from movielens, a movie recommendation service. it contains 20000263 ratings and 465564 tag applications across 27278 movies. these data were created by 138493 users between january 09, 1995 and march 31, 2015. this dataset was generated on october 17, 2016.\n",
      "users were selected at random for inclusion. all selected users had rated at least 20 movies. no demographic information is included. each user is represented by an id, and no other information is provided.\n",
      "the data are contained in six files, genome-scores.csv, genome-tags.csv, links.csv, movies.csv, ratings.csv and tags.csv. more details about the contents and use of all these files follows.\n",
      "usage license\n",
      "neither the university of minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. the data set may be used for any research purposes under the following conditions:\n",
      "the user may not state or imply any endorsement from the university of minnesota or the grouplens research group. - the user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\n",
      "the user may not redistribute the data without separate permission.\n",
      "the user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the grouplens research project at the university of minnesota.\n",
      "the executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. the entire risk as to the quality and performance of them is with you. should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\n",
      "in no event shall the university of minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\n",
      "citation\n",
      "to acknowledge use of the dataset in publications, please cite the following paper:\n",
      "f. maxwell harper and joseph a. konstan. 2015. the movielens datasets: history and context. acm transactions on interactive intelligent systems (tiis) 5, 4, article 19 (december 2015), 19 pages. doi=http://dx.doi.org/10.1145/2827872\n",
      "content and use of files\n",
      "verifying the dataset contents\n",
      "we provide a md5 checksum with the same name as the downloadable .zip file, but with a .md5 file extension. to verify the dataset:\n",
      "on linux\n",
      "md5sum ml-20m.zip; cat ml-20m.zip.md5\n",
      "on osx\n",
      "md5 ml-20m.zip; cat ml-20m.zip.md5\n",
      "windows users can download a tool from microsoft (or elsewhere) that verifies md5 checksums\n",
      "check that the two lines of output contain the same hash value.\n",
      "formatting and encoding\n",
      "the dataset files are written as comma-separated values files with a single header row. columns that contain commas (,) are escaped using double-quotes (\"). these files are encoded as utf-8. if accented characters in movie titles or tag values (e.g. misã©rables, les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for utf-8.\n",
      "user ids\n",
      "movielens users were selected at random for inclusion. their ids have been anonymized. user ids are consistent between ratings.csv and tags.csv (i.e., the same id refers to the same user across the two files).\n",
      "movie ids\n",
      "only movies with at least one rating or tag are included in the dataset. these movie ids are consistent with those used on the movielens web site (e.g., id 1 corresponds to the url https://movielens.org/movies/1). movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).\n",
      "ratings data file structure (ratings.csv)\n",
      "all ratings are contained in the file ratings.csv. each line of this file after the header row represents one rating of one movie by one user, and has the following format: userid,movieid,rating,timestamp\n",
      "the lines within this file are ordered first by userid, then, within user, by movieid.\n",
      "ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
      "timestamps represent seconds since midnight coordinated universal time (utc) of january 1, 1970.\n",
      "tags data file structure (tags.csv)\n",
      "all tags are contained in the file tags.csv. each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n",
      "userid,movieid,tag,timestamp the lines within this file are ordered first by userid, then, within user, by movieid.\n",
      "tags are user-generated metadata about movies. each tag is typically a single word or short phrase. the meaning, value, and purpose of a particular tag is determined by each user.\n",
      "timestamps represent seconds since midnight coordinated universal time (utc) of january 1, 1970.\n",
      "movies data file structure (movies.csv)\n",
      "movie information is contained in the file movies.csv. each line of this file after the header row represents one movie, and has the following format:\n",
      "movieid,title,genres movie titles are entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. errors and inconsistencies may exist in these titles.\n",
      "genres are a pipe-separated list, and are selected from the following:\n",
      "action\n",
      "adventure\n",
      "animation\n",
      "children's\n",
      "comedy\n",
      "crime\n",
      "documentary\n",
      "drama\n",
      "fantasy\n",
      "film-noir\n",
      "horror\n",
      "musical\n",
      "mystery\n",
      "romance\n",
      "sci-fi\n",
      "thriller\n",
      "war\n",
      "western\n",
      "(no genres listed)\n",
      "links data file structure (links.csv)\n",
      "identifiers that can be used to link to other sources of movie data are contained in the file links.csv. each line of this file after the header row represents one movie, and has the following format:\n",
      "movieid,imdbid,tmdbid movieid is an identifier for movies used by https://movielens.org. e.g., the movie toy story has the link https://movielens.org/movies/1.\n",
      "imdbid is an identifier for movies used by http://www.imdb.com. e.g., the movie toy story has the link http://www.imdb.com/title/tt0114709/.\n",
      "tmdbid is an identifier for movies used by https://www.themoviedb.org. e.g., the movie toy story has the link https://www.themoviedb.org/movie/862.\n",
      "use of the resources listed above is subject to the terms of each provider.\n",
      "tag genome (genome-scores.csv and genome-tags.csv)\n",
      "this data set includes a current copy of the tag genome.\n",
      "the tag genome is a data structure that contains tag relevance scores for movies. the structure is a dense matrix: each movie in the genome has a value for every tag in the genome.\n",
      "as described in this article, the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). the tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.\n",
      "the genome is split into two files. the file genome-scores.csv contains movie-tag relevance data in the following format:\n",
      "movieid,tagid,relevance the second file, genome-tags.csv, provides the tag descriptions for the tag ids in the genome file, in the following format:\n",
      "tagid,tag the tagid values are generated when the data set is exported, so they may vary from version to version of the movielens data sets.\n",
      "cross-validation\n",
      "prior versions of the movielens dataset included either pre-computed cross-folds or scripts to perform this computation. we no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. if you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see lenskit for tools, documentation, and open-source code examples.\n",
      "did you know that donald j. trump for president, inc paid for a subscription to the new york times on november 30th, 2016? curious to see where else over six million usd was spent and for what purposes? this dataset was downloaded from pro publica so you can find out.\n",
      "content\n",
      "here's what you get:\n",
      "line number\n",
      "entity type\n",
      "payee name\n",
      "payee state\n",
      "date\n",
      "amount\n",
      "purpose\n",
      "contributions\n",
      "want to contribute to this dataset? download contributions to donald j. trump, inc here and share on the dataset's discussion forum.\n",
      "acknowledgements\n",
      "this data was downloaded from pro publica. you can read about their data terms of use here.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "uk postcodes with the below additional geospace fields:\n",
      "latitude\n",
      "longitude\n",
      "easting\n",
      "northing\n",
      "positional quality indicator\n",
      "postcode area\n",
      "postcode district\n",
      "postcode sector\n",
      "outcode\n",
      "incode\n",
      "country\n",
      "usecases\n",
      "geocode any dataset containing uk postcodes using either latitude and longitude or northing and easting.\n",
      "geocode older data using terminated postcodes.\n",
      "recognise postcodes in a variety of different formats, for example \"rg2 6aa\" and \"rg26aa\".\n",
      "group records into geographical hierarchies using postcode area, postcode district and postcode sector (see below graphic).\n",
      "format of a uk postcode\n",
      "documentation\n",
      "full documentation is available on the open postcode geo homepage:\n",
      "https://www.getthedata.com/open-postcode-geo\n",
      "version\n",
      "august 2016.\n",
      "updated quarterly, for the most up to date version please see:\n",
      "https://www.getthedata.com/open-postcode-geo\n",
      "api\n",
      "open postcode geo is also available as an api.\n",
      "licence\n",
      "uk open government licence\n",
      "attribution required (see ons licences for more info):\n",
      "contains os data © crown copyright and database right (2016)\n",
      "contains royal mail data © royal mail copyright and database right (2016)\n",
      "contains national statistics data © crown copyright and database right (2016)\n",
      "derived from the ons postcode directory.\n",
      "acknowledgements\n",
      "open postcode geo was created and is maintained by getthedata, an open data portal organising uk open data geographically and signposting the source.\n",
      "context\n",
      "the subject matter of this dataset explores tesla's stock price from its initial public offering (ipo) to yesterday.\n",
      "content\n",
      "within the dataset one will encounter the following:\n",
      "the date - \"date\"\n",
      "the opening price of the stock - \"open\"\n",
      "the high price of that day - \"high\"\n",
      "the low price of that day - \"low\"\n",
      "the closed price of that day - \"close\"\n",
      "the amount of stocks traded during that day - \"volume\"\n",
      "the stock's closing price that has been amended to include any distributions/corporate actions that occurs before next days open - \"adj[usted] close\"\n",
      "acknowledgements\n",
      "through python programming and checking sentdex out, i acquired the data from yahoo finance. the time period represented starts from 06/29/2010 to 03/17/2017.\n",
      "inspiration\n",
      "what happens when the volume of this stock trading increases/decreases in a short and long period of time? what happens when there is a discrepancy between the adjusted close and the next day's opening price?\n",
      "this data set contains recent tweets regarding the trump's call to taiwan. i mined twitter for recent 2000 tweets. it's 3rd dec, 2016, 6:19 indian standard time. can be used for sentiment analysis and future predictions. thanx to me and twitter did you like it ??\n",
      "context\n",
      "crime classification\n",
      "content\n",
      "crime classification\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "osu! is a music rythm game that has 4 modes (check for more infos). in this dataset you can examine the rankings of the standard mode, taken on 26/03/2017 around 12pm. ranking is based on pp (performance points) awarded after every play, which are influenced by play accuracy and score; pps are then summed with weights: your top play will award you the whole pp points of the map, then the percentage is decreased (this can maintain balance between strong players and players who play too much). you can find here many other statistics.\n",
      "content\n",
      "the dataset contains some columns (see below) reporting statistics for every player in the top100 of the game in the standard mode. the ranking are ordered by pp. some players seem to have the same points, but there are decimals which are not shown in the ranking chart on the site\n",
      "acknowledgements\n",
      "i created this dataset on my own, so if you find something wrong please report. the data is public and accessible on this link ranking.\n",
      "inspiration\n",
      "i uploaded this just for newcomers in the world (like me) who want an easy stuff to work with, but that can give a lot of results. check out my notebook that will soon follow\n",
      "context\n",
      "financial contributions to presidential campaigns by the residents of north carolina\n",
      "content\n",
      "i acquired the data for free on the website: http://fec.gov/disclosurep/pnational.do. out of all the states, i chose nc since i currently live in raleigh, nc. the data set mostly consists of categorical data such as the 'candidate name', 'contributor name', etc.\n",
      "acknowledgements\n",
      "udacity - exploratory data analysis online class through which i was taken to the above website and i completed a project analyzing this data.\n",
      "inspiration\n",
      "i have analyzed the data and made new variables too. i'm trying to think how linear models can be applied to such a data set and to my knowledge, it is really a waste of time. if you want to see my project, i will be posting it here and also would love to hear your opinion on what further analyses could be done/how it could be done!\n",
      "thank you!\n",
      "context\n",
      "looking at elections on tv or online is something, but that implies a ton of data. i was curious to see just how much. this is only a tiny bit of what analysts use.\n",
      "content\n",
      "this represents all votes for all polling stations in the 42nd general election, per cadidate.\n",
      "acknowledgements\n",
      "i won't take much credit; data comes straight from the elections canada website. i only rearranged the data.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "acknowledgments\n",
      "this dataset was downloaded from the open source sports website. it did not come with an explicit license, but based on other datasets from open source sports, we treat it as follows:\n",
      "this database is copyright 1996-2015 by sean lahman.\n",
      "this work is licensed under a creative commons attribution-sharealike 3.0 unported license. for details see: http://creativecommons.org/licenses/by-sa/3.0/\n",
      "this is modified to only include players info from 2000-2005\n",
      "context\n",
      "i was recently tasked to put together a list of vr games.\n",
      "content\n",
      "this is a single column list with titles of vr capable games.\n",
      "acknowledgements\n",
      "this list was put together from info available on steam website.\n",
      "inspiration\n",
      "wanted to put together a word cloud and see what words stand out.\n",
      "context\n",
      "air passengers per month. workshop dataset\n",
      "context\n",
      "while studying neural networks in machine learning, i found an ingenious 2-d scatter pattern at the cn231 course by andrej karpathy. decision boundaries for the three classes of points cannot be straight lines. he uses it to compare the behavior of a linear classifier and a neural network classifier. often, we are content with the percentage of accuracy of our prediction or classification algorithm but a visualization tool helps our intuition about the trends and behavior of the classifier. it was definitely useful to catch something strange in the last example of this description. how does your preferred method or algorithm do with this input?\n",
      "content\n",
      "the 2d scatter train pattern is a collection of 300 points with (x,y) coordinates where -1<x,y<1.\n",
      "the 300 points are divided into 3 classes of 100 points each whose labels are 0, 1 and 2. each class is an arm in the spiral.\n",
      "the test file is simply a grid covering a square patch within the (x,y) coordinate boundaries. the idea is to predict and assign labels to each point and visualize the decision boundaries and class areas.\n",
      "the train pattern is provided in two formats: libsvm and csv formats. the test pattern is also provided in the above formats.\n",
      "if you want to reproduce the data yourself or modify parameters, either run the python script in cn231 or you may check piconn out for a c++ version.\n",
      "here's the output i get with the neural network classifier piconn\n",
      "an svm classifier with radial basis kernel yields the following output. it looks better than the above neural network one.\n",
      "an svm classifier with linear kernel yields the following output. one can see it won't handle curved decision regions.\n",
      "an svm classifier with polynomial kernel yields the following output. interestingly it decided to bundle the center of the spiral.\n",
      "an svm classifier with sigmoid kernel yields the following output. there is definitely something going on. a bug in my code? i could had wrongfully discarded this kernel for bad accuracy. visualization was definitely useful for this case.\n",
      "acknowledgements\n",
      "i thank andrej karpathy for making his excellent cn231 machine learning course freely available.\n",
      "inspiration\n",
      "i was happy with the result of the 100-node neural network. i then tested with libsvm obtaining similar results. how do other algorithms do with it? would you draw yourself a different decision boundary?\n",
      "context\n",
      "in this study, large number of national stock exchange(nse), india stocks under different sectors are mined from various financial websites and data analytic steps are followed. primary goal of this work is to explore the hidden context patterns between diverse group of stocks and discover the predictive analytic knowledge using machine learning algorithms. the transaction dataset are captured for nse stocks using statistical computing software r. the price of the stock is determined by the market forces. buyers and sellers quote the preferred price, so there is a dynamic data day by day. though it is difficult to identify when to buy and sell the stock, technical indicators may support us to forecast the future price\n",
      "content\n",
      "a data frame with 8 variables: index, date, time, open, high, low, close and id. for each year from 2013 to 2016, the number of trading data of each minute of given each date. the currency of the price is indian rupee (inr).\n",
      "code : market id\n",
      "date : numerical value (ex. 20151203- to be converted as 2015/12/03)\n",
      "time : factor (ex. 09:16)\n",
      "open : numeric (opening price)\n",
      "high : numeric (high price)\n",
      "low : numeric (low price)\n",
      "close : numeric (closing price)\n",
      "volume : numeric (total volume traded)\n",
      "acknowledgements\n",
      "references [1] brett lantz, machine learning with r . packt publishing ltd., birmingham, uk , 2015. [2] the r project https://www.r-project.org/ [3] https://finance.yahoo.com/ [4] https://www.google.com/finance [5] https://www.nseindia.com/\n",
      "inspiration\n",
      "machine learning (nse stocks)\n",
      "context\n",
      "real estate data set of philly.\n",
      "content\n",
      "data set included addresses, sales price, crime rate and rank by zipcode, school ratings and rank by zipcode, walkscore and rank by zip code, approximate rehab cost,\n",
      "acknowledgements\n",
      "data from phila.gov and other sites\n",
      "inspiration\n",
      "find out how data could impact house price.\n",
      "this dataset does not have a description yet.\n",
      "i have compiled weekly rankings of college teams, and am making the archived data public.\n",
      "fbs football (since 1997): http://www.masseyratings.com/cf/compare.htm\n",
      "fcs football (since 2000): http://www.masseyratings.com/cf/compare1aa.htm\n",
      "basketball (since 2001): http://www.masseyratings.com/cb/compare.htm\n",
      "baseball (since 2010): http://www.masseyratings.com/cbase/compare.htm\n",
      "each line of the .csv files have the following fields:\n",
      "sportyear\n",
      "unique team id #\n",
      "team name\n",
      "ranking system's 2/3 letter code\n",
      "ranking system's name\n",
      "date of weekly ranking (yyyymmdd)\n",
      "ordinal ranking\n",
      "note: in most cases, the rankings were collected on monday or tuesday of each week, and are based on games played through sunday.\n",
      "please explore, test hypotheses, generate various metrics, time series, correlations, etc and please reference masseyratings.com in your research.\n",
      "this dataset is not novel. it is a copy of the student performance dataset available at uci machine learning repository\n",
      "ditrd - dilma impeachment twitter raw data\n",
      "abstract:\n",
      "dilma impeachment twitter raw data from \"dilma impeachment process period dec 2015 to jan 2016 and march 2016\".\n",
      "date donated: 10 feb 2016, 17 mar 2016.\n",
      "source:\n",
      "authors: caio moreno\n",
      "data set information:\n",
      "this data was collected using the the streaming apis.\n",
      "https://dev.twitter.com/streaming/overview\n",
      "dilma impeachment twitter raw data is a free and open source software. it is available under the terms of the apache license version 2.\n",
      "citation:\n",
      "if you publish material based on databases obtained from this repository, then, in your acknowledgements, please note the assistance you received by using this repository. this will help others to obtain the same data sets and replicate your experiments. we suggest the following pseudo-apa reference format for referring to this repository:\n",
      "moreno, c. (2016). ditrd-v1.0.0 - dilma impeachment twitter raw data [https://github.com/caiomsouza/twitterrawdata]. madrid, spain: u-tad, certificate program in data science.\n",
      "link do download ditrd-v1.0.0.\n",
      "https://github.com/caiomsouza/twitterrawdata/releases/download/ditrd-v1.0.0/twitter_raw_data.zip\n",
      "link do download ditrd-v1.0.1.\n",
      "https://github.com/caiomsouza/twitterrawdata/releases/download/ditrd-v1.0.1/ditrd-v1.0.1.zip\n",
      "csv files extracted using r\n",
      "https://github.com/caiomsouza/twitterrawdata/releases/download/dilmatwitterrawdata-extract-r-csv-v1.0.0/twittersdata-extract-r-csv.zip\n",
      "all releases: https://github.com/caiomsouza/twitterrawdata/releases\n",
      "cousin marriage data\n",
      "there was a story on fivethirtyeight.com about the prevalence of marriage to cousins in the united states. this is called consanguinity and is defined as marriages between individuals who are second cousins or closer. the article included data put together in 2001 for a number of countries. the data source and the article are listed below.\n",
      "the raw data behind the story dear mona: how many americans are married to their cousins? on fivethirtyeight.com.\n",
      "link to fivethirtyeight's public github repository.\n",
      "header | definition\n",
      "country | country names\n",
      "percent | percent of marriages that are consanguineous\n",
      "source: cosang.net\n",
      "data flaws: while the data does compile older sources and some self-reported data, it does match the trends of more recent data based on global genomic data.\n",
      "context:\n",
      "in an attempt to provide transparency to the population of brazil, the dadosaberto initiative publishes data on different topics related to the house of deputies. among those informations, is the amount of refunds requested by each deputy, on expenses related to their activities.\n",
      "this dataset compiles those expenses from 2009 to 2017, translate the categories to english, fix some dates (missing, invalid) for analysis purpouses, and discards some information thats not useful.\n",
      "there are some issues with the dataset provided by the initiative dadosabertos;\n",
      "the dates are related to the receipt, not to the actual refund request.\n",
      "parties/states relations to the candidate sometimes are missing\n",
      "social security numbers sometimes are missing.\n",
      "social security should always have length == 11 for a person or == 14 for a company. that doesnt happen.\n",
      "among other issues.\n",
      "ive converted dates that were missing or corrupted with only the year. ive also created a ohe column to help identify/filter those dates.\n",
      "content:\n",
      "bugged_date: (binary) identify wether date had issues receipt_date: (datetime) receipt date // (int year) for when bugged_date == 1 deputy_id: (deputy_id) id number. (didnt check if it changed across year/legislation period for deputies) political_party: (string) deputy political party state_code: (string) brazil's state that elected the deputy deputy_name: (string)\n",
      "receipt_social_security_number: might be a persons ss number (11 digits long) or a business id number (14 digits long). many cases with issues. receipt_description: (str / classes) class of spending under which the receipt fits establishment_name: (string) receipt_value: (int) $br, 3br$ ~ 1usd\n",
      "acknowledgments:\n",
      "this data is from dadosabertos.camara.leg.br\n",
      "** inspiration: **\n",
      "i have only slightly fiddled with the dataset, but there are some funny patterns to say the least. brazil faces a huge problem with corruption and neglect with public funds, this dataset helps to show that.\n",
      "i invite fellow kagglers to toy with the dataset, try to identify suspicious spendings as well as odd patterns.\n",
      "ps: this is a more complete version of a dataset i posted a year ago, which did not include translations or other years spendings.\n",
      "this research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from france between jan-2016 and aug-2016 were interviewed in order to have exploitable data. therefore, this sample might not be representative of the true population.\n",
      "what can you do with the data?\n",
      "look up whether kagglers has \"stronger\" hardware than non-kagglers\n",
      "whether there is a correlation between a preferred data set size and hardware\n",
      "is proficiency a predictor of specific preferences?\n",
      "are data scientists more intel or amd?\n",
      "how spread is gpu computing, and is there any relationship with kaggling?\n",
      "are you able to predict the amount of euros a data scientist might invest, provided their current workstation details?\n",
      "i did not find any past research on a similar scale. you are free to play with this data set. for re-usage of this data set out of kaggle, please contact the author directly on kaggle (use \"contact user\"). please mention:\n",
      "your intended usage (research? business use? blogging?...)\n",
      "your first/last name\n",
      "arbitrarily, we chose characteristics to describe data scientists and data set sizes.\n",
      "data set size:\n",
      "small: under 1 million values\n",
      "medium: between 1 million and 1 billion values\n",
      "large: over 1 billion values\n",
      "for the data, it uses the following fields (ds = data scientist, w = workstation):\n",
      "ds_1 = are you working with \"large\" data sets at work? (large = over 1 billion values) => yes or no\n",
      "ds_2 = do you enjoy working with large data sets? => yes or no\n",
      "ds_3 = would you rather have small, medium, or large data sets for work? => small, medium, or large\n",
      "ds_4 = do you have any presence at kaggle or any other data science platforms? => yes or no\n",
      "ds_5 = do you view yourself proficient at working in data science? => yes, a bit, or no\n",
      "w_1 = what is your cpu brand? => intel or amd\n",
      "w_2 = do you have access to a remote server to perform large workloads? => yes or no\n",
      "w_3 = how much euros would you invest in data science brand new hardware? => numeric output, rounded by 100s\n",
      "w_4 = how many cores do you have to work with data sets? => numeric output\n",
      "w_5 = how much ram (in gb) do you have to work with data sets? => numeric output\n",
      "w_6 = do you do gpu computing? => yes or no\n",
      "w_7 = what programming languages do you use for data science? => r or python (any other answer accepted)\n",
      "w_8 = what programming languages do you use for pure statistical analysis? => r or python (any other answer accepted)\n",
      "w_9 = what programming languages do you use for training models? => r or python (any other answer accepted)\n",
      "you should expect potential noise in the data set. it might not be \"free\" of internal contradictions, as with all researches.\n",
      "these data sets contain data on current driving licences issued by the driver and vehicle licensing agency (dvla). the dvla is responsible for issuing driving licences in great britain (gb). driving licences issued in northern ireland are the responsibility of the northern ireland driver & vehicle agency and are outside the scope of this release.\n",
      "dvla’s drivers database changes constantly as the agency receives driving licence applications and other information that updates the records of individual drivers. therefore, it is only possible only to provide a snapshot of the state of the record at a particular time.\n",
      "contact dvla for further information about driving licensing which can be found at: https://www.gov.uk/browse/driving/driving-licences\n",
      "this dataset includes the listing prices for the sale of properties (mostly houses) in ontario. they are obtained for a short period of time in july 2016 and include the following fields: - price in dollars - address of the property - latitude and longitude of the address obtained by using google geocoding service - area name of the property obtained by using google geocoding service\n",
      "this dataset will provide a good starting point for analyzing the inflated housing market in canada although it does not include time related information. initially, it is intended to draw an enhanced interactive heatmap of the house prices for different neighborhoods (areas)\n",
      "however, if there is enough interest, there will be more information added as newer versions to this dataset. some of those information will include more details on the property as well as time related information on the price (changes).\n",
      "this is a somehow related articles about the real estate prices in ontario: http://www.canadianbusiness.com/blogs-and-comment/check-out-this-heat-map-of-toronto-real-estate-prices/\n",
      "i am also inspired by this dataset which was provided for king county https://www.kaggle.com/harlfoxem/housesalesprediction\n",
      "this data set only includes the track sections that belong to the standard-gauge track network of the sbb group (sbb infrastructure, sensetalbahn, thurbo).track sections that are managed by other infrastructure operators, for example bls netz ag, deutsche bahn ag (db) or rhätische bahn ag (rhb), are not included.\n",
      "the data on the number of trains includes passenger and freight trains operated by all railway undertakings that have travelled on the track section in question, i.e. also trains run by bls ag, schweizerische südostbahn ag, crossrail ag and railcare ag, for example.\n",
      "https://data.sbb.ch/page/licence\n",
      "the breathing signal is the expansion and contraction of the chest measured using a chest belt. simultaneously, we obtained ventilation from the douglas bag (db) method which is the gold standard. given the breathing signal, we extract the average height (a) between adjacent local minima and maxima and the fundamental period (p). these are our features. we want to find a function f(x) that maps (a,b) into the flow of air calculated (based on the db method) in the time window. the average height seems to have a quadratic/cubic relationship with ventilation while the period seems to be having an inverse relationship with the flow of air. .\n",
      "water pump\n",
      "context: this csv contains the top 100 lap times on the famous nürburgring track in germany.\n",
      "content: - position on the list from 1-100 - model year of the car - car make - car model - lap time\n",
      "acknowledgements: sourced from https://nurburgringlaptimes.com/lap-times-top-100/\n",
      "inspiration: new to the data science world and saw found no data sets related to cars or racing so figured this might be a place to start. hoping to contribute larger, more interesting datasets to the community in the future.\n",
      "campus party bh mg brazil 2016 http://campuse.ro/challenges/hackathon-servicos-para-os-cidadaos/\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "i'm always concerned with the rise of bitcoin price. investing in these coins require data driven knowledge and without the knowledge one is bound to make grave mistakes. it is just the other day bitcoin price hit 1600 usd. what is the future of this coin?\n",
      "content\n",
      "the csv file contains daily closing price of a bitcoin from 28-apr-13 to 3-oct-17. soon i will give the updated data set.\n",
      "date column: the dates of observation\n",
      "close column: the daily closing price in usd of a bitcoin\n",
      "acknowledgements\n",
      "many thanks to block chain info and my friends.\n",
      "inspiration\n",
      "now that bitcoin has hit higher prices we would like to predict the direction of bitcoin price\n",
      "predict actual bitcoin price for a given day. will a neural network model work for my case?\n",
      "context\n",
      "i needed a data set for a hackathon project involving food classification. i gathered this data by scraping various online stores that only sold specific food items (ie only vegan food or only halal food). i then compared those items to walmart's electrobit-backed api that happened to return ingredient information.\n",
      "state wise tree cover can be used to predict area useful for agriculture ,find density of forest cover,number of tree approx. ,environment statistics\n",
      "context\n",
      "this dataset lists the natural rate of unemployment (nairu) in the u.s., which is the rate of unemployment arising from all sources except fluctuations in aggregate demand. estimates of potential gdp are based on the long-term natural rate. the short-term natural rate incorporates structural factors that are temporarily boosting the natural rate beginning in 2008. the short-term natural rate is used to gauge the amount of current and projected slack in labor markets, which is a key input into cbo's projections of inflation.\n",
      "content\n",
      "data includes the date of the quarterly collection and the natural rate of unemployment from january 1, 1949 through october 1, 2016.\n",
      "inspiration\n",
      "what is the general trend of unemployment?\n",
      "can you compare this unemployment data with other factors found in any of the bls databases, such as manufacturing employment rates and gdp?\n",
      "acknowledgement\n",
      "this dataset is part of the us department of labor bureau of labor statistics datasets (the federal reserve economic data database), and the original source can be found here.\n",
      "this is a subset of only asbestos-related maintenance requests for the borough of staten island received by progress queens from the new york city housing authority in response to a request filed under the state's freedom of information law.\n",
      "this subset was derived from the concatenation of the siebel extracts, which were included in the subject foil response. because of the poor condition of the data, the concatenation of the siebel extracts was processed with some possible data loss. a general description of the quality of the data nycha produced was reported in an article published by progress queens.\n",
      "the publisher of progress queens formed this dataset to study how does nycha treat maintenance requests for asbestos, to determine how nycha escalates complaints made by tenants about asbestos to ordering testing for asbestos and to abatement, if necessary.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "narendra modi is a great public speaker. this data set is an opportunity to understand what his speech's contain.\n",
      "content\n",
      "this is an unstructured text data of every month. starting from october 2014 to september 2017 each speech is provided as a text file.\n",
      "acknowledgements\n",
      "mann ki baat\n",
      "inspiration\n",
      "i would like to see what makes his speech's great and attract crores of people.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this data download from kaggle .it's about the fraud in a bank. use some method prevent fraud accident happen is very import for bank.\n",
      "content\n",
      "the fraud id is the positive and the total number is less than the negative sample. so ,please pay more attention to the positive recall and precise.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "how to improve recall in a unbalance is very common status in true business sense. down_sampling? or up_sampling?\n",
      "this is the car sales data set which include information about different cars . this data set is being taken from the analytixlabs for the purpose of prediction in this we have to see two things\n",
      "first we have see which feature has more impact on car sales and carry out result of this\n",
      "secondly we have to train the classifier and to predict car sales and check the accuracy of the prediction.\n",
      "this dataset does not have a description yet.\n",
      "description\n",
      "this is a data set for forecasting growth of investment after a period of some years say after 10 years if data growth pattern remains same.\n",
      "data fields\n",
      "deposit date - gives the date of investment. month - counter on total months pft_perc - percentage of profit earned pft - calculated total profit on investment investment - total investment for month\n",
      "what we already know!\n",
      "their are some deposits made to grow the investment and based on which pft earned is growing.\n",
      "what we need to know.\n",
      "idea here is to know the growth of investment after some years. for instance after 40 months, which will be my investment ?\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this breast cancer databases was obtained from the university of wisconsin hospitals, madison from dr. william h. wolberg.\n",
      "content\n",
      "past usage:\n",
      "attributes 2 through 10 have been used to represent instances. each instance has one of 2 possible classes: benign or malignant.\n",
      "wolberg,~w.~h., \\& mangasarian,~o.~l. (1990). multisurface method of pattern separation for medical diagnosis applied to breast cytology. in {\\it proceedings of the national academy of sciences}, {\\it 87}, 9193--9196. -- size of data set: only 369 instances (at that point in time) -- collected classification results: 1 trial only -- two pairs of parallel hyperplanes were found to be consistent with 50% of the data -- accuracy on remaining 50% of dataset: 93.5% -- three pairs of parallel hyperplanes were found to be consistent with 67% of data -- accuracy on remaining 33% of dataset: 95.9%\n",
      "zhang,~j. (1992). selecting typical instances in instance-based learning. in {\\it proceedings of the ninth international machine learning conference} (pp. 470--479). aberdeen, scotland: morgan kaufmann. -- size of data set: only 369 instances (at that point in time) -- applied 4 instance-based learning algorithms -- collected classification results averaged over 10 trials -- best accuracy result: -- 1-nearest neighbor: 93.7% -- trained on 200 instances, tested on the other 169 -- also of interest: -- using only typical instances: 92.2% (storing only 23.1 instances) -- trained on 200 instances, tested on the other 169\n",
      "relevant information:\n",
      "samples arrive periodically as dr. wolberg reports his clinical cases. the database therefore reflects this chronological grouping of the data. this grouping information appears immediately below, having been removed from the data itself:\n",
      "group 1: 367 instances (january 1989) group 2: 70 instances (october 1989) group 3: 31 instances (february 1990) group 4: 17 instances (april 1990) group 5: 48 instances (august 1990) group 6: 49 instances (updated january 1991) group 7: 31 instances (june 1991) group 8: 86 instances (november 1991)\n",
      "total: 699 points (as of the donated datbase on 15 july 1992)\n",
      "note that the results summarized above in past usage refer to a dataset of size 369, while group 1 has only 367 instances. this is because it originally contained 369 instances; 2 were removed. the following statements summarizes changes to the original group 1's set of data:\n",
      "group 1 : 367 points: 200b 167m (january 1989) revised jan 10, 1991: replaced zero bare nuclei in 1080185 & 1187805 revised nov 22,1991: removed 765878,4,5,9,7,10,10,10,3,8,1 no record : removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial : changed 0 to 1 in field 6 of sample 1219406 : changed 0 to 1 in field 8 of following sample: : 1182404,2,3,1,1,1,2,0,1,1,1\n",
      "number of instances: 699 (as of 15 july 1992)\n",
      "number of attributes: 10 plus the class attribute\n",
      "attribute information: (class attribute has been moved to last column)\n",
      "attribute domain\n",
      "sample code number id number\n",
      "clump thickness 1 - 10\n",
      "uniformity of cell size 1 - 10\n",
      "uniformity of cell shape 1 - 10\n",
      "marginal adhesion 1 - 10\n",
      "single epithelial cell size 1 - 10\n",
      "bare nuclei 1 - 10\n",
      "bland chromatin 1 - 10\n",
      "normal nucleoli 1 - 10\n",
      "mitoses 1 - 10\n",
      "class: (2 for benign, 4 for malignant)\n",
      "missing attribute values: 16\n",
      "there are 16 instances in groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by \"?\".\n",
      "class distribution:\n",
      "benign: 458 (65.5%) malignant: 241 (34.5%)\n",
      "acknowledgements\n",
      "o. l. mangasarian and w. h. wolberg: \"cancer diagnosis via linear programming\", siam news, volume 23, number 5, september 1990, pp 1 & 18.\n",
      "william h. wolberg and o.l. mangasarian: \"multisurface method of pattern separation for medical diagnosis applied to breast cytology\", proceedings of the national academy of sciences, u.s.a., volume 87, december 1990, pp 9193-9196.\n",
      "o. l. mangasarian, r. setiono, and w.h. wolberg: \"pattern recognition via linear programming: theory and application to medical diagnosis\", in: \"large-scale numerical optimization\", thomas f. coleman and yuying li, editors, siam publications, philadelphia 1990, pp 22-30.\n",
      "k. p. bennett & o. l. mangasarian: \"robust linear programming discrimination of two linearly inseparable sets\", optimization methods and software 1, 1992, 23-34 (gordon & breach science publishers).\n",
      "inspiration\n",
      "rouse tek bio informatics cytogenomics project is an attempt to bring the human genome to the understanding of how cancers develop. all of our bodies are composed of cells. the human body has about 100 trillion cells within it. and usually those cells behave in a certain fashion. they observe certain rules, they divide when they’re told to divide, they’re quiescent when they’re told to remain dormant, they stay within a particular position within their tissue and they don’t move out of that.\n",
      "occassionally however, a single cell, of those 100 trillion cells, behave in a different way. that cell keeps dividing when all its signals around it tell it to stop dividing. that cell ignores its counterparts around it and pushes them out of the way. that cell stops observing the rules of the tissue within which it is located and begins to move out of its normal position, invading into the tissues around it and sometimes entering the bloodstream and becoming a metastasis, depositing in another tissue of the body..\n",
      "the reason the cell has gone rogue is because it has acquired within its genome, within its dna, a number of abnormalities that cause it to behave as a cancer cell.\n",
      "all 100 trillion cells in the human body have got a copy of the human genome, they have 2 copies, 1 maternal, 1 paternal. throughout life all those copies of the genome in those 100 trillion cells, are acquiring abnormal changes or somatic mutations. these mutations are present in the cell and are not transmitted from parents to offspring. they are constrained to that individual cell. those mutations occur in every cell of the body, normal and abnormal, for a number of different reasons. they occur because every time a cell divides possibly one letter of code out of 3 billion is replicated incorrectly. and that’s 1 source of somatic mutations.\n",
      "another source is that our 100 trillion cells are being exposed to a number of different onslaughts like radiation, self generated chemicals from inhalation of things like tobacco smoke or even an unhealthy diet over time. occasionally mechanisms in a particular cell make breakdown and the dna of that cell begins to acquire somatic mutations rather more commonly than other cells.\n",
      "so in summary, every cell in the body acquires mutations throughout a lifetime, and as we get older we acquire more and more somatic mutations in which occasionally a particular type of gene is mutated where the protein that it makes is abnormal and drives the cell to behave in a rogue fashion that we call cancer.\n",
      "context\n",
      "the subject of this dataset is multi-instrument observations of solar flares. there are a number of space-based instruments that are able to observe solar flares on the sun; some instruments observe the entire sun all the time, and some only observe part of the sun some of the time. we know roughly where flares occur on the sun but we don't know when they will occur. in this respect solar flares resemble earthquakes on earth. this dataset is a catalog of which solar flares have been observed by currently operational space-based solar observatories.\n",
      "content\n",
      "it includes that start time and end time of each solar flare from 1 may 2010 to 9 october 2017 and which instrument(s) they were observed by. it was collected by doing a retrospective analysis of the known pointing of seven different instruments with the location and times of 12,455 solar flares.\n",
      "acknowledgements\n",
      "the dataset was compiled by dr. ryan milligan based on publicly available data and are freely distributed. the citation of relevance is https://arxiv.org/abs/1703.04412.\n",
      "inspiration\n",
      "this dataset represents the first attempted evaluation of how well space-based instrumentation co-ordinate when it comes to observing solar flares. we are particularly interested in understanding how often combinations of instruments co-observe the same flare. the ultimate purpose is to try to find strategies that optimize the scientific return on solar flare data given the limited space-based instrument resources available. more often than not, our greatest understanding of these explosive events come through simultaneous observations made by multiple instruments.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "the top \"how to\" related searches on google from 2004 to 2017 worldwide. top searches are searches with the highest search interest based on volume.\n",
      "indexed search interest in 'health care' from may 2 to may 4, 2017. 100 is the max value, and every number is relative to that.\n",
      "rankings for halloween costumes in october 2017 in the us\n",
      "context\n",
      "expressionist art works is a very excellent knowledge repository to gather insights about the human aesthetics and perceptions.\n",
      "content\n",
      "this data set is the collection of expressionist art works across the world for the machine learning experiments.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "after participating in many kaggle competitions, we decided to open-source our python automl framework alphapy. you can configure models using yaml, and it's a quick way to develop a classification or regression model with any number of scikit-learn algorithms. more importantly, we needed the ability to easily write custom transformations to engineer features that could be imported by other tools such as h2o.ai and datarobot, which are commercial automl tools.\n",
      "we were very interested in predicting the stock market and sporting events, so we developed some tools for collecting end-of-day stock market data and then transforming that time series data for machine learning. for the sports data, we painstakingly entered game results on a daily basis, and this dataset (albeit limited) is the result. sports apis such as sportradar are becoming more prevalent, but you still need a pipeline to merge all that data to shape it for machine learning.\n",
      "content\n",
      "we collected game results, line, and over/under data for the following sports and seasons:\n",
      "nfl 2014, 2015 (2 seasons)\n",
      "nba 2015-16 (1 season)\n",
      "ncaa football 2015-16 (1 season)\n",
      "ncaa basketball 2015-16 (1 season)\n",
      "using alphapy, we developed a sports pipeline to analyze trend data in the game results. you can find documentation and examples here.\n",
      "inspiration\n",
      "every speculator's dream is to gain an edge, whether it be betting on sports or speculating on stocks. this dataset is just a small step for applying machine learning to the world of sports data.\n",
      "context\n",
      "most of the small to medium business owners are making effective use of gmail based email marketing strategies for offline targeting of converting their prospective customers into leads so that they stay with them in business\n",
      "content\n",
      "we have different aspects of emails to characterize the mail and track the mail is ignored; read; acknowledged by the reader\n",
      "acknowledgements\n",
      "corefactors.in\n",
      "inspiration\n",
      "amount of advertising dollars spent on a product determines the amount of its sales, we could use regression analysis to quantify the precise nature of the relationship between advertising and sales. here we want everyone to experiment with this fun data , what value we can derive from email as a tool for compaign marketing in a multi channel marketing strategy of a small to medium businesses\n",
      "content\n",
      "this data was acquired by scraping truecar.com for used car listings on 9/24/2017. each row represents one used car listing.\n",
      "context\n",
      "penalty kicks in football are the easiest, and perhaps most elegant way of modelling a game theory situation in a real-world scenario. given the limited number of options for both kickers and keepers, it makes for wonderful real-life data, which can be used in a professional context. however, official match records do not record the interesting aspects of the play - the direction of the kicker, the direction the keeper moves, where the ball lands, and so on. i watched all the penalties of the 2016/17 season of the epl (thanks, youtube!) and tagged the direction each player moves in. i believe this dataset will be extremely valuable to those who wish to experiment at the intersection of sports, game theory and data science.\n",
      "content\n",
      "the dataset contains information of all 106 penalty kicks taken during the 2016/17 season of the english premier league, with the following details - teams involved, player who took the kick, his foot, the direction the ball went, and the direction the keeper dove in [important: direction is with reference to the kicker!], and what time the penalty was awarded. the saved column indicates whether the keeper saved it, or the kicker kicked it beyond the goal post.\n",
      "there is missing data for 3 kicks - for one, the penalty was nullified due to a double kick, and for the other two, i simply couldn't find any video evidence of them. (if you do find them, please let me know). also, please let me know if you find any other errors with the data.\n",
      "acknowledgements\n",
      "this entire project was inspired by the brilliant work of ignacio palacios huerta, whose story is wonderful, and whose papers are an absolute joy to read. my current project is basically emulating what huerta has done with his (very vast) dataset.\n",
      "i will be expanding this dataset to previous seasons penalties too, as and when i get time.\n",
      "context\n",
      "as i am trying to learn and build an lstm prediction model for equity prices, i have chosen gold price to begin.\n",
      "content\n",
      "the file composed of simply 2 columns. one is the date (weekend) and the other is gold close price. the period is from 2015-01-04 to 2017-09-24.\n",
      "acknowledgements\n",
      "thanks to jason of his tutorial about lstm forecast: https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
      "inspiration\n",
      "william gann: time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. there is a definite relation between price and time.\n",
      "context\n",
      "before joining the federal executive administration, new government appointees must submit, amongst other things, detailed information regarding their finances and previous job history. such disclosure rules are in place in order to prevent conflicts of interest, and are a fundamental part of the work done by government ethics commissions. this dataset is a condensed collection of information recovered from these forms for a selection of top trump administration appointees.\n",
      "content\n",
      "this dataset is split into five separate csv files:\n",
      "names_and_job_titles.csv -- the names and job titles of appointees. not all appointees are included in this dataset.\n",
      "jobs_before_joining_admin.csv -- positions held by administration members immediately prior to their joining the federal government.\n",
      "clients_before_joining_admin.csv -- former appointee clients before joining the federal government.\n",
      "income_sources_and_assets.csv -- all acknowledged and disclosed income sources and assets. the most important file.\n",
      "debts.csv -- known appointee debt obligations. this record is incomplete.\n",
      "employee_agreements.csv -- agreements that the appointee made as part of the conditions of their entering employment with the federal government.\n",
      "acknowledgements\n",
      "propublica, the new york times, the associated press, and others pooled their resources to collect and condense disclosure forms for many prominent members of the trump administration. these were in turn collected into a public spreadsheet. this dataset is a further condensation of this work.\n",
      "inspiration\n",
      "what can you discover about the finances and potential conflicts of interest of members of the trump administration by looking at the raw government record?\n",
      "context\n",
      "in the early 2000s, billy beane and paul depodesta worked for the oakland athletics. while there, they literally changed the game of baseball. they didn't do it using a bat or glove, and they certainly didn't do it by throwing money at the issue; in fact, money was the issue. they didn't have enough of it, but they were still expected to keep up with teams that had much deeper pockets. this is where statistics came riding down the hillside on a white horse to save the day. this data set contains some of the information that was available to beane and depodesta in the early 2000s, and it can be used to better understand their methods.\n",
      "content\n",
      "this data set contains a set of variables that beane and depodesta focused heavily on. they determined that stats like on-base percentage (obp) and slugging percentage (slg) were very important when it came to scoring runs, however they were largely undervalued by most scouts at the time. this translated to a gold mine for beane and depodesta. since these players weren't being looked at by other teams, they could recruit these players on a small budget. the variables are as follows:\n",
      "team\n",
      "league\n",
      "year\n",
      "runs scored (rs)\n",
      "runs allowed (ra)\n",
      "wins (w)\n",
      "on-base percentage (obp)\n",
      "slugging percentage (slg)\n",
      "batting average (ba)\n",
      "playoffs (binary)\n",
      "rankseason\n",
      "rankplayoffs\n",
      "games played (g)\n",
      "opponent on-base percentage (oobp)\n",
      "opponent slugging percentage (oslg)\n",
      "acknowledgements\n",
      "this data set is referenced in the analytics edge course on edx during the lecture regarding the story of moneyball. the data itself is gathered from baseball-reference.com. sports-reference.com is one of the most comprehensive sports statistics resource available, and i highly recommend checking it out.\n",
      "inspiration\n",
      "it is such an important skill in today's world to be able to see the \"truth\" in a data set. that is what depodesta was able to do with this data, and it unsettled the entire system of baseball recruitment. beane and depodesta defined their season goal as making it to playoffs. with that in mind, consider these questions:\n",
      "how does a team make the playoffs?\n",
      "how does a team win more games?\n",
      "how does a team score more runs?\n",
      "they are all simple questions with simple answers, but now it is time to use the data to find the \"truth\" hidden in the numbers.\n",
      "context\n",
      "human activity recognition - har - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in har with wearable accelerometers), especially for the development of context-aware systems. there are many potential applications for har, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.\n",
      "content\n",
      "this human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict \"which\" activity was performed at a specific point in time (like with the daily living activities dataset above). the approach we propose for the weight lifting exercises dataset is to investigate \"how (well)\" an activity was performed by the wearer. the \"how (well)\" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.\n",
      "ix young health participants were asked to perform one set of 10 repetitions of the unilateral dumbbell biceps curl in five different fashions: exactly according to the specification (class a), throwing the elbows to the front (class b), lifting the dumbbell only halfway (class c), lowering the dumbbell only halfway (class d) and throwing the hips to the front (class e).\n",
      "class a corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. the exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. we made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).\n",
      "acknowledgements\n",
      "this dataset is licensed under the creative commons license (cc by-sa) - wallace ugulino (wugulino at inf dot puc-rio dot br) - eduardo velloso - hugo fuks\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "engineers use numerical models to analyze the behavior of the systems they are studying. by means of numerical models you can prove whether a design is safe or not, instead of making a prototype and testing it. this gives you great flexibility to modify parameters and to find a cheaper design that satisfies all the safety requirements.\n",
      "but when the models are too complex, the numerical simulations can easily last from a few hours to a few days. in addition, during the optimization process you might need a few tens of trials. so in order to simplify the process we can build a simple 'surrogate' model that yields similar results to the original one. here's when machine learning comes in!\n",
      "content\n",
      "the dataset contains the data of about 6000 numerical simulations (finite element models, fem). it must be pointed out that there is no noise in the data, that is, if we run again the simulations we'd get the same results. there are 9 input parameters and 4 output results.\n",
      "inputs (continuous and positive values): (1) load parameters: ecc, n, gammag. (2) material parameters: esoil, econc. (3) geometry parameters: dbot, h1, h2, h3.\n",
      "outputs (continuous values): (1) stress related results: mr_t, mt_t, mr_c, mt_c.\n",
      "acknowledgements\n",
      "the parametric numerical model was self-made.\n",
      "inspiration\n",
      "the data comes from deterministic numerical simulations. under this circumstance, is there any way we can find a regression model that gives accurate results? let's say something like 5% error (true_value / predicted_value within the range of [0.95, 1.05]).\n",
      "what would be the most appropriate regression algorithms? what accuracy can we expect?\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "20 years of daily stock prices of all companies listed on asx (australian securities exchange).\n",
      "date range: 1997-01-02 to 2017-12-31\n",
      "exchange website: https://www.asx.com.au/\n",
      "content\n",
      "total records: 6,475,470\n",
      "total companies: 2,228\n",
      "asx-equity-price.csv fields : ticker, date, open, high, low, close, volume\n",
      "asx-tickers.csv fields: ticker, company, industry\n",
      "acknowledgements\n",
      "the data posted here is merely a concatenation of all files on the below website.\n",
      "all credits to: https://www.asxhistoricaldata.com/\n",
      "inspiration\n",
      "this has been added to help correlate stock prices with my australian news headlines dataset.\n",
      "telecom giant telstra and the mining gaint bhp billiton are highly referenced (1000+ headlines) in the news and would be interesting case studies.\n",
      "context\n",
      "i like to livetweet conferences when i attend them, for my own reference later on and to help other people who aren't attending the conference keep up-to-date. after the last conference i attended, i was curious: do livetweets get more or less engagement than other types of tweets?\n",
      "content\n",
      "this dataset contains information on 314 tweets sent from my personal twitter account between september 29, 2017 and october 26, 2017. for each tweet, the following information is recorded:\n",
      "at_conference?: whether the tweet was sent during the conference\n",
      "day: the day the tweet was sent\n",
      "impressions: how many times the tweet was seen\n",
      "engagements: how many times the tweet was engaged with (sum of the following columns)\n",
      "retweets: how many times the tweet was retweeted\n",
      "likes: how many times the tweet was liked\n",
      "user profile clicks: how many times someone clicked on my profile from the tweet\n",
      "url clicks: how many times someone clicked a url in the tweet (not all tweets have url's)\n",
      "hashtag clicks: how many times someone clicked on a hashtag in the tweet (not all tweets have hashtags)\n",
      "detail expands: how many times someone expanded the tweet\n",
      "follows: how many times someone followed me from the tweet\n",
      "media views: how many times someone viewed media embedded in the tweet (not all tweets have media)\n",
      "media engagements: how many times someone clicked on media embedded in the tweet (not all tweets have media)\n",
      "inspiration\n",
      "do conference tweets get more engagement?\n",
      "does my account get more engagement during a conference?\n",
      "do the types of engagement differ depending on whether i'm at a conference?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "in the modern-world scenario of telecommunication industries, the customers have a range of options to choose from, as far as service providers are concerned. factors such as perceived frequent service disruptions, poor customer service experiences, and better offers from other competing carriers may cause a customer to churn (likely to leave).\n",
      "customer churn includes customers stopping the use of a service, switching to a competitor service, switching to a lower-tier experience in the service or reducing engagement with the service.\n",
      "content\n",
      "necessary shapefiles to create maps of new york city and its boroughs.\n",
      "acknowledgements\n",
      "these files have been made available by the new york city department of city planning and were retrieved from http://www1.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page on 27 september, 2017.\n",
      "inspiration\n",
      "these shapefiles might pair nicely with the new york building and elevator data also on here, as well as the nyc tree census i use it for.\n",
      "context\n",
      "i studied on this data set to predict phishing web sites by using:\n",
      "1- just url string 2- the content broadcasts from the url\n",
      "content\n",
      "this data set just has url list.\n",
      "acknowledgements\n",
      "data gathered from openphish.com.\n",
      "inspiration\n",
      "does a url itself or content of the url show us whether it is phishing or not?\n",
      "context\n",
      "data of hitters in mlb's al east\n",
      "content\n",
      "basic fundamental data on al east hitters sorted descending by plate appearances\n",
      "acknowledgements\n",
      "inspiration\n",
      "i love baseball and stats.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the official archive data set from the dhhs on all reported phi data breaches from medical and dental centers (military and civilian).\n",
      "content\n",
      "the name of the practice, the amount of people effected and the breach description.\n",
      "acknowledgements\n",
      "i did not create this data set, it is from the archives of the department of health and human services.\n",
      "inspiration\n",
      "this can be used to cross reference with osha violations and to determine the most common breach types\n",
      "context\n",
      "the need for music-speech classification is evident in many audio processing tasks which relate to real-life materials such as archives of field recordings, broadcasts and any other contexts which are likely to involve speech and music, concurrent or alternating. segregating the signal into speech and music segments is an obvious first step before applying speech-specific or music-specific algorithms. indeed, speech-music classification has received considerable attention from the research community (for a partial list, see references below) but many of the published algorithms are dataset-specific and are not directly comparable due to non-standardised evaluation.\n",
      "content\n",
      "dataset collected for the purposes of music/speech discrimination. the dataset consists of 120 tracks, each 30 seconds long. each class (music/speech) has 60 examples. the tracks are all 22050hz mono 16-bit audio files in .wav format.\n",
      "this dataset does not have a description yet.\n",
      "content\n",
      "this dataset contains information about players and teams and their statistics from 2005-2013. it also includes every play of every drive for every game played between 2005-2013.\n",
      "acknowledgements\n",
      "thanks to cbfstats.com and j. albert bowden ii for the dataset.\n",
      "context\n",
      "includes data for all candidates from all units of the federation and yours list of property declarations.\n",
      "content\n",
      "data of candidates brazilian national elections of 2014. source: http://www.tse.jus.br/eleitor-e-eleicoes/estatisticas/repositorio-de-dados-eleitorais-1/repositorio-de-dados-eleitorais\n",
      "context\n",
      "i love football and wanted to gather a data-set of a list of football players along with their each game performance from various different sources.\n",
      "content\n",
      "the csv file has the fantasy premier league data of all players who played in 3 seasons and a detailed spreadsheet of each player is provided.\n",
      "acknowledgements\n",
      "thanks to turd from tableau for some of the data.\n",
      "inspiration\n",
      "we all wondered if it is possible to predict the future! well with the player data against each team and conditions we get to check if the future prediction is truly possible!\n",
      "we generated our own dataset (iitm-hetra) from cameras monitoring road traffic in chennai, india. to ensure that data are temporally uncorrelated, we sample a frame every two seconds from multiple video streams. we extracted 2400 frames in total.\n",
      "we manually labeled 2400 frames under different vehicle categories. the number of available frames reduced to 1417 after careful scrutiny and elimination of unclear images. we initially defined eight different vehicle classes commonly seen in indian traffic. few of these classes were similar while two classes had less number of labeled instances; these were merged into similar looking classes. for example, in our dataset, we had different categories for small car, suv, and sedan which were merged under the light motor vehicle (lmv) category.\n",
      "a total of 6319 labeled vehicles are available in the collected dataset. this includes 3294 two-wheelers, 279 heavy motor vehicles (hmv), 2148 cars, and 598 auto-rickshaws. a second dataset was created by merging cars and auto-rickshaws together into light motor vehicle (lmv) class. approximately 25.2\\% of vehicles were occluded.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "there's a story behind every dataset and here's your opportunity to share yours.\n",
      "interactive hand gesture. here are color images and as well depth images of hand gestures grouped by their classes.\n",
      "copyright: author: chengyin liu; email: destin369y at gmail.com; year: 2015.\n",
      "please acknowledge my name if you use this dataset, thank you.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "rgb-d hand gesture images taken by depth camera. grouped by classes. please refer to \"class.txt\" used for hand gesture recognition evaluation.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "this dataset is used for my hand gesture recognition research at national taiwan university, in the intelligent robot lab under the lead of prof. li-chen fu. more details about our lab, please visit http://robotlab.csie.ntu.edu.tw/\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "context\n",
      "automated composition of computer programs has been a standing challenge since the early days of artificial intelligence, and has no clear solutions with modern-day research in deep learning. indeed, modeling latent context representations in language proves to be a difficult task singularly, and combined with applying structured procedural knowledge in a generative fashion quickly becomes intractable for complex domain specific languages. there is a clear need for research in devising efficiently learned combinations of these independent problems. we present this basic dataset of elementary mathematical functions encoded in the python programming language to encourage future research in this field, and to benchmark our own deep learning\n",
      "content\n",
      "this dataset contains a total of 335922 elementary mathematical functions with examples for inputs with corresponding outputs. the first line in the dataset csv file contains a header describing the contents of each column. the first column is labeled function_name, and the following rows contains a function name with a unique integer index. the next twenty columns contain function_input_x and function_output_x for all integers between 0 and 9 inclusively, where the following rows contains string encoded python floating point numbers after executing the corresponding function. the final column is labeled function_code, and contains a single-line lambda statement elementary mathematical function.\n",
      "acknowledgements\n",
      "we thank the guidance from the student instructors of the machine learning @ berkeley research group, and the discussions had with members of the redwood institute of theoretical neuroscience.\n",
      "inspiration\n",
      "we hope to see a neural architecture that is capable of programming new computer code without human supervision, in any programming language, solving even scientific computing tasks.\n",
      "mit license\n",
      "copyright (c) 2017 brandon trabucco\n",
      "permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"software\"), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\n",
      "the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.\n",
      "the software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.\n",
      "my research is taking data from high schooler's 2 mile times and converting it using my formula against other running formulas. i'm trying to find the fatigue decrease by using their time.\n",
      "the actual times are the 1 mile and 2 mile times, the age is the age grading formula, the vo2 max is the formula used for hypothetical running tmes, and the cameron and riegel formulas are for prediction times. taking the average of them all, i incorporated my formula and compared how close my prediction was to the average of all the formulas.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset is a experimentation of how to detect number of pill inside a pill bottle, through audio alone. there are plenty of games that exist that try to guess the number of items inside a container. its usually a game for fun, and just a game of chance. but can we game the system?\n",
      "for this experiment, we decided to use child resistant medicine pill bottles. and filled these bottles with generic acetaminophen gel caps https://www.costco.com/.product.100017017.html\n",
      "the following data contains 10, 10 seconds audio recordings of a generic pill bottle being shaken by one individual with a different number of pills. in our example, we recorded a user shaking a pill bottle with 1 pill, 10 pills, 25 pills and 50 pills. each folder contains 10, 10 second samples of each\n",
      "i work with uk company information on a daily basis, and i thought it would be useful to publish a list of all active companies, in a way that could be used for machine learning.\n",
      "there are 3,838,469 rows in the dataset, one for each active company. each row, has the company name, date of incorporation and the standard industrial classification code.\n",
      "the company list is from the publicly available 1st november 2017 companies house snapshot.\n",
      "the sic code descriptions are from the gov.uk website.\n",
      "in the file allcompanies.csv each row is formatted as follows:\n",
      "companyname - alpha numberic company name\n",
      "incorporationdate - in british date format, dd/mm/yyyy\n",
      "sic - 5 digits or if not known, none - see separate file for description of each code.\n",
      "inspiration\n",
      "possible uses for this data is to use ml to suggest a new unique but suitable name for a company based on what other companies of the same sic are called.\n",
      "perhaps analyse how company names have evolved over time.\n",
      "using ml, perhaps determine what a typical company name looks like, maybe analyse if company names have got longer or more complicated over time.\n",
      "i am sure there are many more possible uses for this data in ways, that i cannot imagine.\n",
      "this is my second go (the first was published a few hours ago) at publishing a dataset on any medium, so any useful tips and hints would be extremely welcome.\n",
      "links to the raw data sources are here:\n",
      "companies house http://download.companieshouse.gov.uk/en_output.html\n",
      "sic codes https://www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic\n",
      "context\n",
      "created for use in the renewable and appropriate energy lab at uc berkeley and lawrence berkeley national laboratory.\n",
      "content\n",
      "geography: all 58 counties of the american state of california\n",
      "time period: 2015\n",
      "unit of analysis: tons per year\n",
      "variables:\n",
      "co: county id as numbered in the county dropdown menu on the california air resources board facility search tool\n",
      "ab\n",
      "facid\n",
      "dis\n",
      "fname\n",
      "fstreet\n",
      "fcity\n",
      "fzip\n",
      "fsic: facility standard industrial classification code specified by the us department of labor\n",
      "coid\n",
      "disn\n",
      "chapis\n",
      "cerr_code\n",
      "togt: total organic gases consist of all hydrocarbons, i.e. compounds containing hydrogen and carbon with or without other chemical elements.\n",
      "rogt: reactive organic gases include all the organic gases exclude methane, ethane, acetone, methyl acetate, methylated siloxanes, and number of low molecular weight halogenated organics that have a low rate of reactivity.\n",
      "cot: the emissions of co are for the single species, carbon monoxide.\n",
      "noxt: the emissions of nox gases (mostly nitric oxide and nitrogen dioxide) are reported as equivalent amounts of no2.\n",
      "soxt: the emissions of sox gases (sulfur dioxide and sulfur trioxide) are reported as equivalent amounts of so2.\n",
      "pmt: particulate matter refers to small solid and liquid particles such as dust, sand, salt spray, metallic and mineral particles, pollen, smoke, mist and acid fumes.\n",
      "pm10t: pm10 refers to the fraction of particulate matter with an aerodynamic diameter of 10 micrometer and smaller. these particles are small enough to penetrate the lower respiratory tract.\n",
      "pm2.5t: pm2.5 refers to the fraction of particulate matter with an aerodynamic diameter of 2.5 micrometer and smaller. these particles are small enough to penetrate the lower respiratory tract.\n",
      "lat: facility latitude geocoded by inputting fstreet, fcity, california fzip into bing’s geocoding service.\n",
      "lon: facility longitude geocoded in the same way.\n",
      "sources: all columns except for lat and lon were scraped from the california air resources board facility search tool using the request module from python’s urllib library. the script used is included below in scripts in case you would like to get additional columns.\n",
      "the lat and lon columns were geocoded using the geocoder library for python with the bing provider.\n",
      "scripts\n",
      "download.py\n",
      "import pandas as pd out_dir = 'arb/' file_ext = '.csv' for i in range(1, 59): facilities = pd.read_csv(\"https://www.arb.ca.gov/app/emsinv/facinfo/faccrit_output.csv?&dbyr=2015&ab_=&dis_=&co_=\" + str(i) + \"&fname_=&city_=&sort=facilitynamea&fzip_=&fsic_=&facid_=&all_fac=c&chapis_only=&cerr=&dd=\") for index, row in facilities.iterrows(): curr_facility = pd.read_csv(\"https://www.arb.ca.gov/app/emsinv/facinfo/facdet_output.csv?&dbyr=2015&ab_=\" + str(row['ab']) + \"&dis_=\" + str(row['dis']) + \"&co_=\" + str(row['co']) + \"&fname_=&city_=&sort=c&fzip_=&fsic_=&facid_=\" + str(row['facid']) + \"&all_fac=&chapis_only=&cerr=&dd=\") facilities.set_value(index, 'pm2.5t', curr_facility.loc[curr_facility['pollutant name'] == 'pm2.5'].iloc[0]['emissions_tons_yr']) facilities.to_csv(out_dir + str(i) + file_ext)\n",
      "geocode.py\n",
      "import geocoder import csv directory = 'arb/' outdirectory = 'arb_out/' for i in range(1, 59): with open(directory + str(i) + \".csv\", 'rb') as csvfile, open(outdirectory + str(i) + '.csv', 'a') as csvout: reader = csv.dictreader(csvfile) fieldnames = reader.fieldnames + ['lat'] + ['lon'] # add new columns writer = csv.dictwriter(csvout, fieldnames) writer.writeheader() for row in reader: address = row['fstreet'] + ', ' + row['fcity'] + ', california ' + row['fzip'] g = geocoder.bing(address, key='api_key') newrow = dict(row) if g.latlng: newrow['lat'] = g.json['lat'] newrow['lon'] = g.json['lng'] writer.writerow(newrow) # only write row if successfully geocoded\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "uk police forces collect data on every vehicle collision in the uk on a form called stats19. data from this form ends up at the dft and is published at https://data.gov.uk/dataset/road-accidents-safety-data\n",
      "content\n",
      "there are 4 csvs and an excel file in this set. accidents is the primary table and has references by accident_index to the other tables.\n",
      "acknowledgements\n",
      "department for transport and the uk's wonderful open gov initiative https://data.gov.uk/\n",
      "inspiration\n",
      "are there patterns for accidents involving different road users?\n",
      "can we predict the safest / most dangerous times to travel\n",
      "can this data help route cyclists around accident hotspots taking into account the time of day, weather, route etc\n",
      "are certain cars more accident prone than others?\n",
      "context\n",
      "train commuter service in stockholm in 2012.\n",
      "content\n",
      "description of the train commuter service during one week-day. the data includes information about: - train timetable. - passengers flow\n",
      "acknowledgements\n",
      "the data is generated with the help of samtrafiken - trafiklab api (see. www.trafiklab.se).\n",
      "inspiration\n",
      "crowdedness: how crowd the trains are? which lines are crowded? how frequent should the train run?\n",
      "context\n",
      "japanese animation, which is known as anime, has become internationally widespread nowadays. this dataset provides data on anime taken from anime news network.\n",
      "content\n",
      "this dataset consists of 4029 anime data in 5 files. all of the csv files use '|' delimiter.\n",
      "- anime title (datatitle-all-share-new.csv)\n",
      "- anime synopsis (datasynopsis-all-share-new.csv)\n",
      "- anime genre (datagenre-all-share-new.csv)\n",
      "- anime staff (datastaff-all-share-new.csv)\n",
      "- anime scores (datascorehist-all-share-new.csv)\n",
      "anime id and staff were taken as what they seen on anime news network system. while the scores are taken based on the histogram of scores on each anime page and normalized.\n",
      "acknowledgements\n",
      "the dataset was collected from http://www.animenewsnetwork.com on 10 may 2016. if you use this dataset in publications, please cite:\n",
      "wibowo, c. p. (2016). a minimum spanning tree representation of anime similarities. arxiv preprint arxiv:1606.03048\n",
      "inspiration\n",
      "this dataset can be used to build recommendation systems, predict a score, visualize anime similarity, etc.\n",
      "context\n",
      "this dataset mainly features the score changes during badminton games in the rally-point system.\n",
      "content\n",
      "the dataset contains 11872 games from 5131 matches in bwf super series tournaments. there are 6 fields:\n",
      "- year: i collected 3 years data: 2015-2017.\n",
      "- tournament: for each year, there are 12 super series tournaments.\n",
      "- round: 1 - round 1; 2 - round 2; q - quarter finals; s - semi-finals; f - finals\n",
      "- match: information about the countries of the players.\n",
      "- type: ms - men's single; ws - women's single; md - men's double; wd - women's double; xd - mixed double\n",
      "- scores: score changes during the games.\n",
      "acknowledgements\n",
      "the dataset was collected from bwfbadminton.com. i wrote codes to scrap the information.\n",
      "inspiration\n",
      "performance of the players is reflected on how the score changes during the games. exploring this information may help us to predict or learn something related to badminton games.\n",
      "context\n",
      "walking around a total wine one day i wondered if there was any data i could find that would help me figure out what new rums to try, i later was able to find some information on rumratings.com. i'm now the hit of every party thanks to this data and a few box plots.\n",
      "content\n",
      "the data was scraped from rumratings, unfortunately a good portion of the data is sparse, i go into more detail about that in the kernel. there is slightly more information that could be grabbed but i didn't want to overburden the site, and well that would take a bit more time to write.\n",
      "acknowledgements\n",
      "mainly i would just like to thank the site for existing as a resource, and of course to any contributors on this data.\n",
      "inspiration\n",
      "i'd like to see some more eda done, i'll be doing my work in python so i like seeing the r kernels, interesting to see another approach.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the united states census bureau conducts regular surveys to assess education levels in the u.s. these surveys sample participants' highest levels of education (i.e. high school diploma, bachelor's degree, etc.) the attached csv file aggregates data for the years 1995, 2005, and 2015.\n",
      "content\n",
      "data is organized into columns representing the survey year, age range, sex of participants, and education level. for example, [1995, 18_24, male, ...] represents the 1995 survey for men ages 18-24.\n",
      "it's worth noting that the surveys varied somewhat in granularity. the 2015 survey divided categories more finely (18-24, 25-29, 29-34...) while the 2005 and 1995 surveys were coarser (18-24, 25-34, ...). this could create some distortion depending on the analysis used.\n",
      "sources\n",
      "main\n",
      "https://www.census.gov/topics/education/educational-attainment/data/tables.all.html\n",
      "2015\n",
      "table 1. educational attainment of the population 18 years and over, by age, sex, race, and hispanic origin: 2015\n",
      "https://www.census.gov/data/tables/2015/demo/education-attainment/p20-578.html\n",
      "2005\n",
      "table 6. educational attainment of employed civilians 18 to 64 years, by occupation, age, sex, race, and hispanic origin: 2005\n",
      "https://www.census.gov/data/tables/2005/demo/educational-attainment/cps-detailed-tables.html\n",
      "1995\n",
      "educational attainment in the united states: march 1995\n",
      "https://www.census.gov/data/tables/1995/demo/educational-attainment/p20-489.html\n",
      "context\n",
      "many many search queries bypassing the limits of twitter api. these files can easily be read using read_delim in r and using ; as the delimiter. the search query is in the name of the csv. the time runs from this year until ~2012, but each set varies.\n",
      "dental pain, alternatives, opioid prescription, patient visits, dental advertisements, misinformation, and more are found within these datasets.\n",
      "this tweet set is great for text analytics, machine learning, and etc.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "i believe healthcare should be a right for all, and that includes dental care? what alternatives do people turn to when they cannot afford the care they need? i hear many of my friends, in their 20s like myself, not receiving coverage or waiting to go to the dentist. although age is not included in these csv. user profiles if public can be grabbed using the twitter api.\n",
      "this dataset has attributes for all available weapons in fortnite: battle royale as of patch 2.1.0 or as of january 10th 2018. all credit for this data goes to soumydev at http://www.fortnitechests.info/\n",
      "the dataset features the following columns:\n",
      "name: name of the weapon\n",
      "dps: the damage per second of the weapon\n",
      "damage: the damage done by the weapon\n",
      "critical %: the critical hit chance of the weapon\n",
      "crit. damage: the critical hit damage of the weapon\n",
      "fire rate: the fire rate of the weapon\n",
      "mag. size: the size of the magazine of the weapon\n",
      "range: the range of the weapon\n",
      "durability: the durability of the weapon\n",
      "reload time: the reload time of the weapon\n",
      "ammo cost: the cost in ammunition to fire a single projectile\n",
      "impact: the impact of the weapon i.e. the damage it does to buildings\n",
      "rarity: the rarity of the weapon\n",
      "type: what type of weapon is in question\n",
      "in total there are 14 columns with 43 rows or weapons\n",
      "as mentioned previously, all credit for this data goes to soumydev at http://www.fortnitechests.info/\n",
      "potential uses for the dataset:\n",
      "find out is some weapon in the game underutilized\n",
      "what is the best weapon for different situations?\n",
      "are there different ways to classify weapons than by their type and rarity?\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "data was grabbed from us-news: https://www.usnews.com\n",
      "the following data points are included in this data set:\n",
      "ranking\n",
      "acceptance-rate\n",
      "act-avg\n",
      "sat-avg\n",
      "photo\n",
      "cost after financial aid\n",
      "city\n",
      "sortname\n",
      "zip\n",
      "percent receiving aid\n",
      "state\n",
      "average high school gpa\n",
      "tied ranking\n",
      "public/private university\n",
      "business reputation score\n",
      "tuition\n",
      "engineering reputation score\n",
      "enrollment size\n",
      "region\n",
      "it includes statistics surrounding the following 311 us universities.\n",
      "princeton university\n",
      "harvard university\n",
      "university of chicago\n",
      "yale university\n",
      "columbia university\n",
      "massachusetts institute of technology\n",
      "stanford university\n",
      "university of pennsylvania\n",
      "duke university\n",
      "california institute of technology\n",
      "dartmouth college\n",
      "johns hopkins university\n",
      "northwestern university\n",
      "brown university\n",
      "cornell university\n",
      "rice university\n",
      "vanderbilt university\n",
      "university of notre dame\n",
      "washington university in st. louis\n",
      "georgetown university\n",
      "emory university\n",
      "university of california--berkeley\n",
      "university of california--los angeles\n",
      "university of southern california\n",
      "carnegie mellon university\n",
      "university of virginia\n",
      "wake forest university\n",
      "university of michigan--ann arbor\n",
      "tufts university\n",
      "new york university\n",
      "university of north carolina--chapel hill\n",
      "boston college\n",
      "college of william and mary\n",
      "brandeis university\n",
      "georgia institute of technology\n",
      "university of rochester\n",
      "boston university\n",
      "case western reserve university\n",
      "university of california--santa barbara\n",
      "northeastern university\n",
      "tulane university\n",
      "rensselaer polytechnic institute\n",
      "university of california--irvine\n",
      "university of california--san diego\n",
      "university of florida\n",
      "lehigh university\n",
      "pepperdine university\n",
      "university of california--davis\n",
      "university of miami\n",
      "university of wisconsin--madison\n",
      "villanova university\n",
      "pennsylvania state university--university park\n",
      "university of illinois--urbana-champaign\n",
      "ohio state university--columbus\n",
      "university of georgia\n",
      "george washington university\n",
      "purdue university--west lafayette\n",
      "university of connecticut\n",
      "university of texas--austin\n",
      "university of washington\n",
      "brigham young university--provo\n",
      "fordham university\n",
      "southern methodist university\n",
      "syracuse university\n",
      "university of maryland--college park\n",
      "worcester polytechnic institute\n",
      "clemson university\n",
      "university of pittsburgh\n",
      "american university\n",
      "rutgers university--new brunswick\n",
      "stevens institute of technology\n",
      "texas a&m university--college station\n",
      "university of minnesota--twin cities\n",
      "virginia tech\n",
      "baylor university\n",
      "colorado school of mines\n",
      "university of massachusetts--amherst\n",
      "miami university--oxford\n",
      "texas christian university\n",
      "university of iowa\n",
      "clark university\n",
      "florida state university\n",
      "michigan state university\n",
      "north carolina state university--raleigh\n",
      "university of california--santa cruz\n",
      "university of delaware\n",
      "binghamton university--suny\n",
      "university of denver\n",
      "university of tulsa\n",
      "indiana university--bloomington\n",
      "marquette university\n",
      "university of colorado--boulder\n",
      "university of san diego\n",
      "drexel university\n",
      "saint louis university\n",
      "yeshiva university\n",
      "rochester institute of technology\n",
      "stony brook university--suny\n",
      "suny college of environmental science and forestry\n",
      "university at buffalo--suny\n",
      "university of oklahoma\n",
      "university of vermont\n",
      "auburn university\n",
      "illinois institute of technology\n",
      "loyola university chicago\n",
      "university of new hampshire\n",
      "university of oregon\n",
      "university of south carolina\n",
      "university of tennessee\n",
      "howard university\n",
      "university of alabama\n",
      "university of san francisco\n",
      "university of the pacific\n",
      "university of utah\n",
      "arizona state university--tempe\n",
      "iowa state university\n",
      "temple university\n",
      "university of kansas\n",
      "university of st. thomas\n",
      "the catholic university of america\n",
      "depaul university\n",
      "duquesne university\n",
      "university of missouri\n",
      "clarkson university\n",
      "colorado state university\n",
      "michigan technological university\n",
      "seton hall university\n",
      "university of arizona\n",
      "university of california--riverside\n",
      "university of dayton\n",
      "university of nebraska--lincoln\n",
      "hofstra university\n",
      "louisiana state university--baton rouge\n",
      "mercer university\n",
      "the new school\n",
      "rutgers university--newark\n",
      "university of arkansas\n",
      "university of cincinnati\n",
      "university of kentucky\n",
      "george mason university\n",
      "new jersey institute of technology\n",
      "san diego state university\n",
      "university of south florida\n",
      "washington state university\n",
      "kansas state university\n",
      "oregon state university\n",
      "st. john fisher college\n",
      "university of illinois--chicago\n",
      "university of mississippi\n",
      "university of texas--dallas\n",
      "adelphi university\n",
      "florida institute of technology\n",
      "ohio university\n",
      "seattle pacific university\n",
      "university at albany--suny\n",
      "oklahoma state university\n",
      "university of massachusetts--lowell\n",
      "university of rhode island\n",
      "biola university\n",
      "illinois state university\n",
      "university of alabama--birmingham\n",
      "university of hawaii--manoa\n",
      "university of la verne\n",
      "university of maryland--baltimore county\n",
      "immaculata university\n",
      "maryville university of st. louis\n",
      "missouri university of science & technology\n",
      "st. john's university\n",
      "university of california--merced\n",
      "university of louisville\n",
      "mississippi state university\n",
      "rowan university\n",
      "university of central florida\n",
      "university of idaho\n",
      "virginia commonwealth university\n",
      "kent state university\n",
      "robert morris university\n",
      "texas tech university\n",
      "union university\n",
      "university of hartford\n",
      "edgewood college\n",
      "lesley university\n",
      "lipscomb university\n",
      "suffolk university\n",
      "university of maine\n",
      "university of wyoming\n",
      "azusa pacific university\n",
      "ball state university\n",
      "montclair state university\n",
      "pace university\n",
      "west virginia university\n",
      "andrews university\n",
      "indiana university-purdue university--indianapolis\n",
      "university of houston\n",
      "university of new mexico\n",
      "university of north dakota\n",
      "widener university\n",
      "new mexico state university\n",
      "north dakota state university\n",
      "nova southeastern university\n",
      "university of north carolina--charlotte\n",
      "bowling green state university\n",
      "california state university--fullerton\n",
      "dallas baptist university\n",
      "university of massachusetts--boston\n",
      "university of nevada--reno\n",
      "central michigan university\n",
      "east carolina university\n",
      "florida a&m university\n",
      "montana state university\n",
      "university of alaska--fairbanks\n",
      "university of colorado--denver\n",
      "university of massachusetts--dartmouth\n",
      "university of montana\n",
      "western michigan university\n",
      "florida international university\n",
      "louisiana tech university\n",
      "south dakota state university\n",
      "southern illinois university--carbondale\n",
      "university of alabama--huntsville\n",
      "university of missouri--kansas city\n",
      "utah state university\n",
      "ashland university\n",
      "benedictine university\n",
      "california state university--fresno\n",
      "gardner-webb university\n",
      "georgia state university\n",
      "shenandoah university\n",
      "university of south dakota\n",
      "wayne state university\n",
      "american international college\n",
      "augusta university\n",
      "barry university\n",
      "boise state university\n",
      "cardinal stritch university\n",
      "clark atlanta university\n",
      "cleveland state university\n",
      "eastern michigan university\n",
      "east tennessee state university\n",
      "florida atlantic university\n",
      "georgia southern university\n",
      "grand canyon university\n",
      "indiana state university\n",
      "indiana university of pennsylvania\n",
      "jackson state university\n",
      "kennesaw state university\n",
      "lamar university\n",
      "liberty university\n",
      "lindenwood university\n",
      "middle tennessee state university\n",
      "morgan state university\n",
      "national louis university\n",
      "north carolina a&t state university\n",
      "northern arizona university\n",
      "northern illinois university\n",
      "oakland university\n",
      "old dominion university\n",
      "portland state university\n",
      "prairie view a&m university\n",
      "regent university\n",
      "sam houston state university\n",
      "san francisco state university\n",
      "spalding university\n",
      "tennessee state university\n",
      "tennessee technological university\n",
      "texas a&m university--commerce\n",
      "texas a&m university--corpus christi\n",
      "texas a&m university--kingsville\n",
      "texas southern university\n",
      "texas state university\n",
      "texas woman's university\n",
      "trevecca nazarene university\n",
      "trinity international university\n",
      "university of akron\n",
      "university of arkansas--little rock\n",
      "university of louisiana--lafayette\n",
      "university of louisiana--monroe\n",
      "university of maryland--eastern shore\n",
      "university of memphis\n",
      "university of missouri--st. louis\n",
      "university of nebraska--omaha\n",
      "university of nevada--las vegas\n",
      "university of new orleans\n",
      "university of north carolina--greensboro\n",
      "university of northern colorado\n",
      "university of north texas\n",
      "university of south alabama\n",
      "university of southern mississippi\n",
      "university of texas--arlington\n",
      "university of texas--el paso\n",
      "university of texas--rio grande valley\n",
      "university of texas--san antonio\n",
      "university of the cumberlands\n",
      "university of toledo\n",
      "university of west florida\n",
      "university of west georgia\n",
      "university of wisconsin--milwaukee\n",
      "valdosta state university\n",
      "wichita state university\n",
      "wright state university\n",
      "alliant international university\n",
      "argosy university\n",
      "california institute of integral studies\n",
      "capella university\n",
      "idaho state university\n",
      "northcentral university\n",
      "trident university international\n",
      "union institute and university\n",
      "university of phoenix\n",
      "walden university\n",
      "wilmington university\n",
      "context\n",
      "i used a bag of words to find 900 public nazi/altright twitter accounts, and this is (up to) 200 tweets scraped from each account, in json. (november 14, 2017)\n",
      "content\n",
      "these are twitter status objects. they contain a variety of twitter user information, hashtags, and other attributes.\n",
      "inspiration\n",
      "i'm working on refining a nazi-detection engine---first by refining the bag of words to help improve the dataset and data collection, then by using the improved dataset to train an ml model: https://github.com/saraislet/sturm/\n",
      "context\n",
      "it is about recognizing songs emotion. generally, for songs, audio features and lyrics could be used. for ground truth data, it tends to use online tags if dataset scale is large. otherwise, music experts or trained candidates could be organized to label songs emotion.\n",
      "content\n",
      "here songs data is from the million song dataset (msd). it contains almost 1 million songs data. and in the stage1, only tempo, loudness and mode are extracted for analysis as audio feature. for checking convenience, artist and title are added.\n",
      "for ground truth data, tags from lastfm is adopted.\n",
      "for two sets of data mentioned above, some preprocessing work is done. if you have more interests, you can find all original data here. https://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset\n",
      "songs data and tags data could be connected by track_id.\n",
      "in the further stage, timbre and pitch could be used in an appropriate way, especially related to time-series analysis. adding genre, similarity or other features to assist songs emotion prediction.\n",
      "acknowledgements\n",
      "the million song dataset was created under a grant from the national science foundation, project iis-0713334. the original data was contributed by the echo nest, as part of an nsf-sponsored goali collaboration. subsequent donations from secondhandsongs.com, musixmatch.com, and last.fm, as well as further donations from the echo nest, are gratefully acknowledged.\n",
      "inspiration\n",
      "use this dataset to do some exploratory data analysis, predict emotion for songs if you like.\n",
      "context\n",
      "disk space captured for several months for a set of windows servers\n",
      "content\n",
      "contents are the server name, disk drive, total disk space, free disk space and percentage of free space\n",
      "acknowledgements\n",
      "thanks to kaggle for providing this development environment\n",
      "inspiration\n",
      "my initial goal is to add a column with the moving average of free disk space (for 7 days), to be used for forcasting\n",
      "context\n",
      "i was searching for the most sought after startups in india and i came to know about these startups. the list contains the overview of the top 50 promising startups in india.\n",
      "content\n",
      "the excel sheet contains the startup and the idea they are working on. data was acquired by economic times newspaper. i have put that data inside this excel sheet to upload on kaggle. this list was published on december 2016. it was meant to represent the startups which are most expected to do well in 2017.\n",
      "the data was collected in 2016. columns contain 1) name of startup 2) industry 3) location 4) founding team 5) business idea\n",
      "acknowledgements\n",
      "the data was collected by economic times india. i have just put that data into an excel sheet. original data : 50 hot startups in 2017\n",
      "inspiration\n",
      "this dataset can be updated to include even more startups. we can have a list of startups, their ideas and whether they succeeded or not. thus this kind of data can be used to predict the possibility of success or failure of a business idea.\n",
      "context\n",
      "my goal with this dataset is to create the largest and most organized dataset of jokes.\n",
      "tools for this dataset are on my github\n",
      "content\n",
      "jokes reduced to only the question and the answer.\n",
      "duplicates not removed\n",
      "offensive jokes not removed\n",
      "acknowledgements\n",
      "question-answer jokes by jiri roznovjak\n",
      "short jokes by abhinav moudgil\n",
      "inspiration\n",
      "humor is one of the most difficult domains of natural language processing.\n",
      "context\n",
      "this dataset has been obtained by scraping ta (the famous tourism website) for information about restaurants for a given city. the scraper goes through the restaurants listing pages and fulfills a raw dataset. the raw datasets for the main cities in europe have been then curated for futher analysis purposes, and aggregated to obtain this dataset.\n",
      "the scraper is a python script, available on the github repository here.\n",
      "it uses principally pandas and beautifulsoup libraries.\n",
      "important: the restaurants list contains the restaurants that are registrered in the ta database only. all the restaurants of a city may not be resgistered in this database.\n",
      "content\n",
      "the dataset contain restaurants information for 31 cities in europe: amsterdam (nl), athens (gr) , barcelona (es) , berlin (de), bratislava (sk), bruxelles (be), budapest (hu), copenhagen (dk), dublin (ie), edinburgh (uk), geneva (ch), helsinki (fi), hamburg (de), krakow (pl), lisbon (pt), ljubljana (si), london (uk), luxembourg (lu), madrid (es), lyon (fr), milan (it), munich (de), oporto (pt), oslo (no), paris (fr), prague (cz), rome (it), stockholm (se), vienna (at), warsaw (pl), zurich (ch).\n",
      "the data is a .csv file comma-separated that contains 125 433 entries (restaurants). it is structured as follow: - name: name of the restaurant\n",
      "city: city location of the restaurant\n",
      "cuisine style: cuisine style(s) of the restaurant, in a python list object (94 046 non-null)\n",
      "ranking: rank of the restaurant among the total number of restaurants in the city as a float object (115 645 non-null)\n",
      "rating: rate of the restaurant on a scale from 1 to 5, as a float object (115 658 non-null)\n",
      "price range: price range of the restaurant among 3 categories , as a categorical type (77 555 non-null)\n",
      "number of reviews: number of reviews that customers have let to the restaurant, as a float object (108 020 non-null)\n",
      "reviews: 2 reviews that are displayed on the restaurants scrolling page of the city, as a list of list object where the first list contains the 2 reviews, and the second le dates when these reviews were written (115 673 non-null)\n",
      "url_ta: part of the url of the detailed restaurant page that comes after 'www.tripadvisor.com' as a string object (124 995 non-null)\n",
      "id_ta: identification of the restaurant in the ta database constructed a one letter and a number (124 995 non-null)\n",
      "missing information for restaurants (for example unrated or unreviewed restaurants) are in the dataset as nan (numpy.nan).\n",
      "acknowledgements\n",
      "this work has been done as a personal interest but also as a training of the skills i got from the datacamp data science bootcamp i have followed.\n",
      "i hope you will find this dataset inspiring and will make great stories out of it that i will be pleased to read :)\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "the data provides customer and date level transactions for few years. it can be used for demonstration of any analysis that require transaction information like rfm. the data also provide response information of customers to a promotion campaign.\n",
      "content\n",
      "transaction data provides customer_id, transaction date and amount of purchase. response data provides the response information of each of the customers. it is a binary variable indicating whether the customer responded to a campaign or not.\n",
      "acknowledgements\n",
      "extremely thankful numerous kernel and data publishers of kaggle and github. learnt a lot from these communities.\n",
      "inspiration\n",
      "more innovative approaches for handling rfm analysis.\n",
      "context\n",
      "fasttext word embeddings trained on english wikipedia\n",
      "fasttext embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.\n",
      "content\n",
      "each line contains a word followed by 300-dimensional embedding\n",
      "acknowledgements\n",
      "p. bojanowski, e. grave, a. joulin, t. mikolov, \"enriching word vectors with subword information\", arxiv 2016\n",
      "fasttext embeddings: https://github.com/facebookresearch/fasttext/blob/master/pretrained-vectors.md\n",
      "inspiration\n",
      "q1: how does fasttext compare with glove and word2vec embeddings?\n",
      "q2: what are the different approaches for learning embeddings with sub-word information?\n",
      "q3: how does fasttext compare with character-level n-gram representation of words?\n",
      "context\n",
      "i made this dataset for coursera assignment (applied plotting, charting & data representation in python).\n",
      "content\n",
      "price transition of crypto-currencies in 2017. these data were downloaded via poloniex api.\n",
      "many women who are initially thought to have angina turn out to have normal coronary angiograms, that is they are found not to have angina after all. a study was carried out to assess the feasibility of a preliminary screening test. for a large number of patients who were thought to have angina, information on a number of possible risk factors was collected and then their subsequent angina status was recorded. the data is available as an r data frame entitled angina and contains the following information:\n",
      "status: whether woman turns out to have angina (yes/no) age: age of a woman smoke: smoking status (1=current-, 2=ex-, 3=non-smoker) cig: current average number of cigarettes per day hyper: hypertension (1=absent, 2=mild, 3=moderate) angfam: family history of angina (yes/no) myofam: family history of myocardial infarction (yes/no) strokefam: family history of stroke (yes/no) diabetes: does woman have diabetes? (yes/no) missing values are coded as na.\n",
      "the main aim of this study was to try to find out which, if any, of the health variables, are associated with angina and whether some subset of them could be used to help predict the dependent variable angina status. the accompanying document on the `model selection through backward elimination’ is going to be useful for that purpose. more specifically, it would be helpful to be able to estimate the risk/probability that a woman with a particular combination of these health variables truly has angina. if such a scheme of estimating risks can be constructed, is it likely to be useful? i.e. is it good at predicting whether a woman has angina or not (since the treatment of angina is expensive)? in addition, it would be of interest to estimate the individual effects of important variables. for example, if smoking seems to be a risk factor, then what is the odds of a smoker having angina relative to a non-smoker? what about ex-smokers and light smokers?\n",
      "first held in 1959, the international mathematical olympiad is an annual math competition for top high school students around the world. it consists of six problems, divided between two days: on each day, contestants are given 4.5 hours to solve three problems.\n",
      "this dataset contains scores of the imo from 1984 to 2017. the data for years 1959-1984 do not always record the scores for each individual problem, so it is omitted from this dataset.\n",
      "source: scraped from imo-official.org.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "i've always had problems with sleep. i felt bad either undersleeping or oversleeping. so last october i promised a friend of mine to get up at 5 am for one month and started logging sleep data to a google spreadsheet. sometimes i broke my promise, sometimes i was sick or on vacation and overslept but mainly kept my promise. now with the help of kaggle community i want to analyze this data and know how much sleep i need. need to say that i'm fan of biphasic and polyphasic sleep.\n",
      "content\n",
      "i put a reminder on todo list to enter data everyday. i've decided not to capture many features besides times, only how easy felt asleep, how easy got up and how felt afterwards. there're usually more than one row per day. i usually get up by 5-6 am and if feel very sleepy go back to sleep again. sometimes i had a nap in the afternoon or in weekends.\n",
      "inspiration\n",
      "i would like to know minimum amount of sleep i need to feel either \"very good\" (4.5) or \"super good\" (5). i guess this will be an average time for the last n days. by the way after how many days body resets it's sleep clock? it would be nice to know optimal time to go to bed too.\n",
      "this dataset does not have a description yet.\n",
      "dataset containing 173 college common data sets\n",
      "contains common data sets for the following schools:\n",
      "alabama state university\n",
      "angelo state university\n",
      "arapahoe community college\n",
      "arkansas tech university\n",
      "aurora university\n",
      "baldwin wallace university\n",
      "beloit college\n",
      "bemidji state university\n",
      "berea college\n",
      "binghamton university\n",
      "boston university\n",
      "bucknell university\n",
      "cabrini university\n",
      "california baptist university\n",
      "california state university, bakersfield\n",
      "california state university, long beach\n",
      "california state university, los angeles\n",
      "california state university, sacramento\n",
      "carnegie mellon university\n",
      "case western reserve university\n",
      "christopher newport university\n",
      "clark university\n",
      "colby college\n",
      "college of charleston\n",
      "collin college\n",
      "colorado college\n",
      "colorado school of mines\n",
      "colorado state university-pueblo\n",
      "columbia college\n",
      "concordia university texas\n",
      "cornell university\n",
      "davidson college\n",
      "delaware technical community college\n",
      "desales university\n",
      "dickinson college\n",
      "drake university\n",
      "drew university\n",
      "duquesne university\n",
      "east central university\n",
      "eastern washington university\n",
      "embry riddle aeronautical university-daytona beach\n",
      "fairfield university\n",
      "florida gulf coast\n",
      "florida international university\n",
      "fort hays state university\n",
      "georgia institute of technology\n",
      "gettysburg college\n",
      "hamilton college\n",
      "hollins university\n",
      "humboldt state university\n",
      "iowa state university\n",
      "jackson state university\n",
      "john jay college of criminal justice\n",
      "kennesaw state university\n",
      "lafayette college\n",
      "lane college\n",
      "lee university\n",
      "le moyne college\n",
      "lenoir rhyne university\n",
      "life university\n",
      "loyola university maryland\n",
      "lubbock christian university\n",
      "lycoming college\n",
      "lynn university common data set\n",
      "malone university\n",
      "marlboro college\n",
      "maryville university\n",
      "massachusetts maritime academy\n",
      "metropolitan state university of denver\n",
      "michigan technological university\n",
      "middlebury college\n",
      "millersville university\n",
      "mississippi state university\n",
      "mott community college\n",
      "neumann university\n",
      "northeastern state university\n",
      "northern arizona university\n",
      "northern kentucky university\n",
      "nyack college\n",
      "oklahoma christian university\n",
      "oklahoma state university\n",
      "old dominion university\n",
      "oral roberts university\n",
      "pepperdine university\n",
      "pomona college\n",
      "prescott college\n",
      "providence college\n",
      "reed college\n",
      "regis university\n",
      "rensselaer polytechnic institute\n",
      "rice university\n",
      "rochester college\n",
      "rutgers university\n",
      "saint vincent college\n",
      "san francisco state university\n",
      "santa clara university\n",
      "scripps college\n",
      "seton hill university\n",
      "sewanee\n",
      "shippensburg university\n",
      "simpson university\n",
      "slippery rock university\n",
      "smith college\n",
      "sonoma state university\n",
      "southeastern community college\n",
      "southeastern oklahoma state university\n",
      "southwestern oklahoma state university\n",
      "springfield college\n",
      "st. ambrose university\n",
      "stanford university\n",
      "stephen f. austin state university\n",
      "suny oneonta\n",
      "suny potsdam\n",
      "sweet briar college\n",
      "taylor university\n",
      "temple university\n",
      "tennessee wesleyan university\n",
      "texas a&m university - kingsville\n",
      "texas a&m university\n",
      "texas wesleyan university\n",
      "the college at brockport\n",
      "the college of new jersey\n",
      "the university of scranton\n",
      "the university of southern mississippi\n",
      "the university of tennessee\n",
      "trinity university\n",
      "tufts university\n",
      "tulane university\n",
      "tulsa community college\n",
      "university at buffalo\n",
      "university enrollment\n",
      "university of california - davis\n",
      "university of california, riverside\n",
      "university of colorado boulder, 2015\n",
      "university of delaware\n",
      "university of kentucky\n",
      "university of louisville\n",
      "university of maine\n",
      "university of missouri\n",
      "university of montana\n",
      "university of mount olive: 2016\n",
      "university of nebraska kearney\n",
      "university of nebraska-lincoln\n",
      "university of nevada, reno\n",
      "university of new hampshire\n",
      "university of new mexico\n",
      "university of north alabama\n",
      "university of north carolina at charlotte\n",
      "university of pennsylvania\n",
      "university of pikeville\n",
      "university of puget sound\n",
      "university of science and arts\n",
      "university of texas rio\n",
      "university of the sciences in philadelphia\n",
      "university of wisconsin\n",
      "university wide common data set 2015\n",
      "villanova university\n",
      "virginia commonwealth university\n",
      "washburn university\n",
      "washington and lee university\n",
      "washington college\n",
      "weber state university\n",
      "wellesley college\n",
      "wesleyan university\n",
      "westfield state university\n",
      "westminster college\n",
      "wheaton college\n",
      "whitman college\n",
      "widener university\n",
      "worcester polytechnic institute\n",
      "xavier university of louisiana\n",
      "xavier university\n",
      "yale university\n",
      "context\n",
      "database of all walmart and sam's club stores in the united states\n",
      "python wrapper surrounding the dataset\n",
      "we are building a system to detect whether a group of words is written by one writer or more than one writes are involved. we need help in improving the efficiency of the classification. help us in improving the classifier for given files.\n",
      "context\n",
      "this is a huge dataset and takes around 400 seconds to load into kernel. if you need quickly imdb data in keras kernel use the following dataset instead:\n",
      "https://www.kaggle.com/pankrzysiu/keras-imdb-reviews\n",
      "content\n",
      "a set of 50,000 highly-polarized reviews from the internet movie database.\n",
      "usage instructions\n",
      "aclimdb_v1.zip file\n",
      "this file is to be used directly in your code. the .zip file will be automatically uncompressed by kaggle.\n",
      "imdb* files\n",
      "from os import listdir, makedirs\n",
      "from os.path import join, exists, expanduser\n",
      "\n",
      "cache_dir = expanduser(join('~', '.keras'))\n",
      "if not exists(cache_dir):\n",
      "    makedirs(cache_dir)\n",
      "datasets_dir = join(cache_dir, 'datasets')\n",
      "if not exists(datasets_dir):\n",
      "    makedirs(datasets_dir)\n",
      "\n",
      "# if you have multiple input files, change the below cp commands accordingly, typically:\n",
      "# !cp ../input/keras-imdb/imdb* ~/.keras/datasets/\n",
      "!cp ../input/imdb* ~/.keras/datasets/\n",
      "acknowledgements\n",
      "the files are on the net in these locations:\n",
      "https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "they are used by keras imdb.py:\n",
      "https://github.com/keras-team/keras/blob/master/keras/datasets/imdb.py\n",
      "inspiration\n",
      "\"python deep learning\" book example is using this:\n",
      "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb\n",
      "context\n",
      "anonymized data from profiles scraped on linkedin. contains data from about 15000 profiles. profiles came from people predominantly located in australia. includes all their work history as well as analysis of their photo and name.\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this dataset does not have a description yet.\n",
      "an extension to kaggle's ted dataset\n",
      "using transcripts provided by ted.com, a dataset that combines combines youtube metadata (at the time of scrapping) and metadata from kaggle's ted dataset.\n",
      "this extension provides not just additional metadata from youtube, but also additional transcripts of the same video, but in different languages (e.g. portugese, french, arabic, chinese, japnese, korean, turkish, dutch...). in total, 111 different languages are available (most videos do not have transcript for all languages).\n",
      "content\n",
      "for each of the 111 languages in teddirector.zip, each language file is a csv file with the following headers:\n",
      "videoid - youtube ids\n",
      "lang - language code\n",
      "title - title of the ted talk\n",
      "transcript - transcript of the ted talk in lang\n",
      "acknowledgements\n",
      "this dataset was developed as part of a larger dataset used for an information retrieval assignment. in that assignment, my team and i used ted talks to evaluate different configuration of search engine algorithms. we also used different languages for the search and retrieve task, to test for reliability of our search engine. more information can be found in our github repository. dataset is downloaded from tedtalksdirector using youtube-dl.\n",
      "code for downloading can be found from the ir project.\n",
      "more about language codes here from w3schools.com.\n",
      "inspiration\n",
      "some of the problem experienced while preparing this dataset: 1. how can we improve the matching of youtube dataset to the data scrapped from ted talk\n",
      "context：\n",
      "this data set contains the communication of students with the smartphones in the schools in a week. we set a evaluation to describe the quality of the user the through the first 3 pieces of information.\n",
      "content：\n",
      "useful information : it means the pieces of useful information between communication. connection frequency: it means the times the user connects to other users. effective connection: it means the successful times of communication with others. evaluation: it is set to judge the popularity of the users.\n",
      "acknowledgements：\n",
      "it's collected by an online survey in several schools.\n",
      "inspiration：\n",
      "can we use the data to find popular student in the school? it's useful for teachers to choose students that good at making friends.\n",
      "context\n",
      "this is the city of new york's list of official dog parks.\n",
      "some ideas for what could be interesting: mapping the most dog-friendly neighborhoods in the city, perhaps to help dog owners find an apartment in an area with access to bigger or multiple dog runs. if one combined this dataset with others like nyc's dog licensing dataset, one could explore the ratio of the area of dog run space to the number of dogs, perhaps by neighborhood. this could show dogs per square foot of dog park space, painting a picture of which neighborhoods are satisfying the needs of canine citizens and which ones have some work to do.\n",
      "content\n",
      "the data were last updated by the city of new york april 11, 2017.\n",
      "these are the fields in the dataset:\n",
      "prop_id: a unique identifier for the property. the first character is a abbreviation of the borough, followed by a 3 digit number. anything after the first 4 characters represents a subproperty. boroughs:\n",
      "x - bronx, b - brooklyn, m - manhattan, q - queens, r - staten island\n",
      "to find more data on each prop_id (exact address, etc.), these parks datasets may be used: http://www.nycgovparks.org/bigapps/dpr_parks_001.xml\n",
      "http://www.nycgovparks.org/bigapps/dpr_parks_001.json\n",
      "name: name of the property\n",
      "address: approximate location of the dog run or off-leash area (note, this is very approximate - to be more precise, cross-reference to the bigger parks dataset above)\n",
      "dogruns_type: dog run or off-leash area\n",
      "accessible: (y)es or (n)o - wheelchair accessible\n",
      "notes: additional notes\n",
      "acknowledgements\n",
      "the dataset comes from the city of new york's open data project.\n",
      "context\n",
      "this dataset originally stems from a walmart recruiting challenge. it is used here for educational purposes only.\n",
      "content\n",
      "the dataset contains anonymized sales by department for 45 walmart stores as well as supporting features.\n",
      "acknowledgements\n",
      "the dataset belongs to walmart and is used here only for educational purposes\n",
      "inspiration\n",
      "try to predict weekly sales.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "some of the most challenging problems in machine learning for me have been in predicting stock market results. so i can use kaggle's kernels on this data, i'm adding the data to kaggle.\n",
      "content\n",
      "acknowledgements\n",
      "this data was gathered using the tidyquant package.\n",
      "context\n",
      "about ease of doing business around the world.\n",
      "content\n",
      "column includes : ease of doing business | starting a business | dealing with construction permits | getting electricity | registering property | getting credit | protecting minority investors | paying taxes | trading across borders | enforcing contracts | resolving insolvency\n",
      "acknowledgements\n",
      "world bank | http://www.doingbusiness.org/rankings\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this directory contains the cross-position activity recognition datasets used in the following paper. please consider citing this article if you want to use the datasets.\n",
      "jindong wang, yiqiang chen, lisha hu, xiaohui peng, and philip s. yu. stratified transfer learning for cross-domain activity recognition. 2018 ieee international conference on pervasive computing and communications (percom).\n",
      "these datasets are secondly constructed based on three public datasets: opportunity (opp) [1], pamap2 (pamap2) [2], and uci dsads (dsads) [3].\n",
      "here are some useful information about this directory. please feel free to contact jindongwang@outlook.com for more information.\n",
      "this is not the raw data, since i have performed feature extraction and normalized the features into [-1,1]. the code for feature extraction can be found in here: https://github.com/jindongwang/activityrecognition/tree/master/code. currently, there are 27 features for a single sensor. there are 81 features for a body part. more information can be found in above percom-18 paper.\n",
      "there are 4 .mat files corresponding to each dataset: dsads.mat for uci dsads, opp_hl.mat and opp_ll.mat for opportunity, and pamap.mat for pamap2. note that opp_hl and opp_loco denotes 'high-level' and 'locomotion' activities, respectively. (1) dsads.mat: 9120 * 408. columns 1~405 are features, listed in the order of 'torso', 'right arm', 'left arm', 'right leg', and 'left leg'. each position contains 81 columns of features. columns 406~408 are labels. column 406 is the activity sequence indicating the executing of activities (usually not used in experiments). column 407 is the activity label (1~19). column 408 denotes the person (1~8). (2) opp_hl.mat and opp_loco.mat: same as dsads.mat. but they contain more body parts: 'back', 'right upper arm', 'right lower arm', 'left upper arm', 'left lower arm', 'right shoe (foot)', and 'left shoe (foot)'. of course we did not use the data of both shoes in our paper. column 460 is the activity label (please refer to opportunity dataset to see the meaning of those activities). column 461 is the activity drill (also check the dataset information). column 462 denotes the person (1~4). (3) pamap.mat: 7312 * 245. columns 1~243 are features, listed in the order of 'wrist', 'chest', and 'ankle'. column 244 is the activity label. column 245 denotes the person (1~9).\n",
      "there are another 3 datasets with the prefix 'cross_', containing only 4 common classes of each dataset. this is for experimenting the cross-dataset activity recognition (see our percom-18 paper). the 4 common classes are lying, standing, walking, and sitting. (1) cross_dsads.mat: 1920*406. columns 1~405 are features. column 406 is labels. (2) cross_opp.mat: 5022*460. columns 1~459 are features. column 460 is labels. (3) cross_pamap.mat: 3063 * 244. columns 1~243 are features. column 244 is labels.\n",
      "-------- original references for the 3 datasets:\n",
      "[1] r. chavarriaga, h. sagha, a. calatroni, s. t. digumarti, g. troster, ¨ j. d. r. millan, and d. roggen, “the opportunity challenge: a bench- ´ mark database for on-body sensor-based activity recognition,” pattern recognition letters, vol. 34, no. 15, pp. 2033–2042, 2013.\n",
      "[2] a. reiss and d. stricker, “introducing a new benchmarked dataset for activity monitoring,” in wearable computers (iswc), 2012 16th international symposium on. ieee, 2012, pp. 108–109.\n",
      "[3] b. barshan and m. c. yuksek, “recognizing daily and sports activities ¨ in two open source machine learning environments using body-worn sensor units,” the computer journal, vol. 57, no. 11, pp. 1649–1667, 2014.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "market segmentation: a company must have different strategies for product promotion for different individuals. not every individual has the same requirement and demand. so the company has to segment, target and position the product according to the tastes of various individuals.\n",
      "segmentation of products based on the revenue generated in different regions is done to understand the market trends.\n",
      "the segmentation will help the company to devise marketing strategies and promotional schemes to position the right product according to the preference of the consumers in any given segment.\n",
      "content\n",
      "the store dataset contains 10,000 observations or we can call it transactions of 5 variables. each row represents the transaction made by the reps. each column contains the attributes of this dataset include: reps - representative who are involved in the promotion and sale of the products in their respective region. products - there are 12 brands of products promoted by the company. qty - quantity sold in units revenue - revenue generated for each transaction region - there are 4 regions - east, north, south and west india.\n",
      "it would also be possible to predict the revenue and make it a regression task.\n",
      "inspiration\n",
      "i am hosting this dataset in order to give it wider exposure, to give the community an opportunity to experiment with different algorithms / models.\n",
      "context\n",
      "introduction: the dataset used for this experiment is real and authentic. the dataset is acquired from uci machine learning repository website [13]. the title of the dataset is ‘crime and communities’. it is prepared using real data from socio-economic data from 1990 us census, law enforcement data from the 1990 us lemas survey, and crimedata from the 1995 fbi ucr [13]. this dataset contains a total number of 147 attributes and 2216 instances.\n",
      "the per capita crimes variables were calculated using population values included in the 1995 fbi data (which differ from the 1990 census values).\n",
      "content\n",
      "the variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units. the crime attributes (n=18) that could be predicted are the 8 crimes considered 'index crimes' by the fbi)(murders, rape, robbery, .... ), per capita (actually per 100,000 population) versions of each, and per capita violent crimes and per capita nonviolent crimes)\n",
      "predictive variables : 125 non-predictive variables : 4 potential goal/response variables : 18\n",
      "acknowledgements\n",
      "http://archive.ics.uci.edu/ml/datasets/communities%20and%20crime%20unnormalized\n",
      "u. s. department of commerce, bureau of the census, census of population and housing 1990 united states: summary tape file 1a & 3a (computer files),\n",
      "u.s. department of commerce, bureau of the census producer, washington, dc and inter-university consortium for political and social research ann arbor, michigan. (1992)\n",
      "u.s. department of justice, bureau of justice statistics, law enforcement management and administrative statistics (computer file) u.s. department of commerce, bureau of the census producer, washington, dc and inter-university consortium for political and social research ann arbor, michigan. (1992)\n",
      "u.s. department of justice, federal bureau of investigation, crime in the united states (computer file) (1995)\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "data available in the dataset may not act as a complete source of information for identifying factors that contribute to more violent and non-violent crimes as many relevant factors may still be missing.\n",
      "however, i would like to try and answer the following questions answered.\n",
      "analyze if number of vacant and occupied houses and the period of time the houses were vacant had contributed to any significant change in violent and non-violent crime rates in communities\n",
      "how has unemployment changed crime rate(violent and non-violent) in the communities?\n",
      "were people from a particular age group more vulnerable to crime?\n",
      "does ethnicity play a role in crime rate?\n",
      "has education played a role in bringing down the crime rate?\n",
      "disclaimer\n",
      "i have prepared the data for rv_forecasting eda and i do have no claim on data's usage rights. all data belongs to jma. please use it at your own risk. i hope you could find this dataset useful as well and share your experiences with me.\n",
      "description\n",
      "since jma does not do all the possible calculations in many weather stations, i have focused on prefecture-centres which nearly all calculations are made in the expense of accuracy in the geographic tagging. it is assumed that all the districts within a prefecture should have similar weather conditions in a given day.\n",
      "content\n",
      "date calendar date in classic y-m-d format.\n",
      "air_district1 could be harvested from air_area_name column via str_split_fixed(air_area_name, \" \", n = 3)[,1] code utilizing stringr package in r\n",
      "avg_temp average temperature data\n",
      "rain_amount amount of precipitation in centimetres\n",
      "sunshine_hours daylight in hours\n",
      "wind_speed wind_speed in kilometres-per-hour\n",
      "snowfall total snowfall during day in centimetres\n",
      "dataset description:\n",
      "this dataset has population data of each indian district from 2001 and 2011 censuses.\n",
      "the special thing about this data is that it has centroids for each district and state.\n",
      "centroids for a district are calculated by mapping border of each district as a polygon of latitude/longitude points in a 2d plane and then calculating their mean center.\n",
      "centroids for a state are calculated by calculating the weighted mean center of all districts that constitutes a state. the population count is the weight assigned to each district.\n",
      "example analysis:\n",
      "the complete code for calculating the centroids and web scraping for the data is shared on github.\n",
      "the purpose of this project was to map population density center for each state.\n",
      "you can also read about the complete project here: https://medium.com/@sumit.arora/plotting-weighted-mean-population-centroids-on-a-country-map-22da408c1397\n",
      "output screenshots: indian districts mapped as polygons\n",
      "mapping centroids for each district\n",
      "mean centers of population by state, 2001 vs. 2011\n",
      "national center of population\n",
      "context\n",
      "i appreciate the thoroughness of stanford mass shootings in america (msa) and the continuous efforts to update us mass shootings last 50 years (1966-2017). this dataset merges the two, adding 14 records to stanford msa.\n",
      "content\n",
      "see original datasets for more information.\n",
      "acknowledgements\n",
      "i acknowledge the contributors of the two original datasets, zeeshan-ul-hassan usmani (us mass shootings) and carlos paradis (stanford msa).\n",
      "context\n",
      "national exam of (higher education) student performance [enade]\n",
      "national exam of upper secondary education [enem]\n",
      "national institute for educational studies and research [inep]\n",
      "content\n",
      "each row is a student that finished higher education education and has his enade and enem scores and more data.\n",
      "for more metadata: see the xlsx files (portuguese).\n",
      "references\n",
      "http://inep.gov.br/microdados\n",
      "http://portal.inep.gov.br/web/guest/about-inep\n",
      "this dataset contains training and testing data for digit recognition which includes hand written images of digits.\n",
      "it contains four zip files which you can easily include in your neural network. so, download all four of them by clicking \"download all\" button.\n",
      "this is the mnist dataset used world-wide to check the performance of neural networks based upon digit recognition.\n",
      "it also contains training and testing labels.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "hepatitis b levels of some random patients that i've collected over the course of 8 years, from random hospitals i've worked at. every patient also had one of the 3 broad categories of diseases. at the time, technical limitations kept me from accurately collecting the exact disease/disorder the patient had, so this has to do.\n",
      "(had to re-upload this dataset because kaggle decided to delete it)\n",
      "context\n",
      "arabic diacritics are often missed in arabic scripts. this feature is a handicap for new learner to read َarabic, text to speech conversion systems, reading and semantic analysis of arabic texts.\n",
      "the automatic diacritization systems are the best solution to handle this issue. but such automation needs resources as diactritized texts to train and evaluate such systems.\n",
      "content\n",
      "data is a collection of arabic vocalized texts, which covers modern and classical arabic language. the data contains over 75 million of fully vocalized words obtained from 97 books, structured in text files.\n",
      "the corpus is collected mostly from islamic classical books [14], and using semi-automatic web crawling process. the modern standard arabic texts crawled from the internet represent 1.15% of the corpus, about 867,913 words, while the most part is collected from shamela library, which represent 98.85%, with 74,762,008 words contained in 97 books\n",
      "acknowledgements\n",
      "we acknowledge the efforts made by shamela library volunteers to write, diacritize and make free texts.\n",
      "inspiration\n",
      "can machine vocalize an arabic text with précision? what semantic data can be extracted from vocalized texts\n",
      "squeezenet 1.0\n",
      "squeezenet: alexnet-level accuracy with 50x fewer parameters and <0.5mb model size\n",
      "recent research on deep neural networks has focused primarily on improving accuracy. for a given accuracy level, it is typically possible to identify multiple dnn architectures that achieve that accuracy level. with equivalent accuracy, smaller dnn architectures offer at least three advantages: (1) smaller dnns require less communication across servers during distributed training. (2) smaller dnns require less bandwidth to export a new model from the cloud to an autonomous car. (3) smaller dnns are more feasible to deploy on fpgas and other hardware with limited memory. to provide all of these advantages, we propose a small dnn architecture called squeezenet. squeezenet achieves alexnet-level accuracy on imagenet with 50x fewer parameters. additionally, with model compression techniques we are able to compress squeezenet to less than 0.5mb (510x smaller than alexnet).\n",
      "authors: forrest n. iandola, song han, matthew w. moskewicz, khalid ashraf, william j. dally, kurt keutzer\n",
      "https://arxiv.org/abs/1602.07360\n",
      "squeezenet architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "vgg-11\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "this database includes the information about the news which appears in the front page of meneame.net, a spanish website similar to reddit. data from 2005/12 to 2017/12/04. there are about 177.000 observations with 17 variables: news, users, post time, positive votes, negative votes, description and so on. the most part of the news are in spanish.\n",
      "esta base de datos incluye la información de las noticias que aparecen en la portada de la web meneame.net, un sitio web similar a reddit. los datos van desde diciembre de 2005 hasta el 4 de diciembre de 2017. hay 177.000 observaciones con 17 variables: noticias, usuarios, hora de envío, votos negativos, votos positivos, descripción, etc. la mayor parte de las noticias están en español.\n",
      "content\n",
      "index - news index\n",
      "  noticia - content of the news\n",
      "  link_noticia - link of the news\n",
      "  web - original website\n",
      "  usuario - user\n",
      "  fecha_envio - date when the post was sent\n",
      "  fecha_publicacion - date when the post was acces to the front page\n",
      "  meneos - positive + anonymous votes\n",
      "  clicks - clicks on the news\n",
      "  comentarios - comments\n",
      "  votos_positivos - positive votes\n",
      "  votos_anonimos - anonymous votes\n",
      "  votos_negativos - negative votes\n",
      "  karma - web points earned\n",
      "  sub - subsection\n",
      "  extracto - summary of the news\n",
      "acknowledgements\n",
      "best regards to meneame.net admins and users.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "content\n",
      "both datasets mononaice and mendotaice are structured the same.\n",
      "winter - the ending year for the winter. for example the the winter of 2016-2017 would be represented by 2017. closed - the date the lake froze over. the lake is considered closed if greater than 50% of the lake's surface is frozen. opened - the date the lake ice melted. the lake is considered opened if less than 50% of the lake's surface is frozen. days - total days in the season that the lake was closed.\n",
      "acknowledgements\n",
      "thank you to the wisconsin state climatology office\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "this data was collected to train an autonomous car prototype as our college project.\n",
      "content\n",
      "the data consists of 14x32 grayscale pixel values along with proper labels as in which direction to maneuver the car. the labels are represented as follows: l- left, r- right, f- front, s- stop\n",
      "inspiration\n",
      "udacity autonomous driving nano degree program, waymo, tesla self-driving car\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "txt files for poetry generation with python\n",
      "content\n",
      "txt files of lyrics and poems\n",
      "acknowledgements\n",
      "free lyric hosting websites\n",
      "inspiration\n",
      "txt files for poetry generation with python\n",
      "this dataset does not have a description yet.\n",
      "310 observations, 13 attributes (12 numeric predictors, 1 binary class attribute - no demographics)\n",
      "lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. typical sources of low back pain include:\n",
      "the large nerve roots in the low back that go to the legs may be irritated the smaller nerves that supply the low back may be irritated the large paired lower back muscles (erector spinae) may be strained the bones, ligaments or joints may be damaged an intervertebral disc may be degenerating an irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.\n",
      "while lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. a simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\n",
      "this data set is about to identify a person is abnormal or normal using collected physical spine details/data.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "context\n",
      "nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. the sting’s even more painful when you know you’re a good driver. it doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.\n",
      "try to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year! good luck\n",
      "content\n",
      "what's inside is more than just rows and columns. make it easy for others to get started by describing how you acquired the data and what time period it represents, too.\n",
      "acknowledgements\n",
      "we wouldn't be here without the help of others. if you owe any attributions or thanks, include them here along with any citations of past research.\n",
      "inspiration\n",
      "your data will be in front of the world's largest data science community. what questions do you want to see answered?\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "this dataset does not have a description yet.\n",
      "vgg-11\n",
      "very deep convolutional networks for large-scale image recognition\n",
      "in this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. these findings were the basis of our imagenet challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. we also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. we have made our two best-performing convnet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "authors: karen simonyan, andrew zisserman\n",
      "https://arxiv.org/abs/1409.1556\n",
      "vgg architectures\n",
      "what is a pre-trained model?\n",
      "a pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. learned features are often transferable to different data. for example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n",
      "why use a pre-trained model?\n",
      "pre-trained models are beneficial to us for many reasons. by using a pre-trained model you are saving time. someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n",
      "context\n",
      "this dataset is consolidated from the surveys of alif ailaan & aser pakistan on primary schooling performance.\n",
      "content\n",
      "dataset contains different dimensions and measures across the board that were used to measure the performance. i have compiled individual file of each year into one for your facilitation.\n",
      "acknowledgements\n",
      "this dataset is compiled by me and i am very thankful to aser pakistan & alif ailaan for providing pdfs from which i extracted.\n",
      "inspiration\n",
      "explore this visualization to interpret performance province wise, city wise and identify different needs and room for actionable improvements.\n",
      "for starting see this: https://public.tableau.com/views/pakistaneducationalperformance-dashboard/pakistaneducationperformancedashboard?%3aembed=y&%3ashowvizhome=no&%3adisplay_count=y&%3adisplay_static_image=y&%3abootstrapwhennotified=true\n",
      "context\n",
      "this is an easy-to-use binary classification dataset for predicting whether or not it will rain tomorrow. use for binary classification modeling.\n",
      "content\n",
      "the content is daily weather observations from multiple australian weather stations. the target variable is raintomorrow, which means: did it rain the next day? yes or no. so the dataset is already set up for prediction -- no need to change the target with a lead function.\n",
      "acknowledgements\n",
      "this dataset comes from the rattle package. the rattle package is currently not available to kaggle's kernels.\n",
      "context\n",
      "what makes us, humans, able to tell apart two songs of different genres? maybe you have ever been in the diffcult situation to explain show it sounds the music style that you like to someone. then, could an automatic genre classifcation be possible?\n",
      "content\n",
      "each row is an electronic music song. the dataset contains 100 song for each genre among 23 electronic music genres, they were the top (100) songs of their genres on november 2016. the 71 columns are audio features extracted of a two random minutes sample of the file audio. these features have been extracted using pyaudioanalysis (https://github.com/tyiannak/pyaudioanalysis).\n",
      "context\n",
      "i'm interested in object recognition, and trying to learn how to build cnn models able to recognize specific objects.\n",
      "content\n",
      "so i created this data set by taking pictures around my flat with a basketball in each image. the data is made up of several pairs of image and annotation files (*.annote). the annotation file subdivides the image into 200x200 pixel windows and says whether basketball is present in it or not.\n",
      "acknowledgements\n",
      "the initial analysis has used the mnist deep cnn model given in the tensor flow example adapted for multiple channels and image resolution\n",
      "inspiration\n",
      "i'd like to some feedback both on my model and alternative way of building or cnn models. have fun!\n",
      "extra data for rossman store from http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
      "context\n",
      "fortnite: battle royale has over 20 million unique players, but there are no datasets on kaggle yet! this one is small, but better than nothing! it contains the location coordinates of chests in fortnite: battle royale as of the 1st of december 2017.\n",
      "content\n",
      "there are three columns in the dataset. the first one has an identifier which is a running number starting from 1. the second column has latitudinal coordinates for chests. the third column has longitudinal coordinates for chests.\n",
      "acknowledgements\n",
      "the data originates from: http://www.fortnitechests.info/ which is updated by soumydev (soumyydev@gmail.com)\n",
      "neither soumydev or the uploader of this dataset are affiliated with epic games or any of his partners. all copyrights reserved to their respective owners.\n",
      "inspiration\n",
      "based on chest coordinates, what are the best clusters to land on at the beginning of the match?\n",
      "context\n",
      "this dataset provides the nationalities of passengers on the titanic. it is meant to be merged with the titanic dataset to allow analysis of how a passengers nationality effected survival chances aboard the titanic.\n",
      "the passengers nationality was predicted using the open api from nameprism http://www.name-prism.com/api using the passenger name variable from the titanic dataset.\n",
      "details from site; nameprism is a non-commercial nationality/ethnicity classification tool that aims to support academic research, e.g. sociology and demographic studies. in this project, we learn name embeddings for name parts (first/last names) and classify names to 39 leaf nationalities and 6 u.s. ethnicities.\n",
      "research covered at; nationality classification using name embeddings. junting ye, shuchu han, yifan hu, baris coskun, meizhu liu, hong qin and steven skiena. cikm, singapore, nov. 2017.\n",
      "content\n",
      "the data consists of two variables; passengerid and nationality in two files that constitute training and test data for the titanic dataset.\n",
      "acknowledgements\n",
      "-nameprism\n",
      "inspiration\n",
      "did a passenger nationality on-board the titanic affect their survival chances?\n",
      "lem.json\n",
      "this file contains lementized english words as key holding their original words as value\n",
      "eaxmple content\n",
      "json {\"abbreviation\": [\"abbreviations\", \"abbreviation\"]}\n",
      "stem.json\n",
      "this file contains stemmed english words as key holding their original words as value\n",
      "eaxmple content\n",
      "json {\"abandon\": [\"abandoned\", \"abandonment\", \"abandoning\", \"abandon\", \"abandonable\", \"abandoner\"]}\n",
      "context\n",
      "this data set contains weather data for los angeles (l.a.) during 2014.\n",
      "content\n",
      "day : number of day of an year, type of weather : this column tells type of weather in a day\n",
      "inspiration\n",
      "the goal is to count how many times each type of weather occurred over the course of the year. during this , how to manipulate the data with lists, and make good progress towards that goal.\n",
      "context\n",
      "births in u.s during 1994 to 2003.\n",
      "content\n",
      "the data set has the following structure:\n",
      "year - year\n",
      "month - month\n",
      "date_of_month - day number of the month\n",
      "day_of_week - day of week, where 1 is monday and 7 is sunday\n",
      "births - number of births\n",
      "acknowledgements\n",
      "data set from the centers for disease control and prevention's national national center for health statistics\n",
      "inspiration\n",
      "make a dictionary that shows total number of births on each day of week?\n"
     ]
    }
   ],
   "source": [
    "for i in df['Description'].items():\n",
    "    raw = str(i[1]).lower()\n",
    "    print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the dataset\n",
    "- Tokenize\n",
    "- Stop words removal\n",
    "- Non-alphabetic words removal\n",
    "- Lowercase\n",
    "- Define them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pattern, tokenizer, stop words and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'contains', 'transaction', 'made', 'credit', 'card', 'september', 'european', 'cardholder', 'dataset', 'present', 'transaction', 'occurred', 'two', 'day', 'fraud', 'transaction', 'dataset', 'highly', 'unbalanced', 'positive', 'class', 'fraud', 'account', 'transaction', 'contains', 'numerical', 'input', 'variable', 'result', 'pca', 'transformation', 'unfortunately', 'due', 'confidentiality', 'issue', 'cannot', 'provide', 'original', 'feature', 'background', 'information', 'data', 'feature', 'principal', 'component', 'obtained', 'pca', 'feature', 'transformed', 'pca', 'time', 'amount', 'feature', 'time', 'contains', 'second', 'elapsed', 'transaction', 'first', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'used', 'example', 'dependant', 'cost', 'senstive', 'learning', 'feature', 'class', 'response', 'variable', 'take', 'value', 'case', 'fraud', 'otherwise', 'given', 'class', 'imbalance', 'ratio', 'recommend', 'measuring', 'accuracy', 'using', 'area', 'precision', 'recall', 'curve', 'auprc', 'confusion', 'matrix', 'accuracy', 'meaningful', 'unbalanced', 'classification', 'dataset', 'collected', 'analysed', 'research', 'collaboration', 'worldline', 'machine', 'learning', 'group', 'http', 'mlg', 'ulb', 'ac', 'ulb', 'université', 'libre', 'de', 'bruxelles', 'big', 'data', 'mining', 'fraud', 'detection', 'detail', 'current', 'past', 'project', 'related', 'topic', 'available', 'http', 'mlg', 'ulb', 'ac', 'brufence', 'http', 'mlg', 'ulb', 'ac', 'artml', 'please', 'cite', 'andrea', 'dal', 'pozzolo', 'olivier', 'caelen', 'reid', 'johnson', 'gianluca', 'bontempi', 'calibrating', 'probability', 'undersampling', 'unbalanced', 'classification', 'symposium', 'computational', 'intelligence', 'data', 'mining', 'cidm', 'ieee']\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "\n",
    "for i in df['Description'].items():\n",
    "    # clean and tokenize document string\n",
    "    raw = str(i[1]).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
    "    \n",
    "    # remove word containing only single char\n",
    "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(new_lemma_tokens)\n",
    "\n",
    "\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter low frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index to word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus, num_topics=15, id2word = id2word, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.0181141, 'player'),\n",
      "   (0.014698765, 'match'),\n",
      "   (0.0146566825, 'game'),\n",
      "   (0.01349743, 'time'),\n",
      "   (0.012353201, 'inspiration')],\n",
      "  -0.7918305939969578),\n",
      " ([(0.011850098, 'text'),\n",
      "   (0.009736271, 'file'),\n",
      "   (0.007820071, 'contains'),\n",
      "   (0.007774869, 'http'),\n",
      "   (0.007116005, 'use')],\n",
      "  -0.8615363689063186),\n",
      " ([(0.01577201, 'cell'),\n",
      "   (0.015328783, 'instance'),\n",
      "   (0.010367524, 'name'),\n",
      "   (0.010325557, 'learning'),\n",
      "   (0.010141488, 'group')],\n",
      "  -1.794857334941512),\n",
      " ([(0.013969049, 'year'),\n",
      "   (0.011342459, 'number'),\n",
      "   (0.0103551345, 'information'),\n",
      "   (0.010077194, 'state'),\n",
      "   (0.007924146, 'crime')],\n",
      "  -1.8012512390817939),\n",
      " ([(0.030116025, 'image'),\n",
      "   (0.027993483, 'column'),\n",
      "   (0.015135894, 'activity'),\n",
      "   (0.015083765, 'label'),\n",
      "   (0.01414586, 'csv')],\n",
      "  -1.9007929103337424),\n",
      " ([(0.03862689, 'csv'),\n",
      "   (0.021309918, 'score'),\n",
      "   (0.021087777, 'weapon'),\n",
      "   (0.016905691, 'name'),\n",
      "   (0.01678431, 'time')],\n",
      "  -1.9570779605794606),\n",
      " ([(0.054189567, 'model'),\n",
      "   (0.041089665, 'trained'),\n",
      "   (0.027820287, 'pre'),\n",
      "   (0.024967765, 'restaurant'),\n",
      "   (0.023462983, 'feature')],\n",
      "  -2.0449348142762087),\n",
      " ([(0.022241708, 'song'),\n",
      "   (0.017819535, 'student'),\n",
      "   (0.0170566, 'http'),\n",
      "   (0.01606959, 'school'),\n",
      "   (0.014467929, 'education')],\n",
      "  -2.0487786845203764),\n",
      " ([(0.02359915, 'team'),\n",
      "   (0.022603292, 'player'),\n",
      "   (0.017222399, 'product'),\n",
      "   (0.016505657, 'question'),\n",
      "   (0.013637895, 'game')],\n",
      "  -2.2464393703330843),\n",
      " ([(0.043580495, 'user'),\n",
      "   (0.024204677, 'movie'),\n",
      "   (0.020840125, 'integer'),\n",
      "   (0.02034525, 'id'),\n",
      "   (0.01814154, 'post')],\n",
      "  -2.348320562441688),\n",
      " ([(0.017615175, 'day'),\n",
      "   (0.016400572, 'back'),\n",
      "   (0.013786685, 'woman'),\n",
      "   (0.012603692, 'number'),\n",
      "   (0.012074515, 'lower')],\n",
      "  -2.412308918515808),\n",
      " ([(0.38889524, 'university'),\n",
      "   (0.0718587, 'state'),\n",
      "   (0.04834637, 'college'),\n",
      "   (0.022168586, 'california'),\n",
      "   (0.019973718, 'texas')],\n",
      "  -2.51880602100133),\n",
      " ([(0.057923608, 'de'),\n",
      "   (0.023965618, 'vote'),\n",
      "   (0.022395998, 'population'),\n",
      "   (0.021128464, 'district'),\n",
      "   (0.020898847, 'en')],\n",
      "  -2.879374789460262),\n",
      " ([(0.057582784, 'description'),\n",
      "   (0.05241578, 'yet'),\n",
      "   (0.03312959, 'time'),\n",
      "   (0.026274709, 'tweet'),\n",
      "   (0.014075907, 'season')],\n",
      "  -5.029033388063555),\n",
      " ([(0.03844474, 'word'),\n",
      "   (0.019449312, 'dog'),\n",
      "   (0.016110346, 'service'),\n",
      "   (0.015575185, 'txt'),\n",
      "   (0.015070451, 'customer')],\n",
      "  -7.28200444569768)]\n"
     ]
    }
   ],
   "source": [
    "pprint(ldamodel.top_topics(corpus,topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the 15 topics with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.389*\"university\" + 0.072*\"state\" + 0.048*\"college\" + 0.022*\"california\" + 0.020*\"texas\" + 0.015*\"institute\" + 0.011*\"north\" + 0.011*\"new\" + 0.010*\"solar\" + 0.010*\"technology\"\n",
      "Topic #1: 0.016*\"cell\" + 0.015*\"instance\" + 0.010*\"name\" + 0.010*\"learning\" + 0.010*\"group\" + 0.010*\"file\" + 0.009*\"company\" + 0.009*\"classification\" + 0.008*\"software\" + 0.008*\"attribute\"\n",
      "Topic #2: 0.012*\"text\" + 0.010*\"file\" + 0.008*\"contains\" + 0.008*\"http\" + 0.007*\"use\" + 0.007*\"date\" + 0.006*\"available\" + 0.006*\"time\" + 0.006*\"language\" + 0.005*\"inspiration\"\n",
      "Topic #3: 0.018*\"day\" + 0.016*\"back\" + 0.014*\"woman\" + 0.013*\"number\" + 0.012*\"lower\" + 0.012*\"inspiration\" + 0.011*\"set\" + 0.011*\"city\" + 0.011*\"health\" + 0.011*\"risk\"\n",
      "Topic #4: 0.018*\"player\" + 0.015*\"match\" + 0.015*\"game\" + 0.013*\"time\" + 0.012*\"inspiration\" + 0.012*\"see\" + 0.011*\"others\" + 0.011*\"research\" + 0.011*\"world\" + 0.011*\"get\"\n",
      "Topic #5: 0.058*\"description\" + 0.052*\"yet\" + 0.033*\"time\" + 0.026*\"tweet\" + 0.014*\"season\" + 0.014*\"many\" + 0.013*\"sport\" + 0.011*\"information\" + 0.010*\"find\" + 0.010*\"url\"\n",
      "Topic #6: 0.014*\"year\" + 0.011*\"number\" + 0.010*\"information\" + 0.010*\"state\" + 0.008*\"crime\" + 0.007*\"city\" + 0.007*\"name\" + 0.007*\"review\" + 0.006*\"database\" + 0.006*\"csv\"\n",
      "Topic #7: 0.054*\"model\" + 0.041*\"trained\" + 0.028*\"pre\" + 0.025*\"restaurant\" + 0.023*\"feature\" + 0.018*\"network\" + 0.013*\"image\" + 0.012*\"large\" + 0.012*\"accuracy\" + 0.012*\"architecture\"\n",
      "Topic #8: 0.022*\"song\" + 0.018*\"student\" + 0.017*\"http\" + 0.016*\"school\" + 0.014*\"education\" + 0.012*\"used\" + 0.012*\"survey\" + 0.012*\"educational\" + 0.012*\"language\" + 0.012*\"audio\"\n",
      "Topic #9: 0.038*\"word\" + 0.019*\"dog\" + 0.016*\"service\" + 0.016*\"txt\" + 0.015*\"customer\" + 0.015*\"city\" + 0.015*\"file\" + 0.013*\"character\" + 0.013*\"http\" + 0.013*\"uk\"\n",
      "Topic #10: 0.058*\"de\" + 0.024*\"vote\" + 0.022*\"population\" + 0.021*\"district\" + 0.021*\"en\" + 0.020*\"la\" + 0.020*\"child\" + 0.019*\"news\" + 0.016*\"net\" + 0.015*\"http\"\n",
      "Topic #11: 0.044*\"user\" + 0.024*\"movie\" + 0.021*\"integer\" + 0.020*\"id\" + 0.018*\"post\" + 0.015*\"rating\" + 0.015*\"train\" + 0.013*\"twitter\" + 0.013*\"tag\" + 0.013*\"comment\"\n",
      "Topic #12: 0.030*\"image\" + 0.028*\"column\" + 0.015*\"activity\" + 0.015*\"label\" + 0.014*\"csv\" + 0.013*\"feature\" + 0.012*\"row\" + 0.010*\"datasets\" + 0.010*\"contains\" + 0.010*\"file\"\n",
      "Topic #13: 0.039*\"csv\" + 0.021*\"score\" + 0.021*\"weapon\" + 0.017*\"name\" + 0.017*\"time\" + 0.017*\"value\" + 0.014*\"id\" + 0.013*\"http\" + 0.012*\"file\" + 0.012*\"type\"\n",
      "Topic #14: 0.024*\"team\" + 0.023*\"player\" + 0.017*\"product\" + 0.017*\"question\" + 0.014*\"game\" + 0.013*\"different\" + 0.012*\"vehicle\" + 0.012*\"set\" + 0.012*\"goal\" + 0.011*\"percentage\"\n"
     ]
    }
   ],
   "source": [
    "for idx in range(15):\n",
    "    print(\"Topic #%s:\" % idx, ldamodel.print_topic(idx, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.970*\"university\" + 0.174*\"state\" + 0.076*\"college\" + 0.051*\"texas\" + '\n",
      "  '0.049*\"california\" + 0.039*\"institute\" + 0.031*\"new\" + 0.028*\"technology\" + '\n",
      "  '0.027*\"florida\" + 0.027*\"north\"'),\n",
      " (1,\n",
      "  '-0.389*\"player\" + -0.247*\"team\" + -0.221*\"shot\" + -0.200*\"number\" + '\n",
      "  '-0.177*\"time\" + -0.173*\"file\" + -0.159*\"year\" + -0.156*\"csv\" + '\n",
      "  '-0.146*\"goal\" + -0.126*\"ice\"'),\n",
      " (2,\n",
      "  '0.437*\"player\" + 0.307*\"shot\" + 0.259*\"team\" + -0.250*\"integer\" + '\n",
      "  '-0.224*\"strongly\" + 0.175*\"ice\" + 0.174*\"goal\" + -0.154*\"file\" + '\n",
      "  '0.151*\"attempt\" + -0.133*\"csv\"'),\n",
      " (3,\n",
      "  '0.595*\"integer\" + 0.535*\"strongly\" + 0.263*\"interested\" + 0.261*\"enjoy\" + '\n",
      "  '0.119*\"much\" + 0.116*\"player\" + -0.098*\"file\" + -0.093*\"year\" + '\n",
      "  '0.090*\"shot\" + -0.088*\"csv\"'),\n",
      " (4,\n",
      "  '0.402*\"year\" + -0.325*\"date\" + -0.265*\"element\" + -0.199*\"tag\" + '\n",
      "  '-0.192*\"registration\" + -0.186*\"zero\" + -0.180*\"end\" + -0.174*\"start\" + '\n",
      "  '-0.171*\"one\" + -0.165*\"application\"'),\n",
      " (5,\n",
      "  '0.535*\"csv\" + -0.436*\"year\" + -0.193*\"number\" + 0.175*\"file\" + '\n",
      "  '-0.166*\"date\" + -0.155*\"total\" + -0.122*\"element\" + -0.120*\"child\" + '\n",
      "  '0.104*\"numeric\" + 0.103*\"text\"'),\n",
      " (6,\n",
      "  '-0.680*\"numeric\" + 0.471*\"csv\" + -0.431*\"text\" + -0.103*\"word\" + '\n",
      "  '0.096*\"year\" + 0.063*\"file\" + -0.063*\"reading\" + -0.060*\"language\" + '\n",
      "  '-0.058*\"real\" + -0.053*\"use\"'),\n",
      " (7,\n",
      "  '-0.552*\"csv\" + -0.487*\"numeric\" + -0.282*\"year\" + -0.198*\"text\" + '\n",
      "  '0.139*\"image\" + 0.125*\"http\" + 0.113*\"model\" + 0.098*\"word\" + '\n",
      "  '-0.095*\"total\" + 0.088*\"trained\"'),\n",
      " (8,\n",
      "  '0.367*\"de\" + 0.241*\"value\" + -0.240*\"year\" + 0.214*\"name\" + -0.198*\"image\" '\n",
      "  '+ 0.186*\"el\" + 0.178*\"per\" + 0.169*\"en\" + 0.159*\"number\" + -0.155*\"total\"'),\n",
      " (9,\n",
      "  '0.410*\"de\" + 0.267*\"el\" + 0.252*\"en\" + 0.235*\"com\" + 0.217*\"http\" + '\n",
      "  '0.208*\"per\" + -0.198*\"name\" + -0.182*\"value\" + -0.179*\"station\" + '\n",
      "  '0.149*\"year\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsamodel = LsiModel(corpus, num_topics=10, id2word = id2word)\n",
    "pprint(lsamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.970*\"university\" + 0.174*\"state\" + 0.076*\"college\" + 0.051*\"texas\" + 0.049*\"california\" + 0.039*\"institute\" + 0.031*\"new\" + 0.028*\"technology\" + 0.027*\"florida\" + 0.027*\"north\"\n",
      "Topic #1: -0.389*\"player\" + -0.247*\"team\" + -0.221*\"shot\" + -0.200*\"number\" + -0.177*\"time\" + -0.173*\"file\" + -0.159*\"year\" + -0.156*\"csv\" + -0.146*\"goal\" + -0.126*\"ice\"\n",
      "Topic #2: 0.437*\"player\" + 0.307*\"shot\" + 0.259*\"team\" + -0.250*\"integer\" + -0.224*\"strongly\" + 0.175*\"ice\" + 0.174*\"goal\" + -0.154*\"file\" + 0.151*\"attempt\" + -0.133*\"csv\"\n",
      "Topic #3: 0.595*\"integer\" + 0.535*\"strongly\" + 0.263*\"interested\" + 0.261*\"enjoy\" + 0.119*\"much\" + 0.116*\"player\" + -0.098*\"file\" + -0.093*\"year\" + 0.090*\"shot\" + -0.088*\"csv\"\n",
      "Topic #4: 0.402*\"year\" + -0.325*\"date\" + -0.265*\"element\" + -0.199*\"tag\" + -0.192*\"registration\" + -0.186*\"zero\" + -0.180*\"end\" + -0.174*\"start\" + -0.171*\"one\" + -0.165*\"application\"\n",
      "Topic #5: 0.535*\"csv\" + -0.436*\"year\" + -0.193*\"number\" + 0.175*\"file\" + -0.166*\"date\" + -0.155*\"total\" + -0.122*\"element\" + -0.120*\"child\" + 0.104*\"numeric\" + 0.103*\"text\"\n",
      "Topic #6: -0.680*\"numeric\" + 0.471*\"csv\" + -0.431*\"text\" + -0.103*\"word\" + 0.096*\"year\" + 0.063*\"file\" + -0.063*\"reading\" + -0.060*\"language\" + -0.058*\"real\" + -0.053*\"use\"\n",
      "Topic #7: -0.552*\"csv\" + -0.487*\"numeric\" + -0.282*\"year\" + -0.198*\"text\" + 0.139*\"image\" + 0.125*\"http\" + 0.113*\"model\" + 0.098*\"word\" + -0.095*\"total\" + 0.088*\"trained\"\n",
      "Topic #8: 0.367*\"de\" + 0.241*\"value\" + -0.240*\"year\" + 0.214*\"name\" + -0.198*\"image\" + 0.186*\"el\" + 0.178*\"per\" + 0.169*\"en\" + 0.159*\"number\" + -0.155*\"total\"\n",
      "Topic #9: 0.410*\"de\" + 0.267*\"el\" + 0.252*\"en\" + 0.235*\"com\" + 0.217*\"http\" + 0.208*\"per\" + -0.198*\"name\" + -0.182*\"value\" + -0.179*\"station\" + 0.149*\"year\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, lsamodel.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the topics and documents with the trained Topic Model\n",
    "- Use pyLDAvis from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyLDAvis\n",
    "import pyLDAvis.gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable the notebook for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/_utils.py:38: DeprecationWarning: ast.Num is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  if isinstance(node, ast.Num):  # <number>\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/_utils.py:38: DeprecationWarning: ast.Num is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  if isinstance(node, ast.Num):  # <number>\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/_utils.py:38: DeprecationWarning: ast.Num is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  if isinstance(node, ast.Num):  # <number>\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/_utils.py:39: DeprecationWarning: Attribute n is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return node.n\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/_utils.py:38: DeprecationWarning: ast.Num is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  if isinstance(node, ast.Num):  # <number>\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/_utils.py:39: DeprecationWarning: Attribute n is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return node.n\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/vostok/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=16048) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el160481400251030524964605192992\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el160481400251030524964605192992_data = {\"mdsDat\": {\"x\": [0.06862858377769905, 0.013591824796177834, 0.04403401933635117, 0.031244145821373092, 0.08577274633516212, 0.056283814429602425, 0.03316086528431008, 0.08781536367944641, 0.022433798474853425, -0.042497237467501905, 0.10922145813327631, 0.04177549356493465, 0.009327005402388166, -0.10783617145738995, -0.45295571011068225], \"y\": [0.05866353068597004, 0.11505895170198344, -0.07201104023782202, 0.07195010916891766, -0.05582980174011898, 0.07233965483411832, 0.023022982667160886, -0.035102287363785276, 0.038958451794770345, -0.33962094533907045, -0.08605202032970936, 0.008051328078593547, 0.03815015725548281, 0.18932090343741098, -0.026899974613902183], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [29.156134485837942, 21.922252970160386, 6.764242813399951, 5.2825737989141635, 5.022336529990381, 4.69279460459509, 4.564492140007097, 4.564104793267801, 3.585301144202133, 3.206563078932456, 2.8018002318019266, 2.598891290532992, 2.4195138883227205, 2.1308792053582017, 1.2881190246767598]}, \"tinfo\": {\"Term\": [\"university\", \"state\", \"description\", \"word\", \"csv\", \"user\", \"model\", \"time\", \"image\", \"yet\", \"http\", \"de\", \"file\", \"city\", \"player\", \"feature\", \"trained\", \"column\", \"year\", \"id\", \"population\", \"language\", \"information\", \"news\", \"inspiration\", \"new\", \"movie\", \"www\", \"many\", \"pre\", \"temperature\", \"reading\", \"corpus\", \"translation\", \"speaker\", \"exchange\", \"headline\", \"australian\", \"nlp\", \"bike\", \"semantic\", \"airline\", \"airport\", \"conversion\", \"presidential\", \"disaster\", \"volunteer\", \"narrative\", \"precipitation\", \"remove\", \"climate\", \"disclaimer\", \"literature\", \"calendar\", \"spoken\", \"annotated\", \"mission\", \"classic\", \"voice\", \"phrase\", \"station\", \"text\", \"arabic\", \"sentence\", \"calculation\", \"article\", \"trip\", \"global\", \"candidate\", \"book\", \"mining\", \"debate\", \"scientist\", \"start\", \"version\", \"frequency\", \"google\", \"numeric\", \"language\", \"format\", \"historical\", \"english\", \"wikipedia\", \"speech\", \"volume\", \"system\", \"date\", \"collection\", \"open\", \"license\", \"use\", \"available\", \"end\", \"file\", \"analysis\", \"word\", \"contains\", \"http\", \"also\", \"source\", \"price\", \"please\", \"project\", \"time\", \"one\", \"inspiration\", \"location\", \"information\", \"using\", \"com\", \"csv\", \"used\", \"crime\", \"federal\", \"death\", \"investment\", \"income\", \"violent\", \"administration\", \"caput\", \"policy\", \"incident\", \"growth\", \"housing\", \"household\", \"fbi\", \"role\", \"hotel\", \"joining\", \"employment\", \"museum\", \"conflict\", \"murder\", \"court\", \"victim\", \"agreement\", \"adult\", \"band\", \"disclosure\", \"contribute\", \"dc\", \"ic\", \"review\", \"bureau\", \"act\", \"profit\", \"government\", \"law\", \"job\", \"justice\", \"annual\", \"member\", \"food\", \"department\", \"rate\", \"drug\", \"report\", \"united\", \"plan\", \"year\", \"survey\", \"state\", \"census\", \"city\", \"health\", \"number\", \"country\", \"non\", \"information\", \"record\", \"database\", \"population\", \"variable\", \"total\", \"name\", \"per\", \"may\", \"csv\", \"file\", \"type\", \"source\", \"contains\", \"facility\", \"sensor\", \"street\", \"gas\", \"lat\", \"chest\", \"directory\", \"lon\", \"denotes\", \"cnn\", \"ieee\", \"closed\", \"mnist\", \"dioxide\", \"plane\", \"postcode\", \"vol\", \"carbon\", \"tract\", \"sort\", \"density\", \"molecular\", \"handwritten\", \"array\", \"ray\", \"acid\", \"opened\", \"wang\", \"pixel\", \"scene\", \"label\", \"coordinate\", \"image\", \"left\", \"emission\", \"activity\", \"digit\", \"column\", \"recognition\", \"testing\", \"row\", \"matter\", \"feature\", \"latitude\", \"longitude\", \"right\", \"datasets\", \"training\", \"csv\", \"class\", \"paper\", \"contains\", \"code\", \"file\", \"used\", \"number\", \"http\", \"one\", \"based\", \"using\", \"weapon\", \"fire\", \"defense\", \"attack\", \"driving\", \"critical\", \"epa\", \"matrix\", \"award\", \"respondent\", \"unzip\", \"flavor\", \"pollution\", \"canonical\", \"sql\", \"damage\", \"fips\", \"leaderboard\", \"intermediate\", \"ambient\", \"pick\", \"athlete\", \"descriptive\", \"decimal\", \"venue\", \"competition\", \"excluded\", \"fuel\", \"km\", \"htm\", \"degree\", \"score\", \"monitor\", \"hit\", \"impact\", \"air\", \"staff\", \"csv\", \"battle\", \"value\", \"bank\", \"taken\", \"info\", \"id\", \"share\", \"type\", \"name\", \"time\", \"sample\", \"www\", \"http\", \"new\", \"file\", \"event\", \"mean\", \"hour\", \"statistic\", \"number\", \"per\", \"column\", \"cell\", \"software\", \"kera\", \"cancer\", \"numerical\", \"formula\", \"programming\", \"linear\", \"diagnosis\", \"protein\", \"simulation\", \"nationality\", \"breast\", \"join\", \"divide\", \"exists\", \"notice\", \"remaining\", \"rain\", \"analyse\", \"modify\", \"warranty\", \"shall\", \"artificial\", \"indeed\", \"instruction\", \"averaged\", \"copy\", \"distribute\", \"queen\", \"function\", \"instance\", \"command\", \"imdb\", \"input\", \"classification\", \"group\", \"passenger\", \"learning\", \"company\", \"music\", \"attribute\", \"output\", \"result\", \"algorithm\", \"name\", \"research\", \"file\", \"using\", \"within\", \"time\", \"code\", \"set\", \"model\", \"number\", \"datasets\", \"http\", \"use\", \"class\", \"strongly\", \"plate\", \"anonymous\", \"facebook\", \"bitcoin\", \"film\", \"movie\", \"collaborative\", \"recommender\", \"unix\", \"gz\", \"message\", \"acl\", \"integer\", \"fork\", \"nominal\", \"animal\", \"enjoy\", \"cryptocurrencies\", \"plant\", \"comment\", \"twitter\", \"improved\", \"continue\", \"presence\", \"stamp\", \"tar\", \"rated\", \"specie\", \"club\", \"store\", \"user\", \"reddit\", \"tag\", \"post\", \"train\", \"rating\", \"sentiment\", \"negative\", \"string\", \"api\", \"interested\", \"id\", \"recommendation\", \"positive\", \"line\", \"json\", \"tweet\", \"account\", \"github\", \"information\", \"file\", \"one\", \"http\", \"com\", \"used\", \"set\", \"inspiration\", \"csv\", \"shot\", \"motor\", \"opponent\", \"scored\", \"reduced\", \"manufacturer\", \"save\", \"stream\", \"merging\", \"bot\", \"bet\", \"outlier\", \"card\", \"retail\", \"stack\", \"ft\", \"heavy\", \"trait\", \"segmentation\", \"offensive\", \"vehicle\", \"logistic\", \"gained\", \"variance\", \"brand\", \"frame\", \"win\", \"baseball\", \"spot\", \"server\", \"percentage\", \"product\", \"white\", \"auto\", \"team\", \"ice\", \"player\", \"money\", \"labeled\", \"answer\", \"revenue\", \"goal\", \"allowed\", \"space\", \"question\", \"region\", \"game\", \"car\", \"attempt\", \"different\", \"sale\", \"set\", \"position\", \"class\", \"free\", \"average\", \"market\", \"company\", \"individual\", \"com\", \"used\", \"number\", \"contains\", \"available\", \"total\", \"one\", \"startup\", \"owe\", \"shooting\", \"tournament\", \"super\", \"sheet\", \"premier\", \"li\", \"gather\", \"fantasy\", \"robot\", \"nice\", \"fifa\", \"fan\", \"appreciate\", \"everyday\", \"fair\", \"luck\", \"selecting\", \"manner\", \"motion\", \"mistake\", \"match\", \"searching\", \"grouped\", \"round\", \"double\", \"dot\", \"exercise\", \"put\", \"inside\", \"easy\", \"describing\", \"along\", \"football\", \"attribution\", \"acquired\", \"thanks\", \"player\", \"others\", \"answered\", \"front\", \"game\", \"past\", \"started\", \"get\", \"without\", \"want\", \"citation\", \"largest\", \"world\", \"science\", \"research\", \"include\", \"see\", \"help\", \"community\", \"class\", \"time\", \"inspiration\", \"make\", \"year\", \"column\", \"team\", \"set\", \"every\", \"information\", \"pokemon\", \"pakistan\", \"violation\", \"room\", \"planning\", \"diabetes\", \"treatment\", \"symptom\", \"emergency\", \"judge\", \"predictor\", \"policing\", \"injured\", \"birth\", \"killed\", \"jurisdiction\", \"pain\", \"dependent\", \"died\", \"encounter\", \"racial\", \"raised\", \"delay\", \"prevention\", \"reliably\", \"walk\", \"forfeiture\", \"predictable\", \"severity\", \"sorted\", \"yes\", \"woman\", \"coded\", \"back\", \"cause\", \"wine\", \"lower\", \"risk\", \"ex\", \"wise\", \"thought\", \"patient\", \"stop\", \"problem\", \"nyc\", \"day\", \"york\", \"health\", \"performance\", \"city\", \"status\", \"type\", \"may\", \"set\", \"number\", \"low\", \"inspiration\", \"weather\", \"center\", \"variable\", \"year\", \"new\", \"large\", \"restaurant\", \"transferable\", \"convolutional\", \"layer\", \"bird\", \"imagenet\", \"vgg\", \"horizontal\", \"residual\", \"convolution\", \"beneficial\", \"thorough\", \"secured\", \"whichever\", \"pushing\", \"karen\", \"convnet\", \"generalise\", \"simonyan\", \"localisation\", \"zisserman\", \"inception\", \"coco\", \"achieve\", \"achieves\", \"architecture\", \"trained\", \"pre\", \"ilsvrc\", \"substantially\", \"learned\", \"bias\", \"depth\", \"model\", \"null\", \"deep\", \"else\", \"art\", \"edge\", \"compute\", \"network\", \"saving\", \"benefit\", \"accuracy\", \"feature\", \"arxiv\", \"large\", \"object\", \"weight\", \"representation\", \"scale\", \"image\", \"recognition\", \"time\", \"using\", \"use\", \"learn\", \"contains\", \"example\", \"yet\", \"gold\", \"walking\", \"pipeline\", \"embedded\", \"prove\", \"engineer\", \"sparse\", \"scrapped\", \"intersection\", \"symposium\", \"modelling\", \"purchase\", \"basically\", \"vast\", \"expanded\", \"participating\", \"recommend\", \"derive\", \"largely\", \"sport\", \"attended\", \"saved\", \"unfortunately\", \"thinking\", \"specially\", \"penalty\", \"went\", \"season\", \"tweet\", \"description\", \"sent\", \"conference\", \"someone\", \"direction\", \"url\", \"time\", \"classifier\", \"collecting\", \"transaction\", \"course\", \"whether\", \"medium\", \"ai\", \"many\", \"improving\", \"click\", \"kernel\", \"find\", \"stock\", \"kaggle\", \"learning\", \"like\", \"result\", \"machine\", \"information\", \"able\", \"one\", \"inspiration\", \"price\", \"market\", \"game\", \"using\", \"day\", \"educational\", \"artist\", \"assignment\", \"exam\", \"song\", \"coin\", \"teacher\", \"retrieval\", \"retrieve\", \"talk\", \"apart\", \"matching\", \"student\", \"youtube\", \"audio\", \"math\", \"smartphones\", \"medicine\", \"school\", \"transcript\", \"lang\", \"varied\", \"sound\", \"demo\", \"filled\", \"convenience\", \"genre\", \"education\", \"successful\", \"moreover\", \"alternative\", \"video\", \"search\", \"communication\", \"music\", \"html\", \"metadata\", \"gov\", \"survey\", \"table\", \"language\", \"top\", \"http\", \"feature\", \"could\", \"census\", \"used\", \"different\", \"age\", \"com\", \"contains\", \"user\", \"www\", \"file\", \"set\", \"information\", \"dog\", \"accident\", \"route\", \"fraud\", \"vocabulary\", \"csvs\", \"recall\", \"transport\", \"park\", \"precise\", \"abbreviation\", \"dimensional\", \"leader\", \"gram\", \"loan\", \"island\", \"vector\", \"txt\", \"email\", \"borough\", \"neighborhood\", \"embeddings\", \"customer\", \"marketing\", \"apartment\", \"determines\", \"episode\", \"horse\", \"road\", \"cm\", \"dangerous\", \"approximate\", \"uk\", \"word\", \"character\", \"york\", \"service\", \"property\", \"race\", \"json\", \"sub\", \"business\", \"city\", \"run\", \"area\", \"file\", \"http\", \"new\", \"inspiration\", \"gov\", \"could\", \"information\", \"one\", \"use\", \"de\", \"da\", \"un\", \"border\", \"se\", \"el\", \"la\", \"mother\", \"en\", \"vote\", \"ease\", \"em\", \"van\", \"float\", \"hi\", \"delivery\", \"gdp\", \"child\", \"carry\", \"electricity\", \"net\", \"aged\", \"brazilian\", \"town\", \"voting\", \"district\", \"crypto\", \"indian\", \"proposed\", \"congress\", \"old\", \"weighted\", \"spanish\", \"population\", \"sale\", \"center\", \"news\", \"mean\", \"map\", \"election\", \"party\", \"web\", \"car\", \"calculated\", \"www\", \"per\", \"state\", \"http\", \"com\", \"age\", \"index\", \"year\", \"le\", \"includes\", \"university\", \"texas\", \"florida\", \"san\", \"carolina\", \"southern\", \"massachusetts\", \"georgia\", \"virginia\", \"northern\", \"francisco\", \"pennsylvania\", \"hill\", \"college\", \"carnegie\", \"christian\", \"instrument\", \"boston\", \"solar\", \"st\", \"sun\", \"rock\", \"observe\", \"institute\", \"western\", \"irvine\", \"harvard\", \"eastern\", \"california\", \"mexico\", \"north\", \"michigan\", \"state\", \"technology\", \"john\", \"washington\", \"west\", \"south\", \"new\", \"school\", \"international\", \"community\", \"set\", \"based\"], \"Freq\": [1136.0, 876.0, 635.0, 615.0, 1383.0, 602.0, 607.0, 1450.0, 605.0, 331.0, 1347.0, 279.0, 1607.0, 572.0, 420.0, 580.0, 327.0, 903.0, 1210.0, 574.0, 318.0, 516.0, 1335.0, 325.0, 1175.0, 632.0, 263.0, 588.0, 558.0, 224.0, 120.73059350392059, 119.77456069091492, 315.78854884801757, 77.84361254685794, 77.13431686793426, 76.41597734610993, 70.73450986057456, 69.9744142356873, 64.57153992258311, 60.78561756896919, 57.46255190886399, 56.986477291115825, 55.09375310427499, 46.79932216792667, 45.61376839515203, 45.61151699047388, 44.90130010861739, 43.711243526486456, 40.16595008169871, 41.76576613767094, 39.929522011142964, 39.205536776738626, 38.97457858749769, 38.25867776887311, 38.03379340311872, 37.08026809685136, 36.1347048889619, 35.41742035472868, 34.24150672218882, 34.21317113156217, 216.71282940515144, 778.1800081187415, 170.46830457033184, 128.94437522583056, 64.51196370303545, 239.00233201612633, 88.60374442700692, 122.43441375325047, 126.08214037726563, 157.88445845250882, 91.96344414897179, 54.96699022933213, 61.50482860228184, 168.74254363704176, 273.88832530634244, 123.35903392848873, 144.58117195624462, 206.45290689510682, 401.77233097537334, 265.49634241958506, 112.28302500285147, 180.78112935668466, 141.49975196836863, 189.5975994969521, 124.2669042645086, 321.052508816787, 446.71483293632645, 230.79359588704347, 302.34427625642445, 184.5095166623946, 467.29846648149015, 411.51818402471434, 148.53925605298872, 639.3677988145565, 305.15008455853985, 327.68581209947644, 513.5335270073401, 510.5651894574918, 335.84086617881417, 315.58987480803233, 259.3114853114967, 236.57790937303756, 237.66451601253803, 411.1864898101969, 335.0120434637519, 341.72607623167465, 226.93692820612372, 322.5147827767767, 261.21865824143333, 254.10765963542096, 263.24882131894884, 227.84296378111557, 391.26034346582423, 195.2654669641292, 169.8312525307684, 139.23405831933795, 121.33355411145715, 119.03304232271934, 120.24711925238954, 96.94864188040036, 92.5785362936623, 106.41919378190606, 83.20671357366926, 77.79155362260681, 71.75796249375176, 61.55152870614837, 64.60949496006032, 60.928424331769264, 60.71469887607505, 58.638126897686085, 58.422470083993026, 57.17568077821729, 53.64004255685753, 51.76162446029705, 50.08353877140304, 49.44600669292537, 47.589428461592966, 46.74950615948577, 46.34717152421632, 45.28316283548358, 44.67539436330963, 58.14076772444642, 325.85253662197124, 157.43333671193375, 96.5696358654831, 59.04825306797768, 230.50907166600828, 129.0760694282057, 159.38614657998565, 79.92593413490779, 93.31248682176114, 123.87776189340704, 146.91767871330012, 258.1154430544333, 278.66056266784705, 98.55502590358705, 207.00495065185578, 211.25101821997126, 85.73326322534395, 689.7317188374507, 223.1043600602015, 497.5686336131177, 179.06297132259485, 353.0262378339395, 217.00322313419022, 560.041994893646, 279.3824996893459, 242.24168063895323, 511.2921277975205, 243.29746668596766, 301.44198791271543, 202.37263447632736, 231.42076461089417, 276.6430982915479, 340.1620140201681, 228.35567647436147, 222.85572071020664, 289.40192514023914, 264.9636936329998, 198.1305905707095, 198.90531804417694, 206.31085737484037, 117.40756460999408, 106.48739956974366, 83.66545579354296, 77.95116211479872, 76.92277484291037, 71.48094274364743, 66.58259363408177, 61.468893105556226, 53.66108637790981, 48.806073593256585, 48.657477212237815, 48.209520720853654, 48.07534731577775, 38.65060296074294, 36.727888311164094, 35.415025042795314, 35.659938951058656, 33.93761002061116, 33.34706967753021, 32.75196052160754, 30.829213947094253, 29.08082362848833, 28.34551789652296, 27.151277042936556, 27.15443406423246, 26.718687086669085, 26.279149909943158, 25.511282518443245, 63.81151641799483, 22.43886229883988, 229.8037516535993, 122.52139993268733, 458.8228309849241, 94.20051389670229, 41.613854093376936, 230.59794470077063, 73.44656805234598, 426.4855398897111, 134.73414881349905, 48.18206882443905, 179.118959517655, 42.280138117055, 191.93025125845054, 75.78050778261338, 74.04325217170167, 97.60386798684503, 150.54280880021398, 82.42127015361238, 215.5146192248269, 111.09902020134247, 100.97780857071177, 150.29413484411506, 112.27299226876617, 145.23562188561385, 119.06397209801116, 111.27547995570879, 101.22517715171305, 93.69309189407709, 85.2906194123754, 76.13686099540719, 250.9022452980183, 121.55402481024397, 68.3225921312072, 122.23046763207094, 54.75337362379486, 54.176132295307426, 48.94494750972722, 52.61532519719682, 35.75714641673698, 34.18781258876043, 24.909766458888026, 20.244067583281247, 18.7069416905829, 17.15889698509489, 15.591423358257064, 117.220296349468, 21.724254422807267, 13.283550895162492, 12.495878295045388, 12.494962738118588, 12.695779969462452, 29.215731798573543, 8.60439153209029, 38.96304725457644, 18.566398854438194, 108.19254492292468, 12.725876998906392, 17.70851370337338, 16.036607027201267, 13.471636206277688, 73.0256737086255, 253.54527669643613, 43.63724377276403, 48.84050476463123, 70.78922161136543, 89.48765319338817, 37.51845909670805, 459.58251776796, 33.332726742610994, 199.58598041591952, 61.433631549586835, 103.69237070605229, 61.39023332019121, 165.4780304312218, 91.03329057975549, 145.0810842326883, 201.1438178407305, 199.69962582398324, 87.73023266918761, 116.03996202156758, 152.95351085609718, 107.60171290674322, 145.56125159497623, 78.26365660827035, 72.21158431318625, 68.88513468494848, 72.11617801800016, 87.23302292803835, 75.92649047884461, 76.2087091701448, 178.4107466508705, 92.43879344813087, 68.93495214849386, 58.35775923398418, 52.38815470210012, 50.94041007378693, 46.15065064848059, 37.19892763929218, 33.67603233586213, 32.68535087140876, 39.96593479472878, 61.821858759644094, 28.886114885709613, 25.62296502195194, 24.54062124546963, 24.180680152074405, 23.727622918065357, 21.541596743371052, 20.834943869953623, 20.47470779684552, 17.946056101276586, 17.945168528390518, 17.94468655260966, 16.40307121504212, 16.13664000750246, 21.85574618696336, 12.063392889141705, 64.24228318868288, 11.706225131797007, 11.222385974625064, 60.46069067706016, 173.39702914605755, 16.35708335105519, 60.44767996472436, 62.69943394044139, 97.78802930636132, 114.71908523623055, 35.267458421820535, 116.80124167952823, 106.50167734978405, 48.21875341545765, 91.75082701277468, 43.19627079917638, 79.65131754503766, 49.362310749856704, 117.27596938744395, 78.69449817137404, 114.30902129537326, 86.81955633771072, 58.745425786642365, 88.55186684153688, 69.41405187715401, 74.90418268824197, 68.70617957867353, 79.21869292364018, 64.58446492311448, 75.96592529227678, 70.0109591286442, 59.743352690293264, 122.13557385437487, 46.18746759959174, 43.80126781606733, 40.74069106759276, 58.93051055801351, 38.1669829238803, 255.83444380008814, 20.881898531120445, 19.15657626232948, 16.55516169982843, 15.66463849260623, 98.36243540267365, 13.075102855671865, 220.27238103835043, 10.45967942973839, 58.81490082831151, 54.113248174913174, 69.3586420519114, 15.371517944691453, 26.635824161933908, 133.74197565531054, 141.5812691868956, 26.01857284421379, 11.081089724920222, 11.643777227089386, 9.413424664975697, 13.681311965184463, 24.42278057495452, 79.61253453856418, 25.468372118163405, 88.0818171791959, 460.6296405654854, 61.96660840186579, 140.0908294768884, 191.74933595096138, 158.19056157799494, 159.0413653199911, 80.6178504455965, 74.69315326271162, 83.15554036793048, 93.333931417586, 79.77746629565533, 215.04173598302305, 46.81715155289564, 73.02578228430951, 101.0609374795716, 79.03588370023066, 86.7359523851181, 67.32032473699216, 84.21625188600609, 112.67306833348412, 113.70844208117761, 98.37040882691977, 98.11335941045178, 88.06600814297457, 86.21501215730427, 85.84928792148168, 82.30812779217236, 80.90051325698846, 81.26547614377948, 49.34990655548083, 37.009672430976345, 37.40234365132955, 30.891223516431367, 23.266143813853212, 22.195336981429158, 20.36227925683507, 16.081616723889844, 15.311161593862604, 14.115183584117844, 12.274555939108465, 57.38814473496064, 11.669371620159133, 16.255028341080763, 10.448868265094195, 16.652988332709096, 9.190506930175275, 42.929536900489325, 26.57789363522759, 122.97134528294647, 7.355864010389084, 9.810089058230684, 6.156847261049948, 47.877835296603685, 68.35778642304054, 64.32155512427332, 44.377145048965126, 15.713062731408336, 40.16900654248714, 111.30684593883646, 177.05748235498024, 45.15462674069326, 30.33571223731068, 242.61463519578533, 41.03209954601709, 232.3765697658448, 57.556164653895294, 51.3972726167142, 93.63010875011577, 56.34905616828299, 120.07115976709143, 36.14962415116757, 77.19211453497843, 169.6889062640076, 85.7710066035338, 140.20644433844896, 95.11033278335954, 46.60464290179761, 131.59760291203236, 65.88989422797643, 122.72873442014424, 68.95541398665357, 83.17785945337363, 71.69447329765966, 75.30683213888369, 61.436751207971994, 71.52317886453429, 69.54837871865584, 80.40905663846861, 79.6957056369433, 83.15590488893125, 79.54356040961156, 74.650627154748, 68.9844346080338, 68.3747334701679, 72.56597073872135, 58.02438838771933, 51.833602145174844, 33.4356826653014, 27.573093137591794, 29.19485493411515, 22.443322214769744, 21.974673208051378, 21.17905532511537, 21.08550503234147, 19.406177230940372, 18.14224901412097, 13.867968065872319, 12.29696291581342, 12.292764808728393, 11.453050047266409, 11.45153260148995, 11.040343091231268, 10.960150629495875, 18.507339526938836, 8.816366543204348, 19.35526022348692, 151.10006050314888, 12.590601837519909, 17.24392231118138, 40.875896422399244, 42.128679550483504, 21.738593110879844, 36.43831054461154, 38.27574590409493, 63.60587467551141, 71.89999394487347, 48.612079122439184, 82.38824885598574, 39.17244625939241, 64.87167815852301, 66.39099561100224, 103.49870557952528, 186.20894015383183, 113.97634794399272, 58.65446825450834, 70.17435652264302, 150.667459210688, 79.29523960852005, 64.36294129800353, 107.98547201853485, 78.25725882847054, 80.98195478592909, 65.13539300164878, 58.148694482113875, 112.76373165348026, 81.75261619077183, 112.96135383466421, 90.63424068106858, 118.94733755323394, 97.0162442390276, 85.65077642295212, 90.9251115806292, 138.75059851102043, 126.98818219788807, 84.37843882800088, 105.74812507386955, 82.99157955243138, 74.52353323309505, 74.77600844630561, 70.04861435969032, 74.19821104882409, 73.68574726986314, 50.47470251059898, 42.8774120334366, 36.067681429693025, 32.84681198423002, 31.809086033372292, 31.08451703667032, 26.87584355058406, 23.667400058146807, 22.04320635156867, 20.997736036886902, 16.610462742989814, 17.104001893134402, 68.69675251213582, 23.196744290401426, 15.005178708458349, 74.94354927664004, 16.04323274659146, 9.652354903974734, 9.135052458279771, 8.600429072616631, 8.600367027458537, 12.335841834395675, 20.432314750854832, 7.538082474933033, 7.523639114191046, 7.518273618133061, 7.004378955742557, 19.21511659966033, 16.631875843157804, 72.13661128317317, 111.33037219801052, 16.58494902191864, 132.43805977566694, 61.643308527395206, 43.44218842585574, 97.50424420937796, 84.8822736078021, 27.431739964770397, 30.756993808181115, 32.83519261825951, 49.596947778873144, 44.81052858216959, 68.93942052622677, 35.74186162281697, 142.24623357357197, 61.3348463235353, 86.34496000754642, 51.44883781487829, 87.89535193864874, 55.35782190642801, 80.77578661694196, 78.82607204834041, 91.50258222403804, 101.77745594082081, 50.30276221642368, 94.58242114499487, 54.479521929373455, 51.49456321623798, 61.758509464570814, 76.99263386632144, 66.27636713453845, 50.65187339961732, 180.32155359939708, 68.44498115504936, 54.15255093609792, 53.93396037186101, 50.08206888140213, 45.68815875213884, 40.079679850432726, 36.44555833098839, 34.80285177095261, 23.149254296069845, 36.447962938176865, 20.94295993890338, 20.50621433019307, 35.1320273611636, 19.62189565425036, 19.610982437010353, 18.751200843651027, 18.751200843651027, 18.751200843651027, 18.751200843651027, 18.751197480564052, 16.764072138594518, 15.45347378061358, 36.867750180806205, 13.797447714605825, 85.15322100519704, 296.75672317917935, 200.92296346955797, 9.297425465047711, 8.397805584131895, 74.37505861200755, 41.986301297892965, 80.79626111254885, 391.3664941097357, 71.08257628311055, 81.19291031677719, 39.98293729045552, 59.131725639601434, 40.75178269350322, 35.75011902738525, 127.77845838239442, 35.85882745082623, 41.27228430182949, 86.89909362133591, 169.45375148592555, 49.319919384525406, 88.4716260085119, 62.14614121107522, 61.78824149499461, 52.161371392777745, 52.90610346220116, 92.00367447512066, 53.84112235586397, 74.91212342369498, 65.38684545287602, 66.65399609020251, 45.51489251108948, 54.65105122942137, 49.101224564592215, 330.7709822699678, 28.76848960179067, 16.237097475633753, 15.919961534550712, 14.936731977957244, 14.919005077375495, 15.251086566135085, 11.247203435393377, 11.244192139683284, 10.589503099582348, 10.257648615061456, 9.93992422609752, 9.93491764393985, 9.615843718450066, 9.286614842825948, 9.285999947849284, 9.283619709229935, 8.633649271799394, 9.243102023867932, 10.225153950058063, 81.53305217221892, 8.884918720865924, 14.770553120966202, 12.524596691826986, 3.961296694777207, 3.6209637121372116, 38.669156787424576, 10.65642894471278, 88.82633251006422, 165.8071518756845, 363.37747734854435, 23.81632822079935, 48.81344874216424, 52.43631591055538, 39.12493428166914, 62.25651633802435, 209.06504433900344, 24.099181379348288, 16.912156656610392, 41.2121831583725, 34.70092287249845, 55.425031677994504, 49.141516149499104, 17.70771174448377, 86.16484372002769, 18.15348048226229, 24.44071917412894, 40.550600241974045, 66.04644393326555, 36.32972015971822, 45.57151966411945, 45.419316627960036, 55.77571429077937, 43.24516553312091, 40.62703234875088, 68.16351044116708, 29.505026526169004, 53.73008432712024, 56.63837741488737, 43.41195267331468, 33.06523464754736, 36.718125146770454, 38.142412919529086, 34.72344110526396, 71.28878130038365, 29.04450967323747, 25.154143927309118, 20.982162969195482, 130.19203744527886, 13.386332900815072, 12.733994900195315, 14.159807432892364, 11.990123403566749, 46.100636335904284, 10.474917090603928, 11.052709847913572, 104.30680852650116, 49.200100450897885, 70.08650091902045, 14.040617088747604, 10.777304581450036, 15.407869553898921, 94.06348500036489, 39.24733396815076, 14.651169302946021, 10.01111530447109, 32.6197184591131, 22.02756402605854, 9.33097735370709, 9.843338421320434, 53.0706199925905, 84.688150112716, 8.286366083642179, 2.6080724320215505, 17.278872434393197, 48.87551783856564, 69.59638880650589, 29.019907030821358, 42.12352896693051, 47.96218358084111, 47.56906464667077, 57.09991461967406, 71.62775588612128, 58.55719460255644, 70.82342992393244, 39.67700251811522, 99.84095737278734, 66.98922830906812, 52.72680992184886, 44.99750247739105, 71.73173244247016, 59.30059999550086, 46.90285256562801, 59.50311795763607, 61.419828982455165, 52.1213276073728, 51.235381711159135, 59.20715574696228, 48.350630575159784, 45.55285002541912, 105.98893909652801, 67.93829007128576, 41.4243907526597, 34.227497496491914, 17.076278264920347, 12.968702776899775, 21.186319049876506, 20.253513114881006, 50.94463668358888, 27.615870554307936, 43.08294051863338, 13.566115404606869, 13.055496962270611, 22.256249038253028, 11.823857198062829, 15.176855158316156, 64.77803569939454, 84.87690250115163, 39.752414444137194, 32.13885936085544, 41.038799533553224, 47.36875075792991, 82.12635688009698, 30.17899524301941, 13.113824136809356, 11.266740692526193, 10.541367988338362, 32.19224832866578, 44.498434591568326, 6.077915531820909, 10.56427063493422, 18.89651750577266, 68.68512659205146, 209.50443042498952, 72.17657070522615, 64.38511076114877, 87.7932561528507, 65.09308110809522, 41.29096274146774, 53.73520563276297, 26.675446686836242, 48.41442745195559, 80.79631966727806, 42.403246861088064, 47.488394422282866, 79.0725911552514, 68.97633868235893, 54.3079868608764, 53.45696585118911, 40.11996319158952, 41.92997836517115, 45.15345497880517, 44.00867410925475, 42.751969012590735, 277.99860117549696, 39.219566174204324, 33.320705832588054, 30.657180204362753, 32.17385376905068, 62.933318417339784, 96.31394849217656, 30.107568097305258, 100.30194000202124, 115.0206015308068, 21.855709626783, 8.8150637682864, 18.32869875421996, 56.10200396867756, 8.559991838468397, 11.897762409928571, 17.188098809717783, 93.86732837274205, 10.054004125591653, 14.599964015263309, 78.15849210122794, 17.811844622053773, 22.29565990119736, 23.131586654088075, 11.78886155906613, 101.40396677458847, 11.19625565757087, 36.191821926837854, 11.151499648348741, 14.92550665729504, 25.875072359111005, 27.63557521382906, 30.910887730723744, 107.48736910908893, 57.1133923356714, 61.88813396668185, 91.97455101678456, 70.85557902867593, 50.81489475740911, 40.07161546032826, 34.457880800353344, 61.99915014575406, 49.87509455017773, 35.502678614159414, 62.92719480477269, 60.34003107223807, 70.7382068079465, 71.86861674825171, 52.22541750965204, 42.00138926972942, 40.120166321425906, 43.26460561630728, 36.66322834974925, 36.5805349962474, 1128.279774308466, 57.94861727334386, 29.900017017662297, 25.40916870205105, 24.85240132308037, 23.7657351886163, 17.148257210924598, 16.849128966255535, 15.31561119786751, 15.164679191317834, 13.58942237816256, 11.837102579208928, 7.423995345409113, 140.2645902065638, 6.974066140875679, 9.40138782636992, 21.366223095839697, 13.400705694349561, 30.34962873795814, 20.146481078946, 12.977004740831221, 6.688483665177608, 11.44807225543203, 44.44333497312608, 9.511287359455576, 4.574663141087098, 5.572180377134633, 8.524634163818975, 64.31646482550171, 8.662656045371211, 32.9959503750427, 19.820092421740487, 208.47957233010914, 30.180138088975465, 14.449654701388091, 20.763705562734106, 13.479770077483197, 19.09191192540377, 31.11871876705411, 16.328410233717108, 16.798250428436, 14.539853999782178, 14.369404143102955, 13.711352537357694], \"Total\": [1136.0, 876.0, 635.0, 615.0, 1383.0, 602.0, 607.0, 1450.0, 605.0, 331.0, 1347.0, 279.0, 1607.0, 572.0, 420.0, 580.0, 327.0, 903.0, 1210.0, 574.0, 318.0, 516.0, 1335.0, 325.0, 1175.0, 632.0, 263.0, 588.0, 558.0, 224.0, 121.76826315912594, 120.81340755981667, 319.4765112089022, 78.88127471451193, 78.17162754104798, 77.45334793242266, 71.77181626443827, 71.01203410165297, 65.60920423315672, 61.82295254471638, 58.49986127588801, 58.026364915566546, 56.13201684639346, 47.83665249828423, 46.65114588224635, 46.64884716996757, 45.938617436437276, 44.755995099704556, 41.203273782411834, 42.84930971877055, 40.9670749852448, 40.24286463788413, 40.0149765913154, 39.29598600067315, 39.0711053915239, 38.11893922841477, 37.17211299893453, 36.45558453223095, 35.27885652086988, 35.26316761003989, 224.37215942048306, 818.3805425126699, 177.22073067489805, 134.2148361462079, 66.98111866236651, 258.4808616023274, 92.98251574820017, 130.05554883853355, 134.10302083467136, 170.3726861748602, 97.32763630932746, 57.14096673586089, 64.39639216330183, 189.57803237021832, 322.9803770379374, 138.12561607449572, 165.2668237430425, 246.61911903645074, 516.859145069919, 343.414350427997, 127.65616342453016, 225.55684900960682, 168.970198421425, 243.37232730069218, 148.81384496112827, 501.35712700782625, 771.7148360346134, 328.61322424346395, 474.0067888884173, 256.14349822789063, 936.6682450102962, 795.7368031770865, 191.03822793852913, 1607.7183582490516, 553.0055635548633, 615.9479585470965, 1335.4867949044824, 1347.7430208466933, 682.2897258951291, 646.9304525506775, 475.8954609493308, 412.0397946409664, 439.7277685304356, 1450.0082292757756, 955.0058763332265, 1175.9927811812618, 444.9516556640223, 1335.7528835087544, 747.1986695585127, 825.443164984746, 1383.4520391974575, 887.0117608803897, 392.3149351871055, 196.32466831487832, 171.03924879243226, 140.28767827245153, 122.38736811745203, 120.08675273606177, 121.33970639084323, 98.0087828516951, 93.63849408731615, 107.68688759967615, 84.26034424337399, 78.8486967932885, 72.81266685099332, 62.605190431178464, 65.72455442159338, 61.982043683002246, 61.768316058739394, 59.69227003601533, 59.478291251569644, 58.22948326018617, 54.693684731640865, 52.81656654062743, 51.14288137770072, 50.51448510560291, 48.64306405100542, 47.80462397504317, 47.40087230295505, 46.337212140324496, 45.72916854378309, 59.527622742642485, 342.45999942562685, 165.40402204261872, 100.72078211334309, 60.79804527098374, 250.6393501340001, 137.03815137755575, 174.0104296149876, 83.85091595907392, 100.08743682351988, 138.03214054079854, 167.6107134747867, 323.9326879920117, 360.0947708986041, 108.7226587008756, 262.4666583878629, 275.09014535235485, 93.14683328079205, 1210.7447058162863, 307.30844436705615, 876.5083012716126, 236.37343647440085, 572.5265815509863, 313.302800209118, 1358.0369121814106, 500.2459034650852, 410.6324132278934, 1335.7528835087544, 424.29641784163925, 600.0076095480263, 318.8295993425147, 432.1411196341815, 646.9000243243098, 1000.6618547104168, 526.7547514126264, 563.8309654184227, 1383.4520391974575, 1607.7183582490516, 572.8225911764837, 646.9304525506775, 1335.4867949044824, 118.49596715894587, 107.59482432649597, 84.79727294885947, 79.04060694071597, 78.0115945304001, 72.56934919361684, 67.72850211204515, 62.557265373654886, 54.75068042378132, 49.895460274500195, 49.74646522538686, 49.297938447451344, 49.16374640370353, 39.739065007006005, 37.816282575198514, 36.50338735399862, 36.76473783466107, 35.02715680784642, 34.43577845083735, 33.840709813362636, 31.91995405562253, 30.169192463991646, 29.43389104600254, 28.24724061346861, 28.25910534763921, 27.807059243983904, 27.36750854958255, 26.609529365818386, 66.66539743159343, 23.541439580050227, 250.98067031399856, 138.54698505257576, 605.0916720645788, 112.24041546495494, 45.62318299948833, 327.1473596268634, 91.54192375986693, 903.0627404181968, 222.761877414982, 58.68868990376155, 411.8314866149752, 49.064372470636535, 580.4632280415974, 133.96221289953206, 128.97611790443523, 212.24728495162918, 508.98717809827224, 171.89058864401943, 1383.4520391974575, 364.3926294759481, 288.83569525191524, 1335.4867949044824, 593.7415841345601, 1607.7183582490516, 887.0117608803897, 1358.0369121814106, 1347.7430208466933, 955.0058763332265, 514.0515758795389, 747.1986695585127, 251.96412626429736, 122.6748797667301, 69.38528275162264, 124.35209943540117, 55.81524664838002, 55.23809552093268, 50.00977008213741, 53.78953669709176, 36.83010304096274, 35.25455059952041, 25.971656942193313, 21.317462119829052, 19.774374789564394, 18.220833019924964, 16.653496033021213, 126.12238464473717, 23.442302689716602, 14.345717925757144, 13.557723748001928, 13.556900523893045, 14.025560216272368, 32.558505169213795, 9.666352531315084, 46.04454502115506, 22.64091318171424, 133.79215170459216, 15.935385736638281, 22.25029887479871, 20.41552242082012, 17.194006497565493, 93.90392984011652, 350.2815015554534, 58.58993197934792, 66.33264648364776, 100.64559733574761, 139.79481486574355, 55.42224926147272, 1383.4520391974575, 49.53368985110361, 607.6850348392297, 120.52266955468144, 258.7198153139293, 122.38431466140969, 574.9846541794057, 253.77781059546868, 572.8225911764837, 1000.6618547104168, 1450.0082292757756, 291.4106542544629, 588.2714638193487, 1347.7430208466933, 632.1819795258772, 1607.7183582490516, 301.22442259858764, 259.19502061175297, 229.6611546796937, 305.78722996373205, 1358.0369121814106, 526.7547514126264, 903.0627404181968, 179.5317610396332, 93.55957616843797, 70.05566936312356, 59.47856907143053, 53.5088799992152, 52.06262770061635, 47.27146920542372, 38.32355314854606, 34.797732602612236, 33.806146892757646, 41.40687101869655, 64.20347384690945, 30.00704564643789, 26.743783458798948, 25.661397607540238, 25.30266013330241, 24.848391080466264, 22.68642557159656, 21.955885540457015, 21.597155147409392, 19.066750383679782, 19.065860959610983, 19.065442931394852, 17.52387794333133, 17.25733377512409, 23.74768346584893, 13.184921322022708, 70.29779061067529, 12.826925235719981, 12.343147326600146, 70.16720848888967, 232.53157207212251, 18.405793687062314, 85.98901978001378, 92.22297466623735, 204.82671460748435, 324.3578750932304, 53.277166544470695, 341.720837940058, 340.7224298964451, 91.4105821987618, 297.0355819243362, 83.53816624578596, 331.8076236233946, 125.13638266349906, 1000.6618547104168, 509.49964541123217, 1607.7183582490516, 747.1986695585127, 258.27471081528114, 1450.0082292757756, 593.7415841345601, 883.5780594979415, 607.9934531351568, 1358.0369121814106, 508.98717809827224, 1347.7430208466933, 936.6682450102962, 364.3926294759481, 123.18818498723104, 47.23650453082868, 44.851169457929664, 41.83068943446784, 60.56139606739267, 39.236922328359086, 263.6705544809133, 21.930951491817364, 20.20560961486466, 17.607054883523155, 16.730244750237322, 105.54030150120116, 14.134354265633403, 239.82511097107886, 11.508757682518759, 64.7386063633316, 60.30113988762684, 79.51572111342195, 18.036872008124593, 31.343941386312977, 157.71311961294504, 168.67680627146612, 31.142267117704538, 13.349981004730974, 14.060112792011303, 11.436415737743685, 16.685050647847678, 30.102265352609603, 99.75959654780587, 32.35249225522462, 111.92232664277209, 602.0892417390933, 80.99519684027112, 192.96748696626125, 276.9119966599733, 254.3161980630367, 266.2018655546901, 128.07668299761306, 116.98372773332015, 141.92073359467582, 170.16177239404013, 139.24708807278142, 574.9846541794057, 69.36752726087278, 136.98205469008224, 244.67581409251753, 178.22671499403387, 261.2179638583866, 149.64581664405486, 311.38042194312726, 1335.7528835087544, 1607.7183582490516, 955.0058763332265, 1347.7430208466933, 825.443164984746, 887.0117608803897, 883.5780594979415, 1175.9927811812618, 1383.4520391974575, 82.35088032225093, 50.4480127538229, 38.09507487544074, 38.53531843066623, 31.977172254836884, 24.35361665444823, 23.28111849435715, 21.448114310439667, 17.166975758051453, 16.41218402934103, 15.201086822315956, 13.359965594419934, 62.71634841498577, 12.75492272172971, 17.788733049982792, 11.534225692637015, 18.622615154841796, 10.291923087263207, 48.64515806748992, 30.235424729973996, 140.8411778378525, 8.467883255990985, 11.500962243303512, 7.258325627918643, 57.65632837831735, 82.44597282342743, 78.2664482752785, 55.27259665728345, 19.588654364854584, 51.395571649504916, 155.80512274068633, 256.6285722259762, 61.45277495780284, 39.863824073121854, 414.2525790579358, 56.343015065916354, 420.1408193989178, 85.50264068556316, 74.6447748141546, 155.46026830073552, 85.85309272346281, 225.12511850391755, 50.2112377461967, 143.56866106076978, 461.9915438198429, 185.57453186861355, 392.92212661053793, 235.08494449090267, 78.41760859817072, 617.7495642045072, 162.39166175565433, 883.5780594979415, 213.1364129779463, 364.3926294759481, 239.84948446145734, 290.4000438843018, 175.93509332906845, 340.7224298964451, 315.5741956445671, 825.443164984746, 887.0117608803897, 1358.0369121814106, 1335.4867949044824, 795.7368031770865, 646.9000243243098, 955.0058763332265, 73.67857258924198, 59.13714178840302, 52.96673733624186, 34.54827582020332, 28.685689361628377, 30.413899507450253, 23.555982184462373, 23.10128319144683, 22.291743494003963, 22.198104779756143, 20.5188164694212, 19.257065215912117, 14.980638417119788, 13.409578284481052, 13.4053594049143, 12.56566370990898, 12.564579396532313, 12.152952460336905, 12.07275621703505, 20.77829243823448, 9.929134642302932, 21.859381239243845, 170.92532251089844, 14.752470785125302, 20.35505484347089, 48.35912137638783, 50.309867943228724, 26.189857186439898, 44.06110441711175, 46.41408978004932, 80.47178828524065, 95.98689725667111, 64.05754096067587, 115.6541560915002, 53.884896120749815, 104.24218776170765, 108.03516768580408, 190.26445259463338, 420.1408193989178, 228.76081706046955, 99.52895193300083, 128.29419328804516, 392.92212661053793, 172.84920535120727, 128.19073140879678, 289.25946737632177, 182.75523392032957, 195.6467176556074, 143.89736433479018, 122.97718054301815, 475.0294504064073, 253.86802081374617, 509.49964541123217, 320.190237417437, 603.9015709210453, 380.3495211973128, 298.97270157088593, 364.3926294759481, 1450.0082292757756, 1175.9927811812618, 416.3327852786746, 1210.7447058162863, 903.0627404181968, 414.2525790579358, 883.5780594979415, 374.03143951422413, 1335.7528835087544, 74.78232637293974, 51.57097994405417, 43.97376294726334, 37.16395546833076, 33.95545089574601, 32.90579110429831, 32.26964683779073, 27.97219746113343, 24.76379119176395, 23.1396091994053, 22.097227461789924, 17.706714044712626, 18.251175793499222, 73.32204139698993, 24.823068095326082, 16.10143470785697, 83.30775177536133, 17.85485341801134, 10.752930114298561, 10.2320290269685, 9.696682502428674, 9.69662693274083, 13.990330908365364, 23.245226415905066, 8.63433403247679, 8.62024131698972, 8.614606851355116, 8.100637763970875, 22.282500968459253, 19.395670097134676, 84.48346825506971, 136.82313717152178, 19.572263473277793, 187.53834697442878, 84.70342202345124, 60.69205261048979, 157.98815455181398, 138.68489004414786, 37.241974795436185, 44.32002338769775, 51.59858231286775, 95.20384844366545, 83.7855250625959, 172.81067043169142, 61.46277881630654, 602.2183962057653, 159.35754192292873, 313.302800209118, 124.21890810053516, 572.5265815509863, 198.49590927977266, 572.8225911764837, 563.8309654184227, 883.5780594979415, 1358.0369121814106, 172.09458425860456, 1175.9927811812618, 225.5455014501806, 185.40023737338979, 432.1411196341815, 1210.7447058162863, 632.1819795258772, 234.7669581579697, 181.4974608990829, 69.5545547859908, 55.26212673021844, 55.04354750161763, 51.191657423417716, 46.797734373269826, 41.18925142543665, 37.55514498066711, 35.912554849934885, 24.258824897628305, 38.3742371070225, 22.052537248747235, 21.615786961864394, 37.10087637839702, 20.731467609582072, 20.720557012935654, 19.86077060993568, 19.860770756685366, 19.860770790003265, 19.860770949200372, 19.860767653421515, 17.8739781500563, 16.56312335661177, 39.66350089717723, 14.90701978693355, 92.68076381393281, 327.9975669489021, 224.51530663600477, 10.407003238607912, 9.507760575531531, 84.43184528260446, 48.62867796923699, 100.06805415116196, 607.9934531351568, 100.75562244654641, 122.58427860765612, 51.09187584035962, 85.00710379343276, 55.187143025157376, 47.39356373700772, 274.14380049750804, 48.07955188412747, 59.06297423730339, 174.4336665518477, 580.4632280415974, 80.81420352721692, 234.7669581579697, 138.5573853591283, 157.42096064207735, 103.84349493314333, 111.52078037801122, 605.0916720645788, 222.761877414982, 1450.0082292757756, 747.1986695585127, 936.6682450102962, 126.46067546930311, 1335.4867949044824, 372.9228353251261, 331.8978544320697, 29.895385526174138, 17.362678754487238, 17.045528515136255, 16.062288945781187, 16.044570051730783, 16.51177345246707, 12.372771186011464, 12.369756102392182, 11.727829522123017, 11.38321326203856, 11.065493688779794, 11.060648622110145, 10.742355849213885, 10.4121784167286, 10.41155890980888, 10.410296215671025, 9.759208730470796, 11.028678504130738, 12.21083423983159, 98.0707666462739, 10.729987515390288, 18.640422932390457, 16.05958772032574, 5.089625854736907, 4.780667905166337, 52.88152701783759, 14.613740340530889, 128.3276224165863, 261.2179638583866, 635.77307371162, 38.33496372387615, 87.73421857703521, 96.79819859217865, 74.6288729554958, 172.10452357076935, 1450.0082292757756, 48.669288003932685, 27.42318383407413, 118.13252391957921, 96.99594435240054, 227.9553688033881, 185.93049201097512, 30.701928228823405, 558.7369759720566, 32.64967648143215, 57.96205501442016, 158.5295879932855, 502.68408962555986, 150.21075316442946, 318.5374105707616, 341.720837940058, 607.5627335048874, 331.8076236233946, 287.0187802858661, 1335.7528835087544, 117.90987336092384, 955.0058763332265, 1175.9927811812618, 475.8954609493308, 175.93509332906845, 392.92212661053793, 747.1986695585127, 602.2183962057653, 72.40824963588487, 30.1663279488823, 26.273580130392453, 22.101615249411143, 139.46780219621837, 14.505758383086732, 13.853483377719007, 15.414604610205263, 13.112265827704052, 50.725150458708114, 11.667856793643804, 12.347538988131637, 119.45011255689668, 56.82115026976768, 85.66951728062618, 17.203945812432924, 13.79443679185336, 19.93631670201234, 127.9320432752234, 53.93290433315962, 21.035434955270308, 14.751173153063805, 49.17587807113474, 34.33791519854544, 14.738870510912964, 15.723425872269436, 89.52535336912642, 152.53386766812363, 14.953498008154149, 4.711975091123124, 32.07182672036231, 93.15146866492267, 143.6049744509043, 56.57356595870191, 91.4105821987618, 119.21490332113297, 133.9505469885178, 206.41102270768675, 307.30844436705615, 287.71804482654636, 516.859145069919, 156.18940179337233, 1347.7430208466933, 580.4632280415974, 339.3513094551738, 236.37343647440085, 887.0117608803897, 617.7495642045072, 302.04473231785596, 825.443164984746, 1335.4867949044824, 602.0892417390933, 588.2714638193487, 1607.7183582490516, 883.5780594979415, 1335.7528835087544, 107.09607426118448, 69.04552336837115, 42.53537793508643, 35.33461322116378, 18.189143158147125, 14.075810406053986, 23.24776802472521, 22.279646761289378, 58.36148749394539, 31.985611522815827, 50.376147968988114, 16.25913953850542, 15.647780060235627, 26.826924300416394, 14.68372751025726, 19.63529629694542, 85.27108030771782, 112.41660185337918, 53.47762787867668, 44.75238790757132, 57.2161120573341, 68.18335856210551, 120.05801007235584, 44.162733150824096, 19.338210119262158, 16.837042373572178, 16.677964111202407, 51.21931527906454, 72.37136098544421, 9.88971734973966, 17.321027202530395, 31.383013398868805, 128.74020170999623, 615.9479585470965, 166.5702297160514, 159.35754192292873, 320.87817083990745, 217.35155811948883, 107.21357534361523, 178.22671499403387, 53.0753976301763, 162.07206826945338, 572.5265815509863, 180.04237486216894, 283.63689935577344, 1607.7183582490516, 1347.7430208466933, 632.1819795258772, 1175.9927811812618, 206.41102270768675, 339.3513094551738, 1335.7528835087544, 955.0058763332265, 936.6682450102962, 279.0943386453064, 40.443480823084506, 34.41302570184663, 31.74964101372319, 33.5059107381102, 66.79211429682303, 102.86703329239218, 32.49506709124047, 114.3480160470061, 136.59642488762134, 27.17002448247195, 11.325390221992512, 23.860513072799794, 73.52029420092678, 11.696634672860652, 17.65800019814688, 26.195393893333875, 144.3961486128898, 15.515836646927223, 23.60415806390874, 127.1313824992231, 30.368119214394, 38.157974910526214, 40.05164888786999, 20.527406711455136, 178.36584585991423, 20.230018144993952, 65.9097271020428, 20.404599044141083, 28.95806393640184, 51.751429760580336, 57.64863326489794, 68.5253775092243, 318.8295993425147, 162.39166175565433, 185.40023737338979, 325.15372834252224, 259.19502061175297, 161.33345997416043, 113.17200406922922, 89.40134372652344, 257.6996907927282, 235.08494449090267, 109.66699859022685, 588.2714638193487, 526.7547514126264, 876.5083012716126, 1347.7430208466933, 825.443164984746, 302.04473231785596, 248.01845574497725, 1210.7447058162863, 210.51419667015534, 467.11710401894413, 1136.2509991531324, 59.09995085993328, 31.051055159309545, 26.564624688001064, 26.003298118479766, 24.92110627334901, 18.299182384477113, 17.999976727969333, 16.466483553279172, 16.315614713007086, 14.762654606805896, 12.98804887765319, 8.574850409246888, 162.7098268513605, 8.125009677457474, 11.88049311463504, 28.16785833243081, 18.56643140824459, 49.97663496091438, 33.87264009102072, 22.056824974534837, 11.6615447407748, 20.462028625865834, 79.5218032086974, 17.546607449047656, 8.870868225564998, 10.986661961234235, 17.14450534123904, 130.0564575626828, 17.965189015853312, 71.00751959797489, 48.36244640505244, 876.5083012716126, 90.03813204011567, 36.94929576803224, 68.20686538305713, 33.71060553593518, 78.5709018888704, 632.1819795258772, 127.9320432752234, 210.2578840170054, 298.97270157088593, 883.5780594979415, 514.0515758795389], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.2988, -6.3068, -5.3373, -6.7377, -6.7468, -6.7562, -6.8334, -6.8442, -6.9246, -6.985, -7.0412, -7.0496, -7.0833, -7.2465, -7.2722, -7.2722, -7.2879, -7.3148, -7.3994, -7.3603, -7.4053, -7.4236, -7.4295, -7.448, -7.4539, -7.4793, -7.5051, -7.5252, -7.5589, -7.5598, -5.7138, -4.4354, -5.9538, -6.233, -6.9255, -5.6159, -6.6082, -6.2848, -6.2554, -6.0305, -6.571, -7.0856, -6.9733, -5.964, -5.4797, -6.2773, -6.1185, -5.7623, -5.0965, -5.5108, -6.3714, -5.8951, -6.1401, -5.8475, -6.2699, -5.3208, -4.9905, -5.6509, -5.3808, -5.8747, -4.9454, -5.0725, -6.0915, -4.6319, -5.3716, -5.3003, -4.8511, -4.8569, -5.2757, -5.3379, -5.5343, -5.6261, -5.6215, -5.0733, -5.2782, -5.2584, -5.6677, -5.3162, -5.527, -5.5546, -5.5193, -5.6637, -4.8378, -5.5329, -5.6724, -5.8711, -6.0087, -6.0278, -6.0177, -6.233, -6.2792, -6.1398, -6.3859, -6.4532, -6.5339, -6.6873, -6.6389, -6.6975, -6.701, -6.7358, -6.7395, -6.7611, -6.8249, -6.8606, -6.8935, -6.9063, -6.9446, -6.9624, -6.9711, -6.9943, -7.0078, -6.7443, -5.0208, -5.7482, -6.2369, -6.7289, -5.3669, -5.9468, -5.7359, -6.4261, -6.2713, -5.9879, -5.8173, -5.2538, -5.1772, -6.2166, -5.4745, -5.4542, -6.356, -4.2709, -5.3996, -4.5975, -5.6195, -4.9407, -5.4273, -4.4792, -5.1746, -5.3173, -4.5703, -5.3129, -5.0986, -5.4971, -5.363, -5.1845, -4.9778, -5.3763, -5.4007, -5.1394, -5.2276, -5.5183, -5.5144, -5.4778, -4.8657, -4.9633, -5.2045, -5.2753, -5.2886, -5.3619, -5.4329, -5.5128, -5.6487, -5.7435, -5.7466, -5.7558, -5.7586, -5.9768, -6.0278, -6.0642, -6.0573, -6.1068, -6.1244, -6.1424, -6.2029, -6.2613, -6.2869, -6.3299, -6.3298, -6.346, -6.3626, -6.3922, -5.4754, -6.5206, -4.1941, -4.8231, -3.5027, -5.0859, -5.9029, -4.1907, -5.3348, -3.5758, -4.7281, -5.7564, -4.4433, -5.887, -4.3742, -5.3035, -5.3267, -5.0504, -4.6171, -5.2195, -4.2583, -4.9209, -5.0165, -4.6188, -4.9104, -4.653, -4.8517, -4.9194, -5.014, -5.0913, -5.1853, -5.2988, -3.8591, -4.5838, -5.1599, -4.5782, -5.3813, -5.3919, -5.4934, -5.4211, -5.8074, -5.8523, -6.1689, -6.3763, -6.4552, -6.5416, -6.6374, -4.6201, -6.3057, -6.7976, -6.8587, -6.8588, -6.8429, -6.0094, -7.2319, -5.7215, -6.4628, -4.7002, -6.8405, -6.5101, -6.6093, -6.7835, -5.0933, -3.8486, -5.6082, -5.4956, -5.1244, -4.89, -5.7593, -3.2538, -5.8776, -4.0879, -5.2662, -4.7427, -5.2669, -4.2753, -4.8729, -4.4068, -4.0801, -4.0873, -4.9099, -4.6302, -4.354, -4.7057, -4.4035, -5.024, -5.1045, -5.1517, -5.1058, -4.9155, -5.0544, -5.0506, -4.1495, -4.8071, -5.1004, -5.267, -5.3749, -5.403, -5.5017, -5.7173, -5.8168, -5.8467, -5.6456, -5.2093, -5.9702, -6.0901, -6.1333, -6.1481, -6.167, -6.2636, -6.297, -6.3144, -6.4462, -6.4463, -6.4463, -6.5361, -6.5525, -6.2491, -6.8434, -5.1709, -6.8735, -6.9157, -5.2316, -4.178, -6.5389, -5.2318, -5.1953, -4.7508, -4.5911, -5.7706, -4.5731, -4.6654, -5.4579, -4.8145, -5.5679, -4.9559, -5.4344, -4.5691, -4.968, -4.5947, -4.8698, -5.2604, -4.85, -5.0935, -5.0174, -5.1038, -4.9614, -5.1656, -5.0033, -5.085, -5.2435, -4.4606, -5.433, -5.4861, -5.5585, -5.1894, -5.6238, -3.7212, -6.2269, -6.3131, -6.459, -6.5143, -4.6771, -6.695, -3.8709, -6.9182, -5.1913, -5.2747, -5.0264, -6.5332, -5.9835, -4.3698, -4.3129, -6.0069, -6.8605, -6.811, -7.0236, -6.6497, -6.0702, -4.8886, -6.0283, -4.7875, -3.1331, -5.1391, -4.3234, -4.0096, -4.2019, -4.1966, -4.876, -4.9524, -4.845, -4.7296, -4.8865, -3.8949, -5.4195, -4.9749, -4.65, -4.8958, -4.8029, -5.0563, -4.8324, -4.5412, -4.5321, -4.677, -4.6796, -4.7877, -4.8089, -4.8131, -4.8553, -4.8725, -4.8403, -5.3391, -5.6268, -5.6163, -5.8075, -6.091, -6.1381, -6.2243, -6.4603, -6.5094, -6.5908, -6.7305, -5.1882, -6.7811, -6.4496, -6.8915, -6.4254, -7.0198, -5.4785, -5.9579, -4.4261, -7.2425, -6.9546, -7.4205, -5.3694, -5.0133, -5.0741, -5.4453, -6.4835, -5.5449, -4.5257, -4.0615, -5.4279, -5.8257, -3.7465, -5.5237, -3.7897, -5.1853, -5.2984, -4.6987, -5.2065, -4.4499, -5.6504, -4.8917, -4.1041, -4.7863, -4.2949, -4.683, -5.3963, -4.3583, -5.05, -4.428, -5.0046, -4.817, -4.9656, -4.9164, -5.12, -4.968, -4.996, -4.8509, -4.8598, -4.8173, -4.8617, -4.9252, -5.0041, -5.013, -4.9534, -5.1771, -5.2899, -5.7283, -5.9211, -5.8639, -6.1269, -6.148, -6.1849, -6.1893, -6.2723, -6.3397, -6.6084, -6.7286, -6.7289, -6.7997, -6.7998, -6.8364, -6.8437, -6.3198, -7.0613, -6.275, -4.22, -6.705, -6.3905, -5.5274, -5.4972, -6.1588, -5.6423, -5.5931, -5.0852, -4.9627, -5.3541, -4.8265, -5.57, -5.0655, -5.0424, -4.5984, -4.0111, -4.5019, -5.1663, -4.987, -4.2229, -4.8648, -5.0734, -4.5559, -4.8779, -4.8437, -5.0615, -5.1749, -4.5126, -4.8342, -4.5109, -4.7311, -4.4593, -4.6631, -4.7877, -4.7279, -4.3053, -4.3938, -4.8026, -4.5769, -4.8192, -4.9268, -4.9234, -4.9887, -4.9312, -4.6967, -5.0751, -5.2382, -5.4112, -5.5047, -5.5368, -5.5598, -5.7053, -5.8325, -5.9035, -5.9521, -6.1865, -6.1572, -4.7669, -5.8525, -6.2882, -4.6798, -6.2213, -6.7294, -6.7844, -6.8447, -6.8447, -6.484, -5.9794, -6.9766, -6.9785, -6.9792, -7.05, -6.0409, -6.1852, -4.718, -4.2841, -6.1881, -4.1104, -4.8752, -5.2251, -4.4167, -4.5553, -5.6849, -5.5704, -5.5051, -5.0926, -5.1941, -4.7633, -5.4202, -4.039, -4.8802, -4.5382, -5.056, -4.5204, -4.9827, -4.6049, -4.6293, -4.4802, -4.3738, -5.0785, -4.4471, -4.9987, -5.0551, -4.8733, -4.6528, -4.8027, -5.0716, -3.6902, -4.6589, -4.8931, -4.8972, -4.9712, -5.0631, -5.194, -5.2891, -5.3352, -5.7429, -5.289, -5.8431, -5.8642, -5.3258, -5.9083, -5.9088, -5.9537, -5.9537, -5.9537, -5.9537, -5.9537, -6.0657, -6.1471, -5.2776, -6.2604, -4.4405, -3.192, -3.582, -6.6552, -6.7569, -4.5758, -5.1476, -4.493, -2.9153, -4.6211, -4.4881, -5.1965, -4.8051, -5.1774, -5.3084, -4.0346, -5.3053, -5.1647, -4.4202, -3.7523, -4.9866, -4.4022, -4.7554, -4.7612, -4.9306, -4.9164, -4.3631, -4.8989, -4.5686, -4.7046, -4.6854, -5.0669, -4.8839, -4.991, -2.9485, -5.3907, -5.9627, -5.9824, -6.0462, -6.0473, -6.0253, -6.3299, -6.3301, -6.3901, -6.422, -6.4534, -6.4539, -6.4866, -6.5214, -6.5215, -6.5217, -6.5943, -6.5261, -6.4251, -4.349, -6.5656, -6.0573, -6.2223, -7.3734, -7.4632, -5.0949, -6.3838, -4.2633, -3.6391, -2.8545, -5.5796, -4.862, -4.7904, -5.0832, -4.6187, -3.4073, -5.5678, -5.9219, -5.0312, -5.2032, -4.7349, -4.8553, -5.876, -4.2937, -5.8511, -5.5537, -5.0474, -4.5596, -5.1573, -4.9307, -4.934, -4.7286, -4.9831, -5.0455, -4.5281, -5.3654, -4.766, -4.7133, -4.9792, -5.2515, -5.1467, -5.1086, -5.2026, -4.4081, -5.306, -5.4498, -5.6311, -3.8058, -6.0806, -6.1305, -6.0244, -6.1907, -4.844, -6.3258, -6.2721, -4.0275, -4.7789, -4.4251, -6.0328, -6.2974, -5.9399, -4.1308, -5.0049, -5.9903, -6.3711, -5.1899, -5.5825, -6.4415, -6.388, -4.7032, -4.2358, -6.5602, -7.7162, -5.8253, -4.7855, -4.4321, -5.3068, -4.9342, -4.8044, -4.8126, -4.63, -4.4033, -4.6048, -4.4146, -4.994, -4.0712, -4.4703, -4.7097, -4.8682, -4.4019, -4.5922, -4.8267, -4.5888, -4.5571, -4.7212, -4.7384, -4.5938, -4.7963, -4.8559, -3.9399, -4.3847, -4.8794, -5.0702, -5.7656, -6.0407, -5.5499, -5.595, -4.6725, -5.2849, -4.8402, -5.9957, -6.0341, -5.5007, -6.1332, -5.8835, -4.4323, -4.1621, -4.9206, -5.1332, -4.8888, -4.7453, -4.195, -5.1961, -6.0296, -6.1814, -6.248, -5.1316, -4.8078, -6.7986, -6.2458, -5.6643, -4.3737, -3.2585, -4.3242, -4.4384, -4.1283, -4.4275, -4.8826, -4.6192, -5.3195, -4.7235, -4.2113, -4.8561, -4.7428, -4.2329, -4.3695, -4.6086, -4.6244, -4.9114, -4.8673, -4.7932, -4.8189, -4.8479, -2.8486, -4.8071, -4.9701, -5.0534, -5.0051, -4.3342, -3.9086, -5.0715, -3.8681, -3.7311, -5.3918, -6.2998, -5.5678, -4.4491, -6.3291, -5.9999, -5.632, -3.9344, -6.1683, -5.7952, -4.1175, -5.5964, -5.3719, -5.335, -6.0091, -3.8571, -6.0607, -4.8874, -6.0647, -5.7732, -5.223, -5.1571, -5.0451, -3.7989, -4.4312, -4.3509, -3.9547, -4.2156, -4.5481, -4.7856, -4.9365, -4.3491, -4.5667, -4.9066, -4.3343, -4.3763, -4.2173, -4.2014, -4.5207, -4.7385, -4.7844, -4.7089, -4.8745, -4.8767, -0.9444, -3.9133, -4.575, -4.7378, -4.7599, -4.8046, -5.131, -5.1486, -5.244, -5.2539, -5.3636, -5.5017, -5.9682, -3.0294, -6.0307, -5.732, -4.9111, -5.3776, -4.5601, -4.9699, -5.4097, -6.0725, -5.5351, -4.1787, -5.7204, -6.4524, -6.2551, -5.8299, -3.8091, -5.8139, -4.4765, -4.9862, -2.6331, -4.5657, -5.3022, -4.9397, -5.3717, -5.0236, -4.5351, -5.18, -5.1516, -5.296, -5.3078, -5.3547], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2239, 1.2239, 1.2209, 1.2193, 1.2191, 1.219, 1.2179, 1.2178, 1.2166, 1.2156, 1.2146, 1.2144, 1.2138, 1.2106, 1.21, 1.21, 1.2097, 1.2089, 1.207, 1.2069, 1.2069, 1.2064, 1.2062, 1.2058, 1.2056, 1.2049, 1.2042, 1.2036, 1.2027, 1.2023, 1.1978, 1.1821, 1.1937, 1.1924, 1.1949, 1.1542, 1.1843, 1.1721, 1.1708, 1.1564, 1.1758, 1.1937, 1.1866, 1.1161, 1.0676, 1.1194, 1.0988, 1.0547, 0.9806, 0.9752, 1.1042, 1.0112, 1.0551, 0.9828, 1.0522, 0.7868, 0.6858, 0.8791, 0.7828, 0.9045, 0.5371, 0.5731, 0.9809, 0.3104, 0.6379, 0.6014, 0.2768, 0.2618, 0.5237, 0.5147, 0.6253, 0.6777, 0.6172, -0.0278, 0.185, -0.0034, 0.5592, -0.1886, 0.1815, 0.0543, -0.4267, -0.1267, 1.515, 1.5123, 1.5106, 1.5101, 1.509, 1.5089, 1.5086, 1.5068, 1.5063, 1.5058, 1.5051, 1.5042, 1.5031, 1.5007, 1.5006, 1.5005, 1.5005, 1.4999, 1.4998, 1.4994, 1.4982, 1.4975, 1.4967, 1.4963, 1.4958, 1.4953, 1.4952, 1.4947, 1.4944, 1.4941, 1.468, 1.4683, 1.4756, 1.4885, 1.4339, 1.4578, 1.4299, 1.4697, 1.4476, 1.4095, 1.3859, 1.2905, 1.2613, 1.4195, 1.2803, 1.2536, 1.4347, 0.955, 1.1975, 0.9515, 1.24, 1.0342, 1.1504, 0.6319, 0.9352, 0.9899, 0.5574, 0.9615, 0.8293, 1.0631, 0.8932, 0.6682, 0.4387, 0.6818, 0.5894, -0.0469, -0.2853, 0.456, 0.3383, -0.35, 2.6843, 2.6832, 2.6801, 2.6796, 2.6795, 2.6784, 2.6765, 2.676, 2.6734, 2.6714, 2.6714, 2.6712, 2.6711, 2.6657, 2.6643, 2.6633, 2.663, 2.6619, 2.6614, 2.6608, 2.6588, 2.6568, 2.6558, 2.6539, 2.6536, 2.6536, 2.6529, 2.6514, 2.6498, 2.6456, 2.6054, 2.5706, 2.4168, 2.5183, 2.6015, 2.3438, 2.4733, 1.9433, 2.1907, 2.4963, 1.861, 2.5447, 1.5868, 2.1238, 2.1385, 1.9167, 1.4753, 1.9585, 0.8342, 1.5057, 1.6426, 0.5091, 1.028, 0.2893, 0.6853, 0.1917, 0.1047, 0.3718, 0.8973, 0.4097, 2.9365, 2.9316, 2.9253, 2.9235, 2.9215, 2.9213, 2.9192, 2.9187, 2.9112, 2.91, 2.899, 2.8891, 2.8853, 2.8807, 2.8749, 2.8676, 2.8646, 2.8638, 2.8592, 2.8592, 2.8411, 2.8324, 2.8244, 2.7738, 2.7424, 2.7284, 2.7159, 2.7124, 2.6993, 2.6968, 2.6893, 2.6176, 2.6461, 2.6346, 2.5889, 2.4947, 2.5506, 1.8387, 2.5446, 1.8273, 2.2669, 2.0264, 2.2508, 1.6953, 1.9155, 1.5675, 1.3364, 0.9582, 1.7403, 1.3175, 0.7647, 1.17, 0.5388, 1.593, 1.6628, 1.7366, 1.4961, 0.1955, 1.0038, 0.4684, 2.985, 2.9792, 2.9751, 2.9723, 2.9701, 2.9695, 2.9673, 2.9615, 2.9585, 2.9576, 2.9559, 2.9535, 2.9532, 2.9485, 2.9466, 2.9459, 2.9451, 2.9395, 2.9389, 2.9379, 2.9307, 2.9307, 2.9307, 2.9252, 2.9241, 2.9083, 2.9024, 2.9012, 2.8998, 2.8961, 2.8424, 2.6978, 2.8733, 2.6388, 2.6054, 2.2519, 1.9519, 2.5787, 1.9178, 1.8284, 2.3517, 1.8165, 2.3317, 1.5644, 2.0611, 0.8474, 1.1234, 0.3476, 0.8388, 1.5105, 0.1955, 0.8449, 0.5235, 0.8109, 0.1497, 0.9268, 0.1154, 0.3976, 1.1831, 3.0506, 3.0367, 3.0355, 3.0327, 3.0318, 3.0315, 3.029, 3.0101, 3.0058, 2.9975, 2.9933, 2.9887, 2.9812, 2.9741, 2.9636, 2.9632, 2.9509, 2.9225, 2.8992, 2.8964, 2.8943, 2.884, 2.8794, 2.8729, 2.8706, 2.8645, 2.8607, 2.8501, 2.8336, 2.8199, 2.8196, 2.7913, 2.7913, 2.7389, 2.6916, 2.5844, 2.5441, 2.5962, 2.6105, 2.5246, 2.4586, 2.5021, 2.0756, 2.666, 2.4301, 2.1749, 2.246, 1.9567, 2.2603, 1.7515, 0.5864, 0.4102, 0.7862, 0.4391, 0.8213, 0.7281, 0.7278, 0.3997, 0.22, 3.0736, 3.0649, 3.058, 3.057, 3.0523, 3.0412, 3.0391, 3.0349, 3.0216, 3.0174, 3.0127, 3.0021, 2.9981, 2.9979, 2.9967, 2.988, 2.9751, 2.9737, 2.9619, 2.9579, 2.9512, 2.9461, 2.9278, 2.9223, 2.901, 2.8995, 2.8906, 2.8673, 2.8664, 2.8404, 2.7505, 2.7157, 2.7787, 2.8137, 2.5519, 2.7698, 2.4946, 2.6911, 2.7137, 2.5798, 2.6658, 2.4583, 2.7583, 2.4663, 2.0853, 2.3151, 2.0564, 2.182, 2.5665, 1.5405, 2.1848, 1.1129, 1.9584, 1.6096, 1.8793, 1.7372, 2.0348, 1.5258, 1.5745, 0.7581, 0.6772, 0.2938, 0.2661, 0.7204, 0.8486, 0.4501, 3.0717, 3.068, 3.0653, 3.0542, 3.0474, 3.046, 3.0386, 3.037, 3.0357, 3.0355, 3.0312, 3.0273, 3.0098, 3.0003, 3.0003, 2.9942, 2.9942, 2.9909, 2.9903, 2.9712, 2.9681, 2.9653, 2.9637, 2.9285, 2.9211, 2.9188, 2.9095, 2.9007, 2.897, 2.8942, 2.8517, 2.798, 2.811, 2.7478, 2.7681, 2.6126, 2.6001, 2.4781, 2.2732, 2.3903, 2.5582, 2.4836, 2.1284, 2.3077, 2.398, 2.1016, 2.2388, 2.2049, 2.2943, 2.338, 1.6489, 1.9538, 1.5806, 1.8249, 1.4622, 1.7207, 1.8369, 1.6988, 0.7403, 0.8612, 1.4908, 0.649, 0.6999, 1.3716, 0.6175, 1.4118, 0.1964, 3.3136, 3.3068, 3.3031, 3.2984, 3.2951, 3.2944, 3.2909, 3.2883, 3.283, 3.2798, 3.2773, 3.2644, 3.2634, 3.2632, 3.2606, 3.2578, 3.2225, 3.2213, 3.2204, 3.2149, 3.2084, 3.2084, 3.2025, 3.1993, 3.1925, 3.1923, 3.1922, 3.1829, 3.1802, 3.1746, 3.1703, 3.1221, 3.1627, 2.9805, 3.0105, 2.9939, 2.8457, 2.8374, 3.0226, 2.963, 2.8763, 2.6762, 2.7025, 2.4094, 2.7862, 1.8853, 2.3735, 2.0395, 2.4469, 1.4544, 2.0514, 1.3694, 1.3608, 1.0607, 0.7373, 2.0983, 0.8079, 1.9076, 2.0473, 1.3828, 0.573, 1.073, 1.7947, 3.4335, 3.4239, 3.4197, 3.4196, 3.4181, 3.416, 3.4127, 3.41, 3.4086, 3.3932, 3.3885, 3.3883, 3.3873, 3.3854, 3.385, 3.3849, 3.3825, 3.3825, 3.3825, 3.3825, 3.3825, 3.3759, 3.3706, 3.3669, 3.3626, 3.3553, 3.3399, 3.3289, 3.3272, 3.3158, 3.3131, 3.2931, 3.2261, 2.9995, 3.0911, 3.028, 3.1948, 3.077, 3.1367, 3.158, 2.6766, 3.1467, 3.0816, 2.7432, 2.2087, 2.9461, 2.4641, 2.6382, 2.5048, 2.7514, 2.6943, 1.5564, 2.0199, 0.477, 1.004, 0.7972, 2.4181, 0.2439, 1.4125, 3.5715, 3.5365, 3.5079, 3.5066, 3.5023, 3.5022, 3.4955, 3.4795, 3.4795, 3.4728, 3.4708, 3.4676, 3.4676, 3.4641, 3.4605, 3.4605, 3.4604, 3.4524, 3.3983, 3.3974, 3.3902, 3.3862, 3.3422, 3.3263, 3.3243, 3.2971, 3.2619, 3.2591, 3.207, 3.1204, 3.0155, 3.0989, 2.9886, 2.9619, 2.9291, 2.5581, 1.6382, 2.872, 3.0916, 2.5218, 2.547, 2.1608, 2.2442, 3.0246, 1.7055, 2.9879, 2.7114, 2.2115, 1.5453, 2.1555, 1.6305, 1.5569, 1.1868, 1.5372, 1.6198, 0.5996, 2.1895, 0.6972, 0.5417, 1.1804, 1.9033, 1.2046, 0.5999, 0.7217, 3.6345, 3.6122, 3.6065, 3.5981, 3.5813, 3.5698, 3.5658, 3.5652, 3.5606, 3.5545, 3.5422, 3.5393, 3.5145, 3.5061, 3.4493, 3.4469, 3.4033, 3.3924, 3.3426, 3.3322, 3.2884, 3.2625, 3.2396, 3.2061, 3.1929, 3.1817, 3.1272, 3.0617, 3.0598, 3.0586, 3.0316, 3.0051, 2.9257, 2.9825, 2.8753, 2.7396, 2.6148, 2.365, 2.1937, 2.0581, 1.6625, 2.2798, 1.0475, 1.4908, 1.7882, 1.9913, 1.1352, 1.3066, 1.7876, 1.0202, 0.5708, 1.2033, 1.2093, 0.3486, 0.7446, 0.2717, 3.7112, 3.7054, 3.6951, 3.6898, 3.6585, 3.6397, 3.6288, 3.6263, 3.5857, 3.5747, 3.5652, 3.5405, 3.5405, 3.5348, 3.505, 3.464, 3.4467, 3.4406, 3.425, 3.3905, 3.3893, 3.3574, 3.3419, 3.3409, 3.3332, 3.3199, 3.2628, 3.2572, 3.2352, 3.2348, 3.2272, 3.2143, 3.0933, 2.6432, 2.8853, 2.8153, 2.4255, 2.5159, 2.7674, 2.5226, 3.0336, 2.5134, 1.7635, 2.2756, 1.9344, 0.7094, 0.7492, 1.2671, 0.6306, 2.0836, 1.6306, 0.3344, 0.6443, 0.6347, 3.8447, 3.8179, 3.8164, 3.8136, 3.8081, 3.7891, 3.7828, 3.7723, 3.7176, 3.6767, 3.631, 3.5981, 3.5849, 3.5782, 3.5364, 3.4538, 3.4273, 3.418, 3.4147, 3.3682, 3.3622, 3.3151, 3.3113, 3.2997, 3.294, 3.2839, 3.257, 3.2492, 3.2444, 3.1859, 3.1555, 3.1134, 3.0525, 2.7614, 2.8037, 2.7514, 2.5858, 2.5517, 2.6934, 2.8104, 2.8952, 2.424, 2.2982, 2.7208, 1.6134, 1.6819, 1.3317, 0.9173, 1.0883, 1.8758, 2.027, 0.517, 2.1009, 1.3016, 4.3449, 4.3323, 4.3142, 4.3075, 4.3067, 4.3045, 4.287, 4.2859, 4.2795, 4.2788, 4.2692, 4.2592, 4.2079, 4.2035, 4.1992, 4.1179, 4.0756, 4.0259, 3.8532, 3.8324, 3.8215, 3.7961, 3.7712, 3.7702, 3.7396, 3.6897, 3.6731, 3.6533, 3.6478, 3.6226, 3.5856, 3.46, 2.9159, 3.2589, 3.4131, 3.1626, 3.4354, 2.9373, 1.3406, 2.2934, 1.8249, 1.3285, 0.2331, 0.7279]}, \"token.table\": {\"Topic\": [2, 4, 13, 2, 3, 4, 7, 9, 10, 11, 12, 13, 1, 2, 6, 11, 13, 1, 5, 10, 14, 2, 10, 10, 3, 6, 2, 7, 8, 2, 15, 2, 3, 5, 8, 2, 2, 1, 2, 5, 8, 9, 12, 14, 2, 8, 14, 2, 1, 11, 1, 3, 4, 1, 1, 1, 3, 5, 7, 11, 12, 1, 2, 4, 7, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 3, 12, 4, 5, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 6, 7, 14, 1, 1, 2, 7, 6, 1, 2, 7, 2, 8, 12, 3, 13, 1, 5, 6, 8, 12, 8, 3, 4, 13, 1, 11, 5, 10, 1, 2, 3, 4, 8, 13, 14, 3, 8, 10, 12, 15, 1, 2, 3, 5, 12, 4, 10, 13, 14, 15, 12, 4, 8, 4, 6, 1, 2, 5, 7, 11, 14, 2, 4, 5, 6, 7, 9, 1, 8, 13, 5, 12, 1, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 15, 1, 2, 4, 5, 7, 8, 9, 15, 5, 4, 1, 3, 8, 9, 2, 2, 4, 14, 7, 9, 1, 2, 3, 4, 5, 6, 7, 11, 13, 15, 11, 4, 12, 2, 10, 2, 10, 7, 6, 10, 1, 10, 2, 9, 6, 14, 1, 2, 5, 14, 9, 13, 9, 15, 7, 6, 7, 8, 1, 14, 5, 2, 12, 1, 2, 8, 13, 14, 15, 1, 2, 4, 14, 1, 14, 1, 3, 15, 5, 1, 12, 4, 2, 4, 7, 8, 10, 13, 14, 3, 2, 7, 15, 15, 8, 14, 5, 9, 13, 14, 5, 2, 12, 14, 1, 2, 3, 9, 14, 1, 2, 3, 13, 3, 2, 12, 14, 10, 15, 1, 2, 3, 8, 15, 1, 2, 3, 9, 10, 13, 15, 3, 4, 5, 7, 8, 9, 1, 2, 3, 5, 6, 10, 11, 3, 11, 14, 1, 6, 11, 14, 1, 3, 6, 8, 4, 7, 13, 3, 10, 1, 2, 3, 4, 5, 6, 8, 12, 14, 2, 9, 12, 6, 1, 2, 11, 1, 2, 6, 8, 4, 15, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 2, 5, 1, 2, 6, 14, 1, 3, 10, 12, 2, 5, 7, 8, 11, 14, 15, 1, 2, 5, 7, 2, 4, 6, 7, 9, 10, 11, 4, 10, 3, 5, 11, 2, 2, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 6, 12, 2, 2, 12, 1, 10, 10, 10, 1, 2, 3, 1, 5, 1, 15, 1, 2, 5, 7, 8, 9, 11, 12, 13, 1, 2, 4, 8, 9, 14, 1, 2, 3, 7, 9, 11, 12, 2, 2, 4, 12, 14, 6, 12, 1, 2, 3, 4, 5, 6, 8, 12, 14, 13, 10, 11, 13, 14, 4, 5, 7, 13, 1, 2, 3, 4, 5, 6, 14, 1, 2, 3, 5, 8, 10, 13, 14, 1, 2, 3, 4, 5, 6, 11, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 2, 14, 2, 1, 9, 1, 4, 14, 3, 5, 10, 4, 1, 2, 4, 12, 9, 14, 2, 3, 14, 2, 12, 3, 3, 2, 3, 9, 12, 13, 9, 14, 8, 10, 7, 11, 2, 5, 8, 1, 2, 3, 4, 5, 6, 9, 11, 14, 4, 1, 13, 9, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 13, 3, 13, 3, 4, 11, 3, 1, 1, 2, 5, 1, 2, 3, 9, 14, 5, 13, 3, 8, 1, 8, 4, 2, 9, 8, 10, 14, 1, 15, 1, 5, 8, 10, 11, 2, 12, 12, 14, 15, 1, 14, 1, 14, 6, 10, 12, 3, 14, 8, 13, 11, 5, 13, 9, 3, 14, 2, 1, 6, 7, 14, 9, 1, 2, 3, 4, 11, 13, 11, 1, 6, 11, 13, 2, 6, 4, 2, 13, 1, 2, 4, 5, 7, 11, 15, 1, 2, 4, 5, 7, 8, 10, 11, 13, 8, 1, 9, 12, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 14, 1, 3, 4, 3, 8, 5, 11, 6, 3, 8, 8, 8, 2, 1, 3, 4, 8, 10, 11, 12, 2, 8, 1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 2, 12, 6, 1, 2, 4, 5, 6, 7, 9, 11, 12, 13, 3, 4, 4, 4, 6, 10, 14, 15, 1, 2, 8, 11, 9, 6, 1, 2, 3, 4, 5, 6, 14, 5, 7, 9, 15, 13, 1, 2, 3, 5, 6, 7, 13, 1, 2, 12, 2, 4, 8, 14, 7, 4, 9, 14, 5, 10, 3, 7, 4, 7, 8, 11, 12, 3, 8, 2, 14, 10, 4, 6, 12, 15, 1, 2, 3, 5, 6, 8, 11, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13, 14, 1, 2, 4, 1, 2, 3, 4, 7, 9, 11, 11, 1, 2, 5, 8, 1, 2, 3, 5, 12, 13, 1, 2, 5, 7, 13, 1, 2, 4, 5, 6, 11, 3, 8, 2, 6, 3, 2, 15, 1, 2, 8, 9, 2, 7, 1, 2, 6, 7, 8, 9, 11, 13, 9, 14, 15, 1, 2, 7, 4, 7, 11, 10, 4, 7, 13, 2, 1, 2, 4, 5, 9, 14, 2, 2, 4, 14, 1, 2, 4, 5, 12, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 7, 1, 2, 3, 4, 5, 6, 12, 3, 10, 1, 3, 4, 7, 10, 10, 5, 6, 1, 2, 4, 8, 6, 9, 11, 10, 11, 10, 2, 1, 2, 3, 4, 7, 8, 9, 1, 2, 3, 6, 7, 8, 9, 13, 14, 15, 2, 5, 1, 2, 3, 4, 5, 14, 1, 2, 7, 14, 1, 2, 5, 7, 9, 12, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 9, 2, 5, 10, 2, 8, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 2, 5, 6, 7, 1, 2, 5, 12, 15, 2, 5, 4, 15, 5, 6, 1, 2, 3, 6, 11, 4, 1, 2, 3, 5, 15, 11, 2, 2, 15, 13, 15, 1, 2, 1, 15, 5, 2, 1, 3, 5, 6, 13, 9, 9, 2, 15, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 14, 10, 5, 1, 2, 4, 5, 6, 11, 2, 9, 4, 7, 14, 15, 3, 4, 12, 5, 7, 6, 12, 1, 5, 7, 12, 1, 2, 3, 5, 8, 9, 10, 12, 4, 11, 1, 2, 7, 8, 3, 1, 2, 3, 4, 2, 9, 10, 1, 2, 3, 4, 7, 10, 11, 13, 14, 15, 11, 13, 4, 1, 2, 3, 5, 8, 10, 11, 5, 10, 2, 3, 4, 5, 8, 10, 11, 13, 1, 3, 4, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 4, 5, 6, 10, 11, 13, 5, 1, 4, 13, 10, 1, 2, 3, 5, 6, 7, 8, 13, 15, 7, 3, 1, 2, 3, 4, 1, 3, 9, 3, 9, 10, 8, 1, 2, 3, 5, 8, 11, 1, 2, 5, 7, 8, 9, 12, 2, 8, 7, 1, 2, 5, 6, 8, 9, 10, 11, 1, 3, 9, 10, 12, 14, 1, 2, 7, 11, 7, 13, 15, 1, 3, 4, 8, 12, 3, 12, 4, 1, 3, 7, 1, 2, 4, 6, 8, 9, 13, 15, 1, 2, 4, 5, 12, 14, 3, 12, 1, 2, 5, 6, 11, 13, 14, 2, 5, 6, 7, 1, 6, 1, 2, 4, 12, 2, 3, 15, 2, 15, 1, 7, 9, 1, 3, 8, 3, 3, 5, 6, 7, 8, 9, 10, 11, 11, 5, 3, 1, 2, 6, 7, 1, 2, 4, 11, 12, 12, 14, 8, 7, 5, 6, 2, 2, 5, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 1, 5, 7, 14, 2, 6, 13, 14, 2, 13, 1, 2, 4, 5, 6, 7, 10, 14, 1, 3, 4, 9, 10, 11, 1, 2, 3, 4, 5, 8, 9, 10, 11, 13, 15, 1, 4, 5, 6, 14, 15, 8, 1, 6, 12, 1, 2, 4, 5, 9, 10, 1, 2, 4, 7, 15, 15, 5, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 1, 6, 9, 5, 9, 13, 3, 6, 10, 5, 15, 1, 2, 7, 1, 2, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3, 5, 9, 11, 13, 14, 3, 7, 1, 2, 8, 12, 13, 7, 1, 5, 14, 8, 9, 12, 9, 1, 2, 3, 6, 9, 10, 11, 13, 15, 11, 1, 2, 11, 14, 5, 14, 1, 2, 4, 5, 8, 11, 2, 3, 8, 9, 11, 12, 7, 11, 15, 1, 2, 3, 4, 7, 8, 9, 13, 14, 2, 7, 14, 3, 8, 9, 10, 12, 1, 4, 11, 3, 4, 2, 4, 3, 9, 1, 2, 6, 6, 4, 7, 8, 1, 2, 3, 4, 6, 8, 9, 11, 13, 9, 9, 2, 4, 2, 12, 14, 2, 3, 4, 5, 7, 2, 4, 5, 6, 13, 14, 1, 2, 6, 7, 11, 14, 3, 2, 10, 1, 1, 13, 9, 9, 8, 2, 6, 1, 2, 9, 1, 2, 7, 11, 12, 14, 1, 2, 3, 5, 8, 9, 10, 11, 12, 2, 7, 12, 13, 1, 2, 5, 1, 2, 4, 5, 6, 9, 11, 12, 13, 14, 1, 2, 7, 13, 14, 2, 3, 10, 14, 5, 11, 11, 10, 1, 8, 5, 1, 2, 4, 6, 7, 8, 2, 4, 12, 13, 9, 5, 9, 2, 3, 4, 9, 14, 15, 2, 4, 6, 2, 4, 6, 3, 1, 4, 13, 3, 5, 8, 10, 11, 2, 4, 6, 6, 1, 2, 4, 5, 8, 9, 11, 6, 14, 7, 1, 2, 3, 7, 14, 15, 9, 5, 1, 1, 2, 5, 4, 5, 10, 12, 13, 1, 2, 5, 6, 8, 9, 10, 12, 10, 4, 10, 1, 2, 4, 5, 8, 10, 11, 14, 7, 12, 12, 2, 6, 7, 2, 5, 1, 2, 3, 4, 5, 7, 8, 1, 2, 9, 7, 8, 13, 8, 2, 12, 15, 2, 9, 4, 8, 13, 1, 2, 3, 4, 5, 7, 8, 12, 4, 5, 6, 7, 8, 13, 6, 7, 12, 13, 14, 1, 2, 3, 4, 5, 7, 12, 15, 7, 7, 11, 2, 10, 1, 2, 5, 6, 9, 10, 12, 3, 2, 12, 15, 1, 2, 5, 8, 11, 12, 15, 1, 2, 2, 4, 7, 8, 12, 15, 7, 11, 14, 2, 3, 8, 12, 2, 8, 3, 8, 11, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 7, 10, 8, 1, 3, 6, 11, 13, 14, 1, 2, 1, 6, 7, 10, 1, 2, 3, 6, 9, 13, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 9, 5, 1, 2, 4, 8, 14, 8, 8, 7, 10, 5, 2, 12, 5, 1, 15, 10, 11, 12, 10, 12, 3, 4, 9, 9, 12, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 1, 7, 15, 15, 1, 7, 13, 15, 1, 14, 11, 1, 11, 3, 6, 1, 5, 1, 8, 11, 2, 7, 4, 2, 15, 1, 7, 2, 4, 4, 6, 1, 2, 3, 4, 7, 15, 1, 2, 8, 8, 1, 2, 3, 9, 10, 12, 14, 15, 1, 5, 15, 2, 4, 7, 8, 9, 15, 1, 2, 4, 6, 9, 13, 1, 11, 14, 4, 5, 9, 6, 7, 7, 3, 2, 5, 6, 11, 14, 6, 1, 2, 5, 12, 2, 4, 6, 13, 14, 10, 7, 12, 3, 10, 11, 15, 8, 2, 9, 12, 11, 9, 1, 2, 4, 5, 7, 8, 11, 12, 1, 2, 4, 12, 13, 1, 3, 6, 12, 1, 2, 3, 4, 7, 8, 11, 13, 14, 1, 11, 12, 3, 6, 12, 4, 5, 7, 8, 9, 10, 11, 1, 2, 15, 1, 3, 5, 15, 1, 5, 6, 8, 1, 2, 4, 7, 8, 11, 11, 10, 2, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 2, 5, 8, 10, 12, 1, 2, 3, 4, 5, 7, 9, 11, 12, 13, 14, 8, 1, 2, 14, 3, 1, 4, 6, 10, 14, 5, 10, 12, 13, 1, 2, 3, 5, 6, 7, 8, 10, 7, 2, 6, 7, 11, 14, 1, 12, 10, 1, 6, 13, 9, 1, 3, 6, 8, 11, 6, 8, 11, 3, 8, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 1, 5, 10, 13, 14, 3, 11, 1, 2, 6, 9, 12, 5, 15, 6, 4, 1, 2, 6, 10, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 6, 11, 12, 13, 14, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 9, 13, 2, 10, 14, 2, 3, 5, 7, 9, 11, 14, 7, 2, 12, 11, 3, 13, 4, 7, 13, 1, 4, 1, 2, 3, 4, 6, 10, 2, 1, 7, 11, 12, 9, 2, 15, 13, 1, 3, 1, 5, 1, 6, 14, 1, 4, 14, 9, 11, 3, 1, 2, 3, 8, 11, 13, 5, 2, 15, 4, 1, 3, 5, 9, 13, 1, 2, 6, 11, 12, 14, 2, 3, 8, 10, 14, 7, 14, 8, 11, 7, 15, 2, 15, 1, 2, 3, 5, 6, 8, 9, 11, 6, 10, 1, 2, 7, 1, 6, 13, 14, 4, 7, 8, 7, 9, 11, 2, 9, 1, 2, 4, 5, 7, 8, 1, 3, 4, 5, 8, 2, 8, 9, 14, 15, 1, 5, 6, 13, 1, 2, 3, 4, 7, 8, 11, 13, 14, 1, 2, 3, 4, 5, 6, 9, 12, 13, 14, 15, 1, 2, 3, 4, 7, 8, 9, 11, 12, 14, 2, 5, 9, 11, 2, 9, 13, 15, 11, 12, 10], \"Freq\": [0.01985066425911736, 0.09925332129558681, 0.8535785631420465, 0.016962107947296075, 0.16114002549931272, 0.02544316192094411, 0.23746951126214505, 0.1356968635783686, 0.08481053973648038, 0.25443161920944113, 0.07632948576283234, 0.9848574778296185, 0.046777117843862535, 0.3341222703133038, 0.44772384221982714, 0.10023668109399116, 0.06682445406266077, 0.23504636926159772, 0.1949165013388859, 0.498756929896561, 0.06879405929607738, 0.05042419238747369, 0.9328475591682633, 0.939154854565325, 0.9709764618796031, 0.9197448822694717, 0.2128936391054653, 0.16661241321297282, 0.6109121817809003, 0.9630584469731776, 0.029785312792984873, 0.12838251254084462, 0.7061038189746455, 0.027510538401609563, 0.13755269200804782, 0.9889590437401591, 0.9867799435839173, 0.003310767886352849, 0.5098582544983388, 0.06290458984070413, 0.006621535772705698, 0.11918764390870255, 0.15560609065858388, 0.13905225122681966, 0.06585854019738016, 0.2963634308882107, 0.5927268617764214, 0.970018795550686, 0.3908549297152688, 0.5862823945729032, 0.12876014047650394, 0.22890691640267366, 0.6366473612449361, 0.9823120935274854, 0.979832956127494, 0.1358517773820764, 0.07192152920227575, 0.39157277010127906, 0.2237558686293023, 0.07991281022475083, 0.08790409124722591, 0.09957930185416969, 0.07966344148333575, 0.07966344148333575, 0.7169709733500217, 0.2680405187987731, 0.00864646834834752, 0.7090104045644966, 0.49245941606285404, 0.14363399635166577, 0.07328265119982946, 0.019053489311955663, 0.013190877215969303, 0.008793918143979536, 0.03957263164790791, 0.019053489311955663, 0.0410382846719045, 0.05422916188787381, 0.038106978623911325, 0.032244366527924964, 0.02198479535994884, 0.4365201933169413, 0.5300602347420001, 0.8851580771615811, 0.9260478921178203, 0.5515315217434357, 0.15732210620222592, 0.057865602281278496, 0.01808300071289953, 0.009041500356449765, 0.001808300071289953, 0.02350790092676939, 0.016274700641609578, 0.01265810049902967, 0.04701580185353878, 0.06509880256643831, 0.01808300071289953, 0.02350790092676939, 0.8955054597745711, 0.06633373776107934, 0.016583434440269836, 0.9706461079173818, 0.0499563197808362, 0.9291875479235534, 0.009991263956167241, 0.9810223575390145, 0.1865428402831516, 0.20584037548485692, 0.6046561029867672, 0.40189310972476133, 0.592792336844023, 0.8570554281612037, 0.2585554696719147, 0.6722442211469781, 0.13516549384981352, 0.09990493023681869, 0.5465387360014199, 0.12928873324764772, 0.08815140903248708, 0.8951643620685689, 0.0637287431445984, 0.318643715722992, 0.6054230598736848, 0.9592557222431043, 0.033856084314462506, 0.06473835295580518, 0.9171266668739069, 0.23621750256111806, 0.47596063948882, 0.049358881132173925, 0.03525634366583852, 0.031730709299254665, 0.16570481522944103, 0.003525634366583852, 0.9558455768995039, 0.23527445481027084, 0.694059641690299, 0.023527445481027084, 0.035291168221540624, 0.924633253380675, 0.015475033529383682, 0.05416261735284289, 0.9130399134107621, 0.961336760945559, 0.1979849989440465, 0.6063290592661423, 0.11136656190602616, 0.03712218730200872, 0.03712218730200872, 0.951526205257455, 0.8907042829294696, 0.06142788158134273, 0.9810851650588895, 0.00804168168081057, 0.2040358063198224, 0.0892656652649223, 0.1020179031599112, 0.5993551810644783, 0.8387707802167594, 0.09319675335741771, 0.22219560219829879, 0.08416500083268894, 0.3097272030642953, 0.18516300183191567, 0.09089820089930405, 0.10773120106584184, 0.35494266567562954, 0.6235479261869168, 0.009593045018260258, 0.17509144998290063, 0.8170934332535362, 0.9857484141321138, 0.7525620207677836, 0.22576860623033507, 0.5177591363815705, 0.21992196326887095, 0.0012566969329649768, 0.035187514123019355, 0.021363847860404607, 0.0037700907988949305, 0.09425226997237326, 0.054037968117494006, 0.03393081719005438, 0.0025133938659299537, 0.012566969329649768, 0.006283484664824884, 0.3340219880911786, 0.1274104490657073, 0.13774102601698088, 0.0482093591059433, 0.2582644237818391, 0.03443525650424522, 0.04476583345551878, 0.010330576951273564, 0.9101305731689472, 0.9774612892057486, 0.1493027983435188, 0.07998364196974221, 0.06398691357579378, 0.7038560493337315, 0.9831684906576562, 0.32359057548344133, 0.5061288488330749, 0.15764669062013809, 0.7960545127420203, 0.18092148016864099, 0.27429154313698956, 0.21593164034188542, 0.1653530579194618, 0.08170386391314582, 0.07392254354046528, 0.05641457270193403, 0.07392254354046528, 0.0019453300931701388, 0.027234621304381944, 0.027234621304381944, 0.9308945021339793, 0.6662132399019081, 0.30282419995541277, 0.02605914997635222, 0.93812939914868, 0.2878283767372999, 0.6941743203664292, 0.9209867796720494, 0.1233839834962337, 0.8636878844736359, 0.9866885596555561, 0.9767216479521018, 0.04091539109988771, 0.9410539952974173, 0.9742179644330664, 0.01651216888869604, 0.9273786987066599, 0.023477941739409112, 0.046955883478818225, 0.9763889924487912, 0.26814211623263595, 0.7150456432870292, 0.21544258624862958, 0.700188405308046, 0.9139551429099024, 0.017344149863973427, 0.8325191934707245, 0.13875319891178742, 0.3931026223265878, 0.5765505127456622, 0.9664396935871814, 0.9491909450638794, 0.04232061538501373, 0.037020567850252165, 0.34552529993568687, 0.1912729338929695, 0.2961645428020173, 0.1110617035507565, 0.018510283925126082, 0.14589621495692018, 0.4741626986099906, 0.045592567174037556, 0.32826648365307043, 0.9704227295403531, 0.029859160908933945, 0.9670198884778982, 0.49978294979064924, 0.49209398133233156, 0.975141145886431, 0.9395761498567496, 0.05219867499204165, 0.9329979579643833, 0.9897072198802676, 0.2594806746646448, 0.4041092474285452, 0.04253781551879423, 0.03828403396691481, 0.04253781551879423, 0.21268907759397115, 0.9706754158357402, 0.06377922345753503, 0.9088539342698742, 0.8615374353856132, 0.961416505171444, 0.2578011157904376, 0.644502789476094, 0.10625308618001565, 0.7319657047956634, 0.11805898464446184, 0.03541769539333855, 0.99146802197693, 0.7572762941126239, 0.19037672198362054, 0.04653653204044057, 0.09169351798489113, 0.1995682450259395, 0.09708725433694354, 0.2750805539546734, 0.33441165382724997, 0.5403126366183263, 0.01200694748040725, 0.01200694748040725, 0.432250109294661, 0.9783744899044116, 0.2770156987166992, 0.0692539246791748, 0.6509868919842431, 0.08417159038357973, 0.7575443134522176, 0.41001445907467204, 0.0347469880571756, 0.07644337372578631, 0.45171084474328277, 0.02084819283430536, 0.06287917654840629, 0.616565258932984, 0.003493287586022572, 0.15370465378499315, 0.006986575172045144, 0.14147814723391416, 0.013973150344090288, 0.30461647964623995, 0.01097717043770234, 0.1646575565655351, 0.22777628658232354, 0.24973062745772823, 0.03842009653195819, 0.9600723853174252, 0.12693656708708426, 0.15622962103025756, 0.47845321440516375, 0.009764351314391097, 0.15134744537306202, 0.07811481051512878, 0.18492154640258476, 0.493124123740226, 0.2876557388484652, 0.10351599850121398, 0.08626333208434499, 0.41406399400485594, 0.379558661171118, 0.9763938483381321, 0.9736715471614524, 0.772737995044655, 0.1854571188107172, 0.20223024877982468, 0.10111512438991234, 0.6066907463394741, 0.9820532715887615, 0.9056262926407662, 0.23747705023141022, 0.2896883165943444, 0.18863425266608472, 0.07579054794619475, 0.11621217351749862, 0.0033684687976086557, 0.018526578386847604, 0.0421058599701082, 0.026947750380869245, 0.10218542187165118, 0.868576085909035, 0.8961958180109769, 0.9575507933541, 0.07293099197019348, 0.2552584718956772, 0.6199134317466446, 0.7029540595385657, 0.16737001417584899, 0.09433582617184215, 0.0334740028351698, 0.12906411620229935, 0.8604274413486623, 0.17274547273178206, 0.01328811328706016, 0.47172802169063566, 0.08415805081804767, 0.04650839650471056, 0.05425979592216232, 0.09190945023549943, 0.0166101416088252, 0.025468883800198638, 0.02325419825235528, 0.30771349352040955, 0.037555583854853136, 0.06057352234653732, 0.06663087458119105, 0.061784992793468066, 0.10660939932990568, 0.0969176357544597, 0.04845881787722985, 0.02301793849168418, 0.03634411340792239, 0.07268822681584478, 0.014537645363168957, 0.06299646324039881, 0.003634411340792239, 0.05433071874009507, 0.8692914998415211, 0.12047190523292703, 0.006340626591206686, 0.8496439632216959, 0.025362506364826745, 0.05302829950988006, 0.24746539771277362, 0.1590848985296402, 0.512606895262174, 0.5485450649450544, 0.03344786981372283, 0.050171804720584245, 0.28765168039801636, 0.026758295850978265, 0.006689573962744566, 0.050171804720584245, 0.434371168475704, 0.04108916458553957, 0.31403861504662384, 0.21131570358277493, 0.01494855994554839, 0.807222237059613, 0.01494855994554839, 0.022422839918322585, 0.007474279972774195, 0.04484567983664517, 0.08969135967329034, 0.23209902637919894, 0.759596813604651, 0.3077476546541826, 0.12537867411837067, 0.558505002890924, 0.9788855543386418, 0.4489250396211158, 0.5179904303320567, 0.38487838439223404, 0.15425087000933893, 0.11231859466699437, 0.007487906311132958, 0.03818832218677808, 0.034444369031211605, 0.05990325048906366, 0.04268106597345786, 0.035193159662324905, 0.04118348471123127, 0.011980650097812733, 0.045676228497911045, 0.02995162524453183, 0.0022463718933398874, 0.8239712098542922, 0.07490647362311748, 0.9711417222021261, 0.31799685644960063, 0.6359937128992013, 0.9825102206239402, 0.9566597577283801, 0.94810857892167, 0.9771610901552892, 0.04330660820748371, 0.06495991231122555, 0.887785468253416, 0.0711259906828524, 0.9104126807405107, 0.9891181007462895, 0.009390361715945787, 0.36834983840400276, 0.10608475346035279, 0.032414785779552244, 0.04125518190124831, 0.038308383194016284, 0.047148779315712354, 0.08840396121696066, 0.15618033148329716, 0.12376554570374493, 0.3558250027976962, 0.5577257066323441, 0.011994101217899872, 0.023988202435799744, 0.015992134957199828, 0.031984269914399656, 0.06185825644628106, 0.03092912822314053, 0.2680524446005513, 0.05154854703856755, 0.2061941881542702, 0.3608398292699729, 0.01030970940771351, 0.9845395754758252, 0.9966482663055426, 0.9775862018909848, 0.3954519438718176, 0.5437464228237492, 0.8316297855439317, 0.11088397140585755, 0.19010416880990444, 0.20889773682913454, 0.15613118046745003, 0.3325015880325325, 0.012288102166419679, 0.058549192675293765, 0.020239227097632415, 0.015902249862425468, 0.005059806774408104, 0.9235702687788917, 0.07496376122322812, 0.23322059047226526, 0.683003157811634, 0.9643086897144474, 0.927670376115761, 0.06343045306774434, 0.34639977928811705, 0.6350662620282146, 0.2683299302175152, 0.5016603043197023, 0.07166575775995748, 0.026666328468821388, 0.04833272034973876, 0.06333253011345079, 0.018333100822314704, 0.16306894077393605, 0.25737387037814, 0.2966675910465584, 0.12770459217235958, 0.06483463910289024, 0.049117150835522906, 0.025540918434471913, 0.013752802233946414, 0.5792295017896363, 0.16845600723188528, 0.06997403377324465, 0.036282832326867596, 0.01943723160367907, 0.04924098672932031, 0.04146609408784868, 0.03498701688662233, 0.35701333827493875, 0.02822896163104167, 0.04981581464301471, 0.033210543095343145, 0.04815528748824756, 0.06642108619068629, 0.05645792326208334, 0.04815528748824756, 0.2357948559769363, 0.0581184504168505, 0.01826579870243873, 0.9840546293098473, 0.9960789650889438, 0.9939239162954144, 0.9625318425962638, 0.017500578956295705, 0.021718099278439006, 0.8470058718591212, 0.10859049639219502, 0.03263061173449836, 0.2936755056104852, 0.6607698876235918, 0.9800349195580638, 0.09584263422546484, 0.010649181580607204, 0.7773902553843259, 0.11714099738667924, 0.857735251481774, 0.07147793762348116, 0.22652621786807886, 0.056631554467019715, 0.6795786536042366, 0.32034559863045975, 0.6406911972609195, 0.9862891124279936, 0.9711793427390449, 0.7964617637055583, 0.04321885539487526, 0.08335064969011657, 0.030870610996339472, 0.04013179429524131, 0.8961148896277003, 0.056007180601731266, 0.1798775858358288, 0.8094491362612296, 0.09067269479525175, 0.8160542531572658, 0.10927675173009238, 0.12488771626296272, 0.7649372621106467, 0.09751907176258073, 0.16829904320316352, 0.0015728882542351732, 0.015728882542351733, 0.02516621206776277, 0.04718664762705519, 0.022020435559292423, 0.5709584362873679, 0.05033242413552554, 0.9310647393464732, 0.296964270152822, 0.6533213943362085, 0.972473200798385, 0.9770751556797597, 0.9299790748851458, 0.22339149713153794, 0.05180092687108127, 0.003237557929442579, 0.09550795891855608, 0.048563368941638686, 0.006475115858885158, 0.21367882334321023, 0.05827604272996642, 0.050182147906359977, 0.06960749548301545, 0.003237557929442579, 0.09550795891855608, 0.05989482169468771, 0.016187789647212895, 0.003237557929442579, 0.7974488300190613, 0.08739165260482865, 0.09831560918043222, 0.1230077394479293, 0.8610541761355051, 0.981402053448522, 0.45558774578138905, 0.5225859436904169, 0.9892437882231624, 0.9860908209027448, 0.9691159004442712, 0.9704462758828234, 0.9355320764311319, 0.14576781712133385, 0.17940654414933396, 0.022425818018666745, 0.07849036306533361, 0.5662519049713354, 0.9742259709445484, 0.9897655047699377, 0.11454816185684605, 0.840019853616871, 0.13913771365687197, 0.8348262819412318, 0.9853938359617788, 0.9105737588001305, 0.08277943261819368, 0.03680526679853103, 0.14722106719412412, 0.8097158695676826, 0.466621803357427, 0.5249495287771054, 0.1666894175901492, 0.07292662019569028, 0.7501023791556715, 0.7429266628517065, 0.2355621126115167, 0.4392467130367113, 0.5572532926585143, 0.9805512542705223, 0.9432251196605196, 0.04491548188859617, 0.6361997438514598, 0.3534443021396999, 0.3389233362333804, 0.6354812554375883, 0.1761532504330272, 0.7829033352578987, 0.019572583381447465, 0.08829717832222003, 0.7946746048999803, 0.24309230823574984, 0.7479763330330764, 0.9338644106473878, 0.29332670642474723, 0.689317760098156, 0.9691569361956995, 0.9205846071825159, 0.0657560433701797, 0.9884026853795701, 0.00874523262029243, 0.0874523262029243, 0.01749046524058486, 0.874523262029243, 0.87959093707404, 0.7799486082332386, 0.0052345544176727425, 0.04187643534138194, 0.015703663253018228, 0.07851831626509113, 0.07851831626509113, 0.9084426965510968, 0.8024584524688537, 0.00443347211308759, 0.031034304791613128, 0.15517152395806563, 0.113185164819952, 0.8677529302862986, 0.9798085438009626, 0.29979678374781693, 0.6595529242451973, 0.640718300113375, 0.01327913575364508, 0.258943147196079, 0.02655827150729016, 0.01327913575364508, 0.03983740726093524, 0.00331978393841127, 0.32617578928244195, 0.19784433120410413, 0.0026735720432987044, 0.06416572903916891, 0.11496359786184429, 0.18715004303090932, 0.0026735720432987044, 0.06416572903916891, 0.04277715269277927, 0.8754014315475962, 0.2416628025080696, 0.7249884075242088, 0.9501567990855105, 0.31641934690624085, 0.06435647733686255, 0.1287129546737251, 0.05094887789168285, 0.09921623589432976, 0.06167495744782661, 0.03754127844650315, 0.13139447456276104, 0.021452159112287515, 0.05094887789168285, 0.032178238668431274, 0.9812358281311391, 0.12550684577415938, 0.815794497532036, 0.15887028009405618, 0.8170471547694318, 0.9485168703037711, 0.8644238656250574, 0.9801416269801337, 0.9873753749193908, 0.8754769780065921, 0.8948827282575798, 0.9460267085121262, 0.9903332227406328, 0.134375437119697, 0.3307703067561772, 0.06374219453113832, 0.049960098416297595, 0.2911467804260101, 0.013782096114840716, 0.115425054961791, 0.9932526649541884, 0.9345396110756454, 0.39745767454937064, 0.16482986503221161, 0.09018992614970069, 0.09081192564038829, 0.07090794193838537, 0.07090794193838537, 0.009951991851001455, 0.009329992360313864, 0.009329992360313864, 0.036697969950567864, 0.049137959764319686, 0.2713912166497641, 0.6106302374619693, 0.9684755517263116, 0.4316826501543729, 0.13129518391791986, 0.061668950022053276, 0.045754382274426623, 0.0696262338958666, 0.005967962905359995, 0.03381845646370663, 0.13129518391791986, 0.035807777432159967, 0.05172234517978662, 0.04265792542806251, 0.9384743594173752, 0.9944986311132857, 0.938197984712093, 0.09521180615612607, 0.13601686593732296, 0.7616944492490085, 0.9661507425780852, 0.11932411470230125, 0.8770322430619143, 0.7237649658376536, 0.2598130646596705, 0.9286552640230545, 0.8689035146851264, 0.7716625693414696, 0.03785514491109096, 0.034943210687160886, 0.011647736895720295, 0.058238684478601484, 0.078622224046112, 0.005823868447860148, 0.979589433965436, 0.8247825536079728, 0.15767901760152422, 0.9483389250023979, 0.9622292958802106, 0.4878059265489117, 0.025015688540969833, 0.08338562846989944, 0.04169281423494972, 0.01667712569397989, 0.300188262491638, 0.04169281423494972, 0.890493765715854, 0.021719360139411073, 0.07963765384450727, 0.12471336067468712, 0.13250794571685506, 0.5456209529517562, 0.19486462605419863, 0.8669849425942477, 0.8089778973884835, 0.04494321652158242, 0.13482964956474724, 0.8551002853348576, 0.12826504280022863, 0.08694924640607829, 0.869492464060783, 0.0839861075899892, 0.35630469886662086, 0.38430006806328393, 0.09416624184332123, 0.08144107402665621, 0.9868345274537621, 0.942052828018974, 0.3053972019880875, 0.648969054224686, 0.9566597506596958, 0.2345704228992413, 0.15638028193282752, 0.5920110673171327, 0.9444456655093606, 0.20051201962749576, 0.1348270476805575, 0.06914207573361923, 0.05877076437357635, 0.05531366058689538, 0.37336720896154385, 0.11062732117379076, 0.19269034201180069, 0.025692045601573427, 0.11882571090727709, 0.07065312540432692, 0.26976647881652094, 0.08349914820511363, 0.03211505700196678, 0.03211505700196678, 0.07065312540432692, 0.03853806840236014, 0.06744161970413023, 0.93806070628686, 0.04613413309607508, 0.00768902218267918, 0.10660738419370276, 0.1510271276077456, 0.02220987170702141, 0.004441974341404282, 0.5330369209685137, 0.13770120458353272, 0.03997776907263854, 0.9700493734931029, 0.8773690733322652, 0.012101642390789865, 0.054457390758554394, 0.054457390758554394, 0.053291727620466654, 0.18894339792710904, 0.169564587883303, 0.11627286026283634, 0.2761480431242363, 0.19378810043806055, 0.047877557907744346, 0.9216429897240787, 0.0279285754461842, 0.14910393585215856, 0.8200716471868722, 0.17881483525975445, 0.39462584333187184, 0.02466411520824199, 0.35454665611847863, 0.02466411520824199, 0.02466411520824199, 0.09825569203226796, 0.8351733822742777, 0.985042260927232, 0.9563518190475389, 0.9512843530010525, 0.364077825832246, 0.546116738748369, 0.9892462486723957, 0.6926206847023408, 0.02872620351300031, 0.27449483356866966, 0.0536981509678035, 0.9128685664526593, 0.29446599445539484, 0.0026291606647803113, 0.16826628254593992, 0.04995405263082591, 0.2550285844836902, 0.07624565927862903, 0.07361649861384871, 0.07887481994340934, 0.17098935342834462, 0.7694520904275508, 0.8163407716654028, 0.8773567761670511, 0.08616896908783538, 0.02350062793304601, 0.7387011162305942, 0.1507553298429784, 0.09045319790578704, 0.9585903614147229, 0.019523884584391183, 0.3319060379346501, 0.6247643067005179, 0.9841559970493268, 0.4441325749766366, 0.14804419165887886, 0.3004426242489012, 0.07402209582943943, 0.008708481862286992, 0.02177120465571748, 0.9888389357767049, 0.9892363878186413, 0.7560774158042034, 0.17447940364712386, 0.3103638804313914, 0.041941064923161006, 0.15098783372337962, 0.09227034283095421, 0.40263422326234566, 0.008388212984632201, 0.37915239930456046, 0.05268066604818746, 0.07494010240657653, 0.11352312542778424, 0.05639057210791897, 0.07271415877073763, 0.0007419812119463023, 0.0051938684836241155, 0.01929151151060386, 0.0333891545375836, 0.00964575575530193, 0.07419812119463022, 0.051196703624294855, 0.053422647260133765, 0.003709906059731511, 0.9743375819113943, 0.24847800536803427, 0.7276855871492433, 0.21044039892279595, 0.020870122207219434, 0.07478460457586963, 0.2869641803492672, 0.015652591655414574, 0.3739230228793482, 0.015652591655414574, 0.984994607717255, 0.8648022676318377, 0.001652642146913029, 0.7585627454330803, 0.05618983299504299, 0.029747558644434523, 0.15204307751599866, 0.9829535685016948, 0.6977635069395879, 0.2907347945581616, 0.11923025266538712, 0.05961512633269356, 0.7054456616035405, 0.10929439827660485, 0.8348782027246457, 0.12844280041917627, 0.03211070010479407, 0.3981662730224323, 0.5513071472618293, 0.9511033222308406, 0.9843352553195973, 0.2935754717513474, 0.17489602572420698, 0.10931001607762936, 0.021862003215525873, 0.059339723013570225, 0.28420604180183634, 0.05309343638056283, 0.2954291307526246, 0.3039922939628456, 0.042815816051105016, 0.1006171677200968, 0.042815816051105016, 0.07706846889198903, 0.019267117222997256, 0.02568948963066301, 0.07920925969454429, 0.012844744815331505, 0.9886641232768352, 0.9271420607894543, 0.1975659426344619, 0.4677071295019914, 0.12095874038844606, 0.020159790064741008, 0.032255664103585616, 0.16127832051792806, 0.03034453468307582, 0.10620587139076537, 0.2882730794892203, 0.5462016242953648, 0.26301263267255015, 0.36441509346196704, 0.02851944209702351, 0.2218178829768495, 0.09189598009040909, 0.02851944209702351, 0.24512945211155904, 0.008170981737051969, 0.13890668952988347, 0.49842988596017007, 0.09805178084462361, 0.24181119426187866, 0.3825557903028483, 0.055399468654424217, 0.0007486414683030299, 0.02545380992230302, 0.08459648591824238, 0.0014972829366060598, 0.055399468654424217, 0.022459244049090898, 0.05090761984460604, 0.034437507541939374, 0.033688866073636345, 0.011978263492848478, 0.9314468389513365, 0.20602241544216707, 0.6831269564661329, 0.10843285023271951, 0.012426715266416047, 0.795309777050627, 0.18640072899624072, 0.2908181117034304, 0.15306216405443707, 0.025510360675739512, 0.045068303860473134, 0.041666922437041204, 0.06972831918035466, 0.05612279348662692, 0.10799386019396393, 0.08078280880650845, 0.04846968528390507, 0.03231312352260338, 0.045068303860473134, 0.0034013814234319347, 0.18062063411747453, 0.7439849929124546, 0.004300491288511298, 0.06450736932766947, 0.11317650803742914, 0.06287583779857174, 0.13832684315685784, 0.12575167559714348, 0.5533073726274313, 0.0421093704334606, 0.9264061495361331, 0.2130087395779023, 0.7455305885226581, 0.07505469267632557, 0.9173351327106459, 0.05745182977053404, 0.07181478721316756, 0.1292666169837016, 0.5745182977053405, 0.1651740105902854, 0.8851043304203998, 0.29963204611582905, 0.4185336517173485, 0.15219405516994491, 0.04280457801654701, 0.08085309180903323, 0.9379399640189123, 0.9908211591473434, 0.338185612018707, 0.563642686697845, 0.7639304125160304, 0.1527860825032061, 0.08045494761995677, 0.9137383336837946, 0.5683464207772198, 0.37889761385147985, 0.9721885476695992, 0.9875613241907266, 0.028054155630750276, 0.17954659603680176, 0.04488664900920044, 0.4432556589658544, 0.302984880812103, 0.9507507153822379, 0.9315940021593537, 0.9540742529162892, 0.03577778448436084, 0.3610250984144711, 0.003139348681864966, 0.028254138136784696, 0.08162306572848912, 0.08476241441035409, 0.05964762495543436, 0.11929524991086872, 0.02511478945491973, 0.14441003936578845, 0.05650827627356939, 0.03453283550051463, 0.9652250172383968, 0.9849309931270281, 0.4100180970807359, 0.06938767796750915, 0.03153985362159507, 0.17662318028093238, 0.05046376579455211, 0.2586267996970796, 0.040285108841492855, 0.9265575033543357, 0.7837173925896165, 0.14694701111055308, 0.9332435954201855, 0.048606437261467995, 0.9164052343642642, 0.04382807642611699, 0.03585933525773208, 0.2947292701300805, 0.6832360353015502, 0.23769415800681024, 0.7130824740204308, 0.7777747648164739, 0.059977655993310175, 0.02321715715870071, 0.13736817985564587, 0.25983213514635395, 0.025557259194723337, 0.008519086398241112, 0.03833588879208501, 0.03833588879208501, 0.21723670315514837, 0.37483980152260893, 0.03407634559296445, 0.08189448651575439, 0.8189448651575438, 0.09757907887473749, 0.3090004164366687, 0.11384225868719373, 0.47163221456123117, 0.987032766904849, 0.2911268719429778, 0.059718332706251855, 0.5673241607093926, 0.08211270747109631, 0.941343696651236, 0.051080665709756994, 0.9810414199486877, 0.19951148504158983, 0.12825738324102204, 0.08550492216068137, 0.06650382834719662, 0.08550492216068137, 0.16625957086799154, 0.03325191417359831, 0.042752461080340684, 0.1757601177747339, 0.01425082036011356, 0.12781365741984257, 0.8307887732289767, 0.9061937553267402, 0.07116836887515003, 0.18187472045871672, 0.06326077233346669, 0.07907596541683336, 0.10279875504188338, 0.36374944091743344, 0.1344291412086167, 0.10659485138428303, 0.8764465558263271, 0.11120188112925575, 0.21947739696563637, 0.02341092234300121, 0.34238473926639273, 0.07901186290762909, 0.055600940564627874, 0.1316864381793818, 0.038042748807376965, 0.04454723353693537, 0.8374879904943849, 0.10691336048864489, 0.9523280511164602, 0.7222513992348369, 0.06246498587976969, 0.06246498587976969, 0.04684873940982726, 0.03904061617485605, 0.031232492939884844, 0.03904061617485605, 0.306141225823726, 0.16459205689447634, 0.04773169649939814, 0.05760721991306672, 0.06912866389568006, 0.01481328512050287, 0.08065010787829341, 0.039502093654674324, 0.07077458446462483, 0.09217155186090675, 0.036210252516784794, 0.019751046827337162, 0.17574274825439312, 0.020435203285394547, 0.03678336591371019, 0.08582785379865711, 0.4127911063649699, 0.15939458562607747, 0.02860928459955237, 0.07356673182742038, 0.9654637151358113, 0.9746350822172994, 0.13620519712061585, 0.8172311827236951, 0.9566597413865735, 0.5101677836466013, 0.319135794175407, 0.06742305510748035, 0.017979481361994763, 0.011237175851246726, 0.008989740680997381, 0.022474351702493452, 0.03371152755374018, 0.0067423055107480355, 0.8266528704263294, 0.975106562533491, 0.2636146951266767, 0.054273613702551085, 0.5737496305698258, 0.10079385401902344, 0.5229682292892961, 0.18013350119964644, 0.2905379051607201, 0.3481273653460022, 0.6202996691619675, 0.025318353843345614, 0.9051298469158215, 0.28569559078444035, 0.13239551768059432, 0.16026825824492996, 0.1951091839503495, 0.08013412912246498, 0.14284779539222017, 0.41793489764091524, 0.12009623495428598, 0.04083271988445723, 0.11529238555611454, 0.20176167472320045, 0.07926351506982875, 0.021617322291771476, 0.048127150148290505, 0.9144158528175196, 0.9444182490980866, 0.3042576512933377, 0.2129803559053364, 0.04653352313898106, 0.00536925266988243, 0.11454405695749184, 0.08590804271811887, 0.07695928826831483, 0.15391857653662966, 0.3595038500338952, 0.13636352932320162, 0.14876021380712903, 0.024793368967854838, 0.012396684483927419, 0.3161154543401492, 0.40924183252815527, 0.05115522906601941, 0.3467187747807982, 0.18756917324207117, 0.2943658390798083, 0.6793057824918652, 0.9290032550536691, 0.029252540972571184, 0.01755152458354271, 0.07020609833417084, 0.8834267373716497, 0.8908657839082847, 0.11625240057165506, 0.8137668040015855, 0.9853217420046221, 0.08152555099718992, 0.8560182854704942, 0.04076277549859496, 0.28022582953163483, 0.3955086075035099, 0.0798111539805289, 0.03901878639048079, 0.021282974394807707, 0.14011291476581741, 0.040792367590048104, 0.005320743598701927, 0.19290494038809014, 0.05015528450090344, 0.2777831141588498, 0.09259437138628326, 0.10416866780956867, 0.273925015351088, 0.15047915042888463, 0.7523957521444231, 0.32807959221879157, 0.15059391118239612, 0.04840518573719876, 0.0430268317663989, 0.26353934456919326, 0.09143201750359765, 0.0699186016203982, 0.8983414986841343, 0.057957516044137694, 0.03622344752758606, 0.9320220535929788, 0.056850320822057855, 0.9285552400936117, 0.5375118028160931, 0.007465441705779071, 0.09705074217512794, 0.35834120187739543, 0.3339792303162145, 0.11132641010540482, 0.5009688454743217, 0.578961613428117, 0.4135440095915122, 0.9452608065771255, 0.030823721953601918, 0.010274573984533972, 0.9684679480295315, 0.04574694905840765, 0.8691920321097453, 0.9763291756867443, 0.00986852731564891, 0.11348806412996247, 0.05921116389389346, 0.08881674584084019, 0.018092300078689668, 0.023026563736514124, 0.6430990300697873, 0.044408372920420096, 0.9037102438673672, 0.944051798958208, 0.9612454836042718, 0.1520420877737289, 0.1520420877737289, 0.023391090426727525, 0.6783416223750982, 0.10240667290951816, 0.1365422305460242, 0.7509822680031332, 0.2122252305373806, 0.6366756916121419, 0.03077390168766769, 0.9232170506300307, 0.9064234018597787, 0.9712969317366585, 0.026548281107008173, 0.970908566199156, 0.987316913551455, 0.9751457007176441, 0.5251033178590803, 0.4594654031266952, 0.12491732288143825, 0.33977511823751205, 0.05396428348478133, 0.2008670551933527, 0.11692261421702621, 0.03997354332206024, 0.021985448827133134, 0.03997354332206024, 0.013990740162721085, 0.026982141742390664, 0.01798809449492711, 0.9831085176852755, 0.9656798345185568, 0.015575481201912206, 0.015575481201912206, 0.11967476392883332, 0.641114806761607, 0.08548197423488094, 0.1453193561992976, 0.2621639160830968, 0.7165813706271312, 0.047195270609415946, 0.03146351373961063, 0.015731756869805314, 0.07865878434902657, 0.03146351373961063, 0.047195270609415946, 0.11798817652353985, 0.6135385179224072, 0.16779514954020688, 0.16414742889802847, 0.12037478119188753, 0.047420368348319336, 0.46690824219883653, 0.03282948577960569, 0.2546735041716098, 0.17874599982230996, 0.06801838931291442, 0.17083688478592457, 0.026890991123710347, 0.03321828315281866, 0.10440031848028723, 0.015818230072770792, 0.012654584058216635, 0.08541844239296228, 0.04903651322558946, 0.5812635179163755, 0.09533951881168065, 0.021528278441347245, 0.009226405046291675, 0.28294308808627805, 0.009226405046291675, 0.9347218695155374, 0.9907146529168106, 0.9113572768137007, 0.07723366752658481, 0.13881027937355267, 0.5893348703228025, 0.11932813490007159, 0.04383482506533242, 0.05601116536125809, 0.053575897302072954, 0.15491317063712384, 0.02816603102493161, 0.08449809307479482, 0.2534942792243845, 0.46473951191137153, 0.9193646861519563, 0.9658573032870044, 0.27790012428194527, 0.704675315143504, 0.12149890663499051, 0.4123599255490587, 0.0817356280999027, 0.06406305986208591, 0.05817220378281363, 0.037554207505360704, 0.06111763182244977, 0.0066272130891813, 0.07510841501072141, 0.019145282257634868, 0.022090710297271, 0.0287179233864523, 0.0110453551486355, 0.8352961473743357, 0.04460319233552278, 0.1135353986722398, 0.9718013159827428, 0.5857203447893724, 0.4067502394370642, 0.2814718241031722, 0.26703737158506075, 0.4474680280614532, 0.3909680778125432, 0.5375811069922469, 0.06614757417372387, 0.033073787086861935, 0.8929922513452723, 0.30917020213782354, 0.05796941290084191, 0.5024015784739633, 0.11593882580168383, 0.35078318186506086, 0.14554884262460732, 0.09842871371736035, 0.012565367708599194, 0.045025900955813776, 0.10261716962022675, 0.07120375034872876, 0.03979033107723078, 0.01780093758718219, 0.002094227951433199, 0.05654415468869638, 0.011518253732882594, 0.046073014931530376, 0.6371216764810762, 0.08016762816649303, 0.11181274454800344, 0.016877395403472217, 0.078057953741059, 0.016877395403472217, 0.054851535061284706, 0.0063290232763020815, 0.950031675440788, 0.9712541613575693, 0.13988409558592052, 0.19671200941770073, 0.4983370905248419, 0.03934240188354014, 0.12239858363768046, 0.8982059059352704, 0.31123498597638366, 0.5147347844994038, 0.15561749298819183, 0.9807711067188233, 0.9002763656644689, 0.08402579412868376, 0.9695375200207864, 0.5227885697032757, 0.05885699129109727, 0.3496797717882838, 0.0034621759582998393, 0.024235231708098874, 0.010386527874899519, 0.027697407666398714, 0.8738639501827452, 0.1028075235509112, 0.8645287140294768, 0.3020088834748653, 0.2348957982582286, 0.07829859941940953, 0.38030748289427485, 0.6569418433839821, 0.31908603821507703, 0.15620551997991247, 0.13884935109325552, 0.04049772740219953, 0.11570779257771294, 0.4570457806819661, 0.09256623406217035, 0.021007554134573152, 0.15755665600929866, 0.07352643947100604, 0.5251888533643289, 0.19957176427844495, 0.021007554134573152, 0.2458325379979088, 0.7374976139937264, 0.9239263043309611, 0.14048283342779583, 0.4328390002910466, 0.034171500022977364, 0.1442796667636822, 0.06454616671006835, 0.020882583347375056, 0.02467941668326143, 0.020882583347375056, 0.11390500007659121, 0.2374761455153231, 0.7124284365459692, 0.03850964521870104, 0.13685517172829473, 0.2817606476759009, 0.41056551518488416, 0.032201216877245815, 0.12075456328967181, 0.9641788388380557, 0.926879197660674, 0.9386625932890356, 0.960018277333028, 0.030000571166657126, 0.923273470186068, 0.06441442815251637, 0.9784145209520445, 0.9718616342725194, 0.031904092330796406, 0.09571227699238923, 0.8614104929315031, 0.9738231153403469, 0.00238015435260651, 0.5521958098047104, 0.4427087095848109, 0.5751871617315787, 0.029123400594003982, 0.13348225272251824, 0.07280850148500996, 0.002426950049500332, 0.10193190207901394, 0.05581985113850763, 0.02669645054450365, 0.002426950049500332, 0.989538619472223, 0.9600877925216363, 0.9931812862483589, 0.9608394804991226, 0.6335672736049638, 0.025091773212077773, 0.3356024667115402, 0.18767323443760353, 0.3143526676829859, 0.09383661721880177, 0.07506929377504142, 0.32373632940486613, 0.05110158418806966, 0.02920090525032552, 0.05840181050065104, 0.5329165208184408, 0.12410384731388346, 0.19710611043969725, 0.1011151569369595, 0.06500260088804538, 0.6933610761391508, 0.01083376681467423, 0.025278789234239873, 0.1047264125418509, 0.9588151274998337, 0.10244290398110241, 0.8952619000087645, 0.970796646189666, 0.09379217270428156, 0.8753936119066278, 0.8641294925115439, 0.9503454691912265, 0.9339453489021271, 0.07112318477047934, 0.853478217245752, 0.9860422317623253, 0.08603917054692749, 0.8603917054692749, 0.5442371723473447, 0.26686533161433507, 0.0483299419459032, 0.09035597842060163, 0.016810414589879374, 0.03151952735602383, 0.028933398542518817, 0.17938707096361667, 0.05208011737653387, 0.08680019562755645, 0.06944015650204516, 0.3992808998867597, 0.011573359417007526, 0.08101351591905269, 0.0925868753360602, 0.23380093447726683, 0.6897127567079372, 0.031173457930302243, 0.042863504654165586, 0.01644789722338748, 0.9704259361798615, 0.9731028202254852, 0.5412439628167965, 0.11598084917502784, 0.05003095454609044, 0.04093441735589218, 0.04548268595099131, 0.03638614876079305, 0.03411201446324348, 0.047756820248540874, 0.027289611570594785, 0.0591274917362887, 0.23924374156735073, 0.27605047103925084, 0.03680672947190011, 0.2990546769591884, 0.14262607670361294, 0.1470256775695581, 0.049008559189852696, 0.24504279594926348, 0.5390941510883797, 0.9761538368949598, 0.9348957280648289, 0.9041061100169191, 0.9647170367599065, 0.1508162722391442, 0.8187169064410686, 0.8911827517682146, 0.17316334264160602, 0.21212509473596738, 0.05844262814154203, 0.038961752094361354, 0.36797210311341283, 0.1493533830283852, 0.34510555105933627, 0.06529023938960415, 0.19587071816881246, 0.3824142592819672, 0.9281524890338341, 0.9564633574584978, 0.9281578081148345, 0.7747960330103242, 0.049986840839375754, 0.12496710209843939, 0.022216373706389226, 0.01943932699309057, 0.008331140139895958, 0.0332200911886955, 0.132880364754782, 0.7972821885286919, 0.39068095853984025, 0.011269643034803083, 0.5972910808445635, 0.9554442601013058, 0.9932672409772572, 0.043014882071106694, 0.9033125234932406, 0.6060282915846914, 0.03591278764946319, 0.11222746140457249, 0.24241131663387658, 0.9222059132621738, 0.07207983616305556, 0.2306554757217778, 0.6775504599327222, 0.9403329254675034, 0.30167589123454164, 0.5727128247655752, 0.009427371601079426, 0.018854743202158852, 0.023568429002698565, 0.023568429002698565, 0.04949370090566699, 0.7654774902549946, 0.22223540039661133, 0.9694415676580321, 0.08621872753168508, 0.26943352353651584, 0.05927537517803349, 0.46342566048280726, 0.09699606847314571, 0.016166011412190952, 0.9265335311222805, 0.969742894515037, 0.9801791505080302, 0.1790703637890084, 0.7886716022196754, 0.030480061921533345, 0.13481826674856714, 0.09629876196326226, 0.5007535622089637, 0.0770390095706098, 0.18296764773019827, 0.16879305172153136, 0.3002946152720267, 0.1550540823953602, 0.05495587730468463, 0.22178621912247726, 0.013738969326171158, 0.06280671691963957, 0.021589808941126106, 0.9745895313283026, 0.9644145059804713, 0.9917494113049024, 0.17178628802301313, 0.14466213728253738, 0.16274490444285455, 0.2411035621375623, 0.006027589053439058, 0.07835865769470775, 0.12959316464893975, 0.06630347958782963, 0.9408132265322472, 0.9082295883691547, 0.9151736364775325, 0.25625168881059557, 0.08153462825791678, 0.6522770260633343, 0.9519359941212594, 0.04672078498754647, 0.3015350703524217, 0.009422970948513179, 0.46172557647714574, 0.051826340216822475, 0.04711485474256589, 0.08009525306236201, 0.051826340216822475, 0.35331895193774693, 0.02884236342348955, 0.6129002227491529, 0.26253478919404877, 0.11054096387117841, 0.6079753012914813, 0.9259793335700106, 0.17150386543619422, 0.17150386543619422, 0.6002635290266798, 0.9889758945044238, 0.9686805278484787, 0.12407173309252063, 0.8478235094655576, 0.9639035078651568, 0.019425420979234814, 0.004856355244808703, 0.4346437944103789, 0.1554033678338785, 0.09955528251857841, 0.08741439440655666, 0.15783154545628286, 0.04127901958087398, 0.18884443190681433, 0.049988231975333204, 0.13885619993148113, 0.32214638384103617, 0.06109672796985169, 0.23327841588488826, 0.006157951641043422, 0.4064248083088659, 0.09852722625669476, 0.12931698446191187, 0.3510032435394751, 0.09608433868567517, 0.18187392679788514, 0.06520008696527958, 0.3019793501549791, 0.19216867737135035, 0.058336919916302785, 0.10294750573465197, 0.9411011935467778, 0.9449717806870979, 0.16094055434692214, 0.8047027717346107, 0.228787490085394, 0.7487590584612894, 0.008966938687215034, 0.17933877374430066, 0.1076032642465804, 0.035867754748860135, 0.1076032642465804, 0.47524775042239675, 0.0807024481849353, 0.9345222888851499, 0.12506639924118776, 0.7347650955419781, 0.12506639924118776, 0.18513556709248624, 0.27967287794822393, 0.12211069318866115, 0.3230024787571037, 0.003939054618989069, 0.03939054618989069, 0.051207710046857896, 0.9627868567974297, 0.03105764054185257, 0.011419386928049799, 0.7251310699311623, 0.09135509542439839, 0.11990356274452289, 0.034258160784149394, 0.014274233660062249, 0.9601581485974064, 0.8892657146144307, 0.9550553706812884, 0.25068769475188124, 0.22283350644611663, 0.03481773538220573, 0.48744829535088013, 0.06778525540333794, 0.8812083202433932, 0.09351065479141148, 0.21039897328067583, 0.6935373563696352, 0.9715121654857722, 0.3328356965414799, 0.1490309088991701, 0.07120365647404793, 0.003311797975537113, 0.046365171657519584, 0.08610674736396494, 0.008279494938842783, 0.19705197954445822, 0.05133286862082525, 0.0380856767186768, 0.014903090889917009, 0.0016558989877685565, 0.8839523132053991, 0.10278515269830223, 0.9111423938535795, 0.974361284912889, 0.9851774996011288, 0.05217169408078351, 0.6260603289694021, 0.026085847040391755, 0.2869443174443093, 0.961145605836548, 0.029802964522063505, 0.366967654845308, 0.6324336179248925, 0.778277168951098, 0.1945692922377745, 0.13089079849234941, 0.3209941010645712, 0.04674671374726765, 0.1745210646564659, 0.05297960891357001, 0.2742473873173036, 0.207112431134814, 0.1109126680394086, 0.050929286344626394, 0.08488214390771066, 0.09733152501417489, 0.13920671600864548, 0.08488214390771066, 0.10412209652679175, 0.006790571512616853, 0.021503476456620034, 0.054324572100934825, 0.014712904944003182, 0.009054095350155804, 0.015844666862772656, 0.08975653149666583, 0.8526870492183255, 0.9441165392679968, 0.5319614023118635, 0.007880909663879459, 0.3585813897065154, 0.09851137079849323, 0.003940454831939729, 0.9535114033271563, 0.9817482181297131, 0.9835960427263832, 0.956659749054829, 0.9660232472513728, 0.14498598458047585, 0.7974229151926171, 0.9833306623190531, 0.38017765731645353, 0.6002805115522951, 0.361576976731342, 0.5372000797151366, 0.08264616611002103, 0.057360909643823996, 0.9321147817121399, 0.9751568504916328, 0.1031157980097558, 0.8764842830829244, 0.26435725217137995, 0.671060717050426, 0.06100551973185691, 0.4884605427895606, 0.3076064810605144, 0.0695592545111716, 0.006183044845437476, 0.040189791495343595, 0.001545761211359369, 0.004637283634078107, 0.021640656959031165, 0.026277940593109273, 0.012366089690874951, 0.01700337332495306, 0.0077288060567968445, 0.5600037538353957, 0.19091037062570307, 0.24181980279255721, 0.9630391097712201, 0.1602020930616923, 0.5363287463369698, 0.20895925181959865, 0.08358370072783945, 0.53994594914883, 0.45238714658415485, 0.8890490121110858, 0.9850121127331939, 0.8367031718888713, 0.1904578672879355, 0.8019278622649916, 0.7806968117835787, 0.21777332118173512, 0.9725857412839857, 0.15295077741262778, 0.8361309165223653, 0.153149876664449, 0.8167993422103946, 0.9607592284691794, 0.3837905744892369, 0.5904470376757491, 0.05621535818150733, 0.8994457309041173, 0.3067360171507457, 0.6856452148075493, 0.08743998320205279, 0.7869598488184751, 0.8914534974704642, 0.03164923659658453, 0.021099491064389686, 0.015824618298292265, 0.021099491064389686, 0.015824618298292265, 0.4680525599675504, 0.02340262799837752, 0.4992560639653871, 0.9907900958800461, 0.0034226715202214145, 0.5681634723567548, 0.03764938672243556, 0.026240481655030845, 0.027381372161771316, 0.017113357601107073, 0.08100322597857348, 0.2373052254020181, 0.967143163218092, 0.01782752374595561, 0.01337064280946671, 0.542861126083285, 0.23545783781925614, 0.1242694144046074, 0.045783468464855356, 0.04251322071736569, 0.009810743242469005, 0.10075774393824277, 0.3425763293900254, 0.025189435984560693, 0.20655337507339766, 0.2770837958301676, 0.050378871969121386, 0.7256471171586646, 0.23966326805240298, 0.026629252005822557, 0.27451042388069746, 0.17902853731349835, 0.5370856119404951, 0.7862595662514581, 0.20549965936117656, 0.9324829078454319, 0.9905978939990169, 0.27480128528213227, 0.06341568121895359, 0.5848335045747942, 0.06341568121895359, 0.01409237360421191, 0.9903547163443134, 0.02511508725930279, 0.02511508725930279, 0.06697356602480743, 0.8706563583224967, 0.05652336362892049, 0.13188784846748117, 0.07536448483856066, 0.5087102726602845, 0.22609345451568197, 0.8414179066086506, 0.33436992450017367, 0.5349918792002778, 0.04533744095782259, 0.18134976383129037, 0.1360123228734678, 0.5893867324516937, 0.9760964656284121, 0.7256552954778026, 0.03904871545172032, 0.23429229271032193, 0.878486572271172, 0.9652441513583525, 0.6402621658452841, 0.07978344745735627, 0.13563186067750566, 0.019945861864339068, 0.029918792796508602, 0.057842999406583294, 0.015956689491471256, 0.017951275677905162, 0.4657337362374446, 0.024329374281060537, 0.21896436852954484, 0.2050618689403674, 0.0834149975350647, 0.12955550384698247, 0.015546660461637897, 0.7255108215431019, 0.12437328369310317, 0.14301185224295399, 0.15074222263446502, 0.003865185195755513, 0.40197926035857334, 0.16620296341748705, 0.06957333352359923, 0.02319111117453308, 0.003865185195755513, 0.03865185195755513, 0.059142259271208975, 0.019714086423736323, 0.9068479754918709, 0.11986778117799685, 0.839074468245978, 0.9383921462603628, 0.11345735036070048, 0.01689790324521071, 0.5865986412266004, 0.18104896334154333, 0.019311889423097955, 0.04827972355774489, 0.03620979266830866, 0.6108522995067829, 0.04442562178231148, 0.3331921633673361, 0.9936907767328341, 0.8178747911856783, 0.15335152334731467, 0.9813882948474837, 0.9506579880446697, 0.018328881517570752, 0.020772732386580187, 0.009775403476037734, 0.10511685040090417, 0.10511685040090417, 0.015767527560135625, 0.11037269292094938, 0.5413517795646564, 0.12088437796103979, 0.7859123861289737, 0.9522713764463984, 0.07752150971408517, 0.23256452914225548, 0.038760754857042584, 0.6395524551412026, 0.2834466671994537, 0.11655106266838852, 0.021379189010177775, 0.1379302516785663, 0.061378961996962, 0.010344768875892472, 0.0055172100671426515, 0.09586152491660357, 0.017930932718213617, 0.05172384437946236, 0.14413711300410179, 0.024137794043749102, 0.020689537751784944, 0.008965466359106809, 0.17286704276977202, 0.40335643312946806, 0.006402483065547112, 0.12164717824539513, 0.03841489839328267, 0.25609932262188445, 0.22414591829928157, 0.42819599564759303, 0.058741688933604826, 0.07883752988457489, 0.02318750878958085, 0.10666254043207192, 0.02009584095097007, 0.01236667135444312, 0.01236667135444312, 0.00618333567722156, 0.032462512305413194, 0.9551851493758796, 0.02496776107269979, 0.3745164160904968, 0.5742585046720952, 0.9583056194624102, 0.20840198305757554, 0.08650648353333325, 0.6212738362848479, 0.027524790215151487, 0.05111746754242419, 0.042683243446683936, 0.9054945216903663, 0.021341621723341968, 0.03048803103334567, 0.058176541711133, 0.0407235791977931, 0.4770476420312906, 0.0639941958822463, 0.1454413542778325, 0.005817654171113299, 0.034905925026679795, 0.1628943167911724, 0.8744721393359391, 0.016930138573535727, 0.16930138573535727, 0.4147883950516253, 0.3470678407574824, 0.050790415720607184, 0.2595817928424145, 0.7231207086324404, 0.97764984923311, 0.9888278337577396, 0.04488401502565508, 0.8976803005131017, 0.960655074901413, 0.9571691977125575, 0.03226413025997385, 0.3330551954197341, 0.030625765096067507, 0.6354846257434007, 0.8418466245529167, 0.10078445505210974, 0.0533564762040581, 0.16011869869075687, 0.07116386608478083, 0.7561160771507963, 0.16584541438019332, 0.3456567583924029, 0.005237223612006105, 0.25313247458029503, 0.019203153244022383, 0.01047444722401221, 0.006982964816008139, 0.019203153244022383, 0.14140503752416483, 0.03316908287603866, 0.023302744287739145, 0.29516809431136254, 0.13981646572643489, 0.5359631186180004, 0.958939219291874, 0.12453619824055069, 0.8094852885635795, 0.04362206426780412, 0.7670212967088891, 0.07633861246865721, 0.054527580334755146, 0.054527580334755146, 0.00616061064431822, 0.992738400970136, 0.9655220655845606, 0.9625877954434717, 0.40672957658324427, 0.040672957658324425, 0.13363971802020883, 0.05229380270355998, 0.3602461964023021, 0.4985756723234132, 0.071530128577449, 0.07473297015554374, 0.05978637612443499, 0.07473297015554374, 0.0042704554374596426, 0.014946594031108748, 0.008540910874919285, 0.02135227718729821, 0.071530128577449, 0.010676138593649106, 0.044839782093326244, 0.04590739595269115, 0.25704281505095505, 0.09131784218915509, 0.13415831136431428, 0.07215236913711019, 0.055241657620599995, 0.09695474602799183, 0.09019046142138774, 0.030439280729718365, 0.047349992246228566, 0.010146426909906121, 0.019165473052044895, 0.08117141527924897, 0.015783330748742854, 0.09799211796175358, 0.01660883355283959, 0.7656672267859052, 0.013287066842271673, 0.08636593447476587, 0.01826971690812355, 0.003321766710567918, 0.34930469048374185, 0.09769824676365194, 0.1017132432059938, 0.11643489682791396, 0.06022494663512791, 0.012044989327025581, 0.046841625160655036, 0.02944330724384031, 0.08699158958407364, 0.0508566216029969, 0.046841625160655036, 0.14316627037395593, 0.278104594174696, 0.044430911495365634, 0.3291178629286344, 0.0872162336760881, 0.023038250405004405, 0.011519125202502202, 0.024683839719647577, 0.059241215327154186, 0.041910247149713115, 0.12573074144913934, 0.754384448694836, 0.5345476037909732, 0.03702494225392022, 0.057851472271750345, 0.14578571012481087, 0.14347165123394084, 0.01851247112696011, 0.060165531162620356, 0.8266369280707686, 0.2711648733625775, 0.6779121834064438, 0.864372433874189, 0.222818802475994, 0.7622748505757689, 0.007100196230617149, 0.8733241363659093, 0.11360313968987439, 0.1325034894097349, 0.8391887662616544, 0.848348752679225, 0.1362311865616274, 0.006192326661892154, 0.003096163330946077, 0.006192326661892154, 0.9711271415653302, 0.9776531680086559, 0.09661683416258428, 0.25764489110022476, 0.10735203795842697, 0.5260249859962922, 0.9778558194250706, 0.9909502696067539, 0.9109413039806467, 0.9346234647884173, 0.9637500574851867, 0.9791991489752964, 0.8332558038023283, 0.16127531686496677, 0.979568008598953, 0.1537375521890622, 0.8418961191305787, 0.14614607885787526, 0.24357679809645877, 0.584584315431501, 0.9280482652188309, 0.9215167904817069, 0.977093568343927, 0.15844886319313883, 0.23511766796401248, 0.07666880477087364, 0.41401154576271765, 0.05622379016530733, 0.06133504381669891, 0.9440958390565788, 0.6744189128419701, 0.3078868949930733, 0.9961735574083826, 0.6384521042278792, 0.022168475841245806, 0.053204342018989936, 0.2394195390854547, 0.04433695168249161, 0.4928217011410699, 0.1901438059520663, 0.0077609716715129114, 0.02716340085029519, 0.04268534419332101, 0.24059012181690026, 0.057171547952009974, 0.12069549012090995, 0.3366768934951698, 0.39384844144717984, 0.08893351903645996, 0.5030474853886606, 0.48570102037525853, 0.2052862532174303, 0.7527162617972444, 0.563620845663724, 0.3856353154541269, 0.39893751657275067, 0.5699107379610724, 0.04386823636790506, 0.1535388272876677, 0.09212329637260062, 0.11844423819334365, 0.11844423819334365, 0.048255060004695566, 0.17985976910841073, 0.24127530002347783, 0.026953541199427753, 0.9433739419799714, 0.09763594897252337, 0.14645392345878505, 0.7322696172939253, 0.8344666770665373, 0.06510023721795681, 0.07101844060140743, 0.017754610150351856, 0.12776866997756217, 0.8177194878563979, 0.038330600993268654, 0.16476621847308617, 0.7084947394342705, 0.11533635293116032, 0.29332114485319766, 0.6994581146499329, 0.4530060245955662, 0.24005447457200946, 0.02710292454845268, 0.22843893547981545, 0.007743692728129337, 0.042590310004711356, 0.21887198052799747, 0.08754879221119899, 0.043774396105599495, 0.21887198052799747, 0.4268003620295951, 0.014617410778213599, 0.1388654023930292, 0.8112662981908547, 0.007308705389106799, 0.021926116167320396, 0.5325125206578967, 0.011364596477455111, 0.1152694785570447, 0.34093789432365335, 0.341031487334948, 0.298928834577547, 0.04420778539527104, 0.0021051326378700493, 0.023156459016570544, 0.23787998807931557, 0.027366724292310643, 0.021051326378700493, 0.006315397913610148, 0.3654775273376578, 0.03399790951978213, 0.07139560999154246, 0.19718787521473632, 0.04419728237571676, 0.03229801404379302, 0.011899268331923743, 0.08669466927544442, 0.04419728237571676, 0.10709341498731369, 0.005099686427967319, 0.11067568526732227, 0.5698971853317341, 0.02477813849268409, 0.04377471133707523, 0.017344696944878862, 0.08754942267415046, 0.0635972221312225, 0.014040945145854317, 0.03386345594000159, 0.035515331839513865, 0.035509905807163344, 0.10652971742149003, 0.8522377393719203, 0.9972947868746964, 0.18825591583553117, 0.38278702886558, 0.40161262044913315, 0.018825591583553115, 0.12319356378331586, 0.8623549464832111, 0.9566599001386925], \"Term\": [\"abbreviation\", \"abbreviation\", \"abbreviation\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"accident\", \"account\", \"account\", \"account\", \"account\", \"account\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"achieve\", \"achieve\", \"achieves\", \"acid\", \"acl\", \"acquired\", \"acquired\", \"acquired\", \"act\", \"act\", \"activity\", \"activity\", \"activity\", \"activity\", \"administration\", \"adult\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"aged\", \"aged\", \"aged\", \"agreement\", \"ai\", \"ai\", \"air\", \"air\", \"air\", \"airline\", \"airport\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"allowed\", \"allowed\", \"allowed\", \"allowed\", \"along\", \"along\", \"along\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alternative\", \"alternative\", \"ambient\", \"analyse\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"animal\", \"animal\", \"animal\", \"annotated\", \"annual\", \"annual\", \"annual\", \"anonymous\", \"answer\", \"answer\", \"answer\", \"answered\", \"answered\", \"apart\", \"apartment\", \"apartment\", \"api\", \"api\", \"api\", \"api\", \"api\", \"appreciate\", \"approximate\", \"approximate\", \"approximate\", \"arabic\", \"arabic\", \"architecture\", \"architecture\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"array\", \"art\", \"art\", \"art\", \"art\", \"article\", \"article\", \"article\", \"artificial\", \"artist\", \"arxiv\", \"arxiv\", \"arxiv\", \"arxiv\", \"arxiv\", \"assignment\", \"athlete\", \"athlete\", \"attack\", \"attack\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attended\", \"attended\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribute\", \"attribution\", \"attribution\", \"attribution\", \"audio\", \"audio\", \"australian\", \"auto\", \"auto\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"averaged\", \"award\", \"back\", \"back\", \"back\", \"back\", \"band\", \"bank\", \"bank\", \"bank\", \"baseball\", \"baseball\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"basically\", \"battle\", \"battle\", \"beneficial\", \"beneficial\", \"benefit\", \"benefit\", \"bet\", \"bias\", \"bias\", \"bike\", \"bird\", \"birth\", \"birth\", \"bitcoin\", \"bitcoin\", \"book\", \"book\", \"book\", \"border\", \"borough\", \"borough\", \"boston\", \"boston\", \"bot\", \"brand\", \"brand\", \"brand\", \"brazilian\", \"brazilian\", \"breast\", \"bureau\", \"bureau\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"calculated\", \"calculated\", \"calculated\", \"calculated\", \"calculation\", \"calculation\", \"calendar\", \"california\", \"california\", \"cancer\", \"candidate\", \"candidate\", \"canonical\", \"caput\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"carbon\", \"card\", \"card\", \"carnegie\", \"carolina\", \"carry\", \"carry\", \"cause\", \"cause\", \"cause\", \"cause\", \"cell\", \"census\", \"census\", \"census\", \"center\", \"center\", \"center\", \"center\", \"center\", \"character\", \"character\", \"character\", \"character\", \"chest\", \"child\", \"child\", \"child\", \"christian\", \"christian\", \"citation\", \"citation\", \"citation\", \"citation\", \"citation\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classic\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classifier\", \"classifier\", \"classifier\", \"click\", \"click\", \"click\", \"click\", \"climate\", \"closed\", \"club\", \"club\", \"cm\", \"cm\", \"cm\", \"cnn\", \"coco\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coded\", \"coded\", \"coin\", \"collaborative\", \"collecting\", \"collecting\", \"collecting\", \"collection\", \"collection\", \"collection\", \"collection\", \"college\", \"college\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"command\", \"command\", \"comment\", \"comment\", \"comment\", \"comment\", \"communication\", \"communication\", \"communication\", \"communication\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"company\", \"company\", \"company\", \"company\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"compute\", \"compute\", \"conference\", \"conference\", \"conference\", \"conflict\", \"congress\", \"congress\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"contains\", \"continue\", \"continue\", \"contribute\", \"convenience\", \"convenience\", \"conversion\", \"convnet\", \"convolution\", \"convolutional\", \"coordinate\", \"coordinate\", \"coordinate\", \"copy\", \"copy\", \"corpus\", \"corpus\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"course\", \"course\", \"course\", \"course\", \"course\", \"course\", \"course\", \"court\", \"crime\", \"critical\", \"crypto\", \"crypto\", \"cryptocurrencies\", \"cryptocurrencies\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csv\", \"csvs\", \"customer\", \"customer\", \"customer\", \"da\", \"damage\", \"damage\", \"dangerous\", \"dangerous\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"database\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dc\", \"de\", \"death\", \"debate\", \"debate\", \"decimal\", \"decimal\", \"decimal\", \"deep\", \"deep\", \"deep\", \"defense\", \"degree\", \"degree\", \"degree\", \"degree\", \"delay\", \"delay\", \"delivery\", \"delivery\", \"delivery\", \"demo\", \"demo\", \"denotes\", \"density\", \"department\", \"department\", \"department\", \"department\", \"department\", \"dependent\", \"dependent\", \"depth\", \"depth\", \"derive\", \"derive\", \"describing\", \"describing\", \"describing\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"descriptive\", \"determines\", \"determines\", \"diabetes\", \"diagnosis\", \"died\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"digit\", \"digit\", \"digit\", \"dimensional\", \"dimensional\", \"dioxide\", \"direction\", \"direction\", \"directory\", \"disaster\", \"disclaimer\", \"disclosure\", \"distribute\", \"district\", \"district\", \"district\", \"district\", \"district\", \"divide\", \"dog\", \"dot\", \"dot\", \"double\", \"double\", \"driving\", \"drug\", \"drug\", \"ease\", \"ease\", \"ease\", \"eastern\", \"eastern\", \"easy\", \"easy\", \"easy\", \"edge\", \"edge\", \"education\", \"education\", \"educational\", \"el\", \"el\", \"election\", \"election\", \"electricity\", \"electricity\", \"else\", \"else\", \"else\", \"em\", \"em\", \"email\", \"email\", \"embedded\", \"embeddings\", \"embeddings\", \"emergency\", \"emission\", \"emission\", \"employment\", \"en\", \"en\", \"en\", \"en\", \"encounter\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"engineer\", \"english\", \"english\", \"english\", \"english\", \"enjoy\", \"enjoy\", \"epa\", \"episode\", \"episode\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"everyday\", \"ex\", \"ex\", \"exam\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"exchange\", \"excluded\", \"excluded\", \"exercise\", \"exercise\", \"exists\", \"expanded\", \"facebook\", \"facility\", \"fair\", \"fan\", \"fantasy\", \"fbi\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"federal\", \"fifa\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"filled\", \"filled\", \"film\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fips\", \"fips\", \"fire\", \"flavor\", \"float\", \"float\", \"float\", \"florida\", \"food\", \"food\", \"football\", \"football\", \"forfeiture\", \"fork\", \"format\", \"format\", \"format\", \"format\", \"format\", \"format\", \"format\", \"formula\", \"frame\", \"frame\", \"francisco\", \"fraud\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"frequency\", \"frequency\", \"frequency\", \"front\", \"front\", \"front\", \"front\", \"ft\", \"fuel\", \"fuel\", \"fuel\", \"function\", \"function\", \"gained\", \"gained\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gas\", \"gather\", \"gdp\", \"gdp\", \"generalise\", \"genre\", \"genre\", \"genre\", \"georgia\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"github\", \"global\", \"global\", \"global\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gold\", \"google\", \"google\", \"google\", \"google\", \"gov\", \"gov\", \"gov\", \"gov\", \"gov\", \"gov\", \"government\", \"government\", \"government\", \"gram\", \"gram\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"grouped\", \"grouped\", \"growth\", \"gz\", \"handwritten\", \"harvard\", \"harvard\", \"headline\", \"health\", \"health\", \"health\", \"heavy\", \"heavy\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hi\", \"hi\", \"hill\", \"historical\", \"historical\", \"historical\", \"hit\", \"hit\", \"hit\", \"horizontal\", \"horse\", \"horse\", \"horse\", \"hotel\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"household\", \"housing\", \"htm\", \"htm\", \"html\", \"html\", \"html\", \"html\", \"html\", \"html\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"http\", \"ic\", \"ice\", \"ice\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"ieee\", \"ilsvrc\", \"image\", \"image\", \"image\", \"image\", \"image\", \"imagenet\", \"imdb\", \"imdb\", \"impact\", \"impact\", \"impact\", \"impact\", \"improved\", \"improved\", \"improved\", \"improving\", \"improving\", \"inception\", \"incident\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"include\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"includes\", \"income\", \"indeed\", \"index\", \"index\", \"index\", \"index\", \"index\", \"index\", \"indian\", \"indian\", \"indian\", \"indian\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"info\", \"info\", \"info\", \"info\", \"info\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"injured\", \"input\", \"input\", \"input\", \"inside\", \"inside\", \"inside\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"inspiration\", \"instance\", \"instance\", \"instance\", \"instance\", \"institute\", \"institute\", \"institute\", \"institute\", \"institute\", \"instruction\", \"instruction\", \"instrument\", \"instrument\", \"integer\", \"integer\", \"interested\", \"interested\", \"interested\", \"interested\", \"interested\", \"intermediate\", \"international\", \"international\", \"international\", \"international\", \"international\", \"intersection\", \"investment\", \"irvine\", \"irvine\", \"island\", \"island\", \"job\", \"job\", \"john\", \"john\", \"join\", \"joining\", \"json\", \"json\", \"json\", \"json\", \"json\", \"judge\", \"jurisdiction\", \"justice\", \"justice\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"karen\", \"kera\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"killed\", \"killed\", \"km\", \"km\", \"la\", \"la\", \"label\", \"label\", \"label\", \"labeled\", \"labeled\", \"lang\", \"lang\", \"language\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"largely\", \"largely\", \"largest\", \"largest\", \"largest\", \"largest\", \"lat\", \"latitude\", \"latitude\", \"latitude\", \"latitude\", \"law\", \"law\", \"layer\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"leader\", \"leader\", \"leaderboard\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"left\", \"left\", \"left\", \"li\", \"license\", \"license\", \"license\", \"license\", \"license\", \"license\", \"license\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear\", \"literature\", \"loan\", \"loan\", \"localisation\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"logistic\", \"lon\", \"longitude\", \"longitude\", \"longitude\", \"longitude\", \"low\", \"low\", \"low\", \"lower\", \"lower\", \"lower\", \"luck\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"manner\", \"manner\", \"manufacturer\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"market\", \"market\", \"market\", \"market\", \"marketing\", \"marketing\", \"massachusetts\", \"match\", \"match\", \"match\", \"match\", \"matching\", \"math\", \"math\", \"matrix\", \"matter\", \"matter\", \"matter\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"medicine\", \"medicine\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"member\", \"member\", \"member\", \"merging\", \"message\", \"message\", \"metadata\", \"metadata\", \"metadata\", \"metadata\", \"mexico\", \"mexico\", \"mexico\", \"michigan\", \"michigan\", \"mining\", \"mining\", \"mining\", \"mission\", \"mistake\", \"mistake\", \"mnist\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modelling\", \"modify\", \"molecular\", \"money\", \"money\", \"money\", \"money\", \"monitor\", \"monitor\", \"monitor\", \"moreover\", \"moreover\", \"mother\", \"mother\", \"motion\", \"motor\", \"movie\", \"movie\", \"murder\", \"museum\", \"music\", \"music\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"narrative\", \"nationality\", \"nationality\", \"nationality\", \"negative\", \"negative\", \"negative\", \"negative\", \"neighborhood\", \"neighborhood\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"nice\", \"nlp\", \"nominal\", \"nominal\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"north\", \"north\", \"north\", \"north\", \"north\", \"northern\", \"notice\", \"null\", \"null\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numeric\", \"numeric\", \"numeric\", \"numerical\", \"nyc\", \"nyc\", \"object\", \"object\", \"object\", \"observe\", \"observe\", \"offensive\", \"offensive\", \"offensive\", \"old\", \"old\", \"old\", \"old\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"opened\", \"opponent\", \"others\", \"others\", \"others\", \"others\", \"others\", \"outlier\", \"output\", \"output\", \"output\", \"owe\", \"pain\", \"pain\", \"pakistan\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"park\", \"park\", \"participating\", \"party\", \"party\", \"party\", \"party\", \"passenger\", \"passenger\", \"past\", \"past\", \"past\", \"past\", \"past\", \"past\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"penalty\", \"penalty\", \"pennsylvania\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"percentage\", \"percentage\", \"percentage\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"phrase\", \"pick\", \"pipeline\", \"pixel\", \"pixel\", \"plan\", \"plan\", \"plane\", \"planning\", \"plant\", \"plant\", \"plant\", \"plate\", \"player\", \"player\", \"player\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"please\", \"pokemon\", \"policing\", \"policy\", \"pollution\", \"population\", \"population\", \"population\", \"position\", \"position\", \"position\", \"position\", \"position\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"post\", \"post\", \"post\", \"post\", \"post\", \"post\", \"postcode\", \"pre\", \"pre\", \"precipitation\", \"precise\", \"precise\", \"predictable\", \"predictor\", \"premier\", \"presence\", \"presence\", \"presidential\", \"prevention\", \"prevention\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"product\", \"profit\", \"profit\", \"programming\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"property\", \"property\", \"property\", \"property\", \"property\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"protein\", \"prove\", \"purchase\", \"pushing\", \"put\", \"put\", \"queen\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"race\", \"race\", \"race\", \"race\", \"racial\", \"rain\", \"raised\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rated\", \"rated\", \"rated\", \"rating\", \"rating\", \"rating\", \"ray\", \"reading\", \"recall\", \"recall\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recommend\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommender\", \"record\", \"record\", \"record\", \"record\", \"record\", \"record\", \"record\", \"reddit\", \"reddit\", \"reduced\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"reliably\", \"remaining\", \"remove\", \"report\", \"report\", \"report\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"research\", \"residual\", \"respondent\", \"restaurant\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retail\", \"retrieval\", \"retrieve\", \"revenue\", \"revenue\", \"revenue\", \"review\", \"review\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"risk\", \"risk\", \"risk\", \"road\", \"road\", \"road\", \"robot\", \"rock\", \"rock\", \"rock\", \"role\", \"room\", \"round\", \"round\", \"route\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"san\", \"save\", \"saved\", \"saved\", \"saving\", \"saving\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scene\", \"school\", \"school\", \"school\", \"science\", \"science\", \"science\", \"science\", \"science\", \"science\", \"science\", \"scientist\", \"scientist\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scored\", \"scrapped\", \"se\", \"search\", \"search\", \"search\", \"search\", \"searching\", \"searching\", \"season\", \"season\", \"season\", \"secured\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"segmentation\", \"segmentation\", \"selecting\", \"semantic\", \"sensor\", \"sent\", \"sent\", \"sent\", \"sent\", \"sentence\", \"sentence\", \"sentiment\", \"sentiment\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"severity\", \"severity\", \"shall\", \"share\", \"share\", \"share\", \"share\", \"share\", \"sheet\", \"shooting\", \"shot\", \"simonyan\", \"simulation\", \"smartphones\", \"smartphones\", \"software\", \"solar\", \"solar\", \"someone\", \"someone\", \"someone\", \"song\", \"song\", \"sort\", \"sorted\", \"sorted\", \"sound\", \"sound\", \"sound\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"south\", \"south\", \"south\", \"southern\", \"space\", \"space\", \"space\", \"space\", \"spanish\", \"spanish\", \"sparse\", \"speaker\", \"specially\", \"specie\", \"specie\", \"speech\", \"speech\", \"spoken\", \"sport\", \"sport\", \"spot\", \"spot\", \"sql\", \"st\", \"st\", \"stack\", \"stack\", \"staff\", \"staff\", \"stamp\", \"stamp\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"started\", \"started\", \"started\", \"startup\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"station\", \"station\", \"station\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"status\", \"status\", \"status\", \"status\", \"status\", \"status\", \"stock\", \"stock\", \"stock\", \"stop\", \"stop\", \"stop\", \"store\", \"store\", \"stream\", \"street\", \"string\", \"string\", \"string\", \"string\", \"string\", \"strongly\", \"student\", \"student\", \"student\", \"student\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"substantially\", \"successful\", \"successful\", \"sun\", \"sun\", \"sun\", \"sun\", \"super\", \"survey\", \"survey\", \"survey\", \"symposium\", \"symptom\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tag\", \"tag\", \"tag\", \"tag\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"taken\", \"talk\", \"talk\", \"talk\", \"tar\", \"tar\", \"teacher\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"technology\", \"technology\", \"technology\", \"temperature\", \"testing\", \"testing\", \"texas\", \"text\", \"text\", \"text\", \"text\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thinking\", \"thorough\", \"thought\", \"thought\", \"thought\", \"thought\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"tournament\", \"town\", \"town\", \"town\", \"tract\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"trained\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trait\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transcript\", \"transcript\", \"transferable\", \"translation\", \"transport\", \"transport\", \"treatment\", \"trip\", \"trip\", \"tweet\", \"tweet\", \"tweet\", \"twitter\", \"twitter\", \"twitter\", \"txt\", \"txt\", \"txt\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"uk\", \"uk\", \"uk\", \"uk\", \"un\", \"unfortunately\", \"unfortunately\", \"united\", \"united\", \"united\", \"united\", \"united\", \"university\", \"university\", \"unix\", \"unzip\", \"url\", \"url\", \"url\", \"url\", \"url\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"van\", \"van\", \"van\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variance\", \"varied\", \"varied\", \"vast\", \"vector\", \"vector\", \"vehicle\", \"vehicle\", \"vehicle\", \"venue\", \"venue\", \"version\", \"version\", \"version\", \"version\", \"version\", \"vgg\", \"victim\", \"video\", \"video\", \"video\", \"video\", \"violation\", \"violent\", \"virginia\", \"vocabulary\", \"voice\", \"vol\", \"volume\", \"volume\", \"volunteer\", \"vote\", \"vote\", \"voting\", \"voting\", \"voting\", \"walk\", \"walking\", \"wang\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"warranty\", \"washington\", \"washington\", \"weapon\", \"weather\", \"weather\", \"weather\", \"weather\", \"weather\", \"web\", \"web\", \"web\", \"web\", \"web\", \"web\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weighted\", \"weighted\", \"went\", \"went\", \"west\", \"west\", \"western\", \"western\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whether\", \"whichever\", \"whichever\", \"white\", \"white\", \"white\", \"wikipedia\", \"wikipedia\", \"wikipedia\", \"wikipedia\", \"win\", \"win\", \"win\", \"wine\", \"wine\", \"wine\", \"wise\", \"wise\", \"within\", \"within\", \"within\", \"within\", \"within\", \"within\", \"without\", \"without\", \"without\", \"without\", \"without\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"word\", \"word\", \"word\", \"word\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"www\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yes\", \"yes\", \"yes\", \"yet\", \"york\", \"york\", \"york\", \"york\", \"youtube\", \"youtube\", \"zisserman\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 7, 13, 14, 2, 12, 15, 5, 4, 8, 6, 9, 10, 11, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el160481400251030524964605192992\", ldavis_el160481400251030524964605192992_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el160481400251030524964605192992\", ldavis_el160481400251030524964605192992_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el160481400251030524964605192992\", ldavis_el160481400251030524964605192992_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.068629  0.058664       1        1  29.156134\n",
       "6      0.013592  0.115059       2        1  21.922253\n",
       "12     0.044034 -0.072011       3        1   6.764243\n",
       "13     0.031244  0.071950       4        1   5.282574\n",
       "1      0.085773 -0.055830       5        1   5.022337\n",
       "11     0.056284  0.072340       6        1   4.692795\n",
       "14     0.033161  0.023023       7        1   4.564492\n",
       "4      0.087815 -0.035102       8        1   4.564105\n",
       "3      0.022434  0.038958       9        1   3.585301\n",
       "7     -0.042497 -0.339621      10        1   3.206563\n",
       "5      0.109221 -0.086052      11        1   2.801800\n",
       "8      0.041775  0.008051      12        1   2.598891\n",
       "9      0.009327  0.038150      13        1   2.419514\n",
       "10    -0.107836  0.189321      14        1   2.130879\n",
       "0     -0.452956 -0.026900      15        1   1.288119, topic_info=               Term         Freq        Total Category  logprob  loglift\n",
       "580      university  1136.000000  1136.000000  Default  30.0000  30.0000\n",
       "567           state   876.000000   876.000000  Default  29.0000  29.0000\n",
       "773     description   635.000000   635.000000  Default  28.0000  28.0000\n",
       "890            word   615.000000   615.000000  Default  27.0000  27.0000\n",
       "604             csv  1383.000000  1383.000000  Default  26.0000  26.0000\n",
       "...             ...          ...          ...      ...      ...      ...\n",
       "993          school    16.328410   127.932043  Topic15  -5.1800   2.2934\n",
       "167   international    16.798250   210.257884  Topic15  -5.1516   1.8249\n",
       "1136      community    14.539854   298.972702  Topic15  -5.2960   1.3285\n",
       "373             set    14.369404   883.578059  Topic15  -5.3078   0.2331\n",
       "421           based    13.711353   514.051576  Topic15  -5.3547   0.7279\n",
       "\n",
       "[947 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "2391      2  0.019851  abbreviation\n",
       "2391      4  0.099253  abbreviation\n",
       "2391     13  0.853579  abbreviation\n",
       "85        2  0.016962          able\n",
       "85        3  0.161140          able\n",
       "...     ...       ...           ...\n",
       "1792     13  0.401613          york\n",
       "1792     15  0.018826          york\n",
       "1730     11  0.123194       youtube\n",
       "1730     12  0.862355       youtube\n",
       "2919     10  0.956660     zisserman\n",
       "\n",
       "[2428 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 7, 13, 14, 2, 12, 15, 5, 4, 8, 6, 9, 10, 11, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
