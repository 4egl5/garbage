{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(shuffle = True, subset= \"train\", remove = (\"headers\", \"footers\", \"quotes\"))\n",
    "test = fetch_20newsgroups(shuffle = True, subset= \"test\", remove = (\"headers\", \"footers\", \"quotes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.data\n",
    "y_train = train.target\n",
    "X_test = test.data\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stop = set(stopwords.words('english'))\n",
    "\n",
    "# for i,j in enumerate(X_train):\n",
    "#     X_train[i] = re.sub('\\n',' ', j)\n",
    "#     X_train[i] = re.sub(r'\\d','',X_train[i])\n",
    "#     X_train[i] = X_train[i].translate(str.maketrans('','', string.punctuation))\n",
    "#     X_train[i] = X_train[i].lower()\n",
    "#     X_train[i] = word_tokenize(X_train[i])\n",
    "#     X_train[i] = [k for k in X_train[i] if k not in stop]\n",
    "#     tmp = \"\"\n",
    "#     for k in X_train[i]:\n",
    "#         tmp+=k+' '\n",
    "#     X_train[i] = tmp\n",
    "\n",
    "# for i,j in enumerate(X_test):\n",
    "#     X_test[i] = re.sub('\\n',' ', j)\n",
    "#     X_test[i] = re.sub(r'\\d','',X_test[i])\n",
    "#     X_test[i] = X_test[i].translate(str.maketrans('','', string.punctuation))\n",
    "#     X_test[i] = X_test[i].lower()\n",
    "#     X_test[i] = word_tokenize(X_test[i])\n",
    "#     X_test[i] = [k for k in X_test[i] if k not in stop]\n",
    "#     tmp = \"\"\n",
    "#     for k in X_test[i]:\n",
    "#         tmp+=k+' '\n",
    "#     X_test[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim.models import Doc2Vec, Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vostok/.local/lib/python3.12/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clf = {'SGDClassifier':SGDClassifier(), 'LogisticRegression':LogisticRegression(), \"LinearSVC\":LinearSVC(),\"DecisionTreeClassifier\":DecisionTreeClassifier()}\n",
    "clf = [SGDClassifier(),LogisticRegression(),LinearSVC(),DecisionTreeClassifier(),MultinomialNB()]\n",
    "# ext = [TfidfTransformer()]\n",
    "score = {}\n",
    "# clf.values()\n",
    "for i in clf:\n",
    "#     print()\n",
    "    score[i] = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',i)]).fit(X_train,y_train).score(X_test,y_test)\n",
    "# params = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__max_iter': (20, 40),\n",
    "    # 'clf__alpha': (0.00001, 0.000001),\n",
    "    # 'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "# }\n",
    "# m2 = GridSearchCV(m1, params, cv = 5, n_jobs=-1, verbose=0)\n",
    "# m1.fit(X_train,y_train)\n",
    "# m2.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{SGDClassifier(): 0.6972915560276155,\n",
       " LogisticRegression(): 0.6736590546999469,\n",
       " LinearSVC(): 0.6919808815719597,\n",
       " DecisionTreeClassifier(): 0.39922995220392987,\n",
       " MultinomialNB(): 0.6062134891131173}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With preproocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for i,j in enumerate(X_train):\n",
    "    X_train[i] = re.sub('\\n',' ', j)\n",
    "    X_train[i] = re.sub(r'\\d','',X_train[i])\n",
    "    X_train[i] = X_train[i].translate(str.maketrans('','', string.punctuation))\n",
    "    X_train[i] = X_train[i].lower()\n",
    "    X_train[i] = word_tokenize(X_train[i])\n",
    "    X_train[i] = [k for k in X_train[i] if k not in stop]\n",
    "    tmp = \"\"\n",
    "    for k in X_train[i]:\n",
    "        tmp+=k+' '\n",
    "    X_train[i] = tmp\n",
    "\n",
    "for i,j in enumerate(X_test):\n",
    "    X_test[i] = re.sub('\\n',' ', j)\n",
    "    X_test[i] = re.sub(r'\\d','',X_test[i])\n",
    "    X_test[i] = X_test[i].translate(str.maketrans('','', string.punctuation))\n",
    "    X_test[i] = X_test[i].lower()\n",
    "    X_test[i] = word_tokenize(X_test[i])\n",
    "    X_test[i] = [k for k in X_test[i] if k not in stop]\n",
    "    tmp = \"\"\n",
    "    for k in X_test[i]:\n",
    "        tmp+=k+' '\n",
    "    X_test[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vostok/.local/lib/python3.12/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clf = {'SGDClassifier':SGDClassifier(), 'LogisticRegression':LogisticRegression(), \"LinearSVC\":LinearSVC(),\"DecisionTreeClassifier\":DecisionTreeClassifier()}\n",
    "clf = [SGDClassifier(),LogisticRegression(),LinearSVC(),DecisionTreeClassifier(),MultinomialNB()]\n",
    "# ext = [TfidfTransformer()]\n",
    "score = {}\n",
    "# clf.values()\n",
    "for i in clf:\n",
    "#     print()\n",
    "    score[i] = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',i)]).fit(X_train,y_train).score(X_test,y_test)\n",
    "# params = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__max_iter': (20, 40),\n",
    "    # 'clf__alpha': (0.00001, 0.000001),\n",
    "    # 'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "# }\n",
    "# m2 = GridSearchCV(m1, params, cv = 5, n_jobs=-1, verbose=0)\n",
    "# m1.fit(X_train,y_train)\n",
    "# m2.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{SGDClassifier(): 0.6894583112055231,\n",
       " LogisticRegression(): 0.6768454593733404,\n",
       " LinearSVC(): 0.6853425385023898,\n",
       " DecisionTreeClassifier(): 0.4200743494423792,\n",
       " MultinomialNB(): 0.6563993627190653}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "# model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.data\n",
    "y_train = train.target\n",
    "X_test = test.data\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import  Word2Vec\n",
    "# X_train = \n",
    "# [j.split() for i in X_train for j in i]\n",
    "# from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for i,j in enumerate(X_train):\n",
    "    X_train[i] = re.sub('\\n',' ', j)\n",
    "    X_train[i] = re.sub(r'\\d','',X_train[i])\n",
    "    X_train[i] = X_train[i].translate(str.maketrans('','', string.punctuation))\n",
    "    X_train[i] = X_train[i].lower()\n",
    "    X_train[i] = word_tokenize(X_train[i])\n",
    "    X_train[i] = [k for k in X_train[i] if k not in stop]\n",
    "    tmp = \"\"\n",
    "    for k in X_train[i]:\n",
    "        tmp+=k+' '\n",
    "    X_train[i] = tmp\n",
    "\n",
    "for i,j in enumerate(X_test):\n",
    "    X_test[i] = re.sub('\\n',' ', j)\n",
    "    X_test[i] = re.sub(r'\\d','',X_test[i])\n",
    "    X_test[i] = X_test[i].translate(str.maketrans('','', string.punctuation))\n",
    "    X_test[i] = X_test[i].lower()\n",
    "    X_test[i] = word_tokenize(X_test[i])\n",
    "    X_test[i] = [k for k in X_test[i] if k not in stop]\n",
    "    tmp = \"\"\n",
    "    for k in X_test[i]:\n",
    "        tmp+=k+' '\n",
    "    X_test[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 382)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0]), len(X_train[1]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model= keyedvectors.load_word2vec_format(model, binary =True)\n",
    "# w2v = Word2Vec(X_train, window=5, min_count=1, workers=-1)\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('~/gensim-data/word2vec-google-news-300/word2vec-google-news-300/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "# w2v\n",
    "# w2v = Word2Vec(X_train, vector_size = 300, window = 5, min_count =2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please email '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in enumerate(X_train):\n",
    "#     X_train[i] = word_tokenize(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_feats(list_of_list):\n",
    "    dimension = 300\n",
    "    feats = []\n",
    "    for tokens in list_of_list:\n",
    "        feat_for_this = np.zeros(dimension)\n",
    "        count_for_this = 0\n",
    "        for token in tokens:\n",
    "            if token in w2v:\n",
    "                feat_for_this +=w2v[token]\n",
    "                count_for_this+=1\n",
    "            # else:\n",
    "            #     feat_for_this = 0\n",
    "            #     count_for_this = 1\n",
    "        feats.append(feat_for_this/count_for_this)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7320/391571462.py:14: RuntimeWarning: invalid value encountered in divide\n",
      "  feats.append(feat_for_this/count_for_this)\n"
     ]
    }
   ],
   "source": [
    "train_vec = embed_feats(X_train)\n",
    "test_vec = embed_feats(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1542666453235554"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_vec:\n",
    "#     print(len(i))\n",
    "train_vec = np.nan_to_num(train_vec,nan = 0)\n",
    "test_vec = np.nan_to_num(test_vec,nan = 0)\n",
    "# ,len(train_vec[1]),len(train_vec[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vostok/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/vostok/.local/lib/python3.12/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # clf = {'SGDClassifier':SGDClassifier(), 'LogisticRegression':LogisticRegression(), \"LinearSVC\":LinearSVC(),\"DecisionTreeClassifier\":DecisionTreeClassifier()}\n",
    "clf = [SGDClassifier(),LogisticRegression(),LinearSVC(),DecisionTreeClassifier()]\n",
    "# # ext = [TfidfTransformer()]\n",
    "\n",
    "score = {}\n",
    "# # clf.values()\n",
    "for i in clf:\n",
    "# #     print()\n",
    "    score[i] = Pipeline([('clf',i)]).fit(train_vec,y_train).score(test_vec,y_test)\n",
    "\n",
    "# # m1 = SGDClassifier()\n",
    "# # m1.fit(train_vec,y_train)\n",
    "# SGDClassifier().fit(train_vec,y_train)\n",
    "# type(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{SGDClassifier(): 0.06386086032926182,\n",
       " LogisticRegression(): 0.1889272437599575,\n",
       " LinearSVC(): 0.21163037705788634,\n",
       " DecisionTreeClassifier(): 0.11059479553903345}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score\n",
    "# len(train_vec),len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.data\n",
    "y_train = train.target\n",
    "X_test = test.data\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vtrain = [TaggedDocument((d),tags=[str(i)])for i,d in enumerate(X_train)]\n",
    "d2vtest = [TaggedDocument((d),tags=[str(i)])for i,d in enumerate(X_test)]\n",
    "\n",
    "model = Doc2Vec(vector_size=500, alpha = 0.025, min_count = 10, dm =1, epochs=100)\n",
    "model.build_vocab(d2vtrain)\n",
    "model.train(d2vtrain, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x7fd2e827ce90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (model.infer_vector(word_tokenize(X_train[0])))\n",
    "# from gensim.models import  Word2Vec\n",
    "# X_train = \n",
    "# [j.split() for i in X_train for j in i]\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "for i,j in enumerate(X_train):\n",
    "    X_train[i] = re.sub('\\n',' ', j)\n",
    "    X_train[i] = re.sub(r'\\d','',X_train[i])\n",
    "    X_train[i] = X_train[i].translate(str.maketrans('','', string.punctuation))\n",
    "    X_train[i] = X_train[i].lower()\n",
    "    X_train[i] = word_tokenize(X_train[i])\n",
    "    X_train[i] = [k for k in X_train[i] if k not in stop]\n",
    "    tmp = \"\"\n",
    "    for k in X_train[i]:\n",
    "        tmp+=k+' '\n",
    "    X_train[i] = tmp\n",
    "\n",
    "for i,j in enumerate(X_test):\n",
    "    X_test[i] = re.sub('\\n',' ', j)\n",
    "    X_test[i] = re.sub(r'\\d','',X_test[i])\n",
    "    X_test[i] = X_test[i].translate(str.maketrans('','', string.punctuation))\n",
    "    X_test[i] = X_test[i].lower()\n",
    "    X_test[i] = word_tokenize(X_test[i])\n",
    "    X_test[i] = [k for k in X_test[i] if k not in stop]\n",
    "    tmp = \"\"\n",
    "    for k in X_test[i]:\n",
    "        tmp+=k+' '\n",
    "    X_test[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [model.infer_vector(word_tokenize(i)) for i in X_train]\n",
    "X_test = [model.infer_vector(word_tokenize(i)) for i in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vostok/.local/lib/python3.12/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/vostok/.local/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # clf = {'SGDClassifier':SGDClassifier(), 'LogisticRegression':LogisticRegression(), \"LinearSVC\":LinearSVC(),\"DecisionTreeClassifier\":DecisionTreeClassifier()}\n",
    "clf = [SGDClassifier(),LogisticRegression(),LinearSVC(),DecisionTreeClassifier()]\n",
    "# # ext = [TfidfTransformer()]\n",
    "\n",
    "score = {}\n",
    "# # clf.values()\n",
    "for i in clf:\n",
    "# #     print()\n",
    "    score[i] = Pipeline([('clf',i)]).fit(X_train,y_train).score(X_test,y_test)\n",
    "\n",
    "# # m1 = SGDClassifier()\n",
    "# # m1.fit(train_vec,y_train)\n",
    "# SGDClassifier().fit(train_vec,y_train)\n",
    "# type(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{SGDClassifier(): 0.08125331917153479,\n",
       " LogisticRegression(): 0.08590015932023366,\n",
       " LinearSVC(): 0.088688263409453,\n",
       " DecisionTreeClassifier(): 0.07023366967604885}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
